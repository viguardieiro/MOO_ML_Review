Title,Authors,Publisher,Year,DOI,Keywords,Citations,Optimization,ML task,Objective functions,Single/Multi Solutions,Abstract
evolutionary model selection in unsupervised learning,"Kim Y., Street W.N., Menczer F.",Intelligent Data Analysis,2002.0,10.3233/ida-2002-6605,,76.0,"['Evolutionary', 'ELSA']","['Feature Selection', 'Clustering']","['cluster cohesiveness', 'distance from glocal centroid', 'number of clusters', 'complexity']",Multi,"Feature subset selection is important not only for the insight gained from determining relevant modeling variables but also for the improved understandability, scalability, and possibly, accuracy of the resulting models. Feature selection has traditionally been studied in supervised learning situations, with some estimate of accuracy used to evaluate candidate subsets. However, we often cannot apply supervised learning for lack of a training signal. For these cases, we propose a new feature selection approach based on clustering. A number of heuristic criteria can be used to estimate the quality of clusters built from a given feature subset. Rather than combining such criteria, we use ELSA, an evolutionary local selection algorithm that maintains a diverse population of solutions that approximate the Pareto front in a multi-dimensional objective space. Each evolved solution represents a feature subset and a number of clusters; two representative clustering algorithms, K-means and EM, are applied to form the given number of clusters based on the selected features. Experimental results on both real and synthetic data show that the method can consistently find approximate Pareto-optimal solutions through which we can identify the significant features and an appropriate number of clusters. This results in models with better and clearer semantic relevance. © 2002-IOS Press. All rights reserved."
pareto neuro-ensembles,Abbass H.A.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2003.0,10.1007/978-3-540-24581-0_47,,28.0,"['Evolutive', 'MPANN']","['Neural Networks', 'Classification', 'Ensemble']",['MSE for two data partitions'],Single - Ensemble,"The formation of a neural network ensemble has attracted much attention in the machine learning literature. A set of trained neural networks are combined using a post-gate to form a single super-network. One main challenge in the literature is to decide on which network to include in, or exclude from the ensemble. Another challenge is how to define an optimum size for the ensemble. Some researchers also claim that for an ensemble to be effective, the networks need to be different. However, there is not a consistent definition of what “different” means. Some take it to mean weakly correlated networks, networks with different bias-variance trade-off, and/or networks which are specialized on different parts of the input space. In this paper, we present a theoretically sound approach for the formation of neural network ensembles. The approach is based on the dominance concept that determines which network to include/exclude, identifies a suitable size for the ensemble, and provides a mechanism for quantifying differences between networks. The approach was tested on a number of standard dataset and showed competitive results. © Springer-Verlag Berlin Heidelberg 2003."
cooperative coevolution of artificial neural network ensembles for pattern classification,N. Garcia-Pedrajas; C. Hervas-Martinez; D. Ortiz-Boyer,IEEE Transactions on Evolutionary Computation,2005.0,10.1109/TEVC.2005.844158,Classification;cooperative coevolution;multiobjective optimization;neural network ensembles,150.0,"['Evolutionary', 'NSGA']","['Classification','Ensemble','Neural Networks']",['Several performanca and diversity measures'],Single - Ensemble,"This paper presents a cooperative coevolutive approach for designing neural network ensembles. Cooperative coevolution is a recent paradigm in evolutionary computation that allows the effective modeling of cooperative environments. Although theoretically, a single neural network with a sufficient number of neurons in the hidden layer would suffice to solve any problem, in practice many real-world problems are too hard to construct the appropriate network that solve them. In such problems, neural network ensembles are a successful alternative. Nevertheless, the design of neural network ensembles is a complex task. In this paper, we propose a general framework for designing neural network ensembles by means of cooperative coevolution. The proposed model has two main objectives: first, the improvement of the combination of the trained individual networks; second, the cooperative evolution of such networks, encouraging collaboration among them, instead of a separate training of each network. In order to favor the cooperation of the networks, each network is evaluated throughout the evolutionary process using a multiobjective method. For each network, different objectives are defined, considering not only its performance in the given problem, but also its cooperation with the rest of the networks. In addition, a population of ensembles is evolved, improving the combination of networks and obtaining subsets of networks to form ensembles that perform better than the combination of all the evolved networks. The proposed model is applied to ten real-world classification problems of a very different nature from the UCI machine learning repository and proben1 benchmark set. In all of them the performance of the model is better than the performance of standard ensembles in terms of generalization error. Moreover, the size of the obtained ensembles is also smaller."
evolutionary multiobjective ensemble learning based on bayesian feature selection,Huanhuan Chen; Xin Yao,2006 IEEE International Conference on Evolutionary Computation,2006.0,10.1109/CEC.2006.1688318,,4.0,"['Evolutive', 'NSGA']","['Neural Networks', 'Classification', 'Ensemble']",['MSE for two data partitions'],Single - Ensemble,"This paper proposes to incorporate evolutionary multiobjective algorithm and Bayesian Automatic Relevance Determination (ARD) to automatically design and train ensemble. The algorithm determines almost all the parameters of ensemble automatically. Our algorithm adopts different feature subsets, selected by Bayesian ARD, to maintain accuracy and promote diversity among individual NNs in an ensemble. The multiobjective evaluation of the fitness of the networks encourages the networks with lower error rate and fewer features. The proposed algorithm is applied to several real-world classification problems and in all cases the performance of the method is better than the performance of other ensemble construction algorithms."
pattern classification by evolutionary rbf networks ensemble based on multi-objective optimization,N. Kondo; T. Hatanaka; K. Uosaki,The 2006 IEEE International Joint Conference on Neural Network Proceedings,2006.0,10.1109/IJCNN.2006.247224,,3.0,"['Evolutionary', 'NSGA-II']","['RBF Networks', 'Ensemble', 'Classification']","['number of hidden layer neuron', 'MSE', 'sum of absolute weights']",Single - Ensemble,"In this paper, evolutionary multi-objective selection method of RBF networks structure and its application to the ensemble learning is considered. The candidates of RBF network structure are encoded into the chromosomes in GAs. Then, they evolve toward Pareto-optimal front defined by several objective functions concerning with model accuracy, model complexity and model smoothness. RBF network ensemble is constructed of the obtained Pareto-optimal models since such models are diverse. This method is applied to the pattern classification problem. Experiments on the benchmark problem demonstrate that the proposed method has comparable generalization ability to conventional ensemble methods."
pattern classification by evolutionary rbf networks ensemble based on multi-objective optimization,N. Kondo; T. Hatanaka; K. Uosaki,The 2006 IEEE International Joint Conference on Neural Network Proceedings,2006.0,10.1109/IJCNN.2006.247224,,3.0,"['Evolutionary', 'NSGA-II']","['RBF Networks', 'Ensemble', 'Classification']","['number of hidden layer neuron', 'MSE', 'sum of absolute weights']",Single - Ensemble,"In this paper, evolutionary multi-objective selection method of RBF networks structure and its application to the ensemble learning is considered. The candidates of RBF network structure are encoded into the chromosomes in GAs. Then, they evolve toward Pareto-optimal front defined by several objective functions concerning with model accuracy, model complexity and model smoothness. RBF network ensemble is constructed of the obtained Pareto-optimal models since such models are diverse. This method is applied to the pattern classification problem. Experiments on the benchmark problem demonstrate that the proposed method has comparable generalization ability to conventional ensemble methods. © 2006 IEEE."
information preserving multi-objective feature selection for unsupervised learning,"Mierswa I., Wurst M.",GECCO 2006 - Genetic and Evolutionary Computation Conference,2006.0,10.1145/1143997.1144248,Multi-objective feature selection; Pareto front segmentation; Unsupervised learning,34.0,"['Evolutive', 'NSGA-II']","['Feature Selection', 'Clustering']","['Number of features', 'DB index']",Multi,"In this work we propose a novel, sound framework for evolutionary feature selection in unsupervised machine learning problems. We show that unsupervised feature selection is inhemulti-objectiverently multi-objective and behaves differently from supervised feature selection in that the number of features must be maximized instead of being minimized. Although this might sound surprising from a supervised learning point of view, we exemplify this relationship on the problem of data clustering and show that existing approaches do not pose the optimization problem in an appropriate way. Another important consequence of this paradigm change is a method which segments the Pareto sets produced by our approach. Inspecting only prototypical points from these segments drastically reduces the amount of work for selecting a final solution. We compare our methods against existing approaches on eight data sets. Copyright 2006 ACM."
a dual-objective evolutionary algorithm for rules extraction in data mining,"Tan, KC; Yu, Q; Ang, JH",COMPUTATIONAL OPTIMIZATION AND APPLICATIONS,2006.0,10.1007/s10589-005-3907-9,data mining; evolutionary algorithm; classification; rules extraction,,['Evolutionary'],"['decision rules', 'Classification']","['Accuracy', 'number of rules']",Multi,"This paper presents a dual-objective evolutionary algorithm (DOEA) for extracting multiple decision rule lists in data mining, which aims at satisfying the classification criteria of high accuracy and ease of user comprehension. Unlike existing approaches, the algorithm incorporates the concept of Pareto dominance to evolve a set of non-dominated decision rule lists each having different classification accuracy and number of rules over a specified range. The classification results of DOEA are analyzed and compared with existing rule-based and non-rule based classifiers based upon 8 test problems obtained from UCI Machine Learning Repository. It is shown that the DOEA produces comprehensible rules with competitive classification accuracy as compared to many methods in literature. Results obtained from box plots and t-tests further examine its invariance to random partition of datasets."
alleviating catastrophic forgetting via multi-objective learning,Yaochu Jin; B. Sendhoff,The 2006 IEEE International Joint Conference on Neural Network Proceedings,2006.0,10.1109/IJCNN.2006.247332,,2.0,"['Evolutionary', 'NSGA-II']","['Catastrophic Forgetting', 'Neural Networks']","['Error on new patterns', 'Error on pseudo-patterns']",Single,"Handling catastrophic forgetting is an interesting and challenging topic in modeling the memory mechanisms of the human brain using machine learning models. From a more general point of view, catastrophic forgetting reflects the stability-plasticity dilemma, which is one of the several dilemmas to be addressed in learning systems: to retain the stored memory while learning new information. Different to the existing approaches, we introduce a Pareto-optimality based multi-objective learning framework for alleviating catastrophic learning. Compared to the single-objective learning methods, multi-objective evolutionary learning with the help of pseudo-rehearsal is shown to be more promising in dealing with the stability-plasticity dilemma."
semi-supervised feature selection via multiobjective optimization,"Handl J., Knowles J.",IEEE International Conference on Neural Networks - Conference Proceedings,2006.0,10.1109/ijcnn.2006.247330,,17.0,"['Evolutive', 'PESA-II']","['Feature Selection', 'Semi-supervised']","['Silhoute Width', 'Feature cardinality', 'Adjusted Rand Index']",Single - Solution that performs best in supervised,"In previous work, we have shown that both unsupervised feature selection and the semi-supervised clustering problem can be usefully formulated as multiobjective optimization problems. In this paper, we discuss the logical extension of this prior work to cover the problem of semi-supervised feature selection. Our extensive experimental results provide evidence for the advantages of semi-supervised feature selection when both labelled and unlabelled data are available. Moreover, the particular effectiveness of a Pareto-based optimization approach can also be seen. © 2006 IEEE."
pattern classification by evolutionary rbf networks ensemble based on multi-objective optimization,"Kondo N., Hatanaka T., Uosaki K.",IEEE International Conference on Neural Networks - Conference Proceedings,2006.0,10.1109/ijcnn.2006.247224,,8.0,"['Evolutionary', 'NSGA-II']","['RBF Networks', 'Ensemble', 'Classification']","['number of hidden layer neuron', 'MSE', 'sum of absolute weights']",Single - Ensemble,"In this paper, evolutionary multi-objective selection method of RBF networks structure and its application to the ensemble learning is considered. The candidates of RBF network structure are encoded into the chromosomes in GAs. Then, they evolve toward Pareto-optimal front defined by several objective functions concerning with model accuracy, model complexity and model smoothness. RBF network ensemble is constructed of the obtained Pareto-optimal models since such models are diverse. This method is applied to the pattern classification problem. Experiments on the benchmark problem demonstrate that the proposed method has comparable generalization ability to conventional ensemble methods."
pattern classification by evolutionary rbf networks ensemble based on multi-objective optimization,"Kondo N., Hatanaka T., Uosaki K.",IEEE International Conference on Neural Networks - Conference Proceedings,2006.0,10.1109/ijcnn.2006.247224,,8.0,"['Evolutionary', 'NSGA-II']","['RBF Networks', 'Ensemble', 'Classification']","['number of hidden layer neuron', 'MSE', 'sum of absolute weights']",Single - Ensemble,"In this paper, evolutionary multi-objective selection method of RBF networks structure and its application to the ensemble learning is considered. The candidates of RBF network structure are encoded into the chromosomes in GAs. Then, they evolve toward Pareto-optimal front defined by several objective functions concerning with model accuracy, model complexity and model smoothness. RBF network ensemble is constructed of the obtained Pareto-optimal models since such models are diverse. This method is applied to the pattern classification problem. Experiments on the benchmark problem demonstrate that the proposed method has comparable generalization ability to conventional ensemble methods. © 2006 IEEE."
multiobjective evolutionary rbf networks and its application to ensemble learning,"Kondo, N; Hatanaka, T; Uosaki, K",FRONTIERS OF COMPUTATIONAL SCIENCE,2007.0,10.1007/978-3-540-46375-7_50,,,"['Evolutionary', 'Genetic']","['Ensebmle', 'RBF Networks', 'Classification']","['Accuracy', 'Complexity']",Single - Ensemble,"This paper considers a pattern classification by the ensemble of evolutionary RBF networks. Mathematical models generally have a dilemma about model complexity, so the structure determination of RBF network can be considered as the multi-objective optimization problem concerning with accuracy, complexity, and smoothness of the model. The set of RBF network are obtained by multi-objective evolutionary computation, and then RBF network ensemble is constructed of all or some RBF networks at the final generation. Some experiments on the benchmark problem of the pattern classification demonstrate that the RBF network ensemble has comparable generalization ability to conventional ensemble methods."
controlling overfitting with multi-objective support vector machines,Mierswa I.,Proceedings of GECCO 2007: Genetic and Evolutionary Computation Conference,2007.0,10.1145/1276958.1277323,Evolution strategies; Kernel methods; Machine learning; Support vector machines,20.0,"['Evolutive', 'NSGA-II']","['Classification', 'SVM']",['Dual of max margin and min error'],Multi,"Recently, evolutionary computation has been successfully integrated into statistical learning methods. A Support Vector Machine (SVM) using evolution strategies for its optimization problem frequently deliver better results with respect to the optimization criterion and the prediction accuracy. Moreover, evolutionary computation allows for the efficient large margin optimization of a huge family of new kernel functions, namely non-positive semi definite kernels as the Epanechnikov kernel. For these kernel functions, evolutionary SVM even outperform other learning methods like the Relevance Vector Machine. In this paper, we will discuss another major advantage of evolutionary SVM compared to traditional SVM solutions: we can explicitly optimize the inherent trade-off between training error and model complexity by embedding multi-objective optimization into the evolutionary SVM. This leads to three advantages: first, it is no longer necessary to tune the SVM parameter C which weighs both conflicting criteria. This is a very time-consuming task for traditional SVM. Second, the shape and size of the Pareto front give interesting insights about the complexity of the learning task at hand. Finally, the user can actually see the point where overfitting occurs and can easily select a solution from the Pareto front best suiting his or her needs. Copyright 2007 ACM."
a multi-objective learning algorithm for rbf neural network,I. Kokshenev; A. P. Braga,2008 10th Brazilian Symposium on Neural Networks,2008.0,10.1109/SBRN.2008.39,multi-objective learning;radial basis functions;generalization;regularization;LASSO,3.0,['Evolutive'],"['Classification', 'RBF-Networks']","['MSE', 'L1']",Single - AIC in Pareto fronteir,"In this paper, the problem of multi-objective supervised learning is discussed within the non-evolutionary optimization framework. The proposed MOBJ learning algorithm performs the search of Pareto-optimal models determining weights,width, prototype vectors, and the quantity of basis functions of the RBF network. In combination with the Akaike information criterion, the algorithm provides high quality solutions."
multi-objective learning of multi-dimensional bayesian classifiers,J. D. Rodríguez; J. A. Lozano,2008 Eighth International Conference on Hybrid Intelligent Systems,2008.0,10.1109/HIS.2008.143,Machine Learning;multi-dimensional classification;Bayesian classifiers;multi-objective;NSGA-II,16.0,"['Evolutive','NSGA-II']","['Classification','Bayesian classifiers']",['Error for each classification variable'],Multi,"Multi-dimensional classification is a generalization of supervised classification that considers more than one class variable to classify. In this paper we review the existing multi-dimensional Bayesian classifiers and introduce a new one: the KDB multi-dimensional classifier. Then we define different classification rules for multi-dimensional scope. Finally, we introduce a structural learning approach of a multi-dimensional Bayesian classifier based on the multi-objective evolutionary algorithm NSGA-II. The solution of the learning approach is a Pareto front representing different multi-dimensional classifiers and their accuracy values for the different classes, so a decision maker can easily choose the classifier which is more interesting for the particular problem and domain."
semi-supervised training of least squares support vector machine using a multiobjective evolutionary algorithm,C. Silva; J. S. Santos; E. F. Wanner; E. G. Carrano; R. H. C. Takahashi,2009 IEEE Congress on Evolutionary Computation,2009.0,10.1109/CEC.2009.4983321,,1.0,"['Evolutionary', 'SPEA2']","['SVM', 'Classification', 'semi-supervised']","['error over the labeled points', 'norm of the vector of weights']",Multi,"Support Vector Machines (SVMs) are considered state-of-the-art learning machines techniques for classification problems. This paper studies the training of SVMs in the special case of problems in which the raw data to be used for training purposes is composed of both labeled and unlabeled data - the semi-supervised learning problem. This paper proposes the definition of an intermediate problem of attributing labels to the unlabeled data as a multiobjective optimization problem, with the conflicting objectives of minimizing the classification error over the training data set and maximizing the regularity of the resulting classifier. This intermediate problem is solved using an evolutionary multiobjective algorithm, the SPEA2. Simulation results are presented in order to illustrate the suitability of the proposed technique."
design of artificial neural networks using a memetic pareto evolutionary algorithm using as objectives entropy versus variation coefficient,J. C. Fernández; C. Hervás; F. J. Martínez; M. Cruz,2009 Ninth International Conference on Intelligent Systems Design and Applications,2009.0,10.1109/ISDA.2009.153,Classification;Neural Networks;Multi-objective;Entropy;Variation Coefficient,,"['Evolutionary', 'MPENSGA2']","['Neural Network','Classification']","['Cross entropy', 'Variation coefficient']",Single,"This paper proposes a multi-classification pattern algorithm using multilayer perceptron neural network models which try to boost two conflicting main objectives of a classifier, a high correct classification rate and a high classification rate for each class. To solve this machine learning problem, we consider a Memetic Pareto Evolutionary approach based on the NSGA2 algorithm (MPENSGA2), where we defined two objectives for determining the goodness of a classifier: the cross-entropy error function and the variation coefficient of its sensitivities, because both measures are continuous functions, making the convergence more robust. Once the Pareto front is built, we use an automatic selection methodology of individuals: the best model in accuracy (upper extreme in the Pareto front). This methodology is applied to solve six benchmark classification problems, obtaining promising results and achieving a high classification rate in the generalization set with an acceptable level of accuracy for each class."
improving generalization of radial basis function network with adaptive multi-objective particle swarm optimization,S. N. Qasem; S. M. H. Shamsuddin,"2009 IEEE International Conference on Systems, Man and Cybernetics",2009.0,10.1109/ICSMC.2009.5346876,Radial basis function network;Adaptive Multi-objective particle swarm optimization;Multi-Objective particle swarm optimization,3.0,"['Evolutive', 'AMPSO']","['Classification', 'RBF Networks']","['MSE', 'Sum of square weights']",Multi,"In this paper, an adaptive evolutionary multi-objective selection method of RBF Networks structure is discussed. The candidates of RBF Network structures are encoded into particles in Particle Swarm Optimization (PSO). These particles evolve toward Pareto-optimal front defined by several objective functions with model accuracy and complexity. The problem of unsupervised and supervised learning is discussed with Adaptive Multi-Objective PSO (AMOPSO). This study suggests an approach of RBF Network training through simultaneous optimization of architectures and weights with Adaptive PSO-based multi-objective algorithm. Our goal is to determine whether Adaptive Multi-objective PSO can train RBF Networks, and the performance is validated on accuracy and complexity. The experiments are conducted on two benchmark datasets obtained from the machine learning repository. The results show that our proposed method provides an effective means for training RBF Networks that is competitive with PSO-based multi-objective algorithm."
radial basis function network based on multi-objective particle swarm optimization,S. N. Qasem; S. M. H. Shamsuddin,2009 6th International Symposium on Mechatronics and its Applications,2009.0,10.1109/ISMA.2009.5164833,,11.0,"['Evolutive', 'MOPSO']","['Classification', 'RBF Networks']","['MSE', 'Sum of square weights']",Multi,"The problem of unsupervised and supervised learning is discussed within the context of multi-objective optimization. In this paper, an evolutionary multi-objective selection method of RBF networks structure is discussed. The candidates of RBF network structure are encoded into the particles in PSO. Then, they evolve toward Pareto-optimal front defined by several objective functions concerning with model accuracy and model complexity. This study suggests an approach of RBF network training through simultaneous optimization of architectures and weights with PSO-based multi-objective algorithm. Our goal is to determine whether multi-objective PSO can train RBF networks, and the performance is validated on accuracy and complexity. The experiments are conducted on benchmark datasets obtained from the UCI machine learning repository. The results show that our proposed method provides an effective means for training RBF networks that is competitive with other evolutionary computational-based methods."
optimal v-svm parameter estimation using multi objective evolutionary algorithms,J. Ethridge; G. Ditzler; R. Polikar,IEEE Congress on Evolutionary Computation,2010.0,10.1109/CEC.2010.5586029,multi-objective optimization;v-SVM;evolutionary algorithms,3.0,"['Evolutionary', 'NSGA-II']","['v-SVM','Classification']","['Sensitivity', 'Specificity', 'Generalization error']",Multi,"Using a machine learning algorithm for a given application often requires tuning design parameters of the classifier to obtain optimal classification performance without overfitting. In this contribution, we present an evolutionary algorithm based approach for multi-objective optimization of the sensitivity and specificity of a v-SVM. The v-SVM is often preferred over the standard C-SVM due to smaller dynamic range of the v parameter compared to the unlimited dynamic range of the C parameter. Instead of looking for a single optimization result, we look for a set of optimal solutions that lie along the Pareto optimality front. The traditional advantage of using the Pareto optimality is of course the flexibility to choose any of the solutions that lies on the Pareto optimality front. However, we show that simply maximizing sensitivity and specificity over the Pareto front leads to parameters that appear to be mathematically optimal yet still cause overfitting. We propose a multiple objective optimization approach with three objective functions to find additional parameter values that do not cause overfitting."
multiobjective neural network ensembles based on regularized negative correlation learning,H. Chen; X. Yao,IEEE Transactions on Knowledge and Data Engineering,2010.0,10.1109/TKDE.2010.26,Multiobjective algorithm;multiobjective learning;neural network ensembles;neural networks;negative correlation learning;regularization.,83.0,['Evolutionary'],"['Ensemble', 'Negative Correlation Learning', 'RBF', 'Classification']","['MSE', 'Correlation measure', 'weight decay']",Single - Ensemble,"Negative Correlation Learning (NCL) [CHECK END OF SENTENCE], [CHECK END OF SENTENCE] is a neural network ensemble learning algorithm which introduces a correlation penalty term to the cost function of each individual network so that each neural network minimizes its mean-square-error (MSE) together with the correlation. This paper describes NCL in detail and observes that the NCL corresponds to training the entire ensemble as a single learning machine that only minimizes the MSE without regularization. This insight explains that NCL is prone to overfitting the noise in the training set. The paper analyzes this problem and proposes the multiobjective regularized negative correlation learning (MRNCL) algorithm which incorporates an additional regularization term for the ensemble and uses the evolutionary multiobjective algorithm to design ensembles. In MRNCL, we define the crossover and mutation operators and adopt nondominated sorting algorithm with fitness sharing and rank-based fitness assignment. The experiments on synthetic data as well as real-world data sets demonstrate that MRNCL achieves better performance than NCL, especially when the noise level is nontrivial in the data set. In the experimental discussion, we give three reasons why our algorithm outperforms others."
feature selection using multiobjective optimization for named entity recognition,A. Ekbal; S. Saha; C. S. Garbe,2010 20th International Conference on Pattern Recognition,2010.0,10.1109/ICPR.2010.477,Multiobjective Optimization;Feature Selection;Maximum Entropy;Named Entity Recognition,17.0,"['Evolutionary', 'NSGA-II']","['Feature Selection', 'Named  Entity Recognition']","['recall', 'precision']",Single - Performance in training,"Appropriate feature selection is a very crucial issue in any machine learning framework, specially in Maximum Entropy (ME). In this paper, the selection of appropriate features for constructing a ME based Named Entity Recognition (NER) system is posed as a multiobjective optimization (MOO) problem. Two classification quality measures, namely recall and precision are simultaneously optimized using the search capability of a popular evolutionary MOO technique, NSGA-II. The proposed technique is evaluated to determine suitable feature combinations for NER in two languages, namely Bengali and English that have significantly different characteristics. Evaluation results yield the recall, precision and F-measure values of 70.76%, 81.88% and 75.91%, respectively for Bengali, and 78.38%, 81.27% and 79.80%, respectively for English. Comparison with an existing ME based NER system shows that our proposed feature selection technique is more efficient than the heuristic based feature selection."
feature selection for multi-purpose predictive models: a many-objective task,"Reynolds A.P., Corne D.W., Chantler M.J.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2010.0,10.1007/978-3-642-15844-5_39,,7.0,"['Evolutionary', 'NSGA-II']","['Feature Selection', 'Classification', 'Binary Classification']","['Specificity', 'Sensitivity', 'Confidence']",Multi,"The target of machine learning is a predictive model that performs well on unseen data. Often, such a model has multiple intended uses, related to different points in the tradeoff between (e.g.) sensitivity and specificity. Moreover, when feature selection is required, different feature subsets will suit different target performance characteristics. Given a feature selection task with such multiple distinct requirements, one is in fact faced with a very-many-objective optimization task, whose target is a Pareto surface of feature subsets, each specialized for (e.g.) a different sensitivity/specificity tradeoff profile. We argue that this view has many advantages. We motivate, develop and test such an approach. We show that it can be achieved successfully using a dominance-based multiobjective algorithm, despite an arbitrarily large number of objectives. © 2010 Springer-Verlag."
on a multiobjective training algorithm for rbf networks using particle swarm optimization,G. R. L. Silva; D. A. G. Vieira; A. C. Lisboa; V. Palade,2010 22nd IEEE International Conference on Tools with Artificial Intelligence,2010.0,10.1109/ICTAI.2010.112,radial basis network;multiobjective least squares;particle swarm optimization,,"['Evolutionary', 'Particle Swarm']",['RBF'],"['Least Squares', 'Complexity - Gradient']",,"This paper presents a novel algorithm for multiobjective training of Radial Basis Function (RBF) networks based on least-squares and Particle Swarm Optimization methods. The formulation is based on the fundamental concept that supervised learning is a bi-objective optimization problem, in which two conflicting objectives should be minimized. The objectives are related to the empirical training error and the machine complexity. The training is done in three steps: i) a conventional minimization of the training error, ii) multiobjective least-squares optimization for the linear parameters and, iii) particle swarm optimization for the nonlinear parameters. Some results are presented and they show the effectiveness of the proposed approach."
an adaptive multiobjective approach to evolving art architectures,A. Kaylani; M. Georgiopoulos; M. Mollaghasemi; G. C. Anagnostopoulos; C. Sentelle; M. Zhong,IEEE Transactions on Neural Networks,2010.0,10.1109/TNN.2009.2037813,ARTMAP;category proliferation;classification;genetic algorithms (GAs);genetic operators;machine learning,31.0,['Evolutionary'],"['Classification', 'ART']","['Accuracy', 'Complexity - Size']",Multi,"In this paper, we present the evolution of adaptive resonance theory (ART) neural network architectures (classifiers) using a multiobjective optimization approach. In particular, we propose the use of a multiobjective evolutionary approach to simultaneously evolve the weights and the topology of three well-known ART architectures; fuzzy ARTMAP (FAM), ellipsoidal ARTMAP (EAM), and Gaussian ARTMAP (GAM). We refer to the resulting architectures as MO-GFAM, MO-GEAM, and MO-GGAM, and collectively as MO-GART. The major advantage of MO-GART is that it produces a number of solutions for the classification problem at hand that have different levels of merit [accuracy on unseen data (generalization) and size (number of categories created)]. MO-GART is shown to be more elegant (does not require user intervention to define the network parameters), more effective (of better accuracy and smaller size), and more efficient (faster to produce the solution networks) than other ART neural network architectures that have appeared in the literature. Furthermore, MO-GART is shown to be competitive with other popular classifiers, such as classification and regression tree (CART) and support vector machines (SVMs)."
sensitivity versus accuracy in multiclass problems using memetic pareto evolutionary neural networks,J. C. Fernandez Caballero; F. J. Martinez; C. Hervas; P. A. Gutierrez,IEEE Transactions on Neural Networks,2010.0,10.1109/TNN.2010.2041468,Accuracy;local search;multiclassification;multiobjective evolutionary algorithms;neural networks;sensitivity,113.0,"['Evolutionary', 'MPENSGA2']","['Classification', 'Multiclass', 'Multilayer Perceptron']","['Accuracy', 'Sensitivity']",Multi,"This paper proposes a multiclassification algorithm using multilayer perceptron neural network models. It tries to boost two conflicting main objectives of multiclassifiers: a high correct classification rate level and a high classification rate for each class. This last objective is not usually optimized in classification, but is considered here given the need to obtain high precision in each class in real problems. To solve this machine learning problem, we use a Pareto-based multiobjective optimization methodology based on a memetic evolutionary algorithm. We consider a memetic Pareto evolutionary approach based on the NSGA2 evolutionary algorithm (MPENSGA2). Once the Pareto front is built, two strategies or automatic individual selection are used: the best model in accuracy and the best model in sensitivity (extremes in the Pareto front). These methodologies are applied to solve 17 classification benchmark problems obtained from the University of California at Irvine (UCI) repository and one complex real classification problem. The models obtained show high accuracy and a high classification rate for each class."
an efficient multi-objective learning algorithm for rbf neural network,"Kokshenev, I; Braga, AP",NEUROCOMPUTING,2010.0,10.1016/j.neucom.2010.06.022,Multi-objective learning; Radial-basis functions; Pareto-optimality; Model selection; Regularization,,['Deterministic'],['RBF'],"['Squared Error', 'Complexity']",Single - Information Criteria,"Most of modern multi-objective machine learning methods are based on evolutionary optimization algorithms. They are known to be global convergent, however, usually deliver nondeterministic results. In this work we propose the deterministic global solution to a multi-objective problem of supervised learning with the methodology of nonlinear programming. As the result, the proposed multi-objective algorithm performs a global search of Pareto-optimal hypotheses in the space of RBF networks, determining their weights and basis functions. In combination with the Akaike and Bayesian information criteria, the algorithm demonstrates a high generalization efficiency on several synthetic and real-world benchmark problems. (C) 2010 Elsevier B.V. All rights reserved."
generalization improvement of radial basis function network based on multi-objective particle swarm optimization,"Qasem S.N., Shamsuddin S.M.",Journal of Artificial Intelligence,2010.0,10.3923/jai.2010.1.16,Elitist non-dominated sorting genetic algorithm; Hybrid learning; Multi-objective optimization; Multi-objective particle swarm optimization; Radial basis function network,16.0,"['Evolutionary', 'Particle Swarm Optimization']","['RBF Network', 'Classification']","['Mean-Squared Error', 'L2']",Multi,"The problem of unsupervised and supervised learning of RBF networks is discussed with Multi-Objective Particle Swarm Optimization (MOPSO). This study presents an evolutionary multi-objective selection method of RBF networks structure. The candidates of RBF networks structures are encoded into particles in PSO. These particles evolve toward Pareto-optimal front defined by several objective functions with model accuracy and complexity. This study suggests an approach of RBF network training through simultaneous optimization of architectures and connections with PSO-based multi-objective algorithm. Present goal is to determine whether MOPSO can train RBF networks and the performance is validated on accuracy and complexity. The experiments are conducted on two benchmark datasets obtained from the machine learning repository. The results show that; the best results are obtained for our proposed method that has obtained 100 and 80.21 % classification accuracy from the experiments made on the data taken from breast cancer and diabetes diseases database, respectively. The results also show that our approach provides an effective means to solve multi-objective RBF networks and outperforms multi-objective genetic algorithm. © 2010 Asian Network for Scientific Information."
classification as clustering: a pareto cooperative-competitive gp approach,A. R. McIntyre; M. I. Heywood,Evolutionary Computation,2011.0,10.1162/EVCO_a_00016,Genetic programming;Pareto multi-objective optimization;coevolution;problem decomposition;classification,5.0,['Evolutionary'],"['Classification', 'Multiclass']","['Sensitivity', 'Specificity']",Multi?,"Intuitively population based algorithms such as genetic programming provide a natural environment for supporting solutions that learn to decompose the overall task between multiple individuals, or a team. This work presents a framework for evolving teams without recourse to prespecifying the number of cooperating individuals. To do so, each individual evolves a mapping to a distribution of outcomes that, following clustering, establishes the parameterization of a (Gaussian) local membership function. This gives individuals the opportunity to represent <italic>subsets</italic> of tasks, where the overall task is that of classification under the supervised learning domain. Thus, rather than each team member representing an entire class, individuals are free to identify unique subsets of the overall classification task. The framework is supported by techniques from evolutionary multiobjective optimization (EMO) and Pareto competitive coevolution. EMO establishes the basis for encouraging individuals to provide accurate yet nonoverlaping behaviors; whereas competitive coevolution provides the mechanism for scaling to potentially large unbalanced datasets. Benchmarking is performed against recent examples of nonlinear SVM classifiers over 12 UCI datasets with between 150 and 200,000 training instances. Solutions from the proposed coevolutionary multiobjective GP framework appear to provide a good balance between classification performance and model complexity, especially as the dataset instance count increases."
a generic optimising feature extraction method using multiobjective genetic programming,"Zhang, Y; Rockett, PI",APPLIED SOFT COMPUTING,2011.0,10.1016/j.asoc.2010.02.008,Feature extraction; Multiobjective optimisation; Genetic programming; Pattern recognition,,"['Evolutionary', 'Genetic', 'SPEA-2']","['Feature Extraction', 'Classification']","['Tree Complexity', 'Missclassification Error', 'Bayes Error']",Multi,"In this paper, we present a generic, optimising feature extraction method using multiobjective genetic programming. We re-examine the feature extraction problem and show that effective feature extraction can significantly enhance the performance of pattern recognition systems with simple classifiers. A framework is presented to evolve optimised feature extractors that transform an input pattern space into a decision space in which maximal class separability is obtained. We have applied this method to real world datasets from the UCI Machine Learning and StatLog databases to verify our approach and compare our proposed method with other reported results. We conclude that our algorithm is able to produce classifiers of superior (or equivalent) performance to the conventional classifiers examined, suggesting removal of the need to exhaustively evaluate a large family of conventional classifiers on any new problem. (C) 2010 Elsevier B.V. All rights reserved."
evolving ensembles in multi-objective genetic programming for classification with unbalanced data,"Bhowan U., Johnston M., Zhang M.","Genetic and Evolutionary Computation Conference, GECCO'11",2011.0,10.1145/2001576.2001756,Class imbalance; Classification; Evolutionary multi-objective optimisation; Genetic programming,25.0,"['Evolutionary', 'Genetic', 'NSGA-II']","['Ensemble', 'Classification']","['Minority Accuracy', 'Majority Accuracy']",Single - Ensemble,"Machine learning algorithms can suffer a performance bias when data sets are unbalanced. This paper proposes a Multi-objective Genetic Programming approach using negative correlation learning to evolve accurate and diverse ensembles of non-dominated solutions where members vote on class membership. We also compare two popular Pareto-based fitness schemes on the classification tasks. We show that the evolved ensembles achieve high accuracy on both classes using six unbalanced binary data sets, and that this performance is usually better than many of its individual members. Copyright 2011 ACM."
simultaneous feature selection and clustering for categorical features using multi objective genetic algorithm,D. Dutta; P. Dutta; J. Sil,2012 12th International Conference on Hybrid Intelligent Systems (HIS),2012.0,10.1109/HIS.2012.6421332,,6.0,"['Evolutive', 'Genetic']","['Feature Selection', 'Clustering']","['Homogeneity', 'Separation']",Multi,"Clustering is unsupervised learning where ideally class levels and number of clusters (K) are not known. K-clustering can be categorized as semi-supervised learning where K is known. Here we have considered K-Clustering with simultaneous feature selection. Feature subset selection helps to identify relevant features for clustering, increase understandability, better scalability and improve accuracy. Here we have used two measures, intra-cluster distance (Homogeneity, H) and inter-cluster distances (Separation, S) for clustering. Measures are using mod distance per feature suitable for categorical features (attributes). Rather than combining H and S to frame the problem as single objective optimization problem, we use multi objective genetic algorithm (MOGA) to find out diverse solutions near to Pareto optimal front in the two-dimensional objective space. Each evolved solution represents a set of cluster modes (CMs) build by selected feature subset. Here, K-modes is hybridized with MOGA. We have used hybridized GA to combine global searching powers of GA with local searching powers of K-modes. Considering context sensitivity, we have used a special crossover operator called “pairwise crossover” and “substitution”. The main contribution of this paper is simultaneous dimensionality reduction and optimization of objectives using MOGA. Results on 3 benchmark data sets from UCI Machine Learning Repository containing categorical features shows the superiority of the algorithm."
semi-supervised clustering using multiobjective optimization,S. Saha; A. Ekbal; A. K. Alok,2012 12th International Conference on Hybrid Intelligent Systems (HIS),2012.0,10.1109/HIS.2012.6421361,Semi-supervised clustering;Multiobjective optimization;Cluster validity index;Adjusted Rand Index (ARI);Sym-index;Con-index;I-Index;AMOSA,11.0,"['Evolutive', 'AMOSA']","['Semi-supervised', 'Clustering']","['Compactedness of partitions', 'Total symmetry', 'Cluster connectedness', 'Adjusted Rand Index']",Single - ARI value,"Semi-supervised clustering uses the information of unsupervised and supervised learning to overcome the problems associated with them. Extracted information are given in the form of class labels and data distribution during clustering process. In this paper the problem of semi-supervised clustering is formulated under the framework of multiobjective optimization (MOO). Thereafter, a multiobjective based clustering technique is extended to solve the semi-supervised clustering problem. The newly developed semi-supervised multiobjective clustering algorithm (Semi-GenClustMOO), is used for appropriate partitioning of data into appropriate number of clusters. Four objective functions are optimized, out of which first three use some unsupervised information and the last one uses supervised information. These four objective functions represent, respectively, the, total compactness of the partitioning, total symmetry present in the clusters, cluster connectedness and Adjust Rand Index. These four objective functions are optimized simultaneously using AMOSA, a newly developed simulated annealing based multiobjective optimization method. Results show that it can easily detect the appropriate number of clusters as well as the appropriate partitioning from data sets having either well-separated clusters of any shape or symmetrical clusters with or without overlaps. Seven artificial and four real-life data sets have been used for evaluation to show the effectiveness of the Semi-GenClustMOO technique. In each case class information of 10% randomly chosen data point is known to us <sup>1</sup>."
clustering by multi objective genetic algorithm,D. Dutta; P. Dutta; J. Sil,2012 1st International Conference on Recent Advances in Information Technology (RAIT),2012.0,10.1109/RAIT.2012.6194619,Clustering;homogeneity and separation;real coded multi objective genetic algorithm;Pareto optimal front,10.0,"['Evolutive', 'Genetic']",['Clustering'],"['Homogeneity', 'Separation']",Multi,"The aim of the paper is to study a real coded multi objective genetic algorithm based K-clustering, where K represents the number of clusters, may be known or unknown. If the value of K is known, it is called K-clustering algorithm. The searching power of Genetic Algorithm (GA) is exploited to get for proper clusters and centers of clusters in the feature space to optimize simultaneously intra-cluster distance (Homogeneity) (H) and inter-cluster distances (Separation) (S). Maximization of 1/H and S are the twin objectives of Multi Objective Genetic Algorithm (MOGA) achieved by measuring H and S using Euclidean distance metric, suitable for continuous features (attributes). We have selected 10 data sets from the UCI machine learning repository containing continuous features only to validate the proposed algorithms. All-important steps of algorithms are shown here. At the end, classification accuracies obtained by best chromosomes are shown."
a computational geometry approach for pareto-optimal selection of neural networks,"Torres L.C.B., Castro C.L., Braga A.P.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2012.0,10.1007/978-3-642-33266-1_13,classification; decision-making; gabriel graph; multiobjective machine learning,2.0,[],"['Neural Networks', 'Classification']","['Training Error', 'Norm of weights']",Single - Separation Margin,"This paper presents a Pareto-optimal selection strategy for multiobjective learning that is based on the geometry of the separation margin between classes. The Gabriel Graph, a method borrowed from Computational Geometry, is constructed in order to obtain margin patterns and class borders. From border edges, a target separator is obtained in order to obtain a large margin classifier. The selected model from the generated Pareto-set is the one that is closer to the target separator. The method presents robustness in both synthetic and real benchmark datasets. It is efficient for Pareto-Optimal selection of neural networks and no claim is made that the obtained solution is equivalent to a maximum margin separator. © 2012 Springer-Verlag."
multi-objective approach based on grammar-guided genetic programming for solving multiple instance problems,"Zafra, A; Ventura, S",SOFT COMPUTING,2012.0,10.1007/s00500-011-0794-0,Multiple instance learning; Multiple objective learning; Grammar guided genetic programming; Evolutionary rule learning,,"['Evolutionary', 'Genetic', 'Grammar Guided Genetic Programming', 'G3P']",['Multiple instance learning'],"['sensitivity', 'specificity']",Multi,"Multiple instance learning (MIL) is considered a generalization of traditional supervised learning which deals with uncertainty in the information. Together with the fact that, as in any other learning framework, the classifier performance evaluation maintains a trade-off relationship between different conflicting objectives, this makes the classification task less straightforward. This paper introduces a multi-objective proposal that works in a MIL scenario to obtain well-distributed Pareto solutions to multi-instance problems. The algorithm developed, Multi-Objective Grammar Guided Genetic Programming for Multiple Instances (MOG3P-MI), is based on grammar-guided genetic programming, which is a robust tool for classification. Thus, this proposal combines the advantages of the grammar-guided genetic programming with benefits provided by multi-objective approaches. First, a study of multi-objective optimization for MIL is carried out. To do this, three different extensions of MOG3P-MI are designed and implemented and their performance is compared. This study allows us on the one hand, to check the performance of multi-objective techniques in this learning paradigm and on the other hand, to determine the most appropriate evolutionary process for MOG3P-MI. Then, MOG3P-MI is compared with some of the most significant proposals developed throughout the years in MIL. Computational experiments show that MOG3P-MI often obtains consistently better results than the other algorithms, achieving the most accurate models. Moreover, the classifiers obtained are very comprehensible."
categorical feature reduction using multi objective genetic algorithm in cluster analysis,"Dutta D., Dutta P., Sil J.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2013.0,10.1007/978-3-642-45318-2_7,Clustering; dimensionality reduction; homogeneity and separation; Pareto optimal front; real coded multi objective genetic algorithm,3.0,"['Evolutionary', 'Genetic']",['Clustering'],"['homogeneity', 'separation']",Multi,"In the paper, real coded multi objective genetic algorithm based K-clustering method has been studied, K represents the number of clusters. In K-clustering algorithm value of K is known. The searching power of Genetic Algorithm (GA) is exploited to search for suitable clusters and centers of clusters so that intra-cluster distance (Homogeneity, H) and inter-cluster distances (Separation, S) are simultaneously optimized. It is achieved by measuring H and S using Mod distance per feature metric, suitable for categorical features (attributes). We have selected 3 benchmark data sets from UCI Machine Learning Repository containing categorical features only. The paper proposes two versions of MOGA based K-clustering algorithm. In proposed MOGA (H, S), all features are taking part in building chromosomes and calculation of H and S values. In MOGA-Feature-Selection (H, S), selected features take part to build chromosomes, relevant for clusters. Here, K-modes is hybridized with GA. We have used hybridized GA to combine global searching capabilities of GA with local searching capabilities of K-modes. Considering context sensitivity, we have used a special crossover operator called ""pairwise crossover"" and ""substitution"". The main contribution of this paper is simultaneous dimensionality reduction and optimization of objectives using MOGA. © 2013 Springer-Verlag Berlin Heidelberg."
evolving diverse ensembles using genetic programming for classification with unbalanced data,U. Bhowan; M. Johnston; M. Zhang; X. Yao,IEEE Transactions on Evolutionary Computation,2013.0,10.1109/TEVC.2012.2199119,Classification;class imbalance learning;genetic programming (GP);multiobjective machine learning (ML),140.0,"['Evolutionary', 'NSGA-II', 'SPEA2']","['Classification', 'Ensemble', 'Unbalanced Data']","['Minority Accuracy', 'Majority Accuracy']",Single - Ensemble,"In classification, machine learning algorithms can suffer a performance bias when data sets are unbalanced. Data sets are unbalanced when at least one class is represented by only a small number of training examples (called the minority class), while the other class(es) make up the majority. In this scenario, classifiers can have good accuracy on the majority class, but very poor accuracy on the minority class(es). This paper proposes a multiobjective genetic programming (MOGP) approach to evolving accurate and diverse ensembles of genetic program classifiers with good performance on both the minority and majority of classes. The evolved ensembles comprise of nondominated solutions in the population where individual members vote on class membership. This paper evaluates the effectiveness of two popular Pareto-based fitness strategies in the MOGP algorithm (SPEA2 and NSGAII), and investigates techniques to encourage diversity between solutions in the evolved ensembles. Experimental results on six (binary) class imbalance problems show that the evolved ensembles outperform their individual members, as well as single-predictor methods such as canonical GP, naive Bayes, and support vector machines, on highly unbalanced tasks. This highlights the importance of developing an effective fitness evaluation strategy in the underlying MOGP algorithm to evolve good ensemble members."
a hybrid meta-learning architecture for multi-objective optimization of svm parameters,"Miranda P.B.C., Prudêncio R.B.C., de Carvalho A.P.L.F., Soares C.",Neurocomputing,2014.0,10.1016/j.neucom.2014.06.026,Meta-learning; Multi-objective optimization; Parameter selection; Particles swarm optimization; Support vector machines,29.0,"['Evolutionary', 'Particle Swarm Optimization']","['SVM', 'Classification', 'Meta-Learning']","['success rate in classification', 'number of support vectors']",Multi,"Support Vector Machines (SVMs) have achieved a considerable attention due to their theoretical foundations and good empirical performance when compared to other learning algorithms in different applications. However, the SVM performance strongly depends on the adequate calibration of its parameters. In this work we proposed a hybrid multi-objective architecture which combines meta-learning (ML) with multi-objective particle swarm optimization algorithms for the SVM parameter selection problem. Given an input problem, the proposed architecture uses a ML technique to suggest an initial Pareto front of SVM configurations based on previous similar learning problems; the suggested Pareto front is then refined by a multi-objective optimization algorithm. In this combination, solutions provided by ML are possibly located in good regions in the search space. Hence, using a reduced number of successful candidates, the search process would converge faster and be less expensive. In the performed experiments, the proposed solution was compared to traditional multi-objective algorithms with random initialization, obtaining Pareto fronts with higher quality on a set of 100 classification problems. © 2014 Elsevier B.V."
reusing genetic programming for ensemble selection in classification of unbalanced data,U. Bhowan; M. Johnston; M. Zhang; X. Yao,IEEE Transactions on Evolutionary Computation,2014.0,10.1109/TEVC.2013.2293393,Classification;ensemble machine learning;genetic programming;unbalanced data,57.0,"['Evolutionary', 'Genetic']","['Classification', 'Ensemble']","['minority acc', 'majority acc']",Single - Ensemble,"Classification algorithms can suffer from performance degradation when the class distribution is unbalanced. This paper develops a two-step approach to evolving ensembles using genetic programming (GP) for unbalanced data. The first step uses multiobjective (MO) GP to evolve a Pareto-approximated front of GP classifiers to form the ensemble by trading-off the minority and the majority class against each other during learning. The MO component alleviates the reliance on sampling to artificially rebalance the data. The second step, which is the focus this paper, proposes a novel ensemble selection approach using GP to automatically find/choose the best individuals for the ensemble. This new GP approach combines multiple Pareto-approximated front members into a single composite genetic program solution to represent the (optimized) ensemble. This ensemble representation has two main advantages/novelties over traditional genetic algorithm (GA) approaches. First, by limiting the depth of the composite solution trees, we use selection pressure during evolution to find small highly-cooperative groups of individuals for the ensemble. This means that ensemble sizes are not fixed a priori (as in GA), but vary depending on the strength of the base learners. Second, we compare different function set operators in the composite solution trees to explore new ways to aggregate the member outputs and thus, control how the ensemble computes its output. We show that the proposed GP approach evolves smaller more diverse ensembles compared to an established ensemble selection algorithm, while still performing as well as, or better than the established approach. The evolved GP ensembles also perform well compared to other bagging and boosting approaches, particularly on tasks with high levels of class imbalance."
multicriteria approaches for predictive model generation: a comparative experimental study,B. Al-Jubouri; B. Gabrys,2014 IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making (MCDM),2014.0,10.1109/MCDM.2014.7007189,,3.0,"['Evolutionary','NSGA-II']","['Neural Networks','Classification']","['Accuracy','Model Complexity','Algorithmic Complexity']",Multi,"This study investigates the evaluation of machine learning models based on multiple criteria. The criteria included are: predictive model accuracy, model complexity, and algorithmic complexity (related to the learning/adaptation algorithm and prediction delivery) captured by monitoring the execution time. Furthermore, it compares the models generated from optimising the criteria using two approaches. The first approach is a scalarized multi objective optimisation, where the models are generated from optimising a single cost function that combines the criteria. On the other hand the second approach uses a Pareto-based multi objective optimisation to trade-off the three criteria and to generate a set of non-dominated models. This study shows that defining universal measures for the three criteria is not always feasible. Furthermore, it was shown that, the models generated from Pareto-based multi objective optimisation approach can be more accurate and more diverse than the models generated from scalarized multi objective optimisation approach."
multiobjective optimization for model selection in kernel methods in regression,D. You; C. F. Benitez-Quiroz; A. M. Martinez,IEEE Transactions on Neural Networks and Learning Systems,2014.0,10.1109/TNNLS.2013.2297686,Kernel methods;kernel optimization;optimization;Pareto optimality;regression.;Kernel methods;kernel optimization;optimization;Pareto optimality;regression,12.0,"['Deterministic', 'ε-Constraint Optimization']","['Kernel methods', 'Regression']","['RMSE', 'Roughness Penalty']",Multi,"Regression plays a major role in many scientific and engineering problems. The goal of regression is to learn the unknown underlying function from a set of sample vectors with known outcomes. In recent years, kernel methods in regression have facilitated the estimation of nonlinear functions. However, two major (interconnected) problems remain open. The first problem is given by the bias-versus-variance tradeoff. If the model used to estimate the underlying function is too flexible (i.e., high model complexity), the variance will be very large. If the model is fixed (i.e., low complexity), the bias will be large. The second problem is to define an approach for selecting the appropriate parameters of the kernel function. To address these two problems, this paper derives a new smoothing kernel criterion, which measures the roughness of the estimated function as a measure of model complexity. Then, we use multiobjective optimization to derive a criterion for selecting the parameters of that kernel. The goal of this criterion is to find a tradeoff between the bias and the variance of the learned function. That is, the goal is to increase the model fit while keeping the model complexity in check. We provide extensive experimental evaluations using a variety of problems in machine learning, pattern recognition, and computer vision. The results demonstrate that the proposed approach yields smaller estimation errors as compared with methods in the state of the art."
feature selection and semi-supervised clustering using multiobjective optimization,"Saha, S; Ekbal, A; Alok, AK; Spandana, R",SPRINGERPLUS,2014.0,10.1186/2193-1801-3-465,Clustering; Multiobjective optimization (MOO); Symmetry; Cluster validity indices; Semi-supervised clustering; Feature selection; Multi-center; Automatic determination of number of clusters,,"['Evolutionary', 'AMOSA']","['Feature selection', 'semi-supervised clustering']","['sym-index', 'xb-index', 'adjusted rand index', 'number of features']",Single - Minkowski Score,"In this paper we have coupled feature selection problem with semi-supervised clustering. Semi-supervised clustering utilizes the information of unsupervised and supervised learning in order to overcome the problems related to them. But in general all the features present in the data set may not be important for clustering purpose. Thus appropriate selection of features from the set of all features is very much relevant from clustering point of view. In this paper we have solved the problem of automatic feature selection and semi-supervised clustering using multiobjective optimization. A recently created simulated annealing based multiobjective optimization technique titled archived multiobjective simulated annealing (AMOSA) is used as the underlying optimization technique. Here features and cluster centers are encoded in the form of a string. We assume that for each data set for 10% data points class level information are known to us. Two internal cluster validity indices reflecting different data properties, an external cluster validity index measuring the similarity between the obtained partitioning and the true labelling for 10% data points and a measure counting the number of features present in a particular string are optimized using the search capability of AMOSA. AMOSA is utilized to detect the appropriate subset of features, appropriate number of clusters as well as the appropriate partitioning from any given data set. The effectiveness of the proposed semi-supervised feature selection technique as compared to the existing techniques is shown for seven real-life data sets of varying complexities."
multi-objective learning of hybrid classifiers,"Piltaver, R; Lustrek, M; Zupancic, J; Dzeroski, S; Gams, M",21ST EUROPEAN CONFERENCE ON ARTIFICIAL INTELLIGENCE (ECAI 2014),2014.0,10.3233/978-1-61499-419-0-717,,,"['Deterministic', 'search']","['Classification', 'hybrid classifiers', 'hybrid trees']","['accuracy', 'comprehensibility']",Multi,"We propose a multi-objective machine learning approach guaranteed to find the Pareto optimal set of hybrid classification models consisting of comprehensible and incomprehensible submodels. The algorithm run-times are below 1 s for typical applications despite the exponential worst-case time complexity. The user chooses the model with the best comprehensibility-accuracy trade-off from the Pareto front which enables a well informed decision or repeats finding new Pareto fronts with modified seeds. For a classification trees as the comprehensible seed, the hybrids include single black-box model, invoked in hybrid leaves. The comprehensibility of such hybrid classifiers is measured with the proportion of examples classified by the regular leaves. We propose one simple and one computationally efficient algorithm for finding the Pareto optimal hybrid trees, starting from an initial classification tree and a black-box classifier. We evaluate the proposed algorithms empirically, comparing them to the baseline solution set, showing that they often provide valuable improvements. Furthermore, we show that the efficient algorithm outperforms the NSGA-II algorithm in terms of quality of the result set and efficiency (for this optimisation problem). Finally we show that the algorithm returns hybrid classifiers that reflect the expert's knowledge on activity recognition problem well."
multi-objective model type selection,"Rosales-P\'{e}rez, Alejandro and Gonzalez, Jesus A. and Coello Coello, Carlos A. and Escalante, Hugo Jair and Reyes-Garcia, Carlos A.",Elsevier Science Publishers B. V.,2014.0,10.1016/j.neucom.2014.05.077,"VC dimension, Multi-objective optimization, Model type selection, Ensemble methods",,"['Evolutionary', 'MOEA/D']","['Classification', 'model selection']","['error', 'vc dimension']",Multi,"Classification is a mainstream within the machine learning community. As a result, a large number of learning algorithms have been proposed. The performance of many of these could highly depend on the chosen values of their hyper-parameters. This paper introduces a novel method for addressing the model selection problem for a given classification task. In our model selection formulation, both the learning algorithm and its hyper-parameters are considered. In our proposed approach, model selection is tackled as a multi-objective optimization problem. The empirical error, or training error, and the model complexity are defined as the objectives. We adopt a multi-objective evolutionary algorithm as the search engine, due to its high performance and its advantages for solving multi-objective problems. The model complexity is estimated experimentally, in a general fashion, for any learning algorithm, through the VC dimension. Strategies for choosing a single model or for constructing an ensemble of models from the resulting non-dominated set are also proposed. Experimental results on benchmark data sets indicate the effectiveness of the proposed approach. Furthermore, a comparative study shows that the obtained models are highly competitive, in terms of generalization performance, with other methods in the state of the art that focus on a single-learning algorithm, or a single-objective approach."
enhancing the effectiveness of ant colony decision tree algorithms by co-learning,"Boryczka, U; Kozak, J",APPLIED SOFT COMPUTING,2015.0,10.1016/j.asoc.2014.12.036,Ant Colony Optimization; Ant Colony Decision Trees; Decision trees; Pareto front; Quality of decision trees; Ant-Miner,,"['Evolutionary', 'Ant Colony Optimization']","['Classification', 'Decision trees']","['Size of decision tree', 'Accuracy']",Multi,"Data mining and visualization techniques for high-dimensional data provide helpful information to substantially augment decision-making. Optimization techniques provide a way to efficiently search for these solutions. ACO applied to data mining tasks - a decision tree construction - is one of these methods and the focus of this paper. The Ant Colony Decision Tree (ACDT) approach generates solutions efficiently and effectively but scales poorly to large problems. This article merges the methods that have been developed for better construction of decision trees by ants. The ACDT approach is tested in the context of the bi-criteria evaluation function by focusing on two problems: the size of the decision trees and the accuracy of classification obtained during ACDT performance. This approach is tested in co-learning mechanism, it means agents-ants can interact during the construction decision trees via pheromone values. This cooperation is a chance of getting better results. The proposed methodology of analysis of ACDT is tested in a number of well-known benchmark data sets from the UCI Machine Learning Repository. The empirical results clearly show that the ACDT algorithm creates good solutions which are located in the Pareto front. The software that implements the ACDT algorithm used to generate the results of this study can be downloaded freely from http://www.acdtalgorithm.com. (C) 2015 Elsevier B.V. All rights reserved."
pareto-path multitask multiple kernel learning,C. Li; M. Georgiopoulos; G. C. Anagnostopoulos,IEEE Transactions on Neural Networks and Learning Systems,2015.0,10.1109/TNNLS.2014.2309939,Machine learning;optimization methods;pattern recognition;supervised learning;support vector machines (SVM).;Machine learning;optimization methods;pattern recognition;supervised learning;support vector machines (SVM),14.0,"['Deterministic', 'Tseng’s algorithm']","['Classification', 'Multitask', 'Multiple Kernel Learning', 'SVM']",['SVM objective'],Multi,"A traditional and intuitively appealing Multitask Multiple Kernel Learning (MT-MKL) method is to optimize the sum (thus, the average) of objective functions with (partially) shared kernel function, which allows information sharing among the tasks. We point out that the obtained solution corresponds to a single point on the Pareto Front (PF) of a multiobjective optimization problem, which considers the concurrent optimization of all task objectives involved in the Multitask Learning (MTL) problem. Motivated by this last observation and arguing that the former approach is heuristic, we propose a novel support vector machine MT-MKL framework that considers an implicitly defined set of conic combinations of task objectives. We show that solving our framework produces solutions along a path on the aforementioned PF and that it subsumes the optimization of the average of objective functions as a special case. Using the algorithms we derived, we demonstrate through a series of experimental results that the framework is capable of achieving a better classification performance, when compared with other similar MTL approaches."
a new ensemble learning methodology based on hybridization of classifier ensemble selection approaches,"Mousavi, R; Eftekhari, M",APPLIED SOFT COMPUTING,2015.0,10.1016/j.asoc.2015.09.009,Ensemble learning system; Static ensemble selection; Dynamic ensemble selection; Classifier combination; Classifier diversity; Multi-objective optimization,,"['Evolutive', 'NSGA-II']","['Classification', 'Ensemble', 'NN', 'DT', 'SVM', 'KNN']","['Accuracy', 'Correlation', 'Disagreement', 'Double Fault', 'Q-statistic']",Single - Ensemble,"Ensemble learning is a system that improves the performance and robustness of the classification problems. How to combine the outputs of base classifiers is one of the fundamental challenges in ensemble learning systems. In this paper, an optimized Static Ensemble Selection (SES) approach is first proposed on the basis of NSGA-II multi-objective genetic algorithm (called SES-NSGAII), which selects the best classifiers along with their combiner, by simultaneous optimization of error and diversity objectives. In the second phase, the Dynamic Ensemble Selection-Performance (DES-P) is improved by utilizing the first proposed method. The second proposed method is a hybrid methodology that exploits the abilities of both SES and DES approaches and is named Improved DES-P (IDES-P). Accordingly, combining static and dynamic ensemble strategies as well as utilizing NSGA-II are the main contributions of this research. Findings of the present study confirm that the proposed methods outperform the other ensemble approaches over 14 datasets in terms of classification accuracy. Furthermore, the experimental results are described from the view point of Pareto front with the aim of illustrating the relationship between diversity and the over-fitting problem. (C) 2015 Elsevier B.V. All rights reserved."
parallel alternatives for evolutionary multi-objective optimization in unsupervised feature selection,"Kimovski, Dragi and Ortega, Julio and Ortiz, Andr\'{e}s and Ba\~{n}os, Ra\'{u}l","Pergamon Press, Inc.",2015.0,10.1016/j.eswa.2015.01.061,"Parallel evolutionary algorithms, Unsupervised classification, Multi-objective clustering, Feature selection, High-dimensional data, Speedup models",,"['Evolutive', 'NSGA-II']","['Feature Selection', 'Classification']",[],Single - Combination,"Multiobjective unsupervised feature selection with many decision variables is tackled.EEG signals for Brain-Computer Interface (BCI) applications are used as benchmarks.Cooperative evolutionary algorithms for multiobjective optimization are given.Parallel implementations obtain quality results in terms of hypervolume and speedup.Superlinear speedups are justified by adjusting models to experimental results. Many machine learning and pattern recognition applications require reducing dimensionality to improve learning accuracy while irrelevant inputs are removed. This way, feature selection has become an important issue on these researching areas. Nevertheless, as in past years the number of patterns and, more specifically, the number of features to be selected have grown very fast, parallel processing constitutes an important tool to reach efficient approaches that make possible to tackle complex problems within reasonable computing times. In this paper we propose parallel multi-objective optimization approaches to cope with high-dimensional feature selection problems. Several parallel multi-objective evolutionary alternatives are proposed, and experimentally evaluated by using some synthetic and BCI (Brain-Computer Interface) benchmarks. The experimental results show that the cooperation of parallel evolving subpopulations provides improvements in the solution quality and computing time speedups depending on the parallel alternative and data profile."
convex hull-based multiobjective genetic programming for maximizing receiver operating characteristic performance,P. Wang; M. Emmerich; R. Li; K. Tang; T. Bäck; X. Yao,IEEE Transactions on Evolutionary Computation,2015.0,10.1109/TEVC.2014.2305671,Classification;evolutionary multiobjective algorithm;genetic programming;memetic algorithm;receiver operating characteristic (ROC) convex hull,26.0,['Evolutive'],"['Classification', 'ROCCH']","['False Positive Rate', 'True Positive Rate']",Multi,"The receiver operating characteristic (ROC) is commonly used to analyze the performance of classifiers in data mining. An important topic in ROC analysis is the ROC convex hull (ROCCH), which is the least convex majorant (LCM) of the empirical ROC curve and covers potential optima for a given set of classifiers. ROCCH maximization problems have been taken as multiobjective optimization problem (MOPs) in some previous work. However, the special characteristics of ROCCH maximization problem makes it different from traditional MOPs. In this paper, the difference will be discussed in detail and a new convex hull-based multiobjective genetic programming (CH-MOGP) is proposed to solve ROCCH maximization problems. Specifically, convex hull-based without redundancy sorting (CWR-sorting) is introduced, which is an indicator-based selection scheme that aims to maximize the area under the convex hull. A novel selection procedure is also proposed based on the proposed sorting scheme. It is hypothesized that by using a tailored indicator-based selection, CH-MOGP becomes more efficient for ROC convex hull approximation than algorithms that compute all Pareto optimal points. Empirical studies are conducted to compare CH-MOGP to both existing machine learning approaches and multiobjective genetic programming (MOGP) methods with classical selection schemes. Experimental results show that CH-MOGP outperforms the other approaches significantly."
an effective multiobjective approach for hard partitional clustering,"Prakash, J; Singh, PK",MEMETIC COMPUTING,2015.0,10.1007/s12293-014-0147-5,Data clustering; Multiobjective optimization; Evolutionary and swarm intelligence; Mutiobjective particle swarm optimization,,"['Evolutive', 'Particle Swarm Optimization']",['Clustering'],"['SSE', 'Connectedness']",Single - Highest F-measure,"Clustering is an unsupervised classification method in the field of data mining. Many population based evolutionary and swarm intelligence optimization methods are proposed to optimize clustering solutions globally based on a single selected objective function which lead to produce a single best solution. In this sense, optimized solution is biased towards a single objective, hence it is not equally well to the data set having clusters of different geometrical properties. Thus, clustering having multiple objectives should be naturally optimized through multiobjective optimization methods for capturing different properties of the data set. To achieve this clustering goal, many multiobjective population based optimization methods, e.g., multiobjective genetic algorithm, mutiobjective particle swarm optimization (MOPSO), are proposed to obtain diverse tradeoff solutions in the pareto-front. As single directional diversity mechanism in particle swarm optimization converges prematurely to local optima, this paper presents a two-stage diversity mechanism in MOPSO to improve its exploratory capabilities by incorporating crossover operator of the genetic algorithm. External archive is used to store non-dominated solutions, which is further utilized to find one best solution having highest F-measure value at the end of the run. Two conceptually orthogonal internal measures SSE and connectedness are used to estimate the clustering quality. Results demonstrate effectiveness of the proposed method over its competitors MOPSO, non-dominated sorting genetic algorithm, and multiobjective artificial bee colony on seven real data sets from UCI machine learning repository."
sprint multi-objective model racing,"Zhang, TT; Georgiopoulos, M; Anagnostopoulos, GC",GECCO'15: PROCEEDINGS OF THE 2015 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE,2015.0,10.1145/2739480.2754791,Racing Algorithm; Model Selection; Multi-objective Optimization; Sequential Probability Ratio Test,,['Deterministic'],"['Racing', 'Model Selection']",['User defined'],Multi,"Multi-objective model selection, which is an important aspect of Machine Learning, refers to the problem of identifying a set of Pareto optimal models from a given ensemble of models. This paper proposes SPRINT-Race, a multi-objective racing algorithm based on the Sequential Probability Ratio Test with an Indifference Zone. In SPRINT-Race, a non-parametric ternary-decision sequential analogue of the sign test is adopted to identify pair-wise dominance and non-dominance relationship. In addition, a Bonferroni approach is employed to control the overall probability of any erroneous decisions. In the fixed confidence setting, SPRINT-Race tries to minimize the computational effort needed to achieve a predefined confidence about the quality of the returned models. The efficiency of SPRTNT-Race is analyzed on artificially-constructed multi-objective model selection problems with known ground-truth. Moreover, SPRINT-Race is applied to identifying the Pareto optimal parameter settings of Ant Colony Optimization algorithms in the context of solving Traveling Salesman Problems. The experimental results confirm the advantages of SPRINT-Race for multi-objective model selection."
surrogate-assisted multi-objective model selection for support vector machines,"Rosales-P\'{e}rez, Alejandro and Gonzalez, Jesus A. and Coello Coello, Carlos A. and Escalante, Hugo Jair and Reyes-Garcia, Carlos A.",Elsevier Science Publishers B. V.,2015.0,10.1016/j.neucom.2014.08.075,"Support vector machines, Surrogate-assisted optimization, Model selection, Multi-objective optimization",,"['Evolutionary','SAMOEA']","['Classification','Pre-Processing','Feature Selection','Model Selection','SVM']","['Bias','Variance']",Single - Performance in validation,"Classification is one of the most well-known tasks in supervised learning. A vast number of algorithms for pattern classification have been proposed so far. Among these, support vector machines (SVMs) are one of the most popular approaches, due to the high performance reached by these methods in a wide number of pattern recognition applications. Nevertheless, the effectiveness of SVMs highly depends on their hyper-parameters. Besides the fine-tuning of their hyper-parameters, the way in which the features are scaled as well as the presence of non-relevant features could affect their generalization performance. This paper introduces an approach for addressing model selection for support vector machines used in classification tasks. In our formulation, a model can be composed of feature selection and pre-processing methods besides the SVM classifier. We formulate the model selection problem as a multi-objective one, aiming to minimize simultaneously two components that are closely related to the error of a model: bias and variance components, which are estimated in an experimental fashion. A surrogate-assisted evolutionary multi-objective optimization approach is adopted to explore the hyper-parameters space. We adopted this approach due to the fact that estimating the bias and variance could be computationally expensive. Therefore, by using surrogate-assisted optimization, we expect to reduce the number of solutions evaluated by the fitness functions so that the computational cost would also be reduced. Experimental results conducted on benchmark datasets widely used in the literature, indicate that highly competitive models with a fewer number of fitness function evaluations are obtained by our proposal when it is compared to state of the art model selection methods."
a flexible cluster-oriented alternative clustering algorithm for choosing from the pareto front of solutions,"Truong D.T., Battiti R.",Machine Learning,2015.0,10.1007/s10994-013-5350-y,Alternative clustering; Cluster-oriented recombination; Genetic algorithms; Multi-objective optimization,4.0,"['Evolutionary', 'Genetic', 'NSGA-II']","['Clustering', 'Supervised Alternative clustering']","['Vector Quantization Error', 'Adjusted Rand Index']",Multi,"Supervised alternative clustering is the problem of finding a set of clusterings which are of high quality and different from a given negative clustering. The task is therefore a clear multi-objective optimization problem. Optimizing two conflicting objectives at the same time requires dealing with trade-offs. Most approaches in the literature optimize these objectives sequentially (one objective after another one) or indirectly (by some heuristic combination of the objectives). Solving a multi-objective optimization problem in these ways can result in solutions which are dominated, and not Pareto-optimal. We develop a direct algorithm, called COGNAC, which fully acknowledges the multiple objectives, optimizes them directly and simultaneously, and produces solutions approximating the Pareto front. COGNAC performs the recombination operator at the cluster level instead of at the object level, as in the traditional genetic algorithms. It can accept arbitrary clustering quality and dissimilarity objectives and provides solutions dominating those obtained by other state-of-the-art algorithms. Based on COGNAC, we propose another algorithm called SGAC for the sequential generation of alternative clusterings where each newly found alternative clustering is guaranteed to be different from all previous ones. The experimental results on widely used benchmarks demonstrate the advantages of our approach. © 2013, The Author(s)."
hybrid multi-objective optimization approach for neural network classification using local search,"Mane S., Sonawani S., Sakhare S.",Advances in Intelligent Systems and Computing,2016.0,10.1007/978-981-10-0419-3_21,Classificationm; Local search; Multi-objective optimization; Neural network; NSGA II; Pareto optimality,2.0,"['Evolutionary', 'NSGA-II']","['Classification', 'Neural Network']","['MSE', 'Accuracy']",Single,"Classification is inherently multi-objective problem. It is one of the most important tasks of data mining to extract important patterns from large volume of data. Traditionally, either only one objective is considered or the multiple objectives are accumulated to one objective function. In the last decade, Pareto-based multi-objective optimization approach have gained increasing popularity due to the use of multi-objective optimization using evolutionary algorithms and population-based search methods. Multi-objective optimization approaches are more powerful than traditional single-objective methods as it addresses various topics of data mining such as classification, clustering, feature selection, ensemble learning, etc. This paper proposes improved approach of non-dominated sorting algorithm II (NSGA II) for classification using neural network model by augmenting with local search. It tries to enhance two conflicting objectives of classifier: Accuracy and mean squared error. NSGA II is improved by augmenting backpropagation as a local search method to deal with the disadvantage of genetic algorithm, i.e. slow convergence to best solutions. By using backpropagation we are able to speed up the convergence. This approach is applied in various classification problems obtained from UCI repository. The neural network modes obtained shows high accuracy and low mean squared error. © Springer Science+Business Media Singapore 2016."
multi-objective cost-sensitive attribute reduction on data with error ranges,"Fang, Y; Liu, ZH; Min, F",INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS,2016.0,10.1007/s13042-014-0296-3,Cost-sensitive learning; Attribute reduction; Test cost; Error range,,"['Heuristic', 'Backtrack reduction', 'k-weighted heuristic reduction']","['Attribute reduction', 'Classification']","['Coverage', 'Accuracy', 'Similarity']",Single,"In current supervised machine learning research spectrum, there are several attribute reduction methodologies to acquire reducts with low test cost. They can deal with symbolic data, or numeric data with error ranges. In many cases, they consider the situation with only one type of cost; therefore the problem is single-objective. This paper addresses the attribute reduction problem on data with multi-type-costs and error ranges. First, we define the multi-objective attribute reduction problem where multi-type-costs are involved. Second, we propose three metrics to evaluate the quality of a reduct set. Third, we design a backtrack algorithm to compute the Pareto optimal set, and a heuristic algorithm to find a sub-optimal reduct set. Finally, we compare these algorithms on seven UCI (University of California-Irvine) datasets. Experimental results indicate that our heuristic algorithm has good capability of tackling the proposed problem."
multi-objective parameter configuration of machine learning algorithms using model-based optimization,D. Horn; B. Bischl,2016 IEEE Symposium Series on Computational Intelligence (SSCI),2016.0,10.1109/SSCI.2016.7850221,,11.0,['Evolutionary'],"['Parameter Configuration', 'Classification']","['False Negative Rate', 'False Positive Rate']",Multi,"The performance of many machine learning algorithms heavily depends on the setting of their respective hyperparameters. Many different tuning approaches exist, from simple grid or random search approaches to evolutionary algorithms and Bayesian optimization. Often, these algorithms are used to optimize a single performance criterion. But in practical applications, a single criterion may not be sufficient to adequately characterize the behavior of the machine learning method under consideration and the Pareto front of multiple criteria has to be considered. We propose to use model-based multi-objective optimization to efficiently approximate such Pareto fronts."
multiobjective fuzzy genetics-based machine learning with a reject option,Y. Nojima; H. Ishibuchi,2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),2016.0,10.1109/FUZZ-IEEE.2016.7737854,Fuzzy genetics-based machine learning;reject option;evolutionary multiobjective optimization,1.0,"['Evolutionary', 'NSGA-II']","['Classification', 'Reject option', 'Fuzzy rule-based classifier']","['Number of rules', 'Error rate except for rejected patterns', 'Number of rejected patterns']",Multi,"Classifier design for a classification problem with M classes can be viewed as finding an optimal partition of its pattern space into M disjoint subspaces. However, this is not always a good strategy especially when training patterns from different classes are heavily overlapping in the pattern space. A simple but practically useful idea is the use of a reject option. In this case, the pattern space is partitioned into (M+1) disjoint subspace where the classification of new patterns is rejected in the (M+1)th subspace. In this paper, we discuss the design of fuzzy rule-based classifiers with a reject option. The rejection subspace is specified by a threshold value for the difference of a kind of matching degrees between the best matching class and the second best matching class. The important research question is how to specify the threshold value. We examine the following two approaches: One is manual specification after designing a fuzzy rule-based classifier, and the other is simultaneous multiobjective optimization of a threshold value and a fuzzy rule-based classifier. In the latter approach, we use three objectives: maximization of the correct classification, and minimization of the rejection and the complexity of the classifier."
ensemble of heterogeneous flexible neural trees using multiobjective genetic programming,"Ojha, VK; Abraham, A; Snasel, V",APPLIED SOFT COMPUTING,2017.0,10.1016/j.asoc.2016.09.035,Pareto-based multiobjectives; Flexible neural tree; Ensemble; Approximation; Feature selection,,"['Evolutionary', 'Genetic', 'NSGA-II']","['Ensemble', 'Flexible neural tree', 'Classification', 'Regression', 'Time-Series']","['MSE', 'Tree size', 'Diversity index']",Single - Ensemble,"Machine learning algorithms are inherently multiobjective in nature, where approximation error minimization and models complexity simplification are two conflicting objectives. We proposed a multiobjective genetic programming (MOGP) for creating a heterogeneous flexible neural tree (HFNT), tree-like flexible feedforward neural network model. The functional heterogeneity in neural tree nodes was introduced to capture a better insight of data during learning because each input in a dataset possess different features. MOGP guided an initial HFNT population towards Pareto-optimal solutions, where the final population was used for making an ensemble system. A diversity index measure along with approximation error and complexity was introduced to maintain diversity among the candidates in the population. Hence, the ensemble was created by using accurate, structurally simple, and diverse candidates from MOGP final population. Differential evolution algorithm was applied to fine-tune the underlying parameters of the selected candidates. A comprehensive test over classification, regression, and time-series datasets proved the efficiency of the proposed algorithm over other available prediction methods. Moreover, the heterogeneous creation of HFNT proved to be efficient in making ensemble system from the final population. (C) 2016 Elsevier B.V. All rights reserved."
multiobjective fuzzy genetics-based machine learning based on moea/d with its modifications,Y. Nojima; K. Arahari; S. Takemura; H. Ishibuchi,2017 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),2017.0,10.1109/FUZZ-IEEE.2017.8015749,Fuzzy classifier design;evolutionary fuzzy systems;MOEA/D;accuracy-oriented scalarizingfunction,3.0,"['Evolutionary', 'MOEA/D']","['Fuzzy Systems', 'Classification']","['Error', 'Number of rules', 'Total rule length']",Multi,"Various evolutionary multiobjective optimization (EMO) algorithms have been used in the field of evolutionary fuzzy systems (EFS), because EMO algorithms can easily handle multiple objective functions such as the accuracy maximization and complexity minimization for fuzzy system design. Most EMO algorithms used in EFS are Pareto dominance-based algorithms such as NSGA-II, SPEA2, and PAES. There are a few studies where other types of EMO algorithms are used in EFS. In this paper, we apply a multiobjective evolutionary algorithm based on decomposition called MOEA/D to EFS for fuzzy classifier design. MOEA/D is one of the most well-known decomposition-based EMO algorithms. The key idea is to divide a multiobjective optimization problem into a number of single-objective problems using a set of uniformly distributed weight vectors in a scalarizing function. We propose a new scalarizing function called an accuracy-oriented function (AOF) which is specialized for classifier design. We examine the effects of using AOF in MOEA/D on the search ability of our multiobjective fuzzy genetics-based machine learning (GBML). We also examine the synergy effect of MOEA/D with AOF and parallel distributed implementation of fuzzy GBML on the generalization ability."
a pareto-based ensemble with feature and instance selection for learning from multi-class imbalanced datasets,"Fernández A., Carmona C.J., José Del Jesus M., Herrera F.",International Journal of Neural Systems,2017.0,10.1142/S0129065717500289,ensembles; feature selection; Imbalanced classification; instance selection; multi-class; multi-objective evolutionary algorithms; overlapping,36.0,"['Evolutionary','NSGA-II']","['Ensemble','Classification','Multi-Class','Feature Selection','Instance Selection']","['M-AUC','RED']",Single - Ensemble,"Imbalanced classification is related to those problems that have an uneven distribution among classes. In addition to the former, when instances are located into the overlapped areas, the correct modeling of the problem becomes harder. Current solutions for both issues are often focused on the binary case study, as multi-class datasets require an additional effort to be addressed. In this research, we overcome these problems by carrying out a combination between feature and instance selections. Feature selection will allow simplifying the overlapping areas easing the generation of rules to distinguish among the classes. Selection of instances from all classes will address the imbalance itself by finding the most appropriate class distribution for the learning task, as well as possibly removing noise and difficult borderline examples. For the sake of obtaining an optimal joint set of features and instances, we embedded the searching for both parameters in a Multi-Objective Evolutionary Algorithm, using the C4.5 decision tree as baseline classifier in this wrapper approach. The multi-objective scheme allows taking a double advantage: the search space becomes broader, and we may provide a set of different solutions in order to build an ensemble of classifiers. This proposal has been contrasted versus several state-of-the-art solutions on imbalanced classification showing excellent results in both binary and multi-class problems. © 2017 World Scientific Publishing Company."
feature weighting and selection with a pareto-optimal trade-off between relevancy and redundancy,"Das, A; Das, S",PATTERN RECOGNITION LETTERS,2017.0,10.1016/j.patrec.2017.01.004,Feature selection; Feature weighting; Multi-objective optimization; Information measure; Classification,,"['Evolutionary', 'MOEA/D']","['Feature Selection', 'Classification']","['relevancy', 'redundancy']",Single,"Feature Selection (FS) is an important pre-processing step in machine learning and it reduces the number of features/variables used to describe each member of a dataset. Such reduction occurs by eliminating some of the non-discriminating and redundant features and selecting a subset of the existing features with higher discriminating power among various classes in the data. In this paper, we formulate the feature selection as a bi-objective optimization problem of some real-valued weights corresponding to each feature. A subset of the weighted features is thus selected as the best subset for subsequent classification of the data. Two information theoretic measures, known as 'relevancy' and 'redundancy' are chosen for designing the objective functions for a very competitive Multi-Objective Optimization (MOO) algorithm called 'Multi-Objective Evolutionary Algorithm based on Decomposition (MOEA/D)'. We experimentally determine the best possible constraints on the weights to be optimized. We evaluate the proposed bi-objective feature selection and weighting framework on a set of 15 standard datasets by using the popular k-Nearest Neighbor (k-NN) classifier. As is evident from the experimental results, our method appears to be quite competitive to some of the state-of-the-art FS methods of current interest. We further demonstrate the effectiveness of our framework by changing the choices of the optimization scheme and the classifier to Non-dominated Sorting Genetic Algorithm (NSGA)-II and Support Vector Machines (SVMs) respectively. (C) 2017 Elsevier B.V. All rights reserved."
a method for entity resolution in high dimensional data using ensemble classifiers,"Liu, Y; Diao, XC; Cao, JJ; Zhou, X; Shang, YL",MATHEMATICAL PROBLEMS IN ENGINEERING,2017.0,10.1155/2017/4953280,,,"['Evolutionary', 'MOACO']","['Ensemble', 'Classification', 'Entity Resolution', 'Feature Selection']","['Accuracy', 'Dissimilarity between classifiers', 'Cardinality of features']",Single - Ensemble,"In order to improve utilization rate of high dimensional data features, an ensemble learning method based on feature selection for entity resolution is developed. Entity resolution is regarded as a binary classification problem, an optimization model is designed to maximize each classifier's classification accuracy and dissimilarity between classifiers and minimize cardinality of features. A modified multiobjective ant colony optimization algorithm is employed to solve the model for each base classifier, two pheromone matrices are set up, weighted product method is applied to aggregate values of two pheromone matrices, and feature's Fisher discriminant rate of records' similarity vector is calculated as heuristic information. A solution which is called complementary subset is selected from Pareto archive according to the descending order of three objectives to train the given base classifier. After training all base classifiers, their classification outputs are aggregated by max-wins voting method to obtain the ensemble classifiers' final result. A simulation experiment is carried out on three classical datasets. The results show the effectiveness of our method, as well as a better performance compared with the other two methods."
a pso algorithm for multi-objective cost-sensitive attribute reduction on numeric data with error ranges,"Fang, Y; Liu, ZH; Min, F",SOFT COMPUTING,2017.0,10.1007/s00500-016-2260-5,Attribute reduction; Cost-sensitive learning; Particle swarm optimization; Rough sets,,"['Evolutionary', 'Particle Swarm Optimization']","['Cost-Sensitive Learning', 'Attribute reduction']","['Accuracy', 'Coverage', 'Similarity']",Multi,"Multi-objective cost-sensitive attribute reduction is an attractive problem in supervised machine learning. Most research has focused on single-objective minimal test cost reduction or dealt with symbolic data. In this paper, we propose a particle swarm optimization algorithm for the attribute reduction problem on numeric data with multiple costs and error ranges and use three metrics with which to evaluate the performance of the algorithm. The proposed algorithm benefits from a fitness function based on the positive region, the selected n types of the test cost, a set of constant weight values , and a designated non-positive exponent . We design a learning strategy by setting dominance principles, which ensures the preservation of Pareto-optimal solutions and the rejection of redundant solutions. With different parameter settings, our PSO algorithm searches for a sub-optimal reduct set. Finally, we test our algorithm on seven UCI (University of California, Irvine) datasets. Comparisons with alternative approaches including the -weighted method and exhaustive calculation method of reduction are analyzed. Experimental results indicate that our heuristic algorithm outperforms existing algorithms."
robust multiobjective evolutionary feature subset selection algorithm for binary classification using machine learning techniques,"Deniz, A; Kiziloz, HE; Dokeroglu, T; Cosar, A",NEUROCOMPUTING,2017.0,10.1016/j.neucom.2017.02.033,Multiobjective feature selection; Evolutionary algorithm; Binary classification; Supervised/unsupervised machine learning,,"['Evolutionary', 'Genetic', 'NSGA-II']","['Feature subset selection', 'Binary Classification']","['Number of features', 'Accuracy']",Multi,"This study investigates the success of a multiobjective genetic algorithm (GA) combined with state-of-the-art machine learning (ML) techniques for the feature subset selection (FSS) in binary classification problem (BCP). Recent studies have focused on improving the accuracy of BCP by including all of the features, neglecting to determine the best performing subset of features. However, for some problems, the number of features may reach thousands, which will cause too much computation power to be consumed during the feature evaluation and classification phases, also possibly reducing the accuracy of the results. Therefore, selecting the minimum number of features while preserving and/or increasing the accuracy of the results at a high level becomes an important issue for achieving fast and accurate binary classification. Our multiobjective evolutionary algorithm includes two phases, FSS using a GA and applying ML techniques for the BCP. Since exhaustively investigating all of the feature subsets is intractable, a GA is preferred for the first phase of the algorithm for intelligently detecting the most appropriate feature subset. The GA uses multiobjective crossover and mutation operators to improve a population of individuals (each representing a selected feature subset) and obtain (near-) optimal solutions through generations. In the second phase of the algorithms, the fitness of the selected subset is decided by using state-of-the-art ML techniques; Logistic Regression, Support Vector Machines, Extreme Learning Machine, K-means, and Affinity Propagation. The performance of the multiobjective evolutionary algorithm (and the ML techniques) is evaluated with comprehensive experiments and compared with state-of-the-art algorithms, Greedy Search, Particle Swarm Optimization, Tabu Search, and Scatter Search. The proposed algorithm was observed to be robust and it performed better than the existing methods on most of the datasets. (C) 2017 Elsevier B.V. All rights reserved."
solving test case based problems with fuzzy dominance,"Zutty, J; Rohling, G",PROCEEDINGS OF THE 2017 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE (GECCO'17),2017.0,10.1145/3071178.3071234,Genetic Algorithms; Machine Learning; Markov Chain Monte Carlo; Pareto Dominance,,['Evolutionary'],"['Feature subset selection', 'Binary Classification']","['True Positive Rate', 'False Positive Rate']",Multi,"Genetic algorithms and genetic programming lend themselves well to the field of machine learning, which involves solving test case based problems. However, most traditional multi-objective selection methods work with scalar objectives, such as minimizing false negative and false positive rates, that are computed from underlying test cases. In this paper, we propose a new fuzzy selection operator that takes into account the statistical nature of machine learning problems based on test cases. Rather than use a Pareto rank or strength computed from scalar objectives, such as with NSGA2 or SPEA2, we will compute a probability of Pareto optimality. This will be accomplished through covariance estimation and Markov chain Monte Carlo simulation in order to generate probabilistic objective scores for each individual. We then compute a probability that each individual will generate a Pareto optimal solution. This probability is directly used with a roulette wheel selection technique. Our method's performance is evaluated on the evolution of a feature selection vector for a binary classification on each of eight different activities. Fuzzy selection performance varies, outperforming both NSGA2 and SPEA2 in both speed (measured in generations) and solution quality (measured by area under the curve) in some cases, while underperforming in others."
classification rule mining approach based on multiobjective optimization,T. Sağ; H. Kahramanli,2017 International Artificial Intelligence and Data Processing Symposium (IDAP),2017.0,10.1109/IDAP.2017.8090264,Rule extraction;multiobjective optimization;genetic algorithms,,"['Evolutionary','NSGA-II']","['Rule Mining','Classification']","['Rate of the correct classification','Rate of wrong classification']",Single,"In this paper, a novel approach for classification rule mining is presented. The remarkable relationship between the rule extraction procedure and the concept of multiobjective optimization is emphasized. The range values of features composing the rules are handled as decision variables in the modelled multiobjective optimization problem. The proposed method is applied to three well-known datasets in literature. These are Iris, Haberman's Survival Data and Pima Indians Diabetes Datasets obtained from machine learning repository of University of California at Irvine (UCI). The classification rules are extracted with 100% accuracy for all datasets. These experimental results are the best outcomes found in literature so far."
multi-objective decision in machine learning,"de Medeiros, TH; Rocha, HP; Torres, FS; Takahashi, RHC; Braga, AP",JOURNAL OF CONTROL AUTOMATION AND ELECTRICAL SYSTEMS,2017.0,10.1007/s40313-016-0295-6,Machine learning; Multi-objective optimization; Decision-making; Classification,,"['Deterministic','Levenberg–Marquardt algorithm']","['Prior knowledge','Binary Classification']","['Training Error','Complexity']",Single - Prior Knowledge,"This work presents a novel approach for decisionmaking for multi-objective binary classification problems. The purpose of the decision process is to select within a set of Pareto-optimal solutions, one model that minimizes the structural risk (generalization error). This new approach utilizes a kind of prior knowledge that, if available, allows the selection of a model that better represents the problem in question. Prior knowledge about the imprecisions of the collected data enables the identification of the region of equivalent solutions within the set of Pareto-optimal solutions. Results for binary classification problems with sets of synthetic and real data indicate equal or better performance in terms of decision efficiency compared to similar approaches."
optimal set of overlapping clusters using multi-objective genetic algorithm,"Das, Sunanda and Chaudhuri, Shreya and Das, Asit K.",Association for Computing Machinery,2017.0,10.1145/3055635.3056653,"Cluster Analysis, Cluster validation index, Fuzzy Clustering, Multi-objective Genetic Algorithm, Overlapping Cluster",,"['Evolutionary', 'Genetic', 'NSGA']","['Clustering', 'Overlapping groups']","['Dunn index', 'Overlapping']",Single,"Clustering is an important unsupervised machine learning techniqueused in diverse fields to explore the inherent structure of the data. In most of the real life datasets, one object resides in many clusters with different membership values. Many clustering algorithms have been proposed for finding such overlapping clusters for knowledge extraction and future trend prediction. In the paper, multi-objective genetic algorithm based cluster analysis technique is proposed for finding the optimal set of overlapping clusters. As most of the real world search and optimization problems involve multiple objectives, multi-objective Genetic Algorithm is an obvious choice for capturing multiple optimal solutions. Thus the usefulness of applying the multi-objective Genetic Algorithm is to grouping the objects based on different objective functions for finding optimal set of overlapping clusters. The advantage of this algorithm is that it assigns a membership value only to the objects which are the members of several clusters, instead of assigning membership values for all clusters like fuzzy clustering algorithm. If any object positively belongs only to a single cluster, its membership value for this cluster is ' 1' and '0' for all other clusters. The overall performance of the method is investigated on some popular UCI and microarray datasets and the optimality of the clusters is measured by some important cluster validation indices. The experimental results show the effectiveness of the proposed method."
a multi-objective evolutionary approach to training set selection for support vector machine,"Acampora, G; Herrera, F; Tortora, G; Vitiello, A",KNOWLEDGE-BASED SYSTEMS,2018.0,10.1016/j.knosys.2018.02.022,Training set selection; Multi-objective optimization; Support vector machine,,"['Evolutionary', 'PESA-II']","['SVM', 'Classification', 'Training Set Selection']","['Accuracy', 'Reduction rate']",Single - Model with highiest sum of objectives,"The Support Vector Machine (SVM) is one of the most powerful algorithms for machine learning and data mining in numerous and heterogenous application domains. However, in spite of its competitiveness, SVM suffers from scalability problems which drastically worsens its performance in terms of memory requirements and execution time. As a consequence, there is a strong emergence of approaches for supporting SVM in efficiently addressing the aforementioned problems without affecting its classification capabilities. In this scenario, methods for Training Set Selection (TSS) represent a suitable and consolidated pre-processing technique to compute a reduced but representative training dataset, and improve SVM's scalability without deprecating its classification accuracy. Recently, TSS has been formulated as an optimization problem characterized by two objectives (the classification accuracy and the reduction rate) and solved through the application of evolutionary algorithms. However, so far, all the evolutionary approaches for TSS have been based on a so-called multi-objective a priori technique, where multiple objectives are aggregated together into a single objective through a weighted combination. This paper proposes to apply, for the first time, a Pareto-based multi-objective optimization approach to the TSS problem in order to explicitly deal with both its objectives and offer a better trade-off between SVM's classification and reduction performance. The benefits of the proposed approach are validated by a set of experiments involving well-known datasets taken from the UCI Machine Learning Database Repository. As shown by statistical tests, the application of a Pareto-based multi-objective optimization approach improves on state-of-the-art TSS techniques and enhances SVM efficiency. (c) 2018 Elsevier B.V. All rights reserved."
a multiobjective optimization-based sparse extreme learning machine algorithm,"Wu, Y; Zhang, YS; Liu, XB; Cai, ZH; Cai, YM",NEUROCOMPUTING,2018.0,10.1016/j.neucom.2018.07.060,Extreme learning machine; Sparse connecting structure; Parameter optimization; Structure learning; Multiobjective optimization,,"['Evolutionary', 'MOEA/D']","['Extreme Learning Machine', 'Classification', 'Regression']","['RMSE', 'Connecting sparsity']",Single - Ensemble,"Extreme Learning Machine (ELM) is a popular machine learning method and has been widely applied to real-world problems due to its fast training speed and good generalization performance. However, in ELM, the randomly assigned input weights and hidden biases usually degrade the generalization performance. Furthermore, ELM is considered as an empirical risk minimization model and easily leads to overfitting when dataset exists some outliers. In this paper, we proposed a novel algorithm named Multiobjective Optimization-based Sparse Extreme Learning Machine (MO-SELM), where parameter optimization and structure learning are integrated into the learning process to simultaneously enhance the generalization performance and alleviate the overfitting problem. In MO-SELM, the training error and the connecting sparsity are taken as two conflicting objectives of the multiobjective model, which aims to find sparse connecting structures with optimal weights and biases. Then, a hybrid encoding-based MOEA/D is used to optimize the multiobjective model. In addition, ensemble learning is embedded into this algorithm to make decisions after multiobjective optimization. Experimental results of several classification and regression applications demonstrate the effectiveness of the proposed MO-SELM. (C) 2018 Elsevier B.V. All rights reserved."
multiobjective sparse ensemble learning by means of evolutionary algorithms,"Zhao, JQ; Jiao, LC; Xia, SX; Fernandes, VB; Yevseyeva, I; Zhou, Y; Emmerich, MTM",DECISION SUPPORT SYSTEMS,2018.0,10.1016/j.dss.2018.05.003,Ensemble learning; Sparse representation; Classification; Multiobjective optimization; Change detection,,"['Evolutionary','TwoArch2','NSGA-III','MOEA/DD','RVEA','AR-MOEA','3DFCH-EMOA']","['Ensemble Learning','CLassification']","['Sparsity rate','False Positive Rate','False Negative Rate']",Single - Ensemble,"Ensemble learning can improve the performance of individual classifiers by combining their decisions. The sparseness of ensemble learning has attracted much attention in recent years. In this paper, a novel multi objective sparse ensemble learning (MOSEL) model is proposed. Firstly, to describe the ensemble classifiers more precisely the detection error trade-off (DET) curve is taken into consideration. The sparsity ratio (sr) is treated as the third objective to be minimized, in addition to false positive rate (fpr) and false negative rate (fnr) minimization. The MOSEL turns out to be augmented DET (ADET) convex hull maximization problem. Secondly, several evolutionary multiobjective algorithms are exploited to find sparse ensemble classifiers with strong performance. The relationship between the sparsity and the performance of ensemble classifiers on the ADET space is explained. Thirdly, an adaptive MOSEL classifiers selection method is designed to select the most suitable ensemble classifiers for a given dataset. The proposed MOSEL method is applied to well-known MNIST datasets and a real-world remote sensing image change detection problem, and several datasets are used to test the performance of the method on this problem. Experimental results based on both MNIST datasets and remote sensing image change detection show that MOSEL performs significantly better than conventional ensemble learning methods."
a novel deep learning approach: stacked evolutionary auto-encoder,Y. Cai; Z. Cai; M. Zeng; X. Liu; J. Wu; G. Wang,2018 International Joint Conference on Neural Networks (IJCNN),2018.0,10.1109/IJCNN.2018.8489138,Deep learning;Auto-encoder;Evolutionary Multiobjective Optimization,4.0,"['Evolutionary', 'NSGA-II']","['Neural Networks', 'Autoencoder', 'Extreme Learning Machine']","['MSE', 'L1-norm of the hidden-layer outputs']",Single,"Deep neural networks have been successfully applied to many data mining problems in recent works. The training of deep neural networks relies heavily upon gradient descent methods, however, which may lead to the failure of training due to the vanishing gradient (or exploding gradient) and local optima problems. In this paper, we present SEvoAE method based on using Evolutionary Multiobjective optimization (EMO) algorithm to train single layer auto-encoder, and sequentially learning deeper representation in a stacking way. SEvoAE is able to achieve accurate feature representation with good sparseness by globally simultaneously optimizing two conflicting objective functions and allows users to flexibly design objective functions and evolutionary optimizers. We compare results of the proposed method with existing architectures for seven classification problems, showing that the proposed method is able to outperform existing methods with a reduced risk of overfitting the training data."
feature learning in feature-sample networks using multi-objective optimization,F. A. Neto Verri; R. Tinós; L. Zhao,2018 IEEE Congress on Evolutionary Computation (CEC),2018.0,10.1109/CEC.2018.8477891,Feature learning;complex networks;multiobjective optimization;genetic algorithm,,"['Evolutionary','SPEA2']","['Feature Learning','Feature–Sample Networks','Clustering']","'Number of features','Disproportion between networks']",Single - Max number of features,"Data and knowledge representation are fundamental concepts in machine learning. The quality of the representation impacts the performance of a learning model directly. Feature learning transforms or enhances raw data to structures that are effectively exploited by those methods. In recent years, several works have been using complex networks for data representation and analysis. However, no feature learning method has been proposed to enhance such category of representation. Here, we present an unsupervised feature learning mechanism that works on datasets with binary features. First, the dataset is mapped into a feature-sample network. Then, a multi-objective optimization process selects a set of new vertices to produce an enhanced version of the network. The new features depend on a nonlinear function of a combination of preexisting features. Effectively, the process projects the input data into a higher-dimensional space. To solve the optimization problem, we design two metaheuristics based on the lexicographic genetic algorithm and the improved strength Pareto evolutionary algorithm (SPEA2). We show that the enhanced network contains more useful information and can be exploited to improve the performance of machine learning methods. The advantages and disadvantages of each optimization strategy are discussed."
dynamic feature selection based on pareto front optimization,J. Jesus; A. Canuto; D. Araújo,2018 International Joint Conference on Neural Networks (IJCNN),2018.0,10.1109/IJCNN.2018.8489680,,2.0,"['Deterministic', 'Test all possibilities']","['Feature Selection', 'Clustering', 'Classification']","['Mutual Information', 'Kullback-Leibler Divergence', 'Spearman Correlation']",Single - All of the selected features,"One of the main issues of machine learning algorithms is the curse of dimensionality. With the fast growing of complex data in real world scenarios, the feature selection becomes a mandatory preprocessing step in any application to reduce both the complexity of the data and the computing time. Based on that, several works have been produced in order to develop efficient methods to perform this task. Most feature selection methods select the best attributes based on some specific criteria. Additionally, recent studies have successfully constructed models to select features considering the particularities of the data, assuming that similar samples should be treated separately. Although some advance has been made, a bad choice of one single criteria to evaluate the importance of the attributes and the arbitrary choice of the number of features made by the user can lead to a poor analysis. In order to overcome some of these issues, this work brings an improvement of a dynamic feature selection algorithm (DFS) by using the idea of pareto front multi-objective optimization, which allow us to both consider distinct perspectives of the features relevance and automatically set the number of attributes to select. We tested our approach using 15 artificial and real world data and results have shown that when compared to the original DFS method, the performance of the proposed method is remarkable superior. In fact, the results are very promising since the proposed method also achieved better performance than well-established dimensionality reduction methods and when using the original datasets, showing that the reduction of noisy and/or redundant attributes can have a positive effect in the performance of a classification task."
a multi-objective optimization design framework for ensemble generation,"Ribeiro, Victor Henrique Alves and Reynoso-Meza, Gilberto",Association for Computing Machinery,2018.0,10.1145/3205651.3208219,"logistic regression, decision trees, multi-objective optimization, ensemble methods",,"['Evolutionary', 'sp-MODE']","['Ensemble', 'Classification']","['Accuracy', 'True Positive Rate', 'True Negative Rate', 'F1 Score', 'Model Complexity', 'Ensemble Complexity']",Single - Ensemble,"Machine learning algorithms have found to be useful for the solution of complex engineering problems. However, due to problem's characteristics, such as class imbalance, classical methods may not be formidable. The authors believe that the application of multi-objective optimization design can improve the results of machine learning algorithms on such scenarios. Thus, this paper proposes a novel methodology for the creation of ensembles of classifiers. To do so, a multi-objective optimization design approach composed of two steps is used. The first step focus on generating a set of diverse classifiers, while the second step focus on the selection of such classifiers as ensemble members. The proposed method is tested on a real-world competition data set, using both decision trees and logistic regression classifiers. Results show that the ensembles created with such technique outperform the best ensemble members."
novel multiobjective tlbo algorithms for the feature subset selection problem,"Kiziloz, HE; Deniz, A; Dokeroglu, T; Cosar, A",NEUROCOMPUTING,2018.0,10.1016/j.neucom.2018.04.020,Teaching learning based optimization; Multiobjective feature selection; Supervised learning,,"['Evolutionary', 'Teaching Learning Based Optimization', 'MTLBO-MD']","['Feature Subset Selection', 'Binary Classification']","['Accuracy', 'Number of features']",Single - Accuracy in Validation,"Teaching Learning Based Optimization (TLBO) is a new metaheuristic that has been successfully applied to several intractable optimization problems in recent years. In this study, we propose a set of novel multiobjective TLBO algorithms combined with supervised machine learning techniques for the solution of Feature Subset Selection (FSS) in Binary Classification Problems (FSS-BCP). Selecting the minimum number of features while not compromising the accuracy of the results in FSS-BCP is a multiobjective optimization problem. We propose TLBO as a FSS mechanism and utilize its algorithm-specific parameterless concept that does not require any parameters to be tuned during the optimization. Most of the classical metaheuristics such as Genetic and Particle Swarm Optimization algorithms need additional efforts for tuning their parameters (crossover ratio, mutation ratio, velocity of particle, inertia weight, etc.), which may have an adverse influence on their performance. Comprehensive experiments are carried out on the well-known machine learning datasets of UCI Machine Learning Repository and significant improvements have been observed when the proposed multiobjective TLBO algorithms are compared with state-of-the-art NSGA-II, Particle Swarm Optimization, Tabu Search, Greedy Search, and Scatter Search algorithms. (C) 2018 Elsevier B.V. All rights reserved."
automatic clustering by multi-objective genetic algorithm with numeric and categorical features,"Dutta D., Sil J., Dutta P.",Expert Systems with Applications,2019.0,10.1016/j.eswa.2019.06.056,Automatic clustering; Multi-Objective Genetic Algorithm (MOGA); Pareto approach; Statistical test,19.0,"['Evolutionary', 'MOGA']","['Clustering', 'Automatic clustering']","['Compactness', 'Separateness']",Single,"Many clustering algorithms categorized as K-clustering algorithm require the user to predict the number of clusters (K) to do clustering. Due to lack of domain knowledge an accurate value of K is difficult to predict. The problem becomes critical when the dimensionality of data points is large; clusters differ widely in shape, size, and density; and when clusters are overlapping in nature. Determining the suitable K is an optimization problem. Automatic clustering algorithms can discover the optimal K. This paper presents an automatic clustering algorithm which is superior to K-clustering algorithm as it can discover an optimal value of K. Iterative hill-climbing algorithms like K-Means work on a single solution and converge to a local optimum solution. Here, Genetic Algorithms (GAs) find out near global optimum solutions, i.e. optimal K as well as the optimal cluster centroids. Single-objective clustering algorithms are adequate for efficiently grouping linearly separable clusters. For non-linearly separable clusters they are not so good. So for grouping non-linearly separable clusters, we apply Multi-Objective Genetic Algorithm (MOGA) by minimizing the intra-cluster distance and maximizing inter-cluster distance. Many existing MOGA based clustering algorithms are suitable for either numeric or categorical features. This paper pioneered employing MOGA for automatic clustering with mixed types of features. Statistical testing on experimental results on real-life benchmark data sets from the University of California at Irvine (UCI) machine learning repository proves the superiority of the proposed algorithm. © 2019 Elsevier Ltd"
instance selection using multi-objective chc evolutionary algorithm,"Rathee, S; Ratnoo, S; Ahuja, J",INFORMATION AND COMMUNICATION TECHNOLOGY FOR COMPETITIVE STRATEGIES,2019.0,10.1007/978-981-13-0586-3_48,Multi-objective optimization; CHC algorithm; Instance selection; KNN,,"['Evolutionary', 'MOGA']","['Instance Selection', 'Classification']","['Accuracy', 'Percentage of data reduction']",Multi,"Data reduction has always been an important field of research to enhance the performance of data mining algorithms. Instance selection, a data reduction technique, relates to selecting a subset of informative and non-redundant examples from data. This paper deals with the problem of instance selection in a multi-objective perspective and, hence, proposes a multi-objective cross-generational elitist selection, heterogeneous recombination, and cataclysmic mutation (CHC) for discovering a set of Pareto-optimal solutions. The suggested MOCHC algorithm integrates the concept of non-dominating sorting with CHC. The algorithm has been employed to eight datasets available from UCI machine learning repository. The MOCHC has been successful in finding a range of multiple optimal solutions instead of yielding a single solution. These solutions provide a user with several choices of reduced datasets. Further, the solutions may be combined into a single instance subset by exploiting the promising characteristics across the potentially good solutions based on some user-defined criteria."
bi-level multi-objective evolution of a multi-layered echo-state network autoencoder for data representations,"Chouikhi, N; Ammar, B; Hussain, A; Alimi, AM",NEUROCOMPUTING,2019.0,10.1016/j.neucom.2019.03.012,Multi-Layered Echo State Network; Autoencoder; Data representation; PSO; Multi-objective optimization; Architecture optimization; Weights optimization,,"['Evolutionary', 'Particle Swarm Optimization']","['Autoencoder', 'Multi-LayeredEcho-StateNetwork', 'Architecture optimization', 'Classification']","['RMSE', 'Average Reservoirs Connectivity Rate', 'Average reservoir sizes']",Single - Optimize RMSE,"The Multi-Layered Echo-State Network (ML-ESN) is a recently developed, highly powerful type of recurrent neural network. It has succeeded in dealing with several non-linear benchmark problems. On account of its rich dynamics, ML-ESN is exploited in this paper, for the first time, as a recurrent Autoencoder (ML-ESNAE) to extract new features from original data representations. Further, the challenging and crucial task of optimally determining the ML-ESNAE architecture and training parameters is addressed, in order to extract more efficient features from the data. Traditionally, in a ML-ESN, the number of parameters (hidden neurons, sparsity rates, weights) are randomly chosen and manually altered to achieve a minimum learning error. On one hand, this random setting may not guarantee best generalization results. On the other, it can increase the network's complexity. In this paper, a novel bi-level evolutionary optimization approach is thus proposed for the ML-ESNAE, to deal with these challenges. The first level offers Pareto multi-objective architecture optimization, providing maximum learning accuracy while maintaining a reduced complexity target. Next, every Pareto optimal solution obtained from the first level undergoes a mono-objective weights optimization at the second level. Particle Swarm Optimization (PSO) is used as an evolutionary tool for both levels 1 and 2. An empirical study shows that the evolved ML-ESNAE produces a noticeable improvement in extracting new, more expressive data features from original ones. A number of application case studies, using a range of benchmark datasets, show that the extracted features produce excellent results in terms of classification accuracy. The effectiveness of the evolved ML-ESNAE is demonstrated for both noisy and noise-free data. In conclusion, the evolutionary ML-ESNAE is proposed as a new benchmark for the evolutionary AI and machine learning research community. (C) 2019 Elsevier B.V. All rights reserved."
an effective metaheuristic for bi-objective feature selection in two-class classification problem,"Lyubchenko, AA; Pacheco, JA; Casado, S; Nunez, L",XII INTERNATIONAL SCIENTIFIC AND TECHNICAL CONFERENCE APPLIED MECHANICS AND SYSTEMS DYNAMICS,2019.0,10.1088/1742-6596/1210/1/012086,bi-objective feature selection; classification; tabu search; MOAMP,,"['Metaheuristic', 'Multi-Objective  Adaptive  Memory  Programming', 'MOAMP']","['Feature  Selection', 'Classification']","['Misclassification  error', 'Subset size']",Multi,"Feature selection is known as a very useful technique in machine learning practice as it may result in the development of more straightforward models with better accuracy. Traditionally, feature selection is considered as a single-objective problem, however, it can be easily formulated in terms of two objectives. The solving of such problems requires the application of appropriate multi-objective optimization methods that do not always offer equally good solutions even under the same conditions. This paper focuses on the development of a metaheuristic optimization approach for bi-objective feature selection problem in two-class classification. We consider the solving of this problem in terms of minimization of both misclassification error and feature subset size. For solving the considered problem, an adaptation of the Multi-Objective Adaptive Memory Programming (MOAMP) metaheuristic based on the tabu search strategy is proposed. Our MOAMP adaption has been utilized to obtain the sets of most relevant features for two real classification problems with two classes. Finally, using popular Pareto front quality indicators, the obtained results have been compared with the sets of non-dominated solutions derived by the well-known NSGA2 algorithm. The conducted research allows concluding about the ability of the MOAMP adaptation to get a better efficient frontier for the same number of objective function calls."
multilayer perceptron: nsga ii for a new multi-objective learning method for training and model complexity,"Senhaji K., Ramchoun H., Ettaouil M.",Advances in Intelligent Systems and Computing,2019.0,10.1007/978-3-319-91337-7_15,Multi-objective training; Multilayer perceptron; Non-dominated Sorting Genetic Algorithm II (NSGA II); Non-linear optimization; Pareto front; Supervised learning,1.0,"['Evolutionary', 'NSGA-II']","['Multilayer Perceptron', 'Classification']","['Training error', 'Network weights']",Single - Crowding distance,"The multi-layer perceptron has proved its efficiencies in several fields as pattern and voice recognition. Unfortunately, the classical training for MLP suffers from a poor generalization. In this respect, we have proposed a new multi-objective training model with constraints, satisfies two objectives. The first one is the learning objective: minimizing the perceptron error and the second is the complexity objective: optimizing number of weights and neurons. The proposed model will provide a balance between the multi-layer perceptron learning and the complexity to get a good generalization. Our model has been solved using an evolutionary approach called the Non-Dominated Sorting Genetic Algorithm (NSGA II). This approach has led to a good representation of the Pareto set for the MLP network, from which an improved generalization performance model is selected. © 2019, Springer International Publishing AG, part of Springer Nature."
constrained multi-objective optimization for automated machine learning,S. Gardner; O. Golovidov; J. Griffin; P. Koch; W. Thompson; B. Wujek; Y. Xu,2019 IEEE International Conference on Data Science and Advanced Analytics (DSAA),2019.0,10.1109/DSAA.2019.00051,Multi-objective Optimization;Automated Machine Learning;Distributed Computing System,4.0,"['Evolutionary', 'Genetic', 'Autotune']","['AutoML', 'Classification']","['Misclassification Rate', 'False Positive Rate', 'Black-box functions']",Multi,"Automated machine learning has gained a lot of attention recently. Building and selecting the right machine learning models is often a multi-objective optimization problem. General purpose machine learning software that simultaneously supports multiple objectives and constraints is scant, though the potential benefits are great. In this work, we present a framework called Autotune that effectively handles multiple objectives and constraints that arise in machine learning problems. Autotune is built on a suite of derivative-free optimization methods, and utilizes multi-level parallelism in a distributed computing environment for automatically training, scoring, and selecting good models. Incorporation of multiple objectives and constraints in the model exploration and selection process provides the flexibility needed to satisfy trade-offs necessary in practical machine learning applications. Experimental results from standard multi-objective optimization benchmark problems show that Autotune is very efficient in capturing Pareto fronts. These benchmark results also show how adding constraints can guide the search to more promising regions of the solution space, ultimately producing more desirable Pareto fronts. Results from two real-world case studies demonstrate the effectiveness of the constrained multi-objective optimization capability offered by Autotune."
nsga-net: neural architecture search using multi-objective genetic algorithm,"Lu, Zhichao and Whalen, Ian and Boddeti, Vishnu and Dhebar, Yashesh and Deb, Kalyanmoy and Goodman, Erik and Banzhaf, Wolfgang",Association for Computing Machinery,2019.0,10.1145/3321707.3321729,"deep learning, image classification, multi objective, bayesian optimization, neural architecture search",,"['Evolutionary', 'NSGA-II']","['Architecture Search', 'Neural Network', 'Classification']","['Classification Error', 'Computational complexity']",Multi,"This paper introduces NSGA-Net --- an evolutionary approach for neural architecture search (NAS). NSGA-Net is designed with three goals in mind: (1) a procedure considering multiple and conflicting objectives, (2) an efficient procedure balancing exploration and exploitation of the space of potential neural network architectures, and (3) a procedure finding a diverse set of trade-off network architectures achieved in a single run. NSGA-Net is a population-based search algorithm that explores a space of potential neural network architectures in three steps, namely, a population initialization step that is based on prior-knowledge from hand-crafted architectures, an exploration step comprising crossover and mutation of architectures, and finally an exploitation step that utilizes the hidden useful knowledge stored in the entire history of evaluated neural architectures in the form of a Bayesian Network. Experimental results suggest that combining the dual objectives of minimizing an error metric and computational complexity, as measured by FLOPs, allows NSGA-Net to find competitive neural architectures. Moreover, NSGA-Net achieves error rate on the CIFAR-10 dataset on par with other state-of-the-art NAS methods while using orders of magnitude less computational resources. These results are encouraging and shows the promise to further use of EC methods in various deep-learning paradigms."
a multi-objective competitive co-evolutionary approach for classification problems,V. T. VU; L. T. BUI; T. T. NGUYEN,2019 6th NAFOSTED Conference on Information and Computer Science (NICS),2019.0,10.1109/NICS48868.2019.9023887,competitive co-evolutionary;Prey-Predator;multiobjective optimization;classification;ensemble learning.,,"['Evolutionary', 'Prey-Predator']","['Classification', 'Neural Networks', 'Ensemble']","['Accuracy', 'Diversity']",Single - Ensemble,"This paper proposes a multi-objective competitive co-evolutionary algorithm (MOCPCEA) based on the PreyPredator model to solve classification problems. In the MOCPCEA, a data population acts as preys. To be specific, each prey represents a selected subset of the training dataset. Another population is ANN classifiers which play as Predators. The task of the Predators is to try to classify the data sets as correctly as possible, whereas the Preys try to find the data sets that are difficult to be classified. Through this interaction process, MOCPCEA generates a set of classifiers that are able to classify difficult data sets. The final classification result is given by the ensemble voting mechanism among these sets of classifiers. The performance of the proposed algorithm is performed on seven benchmark problems. Through comparison with other algorithms, the proposed algorithm indicates that it could create an ensemble of ANN networks that give high and stable classification results."
what's inside the black-box? a genetic programming method for interpreting complex machine learning models,"Evans, Benjamin P. and Xue, Bing and Zhang, Mengjie",Association for Computing Machinery,2019.0,10.1145/3321707.3321726,"interpretable machine learning, evolutionary multi-objective optimisation, explainable artificial intelligence",,"['Evolutionary', 'Genetic', 'NSGA-II']","['Expainable AI', 'Interpretable ML', 'Classification']","['f1-score', 'Split points']",Single - Largest f1,"Interpreting state-of-the-art machine learning algorithms can be difficult. For example, why does a complex ensemble predict a particular class? Existing approaches to interpretable machine learning tend to be either local in their explanations, apply only to a particular algorithm, or overly complex in their global explanations. In this work, we propose a global model extraction method which uses multi-objective genetic programming to construct accurate, simplistic and model-agnostic representations of complex black-box estimators. We found the resulting representations are far simpler than existing approaches while providing comparable reconstructive performance. This is demonstrated on a range of datasets, by approximating the knowledge of complex black-box models such as 200 layer neural networks and ensembles of 500 trees, with a single tree."
multiobjective semisupervised classifier ensemble,Z. Yu; Y. Zhang; C. L. P. Chen; J. You; H. Wong; D. Dai; S. Wu; J. Zhang,IEEE Transactions on Cybernetics,2019.0,10.1109/TCYB.2018.2824299,Ensemble learning;feature selection;multiobjective optimization;semisupervised learning,13.0,"['Evolutionary', 'Genetic', 'NSGA-II']","['Classification', 'Ensemble', 'Feature Selection']","['Relevance of features', 'Redundancy between features', 'Data reconstruction error']",Single - Ensemble,"Classification of high-dimensional data with very limited labels is a challenging task in the field of data mining and machine learning. In this paper, we propose the multiobjective semisupervised classifier ensemble (MOSSCE) approach to address this challenge. Specifically, a multiobjective subspace selection process (MOSSP) in MOSSCE is first designed to generate the optimal combination of feature subspaces. Three objective functions are then proposed for MOSSP, which include the relevance of features, the redundancy between features, and the data reconstruction error. Then, MOSSCE generates an auxiliary training set based on the sample confidence to improve the performance of the classifier ensemble. Finally, the training set, combined with the auxiliary training set, is used to select the optimal combination of basic classifiers in the ensemble, train the classifier ensemble, and generate the final result. In addition, diversity analysis of the ensemble learning process is applied, and a set of nonparametric statistical tests is adopted for the comparison of semisupervised classification approaches on multiple datasets. The experiments on 12 gene expression datasets and two large image datasets show that MOSSCE has a better performance than other state-of-the-art semisupervised classifiers on high-dimensional data."
multi-task learning by pareto optimality,"Dyankov D., Riccio S.D., Di Fatta G., Nicosia G.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019.0,10.1007/978-3-030-37599-7_50,Atari 2600 Games; Deep artificial neural networks; Deep neuroevolution; Evolution Strategy; Hypervolume; Kullback-Leibler Divergence; Multitask learning; Neural and evolutionary computing,1.0,['Evolutionary'],"['Multitask Learning', 'Deep Neural Networks']",['Utility function of each task'],Multi,"Deep Neural Networks (DNNs) are often criticized because they lack the ability to learn more than one task at a time: Multitask Learning is an emerging research area whose aim is to overcome this issue. In this work, we introduce the Pareto Multitask Learning framework as a tool that can show how effectively a DNN is learning a shared representation common to a set of tasks. We also experimentally show that it is possible to extend the optimization process so that a single DNN simultaneously learns how to master two or more Atari games: using a single weight parameter vector, our network is able to obtain sub-optimal results for up to four games. © Springer Nature Switzerland AG 2019."
evolutionary discovery of coresets for classification,"Barbiero, Pietro and Squillero, Giovanni and Tonda, Alberto",Association for Computing Machinery,2019.0,10.1145/3319619.3326846,"evolutionary algorithms, multi-objective, explain AI, coreset discovery, classification, machine learning",,"['Evolutionary', 'NSGA-II']","['Classification', 'Coreset discovery', 'Explainable AI']","['Number of samples', 'Accuracy']",Single - Accuracy in Validation,"When a machine learning algorithm is able to obtain the same performance given a complete training set, and a small subset of samples from the same training set, the subset is termed coreset. As using a coreset improves training speed and allows human experts to gain a better understanding of the data, by reducing the number of samples to be examined, coreset discovery is an active line of research. Often in literature the problem of coreset discovery is framed as i. single-objective, attempting to find the candidate coreset that best represents the training set, and ii. independent from the machine learning algorithm used. In this work, an approach to evolutionary coreset discovery is presented. Building on preliminary results, the proposed approach uses a multi-objective evolutionary algorithm to find compromises between two conflicting objectives, i. minimizing the number of samples in a candidate coreset, and ii. maximizing the accuracy of a target classifier, trained with the coreset, on the whole original training set. Experimental results on popular classification benchmarks show that the proposed approach is able to identify candidate coresets with better accuracy and generality than state-of-the-art coreset discovery algorithms found in literature."
margin-based pareto ensemble pruning: an ensemble pruning algorithm that learns to search optimized ensembles,"Hu, RH; Zhou, SB; Liu, YS; Tang, ZR",COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE,2019.0,10.1155/2019/7560872,,,"['Evolutionary', 'NSGA-II']","['Classification', 'Ensemble', 'Ensemble Pruning']","['Classification Error', 'Number of models selected']",Single,"The ensemble pruning system is an effective machine learning framework that combines several learners as experts to classify a test set. Generally, ensemble pruning systems aim to define a region of competence based on the validation set to select the most competent ensembles from the ensemble pool with respect to the test set. However, the size of the ensemble pool is usually fixed, and the performance of an ensemble pool heavily depends on the definition of the region of competence. In this paper, a dynamic pruning framework called margin-based Pareto ensemble pruning is proposed for ensemble pruning systems. The framework explores the optimized ensemble pool size during the overproduction stage and finetunes the experts during the pruning stage. The Pareto optimization algorithm is used to explore the size of the overproduction ensemble pool that can result in better performance. Considering the information entropy of the learners in the indecision region, the marginal criterion for each learner in the ensemble pool is calculated using margin criterion pruning, which prunes the experts with respect to the test set. The effectiveness of the proposed method for classification tasks is assessed using datasets. The results show that margin-based Pareto ensemble pruning can achieve smaller ensemble sizes and better classification performance in most datasets when compared with state-of-the-art models."
evolutionary inversion of class distribution in overlapping areas for multi-class imbalanced learning,"Fernandes, ERQ; de Carvalho, ACPLF",INFORMATION SCIENCES,2019.0,10.1016/j.ins.2019.04.052,Multi-Class imbalanced learning; Ensemble of classifiers; Evolutionary algorithms,,"['Evolutionary', 'NSGA-II']","['Ensemble', 'Classification', 'Multi-Class', 'Imbalanced Learning']","['Accuracy', 'percentage of instances of the majority classes that are at the border of separation with the minority classes']",Single - Ensemble,"Inductive learning from multi-class and imbalanced datasets is one of the main challenges for machine learning. Most machine learning algorithms have their predictive performance negatively affected by imbalanced data. Although several techniques have been proposed to deal with this difficulty, they are usually restricted to binary classification datasets. Thus, one of the research challenges in this area is how to deal with imbalanced multiclass classification datasets. This challenge become more difficult when classes containing fewer instances are located in overlapping regions of the data attribute space. In fact, several studies have indicated that the degree of class overlapping has a higher effect on predictive performance than the global class imbalance ratio. This paper proposes a novel evolutionary ensemble-based method for multi-class imbalanced learning called the evolutionary inversion of class distribution in overlapping areas for multi-class imbalanced learning (EVINCI). EVINCI uses a multiobjective evolutionary algorithm (MOEA) to evolve a set of samples taken from an imbalanced dataset. It selectively reduces the concentration of less representative instances of the majority classes in the overlapping areas while selecting samples that produce more accurate models. In experiments performed to evaluate its predictive accuracy, EVINCI was superior to state-of-the-art ensemble-based methods for imbalanced learning. (C) 2019 Published by Elsevier Inc."
automatic decision making for parameters in kernel method,Y. Pei,2019 IEEE Symposium Series on Computational Intelligence (SSCI),2019.0,10.1109/SSCI44817.2019.9002691,kernel method;kernel function;reproducing kernel Hilbert space;machine learning;decision making;evolutionary multi-objective optimization,2.0,"['Evolutionary', 'Chaotic evolution']","['Kernel methods', 'AutoML', 'Classification']","['Distance', 'Angle']",Multi,"We propose to use the relationship between the parameter of kernel function and its decisional angle or distance metrics for selecting the optimal setting of the parameter of kernel functions in kernel method-based algorithms. Kernel method is established in the reproducing kernel Hilbert space, the angle and distance are two metrics in such space. We analyse and investigate the relationship between the parameter of kernel function and the metrics (distance or angle) in the reproducing kernel Hilbert space. We design a target function of optimization to model the relationship between these two variables, and found that (1) the landscape shapes of parameter and the metrics are the same in Gaussian kernel function because the norm of all the vectors are equal to one in reproducing kernel Hilbert space; (2) the landscape monotonicity of that are opposite in polynomial kernel function from that of Gaussian kernel. The monotonicity of designed target functions of optimization using Gaussian kernel and polynomial kernel is different as well. The distance metric and angle metric have different distribution characteristics for the decision of parameter setting in kernel function. It needs to balance these two metrics when selecting a proper parameter of the kernel function in kernel-based algorithms. We use evolutionary multi-objective optimization algorithms to obtain the Pareto solutions for optimal selection of the parameter in kernel functions. We found that evolutionary multi-objective optimization algorithms are useful tools to balance the distance metric and angle metric in the decision of parameter setting in kernel method-based algorithms."
single-objective/multiobjective cat swarm optimization clustering analysis for data partition,D. Yan; H. Cao; Y. Yu; Y. Wang; X. Yu,IEEE Transactions on Automation Science and Engineering,2020.0,10.1109/TASE.2020.2969485,Clustering analysis;data partition;quantum model;single-objective/multiobjective optimization,9.0,"['Evolutionary', 'Cat Swarm Optimization']","['Clustering', 'Data Partition']","['Cohesion', 'Connectivity']",Single - Max crowding-distance,"This article proposes single-objective/multiobjective cat swarm optimization clustering algorithms for data partition. The proposed methods use the cat swarm to search the optimal. The position of the cat tightly associates with the clustering centers and is updated by two submodes: the seeking mode and the tracing mode. The seeking mode uses the simulated annealing strategy to update the cat position at a probability. Inspired by the quantum theories, the tracing mode adopts the quantum model to update the cat position in the whole solution space. First, the single-objective method is proposed and adopts the cohesion of clustering as the objective function, in which the kernel method is applied. For considering more objective functions to reveal diverse aspects of data, the multiobjective method is proposed and adopts both the cohesion and the connectivity as the objective functions. The Pareto optimization method is applied to balance the objectives. In the experiments, three kinds of data sets are used to examine the effectiveness of the proposed methods, which are three synthetic data sets, four data sets from the UCI Machine Learning Repository, and a field data set. Experimental results verified that the proposed methods perform better than the traditional clustering algorithms, and the proposed multiobjective method has the highest accuracy. Note to Practitioners-This article presents single-objective/multiobjective cat swarm optimization clustering analysis methods for data partition. Through automatically extracting meaningful or useful classes, clustering analysis could help the practitioners or the intelligent devices find the specific meanings of data, natural data structure, the data relationships, or other characteristics. The proposed methods use the cat swarm to search the optimal clustering result. One or more criterion functions could be selected as the optimization objectives. The time complexity of the multiobjective type is higher than that of the single-objective type. Therefore, in the industrial field, engineers should choose the number of the optimization objectives based on the actual requirements. The proposed methods could be widely used into industrial applications to deal with complex data sets. Future research could consider some more progressive optimization schemes to improve the effectiveness."
automated test input generation for convolutional neural networks by implementing multi-objective evolutionary algorithms,L. ZHANG; H. SATO,2020 Eighth International Symposium on Computing and Networking Workshops (CANDARW),2020.0,10.1109/CANDARW51189.2020.00040,Deep learning testing;Automated test input generation;Evolutionary algorithms,,"['Evolutionary', 'NSGA-II']","['Deep Neural Networks', 'Input generation', 'Classification']","['Neuron coverage', 'Behavioral divergence', 'Pertubation degree']",Single - Crowded tournament selection,"Deep Neural Networks (DNNs) have been widely applied in safety- and security-critical aspects, where the robustness of the system is of great significance, especially for corner case inputs. Traditionally, a DNN is tested with manually labeled data, which is not only labor-consuming, but also unable to contain statistically rare case inputs.In our work, we design, implement and evaluate the test input generation framework guided by multi-objective functions. The multi-objective functions are formed from neuron coverage, behavioral divergence and perturbation degree. We leverage evolutionary algorithms (EAs) to resolve such optimization problem by generating approximation to Pareto-optimal solutions. By implementing our framework, we successfully generated more than 6,000 test inputs for a convolutional neural network. And the generated test inputs help to improve the system’s accuracy by up to 4.4%."
machine learning with incomplete datasets using multi-objective optimization models,H. A. Khorshidi; M. Kirley; U. Aickelin,2020 International Joint Conference on Neural Networks (IJCNN),2020.0,10.1109/IJCNN48605.2020.9206742,incomplete data;multi-objective model;uncertainty;model selection;classification,,"['Evolutionary', 'NSGA-II']","['Incomplete data', 'SVM', 'Classification']","['Cluster validity', 'Correlation', 'Variance ratio']",Multi,"Machine learning techniques have been developed to learn from complete data. When missing values exist in a dataset, the incomplete data should be preprocessed separately by removing data points with missing values or imputation. In this paper, we propose an online approach to handle missing values while a classification model is learnt. To reach this goal, we develop a multi-objective optimization model with two objective functions for imputation and model selection. We also propose three formulations for imputation objective function. We use an evolutionary algorithm based on NSGA II to find the optimal solutions as the Pareto solutions. We investigate the reliability and robustness of the proposed model using experiments by defining several scenarios in dealing with missing values and classification. We also describe how the proposed model can contribute to medical informatics. We compare the performance of three different formulations via experimental results. The proposed model results get validated by comparing with a comparable literature."
multi-objective hyperparameter tuning and feature selection using filter ensembles,"Binder, Martin and Moosbauer, Julia and Thomas, Janek and Bischl, Bernd",Association for Computing Machinery,2020.0,10.1145/3377930.3389815,"model-based optimization, multiobjective optimization, hyperparameter optimization, evolutionary algorithms, feature selection",,"['Evolutionary', 'NSGA-II', 'Bayesian']","['Feature Selection', 'Hyperparameter tuning', 'Classification', 'Filter Ensemble']","['Generalization error', 'Fraction of selected features']",Single - Ensemble,"Both feature selection and hyperparameter tuning are key tasks in machine learning. Hyperparameter tuning is often useful to increase model performance, while feature selection is undertaken to attain sparse models. Sparsity may yield better model interpretability and lower cost of data acquisition, data handling and model inference. While sparsity may have a beneficial or detrimental effect on predictive performance, a small drop in performance may be acceptable in return for a substantial gain in sparseness. We therefore treat feature selection as a multi-objective optimization task. We perform hyperparameter tuning and feature selection simultaneously because the choice of features of a model may influence what hyperparameters perform well.We present, benchmark, and compare two different approaches for multi-objective joint hyperparameter optimization and feature selection: The first uses multi-objective model-based optimization. The second is an evolutionary NSGA-II-based wrapper approach to feature selection which incorporates specialized sampling, mutation and recombination operators. Both methods make use of parameterized filter ensembles.While model-based optimization needs fewer objective evaluations to achieve good performance, it incurs computational overhead compared to the NSGA-II, so the preferred choice depends on the cost of evaluating a model on given data."
transfer clustering ensemble selection,Y. Shi; Z. Yu; C. L. P. Chen; J. You; H. -S. Wong; Y. Wang; J. Zhang,IEEE Transactions on Cybernetics,2020.0,10.1109/TCYB.2018.2885585,Clustering ensemble selection (CES);machine learning;multiobjective;transfer learning,7.0,"['Evolutionary', 'NSGA-II', 'Bayesian']","['Clustering', 'Ensemble', 'Transfer Learning']","['Performance', 'Quality index', 'Diversity index']",Single - Ensemble,"Clustering ensemble (CE) takes multiple clustering solutions into consideration in order to effectively improve the accuracy and robustness of the final result. To reduce redundancy as well as noise, a CE selection (CES) step is added to further enhance performance. Quality and diversity are two important metrics of CES. However, most of the CES strategies adopt heuristic selection methods or a threshold parameter setting to achieve tradeoff between quality and diversity. In this paper, we propose a transfer CES (TCES) algorithm which makes use of the relationship between quality and diversity in a source dataset, and transfers it into a target dataset based on three objective functions. Furthermore, a multiobjective self-evolutionary process is designed to optimize these three objective functions. Finally, we construct a transfer CE framework (TCE-TCES) based on TCES to obtain better clustering results. The experimental results on 12 transfer clustering tasks obtained from the 20newsgroups dataset show that TCE-TCES can find a better tradeoff between quality and diversity, as well as obtaining more desirable clustering results."
plausible counterfactuals: auditing deep learning classifiers with realistic adversarial examples,A. Barredo-Arrieta; J. Del Ser,2020 International Joint Conference on Neural Networks (IJCNN),2020.0,10.1109/IJCNN48605.2020.9206728,Explainable Artificial Intelligence;Deep Learning;Counterfactuals;Generative Adversarial Networks;Multiobjective Optimization;Meta-heuristics,2.0,['Heuristic'],"['Explainable AI','Deep Learning','Counterfactuals','GAN']","['Unlikeliness','Probability of not confusion','Intensity of adversarial changes']",Multi,"The last decade has witnessed the proliferation of Deep Learning models in many applications, achieving unrivaled levels of predictive performance. Unfortunately, the black-box nature of Deep Learning models has posed unanswered questions about what they learn from data. Certain application scenarios have highlighted the importance of assessing the bounds under which Deep Learning models operate, a problem addressed by using assorted approaches aimed at audiences from different domains. However, as the focus of the application is placed more on non-expert users, it results mandatory to provide the means for him/her to trust the model, just like a human gets familiar with a system or process: by understanding the hypothetical circumstances under which it fails. This is indeed the angular stone for this research work: to undertake an adversarial analysis of a Deep Learning model. The proposed framework constructs counterfactual examples by ensuring their plausibility, e.g. there is a reasonable probability that a human could generate them without resorting to a computer program. Therefore, this work must be regarded as valuable auditing exercise of the usable bounds a certain model is constrained within, thereby allowing for a much greater understanding of the capabilities and pitfalls of a model used in a real application. To this end, a Generative Adversarial Network (GAN) and multi-objective heuristics are used to furnish a plausible attack to the audited model, efficiently trading between the confusion of this model, the intensity and plausibility of the generated counterfactual. Its utility is showcased within a human face classification task, unveiling the enormous potential of the proposed framework."
feature selection with dynamic classifier ensembles,H. E. Kiziloz; A. Deniz,"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",2020.0,10.1109/SMC42975.2020.9282969,feature selection;multiobjective optimization;machine learning;classifier ensemble,1.0,"['Evolutionary', 'NSGA-II']","['Feature Selection', 'Ensemble', 'Classification']","['Number of features', 'Accuracy']",Single - Ensemble,"With the advance in technology, the volume of available data grows massively. Therefore, feature selection has become an essential preprocessing step to extract valuable information. Feature selection is the task of reducing the number of features by removing redundant features from data while preserving the classification accuracy. It is a multiobjective problem as there are two objectives. In general, multiobjective selection algorithms with machine learning techniques are utilized to find the most promising feature subsets; however, classification performances of these machine learning techniques are analyzed separately. In this study, we propose a new multiobjective selection model that dynamically searches for the best ensemble of five classifiers to extract the best representative feature subsets. We present the experiment results on 12 well-known datasets. The results show that the proposed method performs significantly better than all the machine learning techniques when they are executed separately. Moreover, the proposed method outperforms two existing ensemble algorithms, namely AdaBoost and Gradient Boosting."
ensemble of classifiers based on multiobjective genetic sampling for imbalanced data,E. R. Q. Fernandes; A. C. P. L. F. de Carvalho; X. Yao,IEEE Transactions on Knowledge and Data Engineering,2020.0,10.1109/TKDE.2019.2898861,Imbalanced datasets;ensemble of classifiers;evolutionary algorithm,18.0,"['Evolutionary', 'NSGA-II']","['Imbalanced Data', 'Ensemble', 'Classification']",['Positive Predictive Value by class'],Single - Ensemble,"Imbalanced datasets may negatively impact the predictive performance of most classical classification algorithms. This problem, commonly found in real-world, is known in machine learning domain as imbalanced learning. Most techniques proposed to deal with imbalanced learning have been proposed and applied only to binary classification. When applied to multiclass tasks, their efficiency usually decreases and negative side effects may appear. This paper addresses these limitations by presenting a novel adaptive approach, E-MOSAIC (Ensemble of Classifiers based on MultiObjective Genetic Sampling for Imbalanced Classification). E-MOSAIC evolves a selection of samples extracted from training dataset, which are treated as individuals of a MOEA. The multiobjective process looks for the best combinations of instances capable of producing classifiers with high predictive accuracy in all classes. E-MOSAIC also incorporates two mechanisms to promote the diversity of these classifiers, which are combined into an ensemble specifically designed for imbalanced learning. Experiments using twenty imbalanced multi-class datasets were carried out. In these experiments, the predictive performance of E-MOSAIC is compared with state-of-the-art methods, including methods based on presampling, active-learning, cost-sensitive, and boosting. According to the experimental results, the proposed method obtained the best predictive performance for the multiclass accuracy measures mAUC and G-mean."
multiobjective fuzzy genetics-based machine learning for multi-label classification,Y. Omozaki; N. Masuyama; Y. Nojima; H. Ishibuchi,2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),2020.0,10.1109/FUZZ48607.2020.9177804,multi-label classification;multiobjective fuzzy genetics-based machine learning;fuzzy rule-based classification system;method adaptation approach,1.0,"['Evolutionary', 'NSGA-II']","['Classification', 'Multi-Label']","['Subset Accuracy', 'Hamming Loss', 'F-Measure', 'Number of rules']",Multi,"In multi-label classification problems, multiple class labels are assigned to each instance. Two approaches have been studied in the literature. One is a data transformation approach, which transforms a multi-label dataset into a number of singlelabel datasets. However, this approach often loses the correlation information among classes in the multi-class assignment. The other is a method adaptation approach where a conventional classification method is extended to multi-label classification. Recently, some explainable classification models for multi-label classification have been proposed. Their high interpretability has also been discussed with respect to the transparency of the classification process. Although the explainability is a well-known advantage of fuzzy systems, their applications to multi-label classification have not been well studied. Since multi-label classification problems often have vague class boundaries, fuzzy systems seem to be a promising approach to multi-label classification. In this paper, we propose a new multiobjective evolutionary fuzzy system, which can be categorized as a method adaptation approach. The proposed algorithm produces nondominated classifiers with different tradeoffs between accuracy and complexity. We examine the behavior of the proposed algorithm using synthetic multi-label datasets. We also compare the proposed algorithm with five representative algorithms. Our experimental results on real-world datasets show that the obtained fuzzy classifiers with a small number of fuzzy rules have high transparency and comparable generalization ability to the other examined multi-label classification algorithms."
multiple reference points-based decomposition for multiobjective feature selection in classification: static and dynamic mechanisms,B. H. Nguyen; B. Xue; P. Andreae; H. Ishibuchi; M. Zhang,IEEE Transactions on Evolutionary Computation,2020.0,10.1109/TEVC.2019.2913831,Classification;feature selection;multiobjective evolutionary algorithm based on decomposition (MOEA/D);multiobjective optimization;partially conflicting,23.0,"['Evolutionary', 'MOEA/D']","['Classification', 'Feature Selection']","['Classification error', 'Number of selected features']",Multi,"Feature selection is an important task in machine learning that has two main objectives: 1) reducing dimensionality and 2) improving learning performance. Feature selection can be considered a multiobjective problem. However, it has its problematic characteristics, such as a highly discontinuous Pareto front, imbalance preferences, and partially conflicting objectives. These characteristics are not easy for existing evolutionary multiobjective optimization (EMO) algorithms. We propose a new decomposition approach with two mechanisms (static and dynamic) based on multiple reference points under the multiobjective evolutionary algorithm based on decomposition (MOEA/D) framework to address the above-mentioned difficulties of feature selection. The static mechanism alleviates the dependence of the decomposition on the Pareto front shape and the effect of the discontinuity. The dynamic one is able to detect regions in which the objectives are mostly conflicting, and allocates more computational resources to the detected regions. In comparison with other EMO algorithms on 12 different classification datasets, the proposed decomposition approach finds more diverse feature subsets with better performance in terms of hypervolume and inverted generational distance. The dynamic mechanism successfully identifies conflicting regions and further improves the approximation quality for the Pareto fronts."
combining a gradient-based method and an evolution strategy for multi-objective reinforcement learning,"Chen D., Wang Y., Gao W.",Applied Intelligence,2020.0,10.1007/s10489-020-01702-7,Multi-objective reinforcement learning; Multi-policy reinforcement learning; Pareto frontier; Sampling efficiency,2.0,"['Evolutionary', 'MO-CMA-ES']","['Reinforcement Learning', 'Multi-policy']",['Task objectives'],Multi,"Multi-objective reinforcement learning (MORL) algorithms aim to approximate the Pareto frontier uniformly in multi-objective decision making problems. In the scenario of deep reinforcement learning (RL), gradient-based methods are often adopted to learn deep policies/value functions due to the fast convergence speed, while pure gradient-based methods can not guarantee a uniformly approximated Pareto frontier. On the other side, evolution strategies straightly manipulate in the solution space to achieve a well-distributed Pareto frontier, but applying evolution strategies to optimize deep networks is still a challenging topic. To leverage the advantages of both kinds of methods, we propose a two-stage MORL framework combining a gradient-based method and an evolution strategy. First, an efficient multi-policy soft actor-critic algorithm is proposed to learn multiple policies collaboratively. The lower layers of all policy networks are shared. The first-stage learning can be regarded as representation learning. Secondly, the multi-objective covariance matrix adaptation evolution strategy (MO-CMA-ES) is applied to fine-tune policy-independent parameters to approach a dense and uniform estimation of the Pareto frontier. Experimental results on three benchmarks (Deep Sea Treasure, Adaptive Streaming, and Super Mario Bros) show the superiority of the proposed method. © 2020, Springer Science+Business Media, LLC, part of Springer Nature."
training feedforward neural network via multiobjective optimization model using non-smooth l-1/2 regularization,"Senhaji, K; Ramchoun, H; Ettaouil, M",NEUROCOMPUTING,2020.0,10.1016/j.neucom.2020.05.066,Multiobjective optimization; NSGAII; Learning algorithm; L-1/2 regularization; Neural network,,"['Evolutionary','NSGA-II']",['Multilayer Perceptron Neural Network'],"['MSE','Modified L1/2regularizer']",Multi,"The paper presents a new approach to optimize the Multilayer Perceptron Neural Network (MLPNN), to deal with the generalization problem. As known, most supervised learning algorithms aim to minimize the training error. However, the mentioned methods, based only on error minimizing, may generate a solution with an insufficient generalization performance. This present work proposes a multiobjective modelling problem involving two objectives: accuracy and complexity since the learning problem is multiobjective by nature. The learning task is carried on by minimizing both objectives simultaneously, according to Pareto domination concept, using NSGAII (Non-dominated Sorting Genetic Algorithm II) as a solver. This method leads us to a set of solutions called Pareto front, being the optimal solutions set, the adequate MLPNN need to be extracted. We show empirically that the proposed method is capable of reducing the neural networks topology and improved generalization performance, in addition to a good classification rate compared to different methods. (C) 2020 Published by Elsevier B.V."
pareto multi-task deep learning,"Riccio, SD; Dyankov, D; Jansen, G; Di Fatta, G; Nicosia, G","ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING, ICANN 2020, PT II",2020.0,10.1007/978-3-030-61616-8_11,Multi-task learning; Multi-objective learning; Deep Neuroevolution; Hypervolume; Kullback-Leibler divergence; Pareto front; Evolution strategy; Atari 2600 games,,"['Evolutionary', 'Proposed', 'MTMO-ES']",['Multi-Task Learning'],['Utility functions for the multi-task learning'],Multi,"Neuroevolution has been used to train Deep Neural Networks on reinforcement learning problems. A few attempts have been made to extend it to address either multi-task or multi-objective optimization problems. This research work presents the Multi-Task Multi-Objective Deep Neuroevolution method, a highly parallelizable algorithm that can be adopted for tackling both multi-task and multi-objective problems. In this method prior knowledge on the tasks is used to explicitly define multiple utility functions, which are optimized simultaneously. Experimental results on some Atari 2600 games, a challenging testbed for deep reinforcement learning algorithms, show that a single neural network with a single set of parameters can outperform previous state of the art techniques. In addition to the standard analysis, all results are also evaluated using the Hypervolume indicator and the Kallback-Leibler divergence to get better insights on the underlying training dynamics. The experimental results show that a neural network trained with the proposed evolution strategy can outperform networks individually trained respectively on each of the tasks."
identifying pareto-based solutions for regression subset selection via a feasible solution algorithm,"Lambert, JW; Hawk, GS",INTERNATIONAL JOURNAL OF DATA SCIENCE AND ANALYTICS,2020.0,10.1007/s41060-020-00218-0,Pareto; Optimal; Feasible solution; Multiple; Objective; Subset selection; Regression,,"['Evolutionary', 'FSA']","['Regression', 'Subset Selection']","['Residual', 'R squared']",Multi,"The concept of Pareto optimality has been utilized in fields such as engineering and economics to understand fluid dynamics and consumer behavior. In machine learning contexts, Pareto-optimality has been used to identify tuning parameters that best optimize a set ofmcriteria (multi-objective optimization). During the process of regression model selection, data scientists are often concerned with choosing a model which has the best single criterion (e.g., Akaike information criterion (AIC) orR-squared (R-2)) before continuing to check a number of other regression model characteristics (e.g., model size, form, diagnostics, and interpretability). This strategy is multi-objective in nature but single objective in its numeric execution. This paper will first introduce a feasible solution algorithm (FSA) and explain how it can be applied to multi-objective problems for regression subset selection. Then we introduce the general framework of Pareto optimality within the regression setting. We then apply the algorithm in a simulation setting where we seek to estimate the first four Pareto boundaries for regression models using two model fit criteria. Finally, we present an application where we use a US communities and crime dataset."
high dimensional restrictive federated model selection with multi-objective bayesian optimization over shifted distributions,"Sun, XD; Bommert, A; Pfisterer, F; Rahenfurher, J; Lang, M; Bischl, B","INTELLIGENT SYSTEMS AND APPLICATIONS, VOL 1",2020.0,10.1007/978-3-030-29516-5_48,Federated learning; Multi-objective Bayesian Optimization; High dimensional data; Differential privacy; Distribution shift; Model selection,,"['Bayesian Optimization', 'Parego algorithm']","['Model Selection', 'Federated Learning']","['Local loss', 'Remote loss']",Single - Dominated Hypervolume Indicator,"A novel machine learning optimization process coined Restrictive Federated Model Selection (RFMS) is proposed under the scenario, for example, when data from healthcare units can not leave the site it is situated on and it is forbidden to carry out training algorithms on remote data sites due to either technical or privacy and trust concerns. To carry out a clinical research in this scenario, an analyst could train a machine learning model only on local data site, but it is still possible to execute a statistical query at a certain cost in the form of sending a machine learning model to some of the remote data sites and get the performance measures as feedback, maybe due to prediction being usually much cheaper. Compared to federated learning, which is optimizing the model parameters directly by carrying out training across all data sites, RFMS trains model parameters only on one local data site but optimizes hyper parameters across other data sites jointly since hyper-parameters play an important role in machine learning performance. The aim is to get a Pareto optimal model with respective to both local and remote unseen prediction losses, which could generalize well across data sites. In this work, we specifically consider high dimensional data with different distributions over data sites. As an initial investigation, Bayesian Optimization especially multi-objective Bayesian Optimization is used to guide an adaptive hyper-parameter optimization process to select models under the RFMS scenario. Empirical results shows that solely using the local data site to tune hyper-parameters generalizes poorly across data sites, compared to methods that utilize the local and remote performances. Furthermore, in terms of hypervolumes, multi-objective Bayesian Optimization algorithms show increased performance across multiple data sites among other candidates."
a two-stage multi-objective deep reinforcement learning framework,"Chen D., Wang Y., Gao W.",Frontiers in Artificial Intelligence and Applications,2020.0,10.3233/FAIA200202,,1.0,"['Evolutionary', 'MO-CMA-ES']",['Reinforcement Learning'],['Soft Actor-Critic (SAC) objective functions'],Multi,"In multi-objective decision making problems, multi-objective reinforcement learning (MORL) algorithms aim to approximate the Pareto frontier uniformly. A naive approach is to learn multiple policies by repeatedly running a single-objective reinforcement learning (RL) algorithm on scalarized rewards. The scalarization methods denote the preferences of objectives, which are different in each run. However, in this way, the model representation and computation are redundant. Furthermore, uniform preferences can not guarantee a uniformly approximated Pareto frontier. To address these problems and leverage the expressive power of deep neural networks, we propose a two-stage MORL framework integrating a multi-policy deep RL algorithm and an evolution strategy algorithm. Firstly, a multi-policy soft actor-critic algorithm is proposed to collaboratively learn multiple policies which are assigned with different scalarization weights. The lower layers of all policy networks are shared. The first-stage learning can be regarded as representation learning. Secondly, the multi-objective covariance matrix adaptation evolution strategy (MO-CMA-ES) is applied to fine-tune policy-independent parameters to approach a dense and uniform estimation of the Pareto frontier. Experimental results on two benchmarks (Deep Sea Treasure and Adaptive Streaming) show the superiority of the proposed method. © 2020 The authors and IOS Press."
learning adversarial attack policies through multi-objective reinforcement learning,"García J., Majadas R., Fernández F.",Engineering Applications of Artificial Intelligence,2020.0,10.1016/j.engappai.2020.104021,Adversarial reinforcement learning; Multi-objective reinforcement learning,6.0,"['Evolutionary','Reinforcement Learning']","['Reinforcement Learning','Adversarial attacks']","['Performance loss of the attacked policy','Cost of the attacks']",Single - Weighted optimization,"Deep Reinforcement Learning has shown promising results in learning policies for complex sequential decision-making tasks. However, different adversarial attack strategies have revealed the weakness of these policies to perturbations to their observations. Most of these attacks have been built on existing adversarial example crafting techniques used to fool classifiers, where an adversarial attack is considered a success if it makes the classifier outputs any wrong class. The major drawback of these approaches when applied to decision-making tasks is that they are blind for long-term goals. In contrast, this paper suggests that it is more appropriate to view the attack process as a sequential optimization problem, with the aim of learning a sequence of attacks, where the attacker must consider the long-term effects of each attack. In this paper, we propose that such an attack policy must be learned with two objectives in view. On the one hand, the attack must pursue the maximum performance loss of the attacked policy. On the other hand, it also should minimize the cost of the attacks. Therefore, in this paper we propose a novel modelization of the process of learning an attack policy as a Multi-objective Markov Decision Process with two objectives: maximizing the performance loss of the attacked policy and minimizing the cost of the attacks. We also reveal the conflicting nature of these two objectives and use a Multi-objective Reinforcement Learning algorithm to draw the Pareto fronts for four well-known tasks: the GridWorld, the Cartpole, the Mountain car and the Breakout. © 2020 Elsevier Ltd"
ensemble learning via multimodal multiobjective differential evolution and feature selection,"Wang J., Wang B., Liang J., Yu K., Yue C., Ren X.",Communications in Computer and Information Science,2020.0,10.1007/978-981-15-3425-6_34,Classifier parameter; Ensemble learning; Feature selection; Multimodal multiobjective optimization,,"['Evolutionary', 'MMODE']","['Ensemble Learning', 'Feature Selection']","['Feature selection rate', 'Error rate']",Single - Ensemble,"Ensemble learning is an important element in machine learning. However, two essential tasks, including training base classifiers and finding a suitable ensemble balance for the diversity and accuracy of these base classifiers, are need to be achieved. In this paper, a novel ensemble method, which utilizes a multimodal multiobjective differential evolution (MMODE) algorithm to select feature subsets and optimize base classifiers parameters, is proposed. Moreover, three methods including minimum error ensemble, all Pareto sets ensemble, and error reduction ensemble are employed to construct ensemble classifiers for executing classification tasks. Experimental results on several benchmark classification databases evidence that the proposed algorithm is valid. © 2020, Springer Nature Singapore Pte Ltd."
feature selection using pso: a multi objective approach,"Vashishtha J., Puri V.H., Mukesh",Communications in Computer and Information Science,2020.0,10.1007/978-981-15-6318-8_10,Feature selection; Multi-objective optimization; PSO,,"['Evolutionary', 'Particle Swarm Optimization']","['Feature Selection', 'Classification']","['Number of features', 'Classification error']",Multi,"Feature selection is a pre-processing technique in which a subset or a small number of features, which are relevant and non-redundant, are selected for better classification performance. Multi-objective optimization is applied in the fields where finest decisions need to be taken in presence of trade-offs between two or more differing objectives. Therefore, feature selection is considered as a multi-objective problem with conflicting measures like classification error rate and feature reduction rate. The existing algorithms, Non-dominated Sorting based particle swarm optimization for Feature Selection (NSPSOFS) and Crowding Mutation Dominance based particle swarm optimization for Feature Selection (CMDPSOFS) are the two multi-objective PSO algorithms for feature selection. This work presents the enhanced form of NSPSOFS and CMDPSOFS. A novel selection mechanism for gbest is incorporated and hybrid mutation is also added to the algorithms in order to generate a better pareto optimal front of non-dominated solutions. The experimental results show that the proposed algorithm generates non-dominated solutions and produce better result than existing algorithms. © 2020, Springer Nature Singapore Pte Ltd."
automated feature engineering for algorithmic fairness,"Salazar, Ricardo and Neutatz, Felix and Abedjan, Ziawasch",VLDB Endowment,2021.0,10.14778/3461535.3463474,,,"['Evolutionary', 'NSGA-II']","['Feature Selection', 'Feature Construction', 'Fairness']","['Ratio of Observational Discrimination', 'F1 score']",Multi,"One of the fundamental problems of machine ethics is to avoid the perpetuation and amplification of discrimination through machine learning applications. In particular, it is desired to exclude the influence of attributes with sensitive information, such as gender or race, and other causally related attributes on the machine learning task. The state-of-the-art bias reduction algorithm Capuchin breaks the causality chain of such attributes by adding and removing tuples. However, this horizontal approach can be considered invasive because it changes the data distribution. A vertical approach would be to prune sensitive features entirely. While this would ensure fairness without tampering with the data, it could also hurt the machine learning accuracy. Therefore, we propose a novel multi-objective feature selection strategy that leverages feature construction to generate more features that lead to both high accuracy and fairness. On three well-known datasets, our system achieves higher accuracy than other fairness-aware approaches while maintaining similar or higher fairness."
automl technologies for the identification of sparse models,"Liuliakov A., Hammer B.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021.0,10.1007/978-3-030-91608-4_7,AutoML; Feature selection; TPOT,,['Greedy search'],"['AutoML', 'Feature Selection']","['Accuracy', 'Number of selected features']",Multi,"Automated machine learning (AutoML) technologies constitute promising tools to automatically infer model architecture, meta-parameters or processing pipelines for specific machine learning tasks given suitable training data. At present, the main objective of such technologies typically relies on the accuracy of the resulting model. Additional objectives such as sparsity can be integrated by pre-processing steps or according penalty terms in the objective function. Yet, sparsity and model accuracy are often contradictory goals, and optimum solutions form a Pareto front. Thereby, it is not guaranteed that solutions at different positions of the Pareto front share the same architectural choices, hence current AutoML technologies might yield sub-optimal results. In this contribution, we propose a novel method, based on the AutoML method TPOT, which enables an automated optimization of ML pipelines with sparse input features along the whole Pareto front. We demonstrate that, indeed, different architectures are found at different points of the Pareto front for benchmark examples from the domain of systems security. © 2021, Springer Nature Switzerland AG."
"feature weighting for na\""{\i}ve bayes using multi objective artificial bee colony algorithm","Chaudhuri, Abhilasha and Sahu, Tirath Prasad",Inderscience Publishers,2021.0,10.1504/ijcse.2021.113655,"na\""{\i}ve Bayes, multi objective optimisation, artificial bee colony, feature weighting",,"['Evolutive','Artificial bee colony']","['Classification','Naïve Bayes','Feature Weighting']","['Relevancy','Redundancy']",Single - Maximum fitness value,"Na\""{\i}ve Bayes (NB) is a widely used classifier in the field of machine learning. However, its conditional independence assumption does not hold true in real-world applications. In literature, various feature weighting approaches have attempted to alleviate this assumption. Almost all of these approaches consider the relationship between feature-class (relevancy) and feature-feature (redundancy) independently, to determine the weights of features. We argue that these two relationships are mutually dependent and both cannot be improved simultaneously, i.e., form a trade-off. This paper proposes a new paradigm to determine the feature weight by formulating it as a multi-objective optimisation problem to balance the trade-off between relevancy and redundancy. Multi-objective artificial bee colony-based feature weighting technique for na\""{\i}ve Bayes (MOABC-FWNB) is proposed. An extensive experimental study was conducted on 20 benchmark UCI datasets. Experimental results show that MOABC-FWNB outperforms NB and other existing state-of-the-art feature weighting techniques."
minimax group fairness: algorithms and experiments,"Diana E., Gill W., Kearns M., Kenthapadi K., Roth A.","AIES 2021 - Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",2021.0,10.1145/3461702.3462523,fair machine learning; game theory; minimax fairness,3.0,"['Evolutive', 'Zero-sum game']","['Fairness', 'Classification', 'Regression']",['Group error'],Single - Minimax,"We consider a recently introduced framework in which fairness is measured by worst-case outcomes across groups, rather than by the more standard differences between group outcomes. In this framework we provide provably convergent oracle-efficient learning algorithms (or equivalently, reductions to non-fair learning) for minimax group fairness. Here the goal is that of minimizing the maximum loss across all groups, rather than equalizing group losses. Our algorithms apply to both regression and classification settings and support both overall error and false positive or false negative rates as the fairness measure of interest. They also support relaxations of the fairness constraints, thus permitting study of the tradeoff between overall accuracy and minimax fairness. We compare the experimental behavior and performance of our algorithms across a variety of fairness-sensitive data sets and show empirical cases in which minimax fairness is strictly and strongly preferable to equal outcome notions. © 2021 ACM."
exploring multiobjective training in multiclass classification,"Raimundo M.M., Drumond T.F., Marques A.C.R., Lyra C., Rocha A., Von Zuben F.J.",Neurocomputing,2021.0,10.1016/j.neucom.2020.12.087,Diversity of Pareto-optimal models; Ensemble learning; Multiclass classification; Multiobjective optimization,1.0,"['Deterministic', 'NISE']","['Classification', 'Multiclass']","['Classification loss', 'L2']",Single - Ensemble,"Multinomial logistic loss and L2 regularization are often conflicting objectives as more robust regularization leads to restrained multinomial parameters. For many practical problems, leveraging the best of both worlds would be invaluable for better decision-making processes. This research proposes a novel framework to obtain representative and diverse L2-regularized multinomial models, based on valuable trade-offs between prediction error and model complexity. The framework relies upon the Non-Inferior Set Estimation (NISE) method – a deterministic multiobjective solver. NISE automatically implements hyperparameter tuning in a multiobjective context. Given the diverse set of efficient learning models, model selection and aggregation of the multiple models in an ensemble framework promote high performance in multiclass classification. Additionally, NISE uses the weighted sum method as scalarization, thus being able to deal with the learning formulation directly. Its deterministic nature and the convexity of the learning problem confer scalability to the proposal. The experiments show competitive performance in various setups, taking a broad set of multiclass classification methods as contenders. © 2021 Elsevier B.V."
the multi-task learning with an application of pareto improvement,"Cai T., Gao X., Song L., Liao M.",ACM International Conference Proceeding Series,2021.0,10.1145/3448734.3450463,multi-objective optimization; Multi-task learning; Pareto improvement,,"['Deterministic', 'MGDA']",['Multi-task Learning'],['Loss of each class'],Multi,"Multi-task learning is a promising field in machine learning, which aims to improve the performance of multiple related learning tasks by taking advantage of useful information between them. Multi-task learning is essentially equivalent to multi-objective optimization problem, the purpose is to find the most appropriate weight, and because the performance of many deep learning systems based on multi-task learning largely depends on the relative weight of each task loss. It's a problem that we need to study how to calculate the weight value under some constraint conditions by reasonable method. Therefore, this paper employs a powerful method based on convex optimization theory, whose purpose is to find the Pareto optimal solution and get the specific task loss weight. The optimization process is closely related to the gradient in deep learning. In addition, to improve the accuracy, we add the modules of gradient normalization and weight standardization. The experimental results show that the performance of our method is better than that of single task experiment or multi-task experiment under fixed weight, and multi-task experiment based on uncertainty based adaptive learning, and the accuracy is further improved after adding the above modules. © 2021 ACM."
multi-task learning with riemannian optimization,"Cai T., Song L., Li G., Liao M.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021.0,10.1007/978-3-030-84529-2_42,Multi-objective optimization; Multi-task learning; Riemannian optimization,,"['Deterministic', 'MGDA']",['Multi-task Learning'],['Loss of each class'],Multi,"Multi-task learning (MTL) is a promising research field of machine learning, in which the training process of the neural network is equivalent to multi-objective optimization. On one hand, MTL trains all the network weights simultaneously to converge the multi-task loss. On the other hand, multi-objective optimization aims to find the optimum solution, which satisfies the constraints and optimizes the vector of objective functions. Therefore, the performance of MTL is dominated by the computation of the multi-objective solution. This paper proposes a method based on Riemannian optimization to solve the multi-objective optimization in MTL. Firstly, multi-objective optimization is reduced to its Karush-Kuhn-Tucker (KKT) condition as the optimum solution of constrained quadratic optimization. Secondly, by mapping the Euclidean space of the constraint into manifold, the quadratic optimization is transformed to an unconstrained problem. Finally, Riemannian optimization algorithm is used to compute the solution of this problem, which gives a Pareto direction towards the KKT condition. We perform experiments on the MultiMNIST and Fashion MNIST datasets, and the experimental results demonstrate the efficiency of our method. © 2021, Springer Nature Switzerland AG."
training-free multi-objective evolutionary neural architecture search via neural tangent kernel and number of linear regions,"Do T., Luong N.H.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021.0,10.1007/978-3-030-92270-2_29,Deep learning; Evolutionary computation; Multi-objective optimization; Neural architecture search; Neural tangent kernels,,"['Evolutionary', 'NSGA-II']","['Neural Architecture Search', 'Neural Network']","['Few floating-point operations (FLOPs)', 'Number of linear regions', 'Condition number of neural tangent kernel']",Multi,"A newly introduced training-free neural architecture search (TE-NAS) framework suggests that candidate network architectures can be ranked via a combined metric of expressivity and trainability. Expressivity is measured by the number of linear regions in the input space that can be divided by a network. Trainability is assessed based on the condition number of the neural tangent kernel (NTK), which affects the convergence rate of training a network with gradient descent. These two measurements have been found to be correlated with network test accuracy. High-performance architectures can thus be searched for without incurring the intensive cost of network training as in a typical NAS run. In this paper, we suggest that TE-NAS can be incorporated with a multi-objective evolutionary algorithm (MOEA), in which expressivity and trainability are kept separate as two different objectives rather than being combined. We also add the minimization of floating-point operations (FLOPs) as the third objective to be optimized simultaneously. On NAS-Bench-101 and NAS-Bench-201 benchmarks, our approach achieves excellent efficiency in finding Pareto fronts of a wide range of architectures exhibiting optimal trade-offs among network expressivity, trainability, and complexity. Network architectures obtained by our approach on CIFAR-10 also show high transferability on CIFAR-100 and ImageNet. © 2021, Springer Nature Switzerland AG."
multi-objective search of robust neural architectures against multiple types of adversarial attacks,"Liu, Jia and Jin, Yaochu",Elsevier Science Publishers B. V.,2021.0,10.1016/j.neucom.2021.04.111,"Adversarial attacks, Robustness, Multi-objective evolutionary algorithm, Neural architecture search",,"['Evolutionary', 'NSGA-II']","['Neural Architecture Search', 'Adversarial attacks', 'Robustness', 'Neural Network']","['Error on clean data', 'Error on adversarial data']",Multi,
facing many objectives for fairness in machine learning,"Villar D., Casillas J.",Communications in Computer and Information Science,2021.0,10.1007/978-3-030-85347-1_27,Decision trees; Fairness in machine learning; Many objective evolutionary algorithm,,"['Evolutionary', 'NSGA-II', 'SMS-EMOA', 'GrEA']","['Fairness', 'Decision tree', 'Classification']","['Accuracy', 'Difference between False Positive Rate', 'Difference between Positive Predictive Val-ues']",Multi,"Fairness is an increasingly important topic in the world of Artificial Intelligence. Machine learning techniques are widely used nowadays to solve huge amounts of problems, but those techniques may be biased against certain social groups due to different reasons. Using fair classification methods we can attenuate this discrimination source. Nevertheless, there are lots of valid fairness definitions which may be mutually incompatible. The aim of this paper is to propose a method which generates fair solutions for machine learning binary classification problems with one sensitive attribute. As we want accurate, fair and interpretable solutions, our method is based on Many Objective Evolutionary Algorithms (MaOEAs). The decision space will represent hyperparameters for training our classifiers, which will be decision trees, while the objective space will be a four-dimensional space representing the quality of the classifier in terms of an accuracy measure, two contradictory fairness criteria and an interpretability indicator. Experimentation have been done using four well known fairness datasets. As we will see, our algorithm generates good solutions compared to previous work, and a presumably well populated pareto-optimal population is found so that different classifiers could be used depending on our needs. © 2021, Springer Nature Switzerland AG."
a robust multiobjective harris' hawks optimization algorithm for the binary classification problem,"Dokeroglu, T; Deniz, A; Kiziloz, HE",KNOWLEDGE-BASED SYSTEMS,2021.0,10.1016/j.knosys.2021.107219,Binary classification; Multiobjective optimization; Feature selection; Harris' Hawks optimization,,"['Evolutionary','Harris’ Hawks optimization']","['Feature Selection','Classification','Logistic Regression','Support Vector Machine','Extreme Learning Machines','Decision Trees']","['Number of features','Performance']",Multi,"The Harris' Hawks Optimization (HHO) is a recent metaheuristic inspired by the cooperative behavior of the hawks. These avians apply many intelligent techniques like surprise pounce (seven kills) while they are catching their prey according to the escaping patterns of the target. The HHO simulates these hunting patterns of the hawks to obtain the best/optimal solutions to the problems. In this study, we propose a new multiobjective HHO algorithm for the solution of the well-known binary classification problem. In this multiobjective problem, we reduce the number of selected features and try to keep the accuracy prediction as maximum as possible at the same time. We propose new discrete exploration (perching) and exploitation (besiege) operators for the hunting patterns of the hawks. We calculate the prediction accuracy of the selected features with four machine learning techniques, namely, Logistic Regression, Support Vector Machines, Extreme Learning Machines, and Decision Trees. To verify the performance of the proposed algorithm, we conduct comprehensive experiments on many benchmark datasets retrieved from the University of California, Irvine (UCI) Machine Learning Repository. Moreover, we apply it to a recent real-world dataset, i.e., a Coronavirus disease (COVID-19) dataset. Significant improvements are observed during the comparisons with state-of-the-art metaheuristic algorithms. (C) 2021 Elsevier B.V. All rights reserved."
learning with privileged tasks,Y. Song; Z. Lou; S. You; E. Yang; F. Wang; C. Qian; C. Zhang; X. Wang,2021 IEEE/CVF International Conference on Computer Vision (ICCV),2021.0,10.1109/ICCV48922.2021.01051,Machine learning architectures and formulations; Recognition and classification,,"['Deterministic','Gradient','P-MGDA']","['Multi-task learning','Privileged tasks']",['Task losses'],Multi,"Multi-objective multi-task learning aims to boost the performance of all tasks by leveraging their correlation and conflict appropriately. Nevertheless, in real practice, users may have preference for certain tasks, and other tasks simply serve as privileged or auxiliary tasks to assist the training of target tasks. The privileged tasks thus possess less or even no priority in the final task assessment by users. Motivated by this, we propose a privileged multiple descent algorithm to arbitrate the learning of target tasks and privileged tasks. Concretely, we introduce a privileged parameter so that the optimization direction does not necessarily follow the gradient from the privileged tasks, but concentrates more on the target tasks. Besides, we also encourage a priority parameter for the target tasks to control the potential distraction of optimization direction from the privileged tasks. In this way, the optimization direction can be more aggressively determined by weighting the gradients among target and privileged tasks, and thus highlight more the performance of target tasks under the unified multi-task learning context. Extensive experiments on synthetic and real-world datasets indicate that our method can achieve versatile Pareto solutions under varying preference for the target tasks."
the trade-off between privacy and utility in local differential privacy,M. Li; Y. Tian; J. Zhang; D. Fan; D. Zhao,2021 International Conference on Networking and Network Applications (NaNA),2021.0,10.1109/NaNA53684.2021.00071,data collecting;local differential privacy;privacy metric;utility metric;Pareto optimality,,"['Evolutionary', 'NSGA-II']","['Privacy', 'Local differential privacy']","['Privacy Loss', 'Utility Loss']",Single - Payoff function,"In statistical queries work, such as frequency estimation, the untrusted data collector could as an honest-but-curious (HbC) or malicious adversary to learn true values. Local differential privacy(LDP) protocols have been applied against the untrusted third party in data collecting. Nevertheless, excessive noise of LDP will reduce data utility, thus affecting the results of statistical queries. Therefore, it is significant to research the trade-off between privacy and utility. In this paper, we first measure the privacy loss by observing the maximum posterior confidence of the adversary (data collector). Then, through theoretical analysis and comparison we obtain the most suitable utility measure that is Wasserstein distance. Based on these, we introduce an originality framework for privacy-utility tradeoff framework, finding that this system conforms to the Pareto optimality state and formalizing a payoff function to find optimal equilibrium point under Pareto efficiency. Finally, we illustrate the efficacy of our system model by the Adult dataset from the UCI machine learning repository."
synthesizing pareto-optimal interpretations for black-box models,H. Torfah; S. Shah; S. Chakraborty; S. Akshay; S. A. Seshia,2021 Formal Methods in Computer Aided Design (FMCAD),2021.0,10.34727/2021/isbn.978-3-85448-046-4_24,,,"['Deterministic', 'quantitative constraint solver', 'MaxSAT']",['Explainability'],"['Explainability measure', 'Correctness measure']",Multi,"We present a new multi-objective optimization approach for synthesizing interpretations that “explain” the behavior of black-box machine learning models. Constructing human-understandable interpretations for black-box models often requires balancing conflicting objectives. A simple interpretation may be easier to understand for humans while being less precise in its predictions vis-a-vis a complex interpretation. Existing methods for synthesizing interpretations use a single objective function and are often optimized for a single class of interpretations. In contrast, we provide a more general and multi-objective synthesis framework that allows users to choose (1) the class of syntactic templates from which an interpretation should be synthesized, and (2) quantitative measures on both the correctness and explainability of an interpretation. For a given black-box, our approach yields a set of Pareto-optimal interpretations with respect to the correctness and explainability measures. We show that the underlying multi-objective optimization problem can be solved via a reduction to quantitative constraint solving, such as weighted maximum satisfiability. To demonstrate the benefits of our approach, we have applied it to synthesize interpretations for black-box neural-network classifiers. Our experiments show that there often exists a rich and varied set of choices for interpretations that are missed by existing approaches."
pareto self-supervised training for few-shot learning,Z. Chen; J. Ge; H. Zhan; S. Huang; D. Wang,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2021.0,10.1109/CVPR46437.2021.01345,,2.0,"['Deterministic', 'Constrained multi-objective subproblems', 'Gradient-based', 'steepest descent algorithm']","['Few-Shot Learning', 'Self-Supervised Learning']",['Loss for each task'],Multi,"While few-shot learning (FSL) aims for rapid generalization to new concepts with little supervision, self-supervised learning (SSL) constructs supervisory signals directly computed from unlabeled data. Exploiting the complementarity of these two manners, few-shot auxiliary learning has recently drawn much attention to deal with few labeled data. Previous works benefit from sharing inductive bias between the main task (FSL) and auxiliary tasks (SSL), where the shared parameters of tasks are optimized by minimizing a linear combination of task losses. However, it is challenging to select a proper weight to balance tasks and reduce task conflict. To handle the problem as a whole, we propose a novel approach named as Pareto self-supervised training (PSST) for FSL. PSST explicitly decomposes the few-shot auxiliary problem into multiple constrained multi-objective subproblems with different trade-off preferences, and here a preference region in which the main task achieves the best performance is identified. Then, an effective preferred Pareto exploration is proposed to find a set of optimal solutions in such a preference region. Extensive experiments on several public benchmark datasets validate the effectiveness of our approach by achieving state-of-the-art performance."
smart multi-objective evolutionary gan,M. Baioletti; G. D. Bari; V. Poggioni; C. A. C. Coello,2021 IEEE Congress on Evolutionary Computation (CEC),2021.0,10.1109/CEC45853.2021.9504858,,,['Evolutionary'],['Generative Adversarial Networ'],"['Jensen-Shannon divergence', 'probability of the discriminator being mistaken', 'Least square']",Multi,"Generative Adversarial Network (GAN) is a family of machine learning algorithms designed to train neural networks able to imitate real data distributions. Unfortunately, GAN suffers from problems such as gradient vanishing and mode collapse. In Multi-Objective Evolutionary Generative Adversarial Network (MO-EGAN) these problems were addressed using an evolutionary technique combined with Multi-Objective selection, obtaining better results on synthetic datasets at the expense of larger computation times. In this works, we present the Smart MultiObjective Evolutionary Generative Adversarial Network (SMO-EGAN) algorithm, which reduces the computational cost of MO-EGAN and achieves better results on real data distributions."
hyperaspo: fusion of model and hyper parameter optimization for multi-objective machine learning,A. Kannan; A. Roy Choudhury; V. Saxena; S. Raje; P. Ram; A. Verma; Y. Sabharwal,2021 IEEE International Conference on Big Data (Big Data),2021.0,10.1109/BigData52589.2021.9671604,Hyperparameter optimization;Model parameters;XGBoost;HyperASPO;Pareto Optimization,,"['Deterministic', 'Gradient based', 'Adaptively Scalarized']",['Hyper Parameter Optimization'],['Chosen given the task'],Multi,"Current state of the art methods for generating Pareto-optimal solutions for multi-objective optimization problems mostly rely on optimizing the hyper-parameters of the models (HPO - hyper-parameter Optimization). Few recent, less studied methods focus on optimizing over the space of model parameters, leveraging the problem specific knowledge. We present a generic first-of-a-kind method, referred to as HyperASPO, that combines optimization over the spaces of both hyper-parameters and model parameters for multi-objective optimization of learning problems. HyperASPO consists of two stages. First, we perform a coarse HPO to determine a set of favorable hyper-parameter configurations. In the second step, for each of these configurations, we solve a sequence of weighted single objective optimization problems for estimating Pareto-optimal solutions. We generate the weights in the second step using an adaptive mesh constructed iteratively based on the metrics of interest, resulting in further refinement of Pareto frontier efficiently. We consider the widely used XGBoost (Gradient Boosted Trees) model and validate our method on multiple classification datasets. Our proposed method shows up to 20% improvement over the hypervolumes of Pareto fronts obtained through state of the art HPO based methods with up to 2× reduction in computational time."
scalable pareto front approximation for deep multi-objective learning,M. Ruchte; J. Grabocka,2021 IEEE International Conference on Data Mining (ICDM),2021.0,10.1109/ICDM51629.2021.00162,Multi-objective optimization;Deep Learning;Fairness,,"['Deterministic', 'Linear scalarization', 'Gradient based']",['Deep Learning'],['Loss for each task'],Multi,"Multi-objective optimization is important for various Deep Learning applications, however, no prior multi-objective method suits very deep networks. Existing approaches either require training a new network for every solution on the Pareto front or add a considerable overhead to the number of parameters by introducing hyper-networks conditioned on modifiable preferences. In this paper, we present a novel method that contextualizes the network directly on the preferences by adding them to the input space. In addition, we ensure a well-spread Pareto front by forcing the solutions to preserve a small angle to the preference vector. Through extensive experiments, we demonstrate that our Pareto fronts achieve state-of-the-art quality despite being computed significantly faster. Furthermore, we demonstrate the scalability as our method approximates the full Pareto front on the CelebA dataset with an EfficientNet network at a marginal training time overhead of 7% compared to a single-objective optimization. We make the code publicly available at https://github.com/ruchtem/cosmos."
multiobjective particle swarm optimization for feature selection with fuzzy cost,Y. Hu; Y. Zhang; D. Gong,IEEE Transactions on Cybernetics,2021.0,10.1109/TCYB.2020.3015756,Feature selection (FS);fuzzy cost;multiobjective optimization;particle swarm optimization (PSO),37.0,"['Evolutionary', 'Particle Swarm Optimization']","['Feature Selection', 'Fuzzy Cost']","['Average error rate', 'Maximal fuzzy cost']",Multi,"Feature selection (FS) is an important data processing technique in the field of machine learning. There have been various FS methods, but all assume that the cost associated with a feature is precise, which restricts their real applications. Focusing on the FS problem with fuzzy cost, a fuzzy multiobjective FS method with particle swarm optimization, called PSOMOFS, is studied in this article. The proposed method develops a fuzzy dominance relationship to compare the goodness of candidate particles and defines a fuzzy crowding distance measure to prune the elitist archive and determine the global leader of particles. Also, a tolerance coefficient is introduced into the proposed method to ensure that the Pareto-optimal solutions obtained satisfy decision makers' preferences. The developed method is used to tackle a series of the UCI datasets and is compared with three fuzzy multiobjective evolutionary methods and three typical multiobjective FS methods. Experimental results show that the proposed method can achieve feature sets with superior performances in approximation, diversity, and feature cost."
an entropy driven multiobjective particle swarm optimization algorithm for feature selection,J. Luo; D. Zhou; L. Jiang; H. Ma,2021 IEEE Congress on Evolutionary Computation (CEC),2021.0,10.1109/CEC45853.2021.9504837,feature selection;multiobjective optimization;particle swarm optimization,1.0,"['Evolutionary', 'Particle Swarm Optimization']","['Feature Selection', 'Classification']","['Number of features', 'Classification error']",Multi,"Feature selection is an important research field in machine learning since high-dimensionality is a common characteristic of real-world data. It has two main objectives, which are to maximize the classification accuracy while minimizing the number of selected features. As the two objectives are usually in conflict with each other, it makes feature selection a multi-objective problem. However, the large search space and discrete Pareto front makes it not easy for existing evolutionary multi-objective algorithms. In order to deal with the above mentioned difficulties in feature selection, an entropy driven multiobjective particle swarm optimization algorithm is proposed to remove redundant feature and decrease computational complexity. First, its basic idea is to model feature selection as a multiobjective optimization problem by optimizing the number of features and the classification accuracy in supervised condition simultaneously. Second, a particle initialization strategy based on information entropy is designed to improve the quality of initial solutions, and an adaptive velocity update rule is used to swap between local search and global search. Besides, a specified discrete nondominated sorting is designed. These strategies enable the proposed algorithm to gain better performance on both the quality and size of feature subset. The experimental results show that the proposed algorithm can maintain or improve the quality of Pareto fronts evolved by the state-of-the-art algorithms for feature selection."
a selective ensemble classifier using multiobjective optimization based extreme learning machine algorithm,L. Bai; H. Li; W. Gao,2021 17th International Conference on Computational Intelligence and Security (CIS),2021.0,10.1109/CIS54983.2021.00017,feedforward neural network;extreme learning machine;multiobjective optimization;ensemble learning;classification,,"['Evolutionary', 'MOEA/D']","['Ensemble Learning', 'Classification', 'Neural Network', 'Extreme learning machine']","['Training RMSE', 'Validation RMSE', 'Network complexity']",Single - Ensemble,"In a single hidden layer feedforward neural network (SLFN), acquiring optimal values for the number of hidden neurons and connection parameters simultaneously is regarded as one of challenges, which has attracted extensive attention. This is because changing the number of hidden neurons and connection parameters greatly affect overall performance of the SLFN and increase the training complexity. In this article, the training error, validation error, and network complexity are treated as three conflicting objectives of multiobjective model for getting a compact network with good generalization ability. For solving the multiobjective model, a hybrid coding scheme is designed for network structure and connection parameters of a SLFN, and then a multiobjective optimization based extreme learning machine (MOELM) is proposed for structure learning and parameter optimization simultaneously. To improve recognition accuracy, a selective ensemble classifier with three base classifiers according to the selection strategy is utilized to make final decision. Experimental results and comparison with other classifiers on several benchmark classification problems indicate the effectiveness and superiority of the proposed MOELM."
a novel feature selection with many-objective optimization and learning mechanism,L. Shu; F. He; X. Hu; H. Li,2021 IEEE 24th International Conference on Computer Supported Cooperative Work in Design (CSCWD),2021.0,10.1109/CSCWD49262.2021.9437707,optimization driven design;intelligent cloud manufacturing;collaborative processing of big data;feature selection;classification;many-objective optimization;extreme learning machine,,"['Evolutionary', 'NSGA-xadIII']","['Feature Selection', 'Extreme learning machine', 'Classification']","['Accuracy', 'Number of features', 'Feature relevance', 'Feature redundancy', 'Feature complementarity']",Multi,"Feature selection is extremely important in machine learning and data mining. Typical two-objective feature selection methods aim to minimize the number of features and maximize classification performance. However, they overlook the fact that there may be multiple subsets with similar information content for a given cardinality. The paper presents a many-objective feature selection approach to address this problem. Firstly, we establish a five-objective optimization model, which consists of classification accuracy, the number of features, feature relevance, feature redundancy, and feature complementarity. Therefore, the proposed model can enlarge the search space with more Pareto solutions. Secondly, we propose a wrapper structure for many-objective feature selection, which integrates a learning algorithm. Thirdly, in order to reduce the computional overhead, we propose a filter structure, which separates the learning algorithm. For implementation, we adopt NSGA-III multi-objective evolutionary algorithm and extreme learning machine. The experiments on mainstream datasets confirm the superiority of the proposed method."
reference-point-based multi-objective optimization algorithm with opposition-based voting scheme for multi-label feature selection,"Bidgoli, AA; Ebrahimpour-Komleh, H; Rahnamayan, S",INFORMATION SCIENCES,2021.0,10.1016/j.ins.2020.08.004,Multi-label classification; Feature selection; Multi-objective optimization; Evolutionary algorithm; Opposition-based computation,,['Evolutionary'],"['Classification','Multi-label','Feature Selection']","['Number of features','Hamming loss']",Multi,"Multi-label classification is a machine learning task to construct a model for assigning an entity in the dataset to two or more class labels. In order to improve the performance of multi-label classification, a multi-objective feature selection algorithm has been proposed in this paper. Feature selection as a preprocessing task for Multi-label classification problems aims to choose a subset of relevant features. Selecting a small number of high-quality features decreases the computational cost and at the same time maximizes the classification performance. However extreme decreasing the number of features causes the failure of classification. As a result, feature selection has two conflicting objectives, namely, minimizing the classification error and minimizing the number of selected features. This paper proposes a multi-objective optimization algorithm to tackle the multi-label feature selection. The task is to find a set of solutions (a subset of features) in a sophisticated large-scale search space using a reference-based multi-objective optimization method. The proposed algorithm utilizes an opposition-based binary operator to generate more diverse solutions. Injection of extreme point of the Pareto-front is another component of the algorithm which aims to find feature subsets with less classification error. The proposed method is compared with two other existing methods on eight multi-label benchmark datasets. The experimental results show that the proposed method outperforms existing algorithms in terms of various multi-objective evaluation measures, such as Hyper-volume indicator, Pure diversity, Two-set coverage, and Pareto-front proportional contribution. The proposed method leads to get a set of well-distributed trade-off solutions which reach less classification error in comparing with competitors, even with the fewer number of features. (C) 2020 Elsevier Inc. All rights reserved."
understanding and improving fairness-accuracy trade-offs in multi-task learning,"Wang, YY; Wang, XZ; Beutel, A; Prost, F; Chen, JL; Chi, EH",KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING,2021.0,10.1145/3447548.3467326,fairness; multi-task learning; Pareto frontier; multi-task-aware fairness treatment,,['Evolutionary'],"['Classification', 'Multi-label', 'Fairness']","['Accuracy loss', 'Task-specific Fairness loss', 'Shared Fairness loss']",Multi,"As multi-task models gain popularity in a wider range of machine learning applications, it is becoming increasingly important for practitioners to understand the fairness implications associated with those models. Most existing fairness literature focuses on learning a single task more fairly, while how ML fairness interacts with multiple tasks in the joint learning setting is largely under-explored. In this paper, we are concerned with how group fairness (e.g., equal opportunity, equalized odds) as an ML fairness concept plays out in the multi-task scenario. In multi-task learning, several tasks are learned jointly to exploit task correlations for a more efficient inductive transfer. This presents a multi-dimensional Pareto frontier on (1) the trade-off between group fairness and accuracy with respect to each task, as well as (2) the trade-offs across multiple tasks. We aim to provide a deeper understanding on how group fairness interacts with accuracy in multi-task learning, and we show that traditional approaches that mainly focus on optimizing the Pareto frontier of multi-task accuracy might not perform well on fairness goals. We propose a new set of metrics to better capture the multi-dimensional Pareto frontier of fairness-accuracy tradeoffs uniquely presented in a multi-task learning setting. We further propose a Multi-Task-Aware Fairness (MTA-F) approach to improve fairness in multi-task learning. Experiments on several real-world datasets demonstrate the effectiveness of our proposed approach."
a projection multi-objective svm method for multi-class classification,"Liu, L; Martin-Barragan, B; Prieto, FJ",COMPUTERS & INDUSTRIAL ENGINEERING,2021.0,10.1016/j.cie.2021.107425,Multiple objective programming; Support vector machine; Multi-class multi-objective SVM; Pareto-optimal solution,,['Evolutionary'],"['Classification', 'Multi-class', 'Support Vector Machine']",['Geometric margin for each class'],Multi,"Support Vector Machines (SVMs), originally proposed for classifications of two classes, have become a very popular technique in the machine learning field. For multi-class classifications, various single-objective models and multi-objective ones have been proposed. However,in most single-objective models, neither the different costs of different misclassifications nor the users' preferences were considered. This drawback has been taken into account in multi-objective models. In these models, large and hard second-order cone programs(SOCPs) were constructed ane weakly Pareto-optimal solutions were offered. In this paper, we propose a Projected Multi-objective SVM (PM), which is a multi-objective technique that works in a higher dimensional space than the object space. For PM, we can characterize the associated Pareto-optimal solutions. Additionally, it significantly alleviates the computational bottlenecks for classifications with large numbers of classes. From our experimental results, we can see PM outperforms the single-objective multi-class SVMs (based on an all-together method, one-against-all method and one-against-one method) and other multi-objective SVMs. Compared to the single-objective multi-class SVMs, PM provides a wider set of options designed for different misclassifications, without sacrificing training time. Compared to other multi-objective methods, PM promises the out-of-sample quality of the approximation of the Pareto frontier, with a considerable reduction of the computational burden."
multietsc: automated machine learning for early time series classification,"Ottervanger, G; Baratchi, M; Hoos, HH",DATA MINING AND KNOWLEDGE DISCOVERY,2021.0,10.1007/s10618-021-00781-5,Early classification; Time series classification; Automated machine learning,,"['Deterministic', 'MO-ParamILS', 'Iterated local search']","['Classification', 'Time Series', 'Early time series classification', 'hyperparameter tuning', ' algorithm selection', 'AutoML']","['Error rate', 'Earliness']",Multi,"Early time series classification (EarlyTSC) involves the prediction of a class label based on partial observation of a given time series. Most EarlyTSC algorithms consider the trade-off between accuracy and earliness as two competing objectives, using a single dedicated hyperparameter. To obtain insights into this trade-off requires finding a set of non-dominated (Pareto efficient) classifiers. So far, this has been approached through manual hyperparameter tuning. Since the trade-off hyperparameters only provide indirect control over the earliness-accuracy trade-off, manual tuning is tedious and tends to result in many sub-optimal hyperparameter settings. This complicates the search for optimal hyperparameter settings and forms a hurdle for the application of EarlyTSC to real-world problems. To address these issues, we propose an automated approach to hyperparameter tuning and algorithm selection for EarlyTSC, building on developments in the fast-moving research area known as automated machine learning (AutoML). To deal with the challenging task of optimising two conflicting objectives in early time series classification, we propose MultiETSC, a system for multi-objective algorithm selection and hyperparameter optimisation (MO-CASH) for EarlyTSC. MultiETSC can potentially leverage any existing or future EarlyTSC algorithm and produces a set of Pareto optimal algorithm configurations from which a user can choose a posteriori. As an additional benefit, our proposed framework can incorporate and leverage time-series classification algorithms not originally designed for EarlyTSC for improving performance on EarlyTSC; we demonstrate this property using a newly defined, naive fixed-time algorithm. In an extensive empirical evaluation of our new approach on a benchmark of 115 data sets, we show that MultiETSC performs substantially better than baseline methods, ranking highest (avg. rank 1.98) compared to conceptually simpler single-algorithm (2.98) and single-objective alternatives (4.36)."
multi-task learning for multi-objective evolutionary neural architecture search,R. Cai; J. Luo,2021 IEEE Congress on Evolutionary Computation (CEC),2021.0,10.1109/CEC45853.2021.9504721,neural architecture search;multi-task learning;surrogate model;multi-objective optimization,,['Evolutionary'],"['Neural Architecture Search', 'Multi-task', 'AutoML', 'Neural Networks']",['Task loss'],Multi,"Neural architecture search (NAS) is an exciting new field in automating machine learning. It can automatically search for the architecture of neural networks. But the current NAS has extremely high requirements for hardware equipment and time costs. In this work, we propose a predictor based on Radial basis function neural network (RBFNN) as a surrogate model of Bayesian optimization to predict the performance of neural architecture. The existing work does not consider the difficulty of directly searching for neural architectures that meet the performance requirements of NAS in real-world applications. Meanwhile, NAS needs to execute multiple times independently when facing multiple similar tasks. Therefore, we further propose a multi-task learning surrogate model with multiple RBFNNs. The model not only functions as a predictor, but also learns knowledge of similar tasks jointly. The performance of NAS is improved by processing multiple tasks simultaneously. Also, the current NAS is committed to searching for very high-performance networks and does not take into account that neural architectures are limited by device memory during actual deployment. The scale of architecture also needs to be considered. We use a multi-objective optimization algorithm to simultaneously balance the performance and the scale, and build a multi-objective evolutionary search framework to find the Pareto optimal front. Once the NAS is completed, decision-makers can choose the appropriate architecture for deployment according to different performance requirements and hardware conditions. Compared with existing NAS work, our proposed MT-ENAS algorithm is able to find a neural architecture with competitive performance and smaller scale in a shorter time."
how fair can we go in machine learning? assessing the boundaries of accuracy and fairness,"Valdivia, A; Sanchez-Monedero, J; Casillas, J",INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS,2021.0,10.1002/int.22354,algorithmic fairness; group fairness; multiobjective optimization,,"['Evolutionary', 'NSGA‐II']","['Fairness', 'Logistic Regression', 'Decision tree', 'Classification']","['Geometric Mean', 'Difference of False-Positive Rate']",Multi,"Fair machine learning has been focusing on the development of equitable algorithms that address discrimination. Yet, many of these fairness-aware approaches aim to obtain a unique solution to the problem, which leads to a poor understanding of the statistical limits of bias mitigation interventions. In this study, a novel methodology is presented to explore the tradeoff in terms of a Pareto front between accuracy and fairness. To this end, we propose a multiobjective framework that seeks to optimize both measures. The experimental framework is focused on logistiregression and decision tree classifiers since they are well-known by the machine learning community. We conclude experimentally that our method can optimize classifiers by being fairer with a small cost on the classification accuracy. We believe that our contribution will help stakeholders of sociotechnical systems to assess how far they can go being fair and accurate, thus serving in the support of enhanced decision making where machine learning is used."
constructing accuracy and diversity ensemble using pareto-based multi-objective learning for evolving data streams,"Sun, YG; Dai, HH",NEURAL COMPUTING & APPLICATIONS,2021.0,10.1007/s00521-020-05386-5,Data streams; Concept drift; Ensemble learning; Diversity; Classifier selection; Multi-objective optimization,,"['Evolutionary', 'NSGA‐II']","['Ensemble Learning', 'Classification', 'Data streams']","['Accuracy', 'Diversity']",Single - Ensemble,"Ensemble learning is one of the most frequently used techniques for handling concept drift, which is the greatest challenge for learning high-performance models from big evolving data streams. In this paper, a Pareto-based multi-objective optimization technique is introduced to learn high-performance base classifiers. Based on this technique, a multi-objective evolutionary ensemble learning scheme, named Pareto-optimal ensemble for a better accuracy and diversity (PAD), is proposed. The approach aims to enhance the generalization ability of ensemble in evolving data stream environment by balancing the accuracy and diversity of ensemble members. In addition, an adaptive window change detection mechanism is designed for tracking different kinds of drifts constantly. Extensive experiments show that PAD is capable of adapting to dynamic change environments effectively and efficiently in achieving better performance."
fairer machine learning through multi-objective evolutionary learning,"Zhang, QQ; Liu, JL; Zhang, ZQ; Wen, JY; Mao, BF; Yao, X","ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2021, PT IV",2021.0,10.1007/978-3-030-86380-7_10,Fairness in machine learning; Discrimination in machine learning; AI ethics; Fairness measures; Multi-objective learning,,"['Evolutionary', 'NSGA‐II']","['Fairness', 'Classification']","['Model Error', 'Individual unfairness', 'Group unfairness']",Multi,"Dilemma between model accuracy and fairness in machine learning models has been shown theoretically and empirically. So far, dozens of fairness measures have been proposed, among which incompatibility and complementarity exist. However, no fairness measure has been universally accepted as the single fairest measure. No one has considered multiple fairness measures simultaneously. In this paper, we propose a multi-objective evolutionary learning framework for mitigating unfairness caused by considering a single measure only, in which a multi-objective evolutionary algorithm is used during training to balance accuracy and multiple fairness measures simultaneously. In our case study, besides the model accuracy, two fairness measures that are conflicting to each other are selected. Empirical results show that our proposed multi-objective evolutionary learning framework is able to find Pareto-front models efficiently and provide fairer machine learning models that consider multiple fairness measures."
multicriteria classifier ensemble learning for imbalanced data,W. Węgier; M. Koziarski; M. Woźniak,IEEE Access,2022.0,10.1109/ACCESS.2022.3149914,Classifier ensemble;imbalanced data;multi-objective optimization;pattern classification,,"['Evolutionary', 'NSGA‐II']","['Multicriteria', 'Classification', 'Imbalanced data']","['Recall', 'Precision']",Multi,"One of the vital problems with the imbalanced data classifier training is the definition of an optimization criterion. Typically, since the exact cost of misclassification of the individual classes is unknown, combined metrics and loss functions that roughly balance the cost for each class are used. However, this approach can lead to a loss of information, since different trade-offs between class misclassification rates can produce similar combined metric values. To address this issue, this paper discusses a multi-criteria ensemble training method for the imbalanced data. The proposed method jointly optimizes <italic>precision</italic> and <italic>recall</italic>, and provides the end-user with a set of Pareto optimal solutions, from which the final one can be chosen according to the user’s preference. The proposed approach was evaluated on a number of benchmark datasets and compared with the single-criterion approach (where the selected criterion was one of the chosen metrics). The results of the experiments confirmed the usefulness of the obtained method, which on the one hand guarantees good quality, i.e., not worse than the one obtained with the use of single-criterion optimization, and on the other hand, offers the user the opportunity to choose the solution that best meets their expectations regarding the trade-off between errors on the minority and the majority class."
efficient and sparse neural networks by pruning weights in a multiobjective learning approach,"Reiners M., Klamroth K., Heldmann F., Stiglmayr M.",Computers and Operations Research,2022.0,10.1016/j.cor.2021.105676,Automated machine learning; l1-regularization; Multiobjective learning; Stochastic multi-gradient descent; Unstructured pruning,,"['Deterministic', 'Stochastic Multi-Gradient Descent', 'Gradient based']","['Neural Networks', 'Pruning', 'AutoML', 'Classification']","['Cross entropy', 'L1']",Multi,"Overparameterization and overfitting are common concerns when designing and training deep neural networks, that are often counteracted by pruning and regularization strategies. However, these strategies remain secondary to most learning approaches and suffer from time and computational intensive procedures. We suggest a multiobjective perspective on the training of neural networks by treating its prediction accuracy and the network complexity as two individual objective functions in a biobjective optimization problem. As a showcase example, we use the cross entropy as a measure of the prediction accuracy while adopting an l1-penalty function to assess the total cost (or complexity) of the network parameters. The latter is combined with an intra-training pruning approach that reinforces complexity reduction and requires only marginal extra computational cost. From the perspective of multiobjective optimization, this is a truly large-scale optimization problem. We compare two different optimization paradigms: On the one hand, we adopt a scalarization-based approach that transforms the biobjective problem into a series of weighted-sum scalarizations. On the other hand we implement stochastic multi-gradient descent algorithms that generate a single Pareto optimal solution without requiring or using preference information. In the first case, favorable knee solutions are identified by repeated training runs with adaptively selected scalarization parameters. Numerical results on exemplary convolutional neural networks confirm that large reductions in the complexity of neural networks with negligible loss of accuracy are possible. © 2022 Elsevier Ltd"
a multi-objective optimization algorithm for feature selection problems,"Abdollahzadeh, B; Gharehchopogh, FS",ENGINEERING WITH COMPUTERS,,10.1007/s00366-021-01369-9,Feature selection; Harris hawks optimization; Fruitfly optimization algorithm; Multiobjective; Bonferroni&#8211; Holm; Family-wise error rate,,"['Evolutionary', 'Harris Hawks Optimization', 'Fruitfly Optimization Algorithm']","['Feature Selection', 'Classification']","['Classification error', 'Number of features']",Multi,"Feature selection (FS) is a critical step in data mining, and machine learning algorithms play a crucial role in algorithms performance. It reduces the processing time and accuracy of the categories. In this paper, three different solutions are proposed to FS. In the first solution, the Harris Hawks Optimization (HHO) algorithm has been multiplied, and in the second solution, the Fruitfly Optimization Algorithm (FOA) has been multiplied, and in the third solution, these two solutions are hydride and are named MOHHOFOA. The results were tested with MOPSO, NSGA-II, BGWOPSOFS and B-MOABC algorithms for FS on 15 standard data sets with mean, best, worst, standard deviation (STD) criteria. The Wilcoxon statistical test was also used with a significance level of 5% and the Bonferroni-Holm method to control the family-wise error rate. The results are shown in the Pareto front charts, indicating that the proposed solutions' performance on the data set is promising."
