Title,Authors,Publisher,Year,Abstract,DOI,Keywords,Citations,Remove - Title
Pareto-Based Multiobjective Machine Learning: An Overview and Case Studies,Y. Jin; B. Sendhoff,"IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",2008.0,"Machine learning is inherently a multiobjective task. Traditionally, however, either only one of the objectives is adopted as the cost function or multiple objectives are aggregated to a scalar cost function. This can be mainly attributed to the fact that most conventional learning algorithms can only deal with a scalar cost function. Over the last decade, efforts on solving machine learning problems using the Pareto-based multiobjective optimization methodology have gained increasing impetus, particularly due to the great success of multiobjective optimization using evolutionary algorithms and other population-based stochastic search methods. It has been shown that Pareto-based multiobjective learning approaches are more powerful compared to learning algorithms with a scalar cost function in addressing various topics of machine learning, such as clustering, feature selection, improvement of generalization ability, knowledge extraction, and ensemble generation. One common benefit of the different multiobjective learning approaches is that a deeper insight into the learning problem can be gained by analyzing the Pareto front composed of multiple Pareto-optimal solutions. This paper provides an overview of the existing research on multiobjective machine learning, focusing on supervised learning. In addition, a number of case studies are provided to illustrate the major benefits of the Pareto-based approach to machine learning, e.g., how to identify interpretable models and models that can generalize on unseen data from the obtained Pareto-optimal solutions. Three approaches to Pareto-based multiobjective ensemble generation are compared and discussed in detail. Finally, potentially interesting topics in multiobjective machine learning are suggested.",10.1109/TSMCC.2008.919172,Ensemble;evolutionary multiobjective optimization;generalization;machine learning;multiobjective learning;multiobjective optimization;neural networks;Pareto optimization,217.0,
Multiobjective Evolutionary Data Mining for Performance Improvement of Evolutionary Multiobjective Optimization,Y. Nojima; Y. Tanigaki; N. Masuyama; H. Ishibuchi,"2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",2018.0,"In recent years, evolutionary multiobjective optimization (EMO) algorithms have frequently been used for engineering problems with some conflicting objective functions to be simultaneously optimized. EMO algorithms can provide a number of Pareto optimal solutions to users. Two scenarios are considered in the practical use of EMO algorithms. One is that a decision maker selects a single solution from the obtained ones after the EMO process. The other is that a decision maker utilizes the solutions to analyze the relationship between design variables and objective functions of the corresponding problem. In this paper, we apply fuzzy genetics-based machine learning to the second scenario in order to generate if-then rule-based classifiers which represent the relationship between design variables and objective functions. We also utilize this method during the EMO process to pre-screen candidate offspring solutions. The classifier detects non-promising offspring solutions. Then, they are discarded before their fitness evaluation, so that the computation resource is used only for promising solutions. We apply this method to one engineering problem and examine its effect on the search performance of an EMO algorithm.",10.1109/SMC.2018.00135,Evolutionary multiobjective optimization;data mining;multiobjective fuzzy genetics-based machine learning;pattern classification,,
Single-Objective/Multiobjective Cat Swarm Optimization Clustering Analysis for Data Partition,D. Yan; H. Cao; Y. Yu; Y. Wang; X. Yu,IEEE Transactions on Automation Science and Engineering,2020.0,"This article proposes single-objective/multiobjective cat swarm optimization clustering algorithms for data partition. The proposed methods use the cat swarm to search the optimal. The position of the cat tightly associates with the clustering centers and is updated by two submodes: the seeking mode and the tracing mode. The seeking mode uses the simulated annealing strategy to update the cat position at a probability. Inspired by the quantum theories, the tracing mode adopts the quantum model to update the cat position in the whole solution space. First, the single-objective method is proposed and adopts the cohesion of clustering as the objective function, in which the kernel method is applied. For considering more objective functions to reveal diverse aspects of data, the multiobjective method is proposed and adopts both the cohesion and the connectivity as the objective functions. The Pareto optimization method is applied to balance the objectives. In the experiments, three kinds of data sets are used to examine the effectiveness of the proposed methods, which are three synthetic data sets, four data sets from the UCI Machine Learning Repository, and a field data set. Experimental results verified that the proposed methods perform better than the traditional clustering algorithms, and the proposed multiobjective method has the highest accuracy. Note to Practitioners-This article presents single-objective/multiobjective cat swarm optimization clustering analysis methods for data partition. Through automatically extracting meaningful or useful classes, clustering analysis could help the practitioners or the intelligent devices find the specific meanings of data, natural data structure, the data relationships, or other characteristics. The proposed methods use the cat swarm to search the optimal clustering result. One or more criterion functions could be selected as the optimization objectives. The time complexity of the multiobjective type is higher than that of the single-objective type. Therefore, in the industrial field, engineers should choose the number of the optimization objectives based on the actual requirements. The proposed methods could be widely used into industrial applications to deal with complex data sets. Future research could consider some more progressive optimization schemes to improve the effectiveness.",10.1109/TASE.2020.2969485,Clustering analysis;data partition;quantum model;single-objective/multiobjective optimization,9.0,
Deep-Learning-Aided Packet Routing in Aeronautical <italic>Ad Hoc</italic> Networks Relying on Real Flight Data: From Single-Objective to Near-Pareto Multiobjective Optimization,D. Liu; J. Zhang; J. Cui; S. -X. Ng; R. G. Maunder; L. Hanzo,IEEE Internet of Things Journal,2022.0,"Data packet routing in aeronautical <italic>ad hoc</italic> networks (AANETs) is challenging due to their high-dynamic topology. In this article, we invoke deep learning (DL) to assist routing in AANETs. We set out from the single objective of minimizing the end-to-end (E2E) delay. Specifically, a deep neural network (DNN) is conceived for mapping the local geographic information observed by the forwarding node into the information required for determining the optimal next hop. The DNN is trained by exploiting the regular mobility pattern of commercial passenger airplanes from historical flight data. After training, the DNN is stored by each airplane for assisting their routing decisions during flight relying solely on local geographic information. Furthermore, we extend the DL-aided routing algorithm to a multiobjective scenario, where we aim for simultaneously minimizing the delay, maximizing the path capacity, and maximizing the path lifetime. Our simulation results based on real flight data show that the proposed DL-aided routing outperforms existing position-based routing protocols in terms of its E2E delay, path capacity, as well as path lifetime, and it is capable of approaching the Pareto front that is obtained using global link information.",10.1109/JIOT.2021.3105357,Aeronautical ad hoc network (AANET);deep learning (DL);multiobjective optimization (MOO);routing,,
Rotation effects of objective functions in parallel distributed multiobjective fuzzy genetics-based machine learning,Y. Takahashi; Y. Nojima; H. Ishibuchi,2015 10th Asian Control Conference (ASCC),2015.0,"Fuzzy genetics-based machine learning (FGBML) is one of data mining techniques using evolutionary computation. It can obtain fuzzy rule-based classifiers that are accurate and linguistically interpretable for human users. However, there are two major problems. One is that it is impossible to design the best classifier with respect to both accuracy and interpretability due to their tradeoff. To solve this problem, we proposed multiobjective FGBML (MoFGBML) where an evolutionary multiobjective optimization algorithm is used to obtain a number of classifiers with different tradeoffs between accuracy and complexity. The other is the heavy computational load of FGBML for large data sets. In the previous study, we applied parallel distributed implementation to our MoFGBML to overcome this problem. We examined the effects of the parallel distributed implementation on the search ability. Although the computational time became much shorter, the number of the obtained non-dominated classifiers became small. As a result, accurate classifiers were not obtained for some data sets. In this paper, we propose a simple idea to bias the search direction of our MoFGBML. We rotate one or two objective functions. This rotation changes the dominance relation in multiobjective optimization. Through computational experiments, we examine the effects of the rotated objective functions on the search ability of our MoFGBML for large data sets.",10.1109/ASCC.2015.7244890,Fuzzy genetics-based machine learning;parallel distributed implementation;multiobjective optimization,1.0,
Multicriteria PM Motor Design Based on ANFIS Evaluation of EV Driving Cycle Efficiency,C. T. Krasopoulos; M. E. Beniakar; A. G. Kladas,IEEE Transactions on Transportation Electrification,2018.0,"This paper proposes a multicriteria design optimization methodology for permanent magnet (PM) motors used in electric vehicle (EV) applications. In the process, an adaptive-network-based fuzzy inference system (ANFIS) is utilized, coupled with a multiobjective optimization algorithm, as a surrogate model of the electric motor. This allows for the consideration of the full drive cycle and respective efficiency map for every motor design. The prediction error of the ANFIS is minimized by employing appropriate membership functions, initial training data, and an adaptive learning scheme via iterative training. The efficiency map is then implemented in a vehicle dynamic model to compute the total consumed energy over the driving cycle. The optimization profile accounts for total energy efficiency, torque density, and additionally considers complementary design criteria via an a posteriori selection procedure on the resulting Pareto set. The methodology developed is applied to optimize a surface PM motor with concentrated fractional slot winding, mounted on a light EV that competes in fuel economy races. The selected motor design has been validated through measurements on a prototype.",10.1109/TTE.2018.2810707,Adaptive-network-based fuzzy inference system (ANFIS);drive cycle;efficiency map;electric vehicle (EV);energy efficiency;machine learning;multicriteria design;multiobjective optimization;permanent magnet (PM) motor design;torque density,31.0,
Machine Learning for Multiobjective Evolutionary Optimization in Python for EM Problems,A. Boryssenko; N. Herscovici,2018 IEEE International Symposium on Antennas and Propagation & USNC/URSI National Radio Science Meeting,2018.0,A highly efficient algorithm for the optimization of EM problems is proposed. The algorithm is mostly based on Python open-source libraries and was successfully used in the development of antennas and arrays using HFSS.,10.1109/APUSNCURSINRSM.2018.8609394,machine learning;surrogate objective function;multiobjective evolutionary algorithm (MOEA);Gaussian process;HFSS,2.0,
Multiple Reference Points-Based Decomposition for Multiobjective Feature Selection in Classification: Static and Dynamic Mechanisms,B. H. Nguyen; B. Xue; P. Andreae; H. Ishibuchi; M. Zhang,IEEE Transactions on Evolutionary Computation,2020.0,"Feature selection is an important task in machine learning that has two main objectives: 1) reducing dimensionality and 2) improving learning performance. Feature selection can be considered a multiobjective problem. However, it has its problematic characteristics, such as a highly discontinuous Pareto front, imbalance preferences, and partially conflicting objectives. These characteristics are not easy for existing evolutionary multiobjective optimization (EMO) algorithms. We propose a new decomposition approach with two mechanisms (static and dynamic) based on multiple reference points under the multiobjective evolutionary algorithm based on decomposition (MOEA/D) framework to address the above-mentioned difficulties of feature selection. The static mechanism alleviates the dependence of the decomposition on the Pareto front shape and the effect of the discontinuity. The dynamic one is able to detect regions in which the objectives are mostly conflicting, and allocates more computational resources to the detected regions. In comparison with other EMO algorithms on 12 different classification datasets, the proposed decomposition approach finds more diverse feature subsets with better performance in terms of hypervolume and inverted generational distance. The dynamic mechanism successfully identifies conflicting regions and further improves the approximation quality for the Pareto fronts.",10.1109/TEVC.2019.2913831,Classification;feature selection;multiobjective evolutionary algorithm based on decomposition (MOEA/D);multiobjective optimization;partially conflicting,23.0,
Expensive Multiobjective Evolutionary Optimization Assisted by Dominance Prediction,Y. Yuan; W. Banzhaf,IEEE Transactions on Evolutionary Computation,2022.0,"We propose a new surrogate-assisted evolutionary algorithm for expensive multiobjective optimization. Two classification-based surrogate models are used, which can predict the Pareto dominance relation and <inline-formula> <tex-math notation=""LaTeX"">$\theta $ </tex-math></inline-formula>-dominance relation between two solutions, respectively. To make such surrogates as accurate as possible, we formulate dominance prediction as an imbalanced classification problem and address this problem using deep learning techniques. Furthermore, to integrate the surrogates based on dominance prediction with multiobjective evolutionary optimization, we develop a two-stage preselection strategy. This strategy aims to select a promising solution to be evaluated among those produced by genetic operations, taking proper account of the balance between convergence and diversity. We conduct an empirical study on a number of well-known multiobjective and many-objective benchmark problems, over a relatively small number of function evaluations. Our experimental results demonstrate the superiority of the proposed algorithm compared with several representative surrogate-assisted algorithms.",10.1109/TEVC.2021.3098257,Deep neural networks;expensive multiobjective optimization;many-objective optimization;metamodeling;surrogate-assisted evolutionary computation,,
Multiobjective Neural Network Ensembles Based on Regularized Negative Correlation Learning,H. Chen; X. Yao,IEEE Transactions on Knowledge and Data Engineering,2010.0,"Negative Correlation Learning (NCL) [CHECK END OF SENTENCE], [CHECK END OF SENTENCE] is a neural network ensemble learning algorithm which introduces a correlation penalty term to the cost function of each individual network so that each neural network minimizes its mean-square-error (MSE) together with the correlation. This paper describes NCL in detail and observes that the NCL corresponds to training the entire ensemble as a single learning machine that only minimizes the MSE without regularization. This insight explains that NCL is prone to overfitting the noise in the training set. The paper analyzes this problem and proposes the multiobjective regularized negative correlation learning (MRNCL) algorithm which incorporates an additional regularization term for the ensemble and uses the evolutionary multiobjective algorithm to design ensembles. In MRNCL, we define the crossover and mutation operators and adopt nondominated sorting algorithm with fitness sharing and rank-based fitness assignment. The experiments on synthetic data as well as real-world data sets demonstrate that MRNCL achieves better performance than NCL, especially when the noise level is nontrivial in the data set. In the experimental discussion, we give three reasons why our algorithm outperforms others.",10.1109/TKDE.2010.26,Multiobjective algorithm;multiobjective learning;neural network ensembles;neural networks;negative correlation learning;regularization.,83.0,
Feature Selection Using Multiobjective Optimization for Named Entity Recognition,A. Ekbal; S. Saha; C. S. Garbe,2010 20th International Conference on Pattern Recognition,2010.0,"Appropriate feature selection is a very crucial issue in any machine learning framework, specially in Maximum Entropy (ME). In this paper, the selection of appropriate features for constructing a ME based Named Entity Recognition (NER) system is posed as a multiobjective optimization (MOO) problem. Two classification quality measures, namely recall and precision are simultaneously optimized using the search capability of a popular evolutionary MOO technique, NSGA-II. The proposed technique is evaluated to determine suitable feature combinations for NER in two languages, namely Bengali and English that have significantly different characteristics. Evaluation results yield the recall, precision and F-measure values of 70.76%, 81.88% and 75.91%, respectively for Bengali, and 78.38%, 81.27% and 79.80%, respectively for English. Comparison with an existing ME based NER system shows that our proposed feature selection technique is more efficient than the heuristic based feature selection.",10.1109/ICPR.2010.477,Multiobjective Optimization;Feature Selection;Maximum Entropy;Named Entity Recognition,17.0,
Neural-Architecture-Search-Based Multiobjective Cognitive Automation System,E. K. Wang; S. P. Xu; C. -M. Chen; N. Kumar,IEEE Systems Journal,2021.0,"Currently, deep-learning-based cognitive automation for decision-making in industrial informatics is a new hot topic in the field of cognitive computing, among which multiobjective architecture optimization is of great difficulty in the research area. When the existing algorithms face multiobjective cognitive model problems, it often takes a lot of time to continuously set different search preference parameters to generate a new search process. This article mainly aims to solve the problem in a multiobjective neural architecture search process, and the key issue is how to adapt user preferences during architectural search. We propose a new algorithm: linear-prefer coevolutionary algorithm. Compared to the original user-constrained method and the Pareto-dominant NSGA-II algorithm, we have faster adaptation time and better quality of adaptation. At the same time, it can respond to user's needs at a relatively faster pace during the reasoning phase. Based on a large number of comparative test results, our algorithm is superior to the traditional cognitive automation algorithms for the multiobjective problem in search quality.",10.1109/JSYST.2020.3002428,Cognitive automation;evolutional algorithm;multiobjective;neural architecture search (NAS);Pareto dominant,1.0,
Collective Personalized Change Classification With Multiobjective Search,X. Xia; D. Lo; X. Wang; X. Yang,IEEE Transactions on Reliability,2016.0,"Many change classification techniques have been proposed to identify defect-prone changes. These techniques consider all developers' historical change data to build a global prediction model. In practice, since developers have their own coding preferences and behavioral patterns, which causes different defect patterns, a separate change classification model for each developer can help to improve performance. Jiang, Tan, and Kim refer to this problem as personalized change classification, and they propose PCC+ to solve this problem. A software project has a number of developers; for a developer, building a prediction model not only based on his/her change data, but also on other relevant developers' change data can further improve the performance of change classification. In this paper, we propose a more accurate technique named collective personalized change classification (CPCC), which leverages a multiobjective genetic algorithm. For a project, CPCC first builds a personalized prediction model for each developer based on his/her historical data. Next, for each developer, CPCC combines these models by assigning different weights to these models with the purpose of maximizing two objective functions (i.e., F1-scores and cost effectiveness). To further improve the prediction accuracy, we propose CPCC+ by combining CPCC with PCC proposed by Jiang, Tan, and Kim To evaluate the benefits of CPCC+ and CPCC, we perform experiments on six large software projects from different communities: Eclipse JDT, Jackrabbit, Linux kernel, Lucene, PostgreSQL, and Xorg. The experiment results show that CPCC+ can discover up to 245 more bugs than PCC+ (468 versus 223 for PostgreSQL) if developers inspect the top 20% lines of code that are predicted buggy. In addition, CPCC+ can achieve F1-scores of 0.60-0.75, which are statistically significantly higher than those of PCC+ on all of the six projects.",10.1109/TR.2016.2588139,Cost effectiveness;developer;machine learning;multiobjective genetic algorithm;personalized change classification (PCC),27.0,
Evolutionary Multiobjective Optimization Driven by Generative Adversarial Networks (GANs),C. He; S. Huang; R. Cheng; K. C. Tan; Y. Jin,IEEE Transactions on Cybernetics,2021.0,"Recently, increasing works have been proposed to drive evolutionary algorithms using machine-learning models. Usually, the performance of such model-based evolutionary algorithms is highly dependent on the training qualities of the adopted models. Since it usually requires a certain amount of data (i.e., the candidate solutions generated by the algorithms) for model training, the performance deteriorates rapidly with the increase of the problem scales due to the curse of dimensionality. To address this issue, we propose a multiobjective evolutionary algorithm driven by the generative adversarial networks (GANs). At each generation of the proposed algorithm, the parent solutions are first classified into real and fake samples to train the GANs; then the offspring solutions are sampled by the trained GANs. Thanks to the powerful generative ability of the GANs, our proposed algorithm is capable of generating promising offspring solutions in high-dimensional decision space with limited training data. The proposed algorithm is tested on ten benchmark problems with up to 200 decision variables. The experimental results on these test problems demonstrate the effectiveness of the proposed algorithm.",10.1109/TCYB.2020.2985081,Deep learning;machine learning;evolutionary algorithm;generative adversarial networks (GANs);machine learning;multiobjective optimization,20.0,
Search ability of evolutionary multiobjective optimization algorithms for multiobjective fuzzy genetics-based machine learning,H. Ishibuchi; Y. Nakashima; Y. Nojima,2009 IEEE International Conference on Fuzzy Systems,2009.0,"Recently evolutionary multiobjective optimization (EMO) algorithms have been actively used for the design of accurate and interpretable fuzzy rule-based systems. This research area is often referred to as multiobjective genetic fuzzy systems where EMO algorithms are used to search for a number of non-dominated fuzzy rule-based systems with respect to their accuracy and interpretability. The main advantage of the use of EMO algorithms for fuzzy system design over single-objective optimizers is that multiple alternative fuzzy rule-based systems with different accuracy-interpretability tradeoffs are obtained by their single run. The decision maker can choose a single fuzzy rule-based system according to their preference. There still exist several important issues to be discussed in this research area such as the definition of interpretability, the formulation of interpretability measures, the visualization of tradeoff relations, and the interpretability of the explanation of fuzzy reasoning results. In this paper, we discuss the ability of EMO algorithms as multiobjective optimizers to search for Pareto optimal or near Pareto optimal fuzzy rule-based systems. More specifically, we examine whether EMO algorithms can find non-dominated fuzzy rule-based systems that approximate the entire Pareto fronts of multiobjective fuzzy system design problems.",10.1109/FUZZY.2009.5277370,,6.0,
Multiobjective evolution for deep learning and its robotic applications,D. Hossain; G. Capi,"2017 8th International Conference on Information, Intelligence, Systems & Applications (IISA)",2017.0,"In numerous industrial applications where robot object recognition and grasping are the primary concern as the most effective and reliable object sorting policy. Deep Learning approaches have produced promising results in object recognition and robot gasping, its performance does not have any influence from handcrafted features. In this paper, we propose a multiobjective deep belief neural network (DBNN) method. It employs a multiobjective evolutionary algorithm integrated with DBNN [10] training technique subject to accuracy and network time as two conflicting objectives. We evaluate the proposed method on the real-time object recognition and robot grasping tasks. Experimental results demonstrate that the proposed method outperforms on the assign tasks.",10.1109/IISA.2017.8316404,deep learning;multiobjective;object recognition;robot grasping;deep belief neural network (DBNN);NSGA-II,4.0,
A Selective Ensemble Classifier Using Multiobjective Optimization Based Extreme Learning Machine Algorithm,L. Bai; H. Li; W. Gao,2021 17th International Conference on Computational Intelligence and Security (CIS),2021.0,"In a single hidden layer feedforward neural network (SLFN), acquiring optimal values for the number of hidden neurons and connection parameters simultaneously is regarded as one of challenges, which has attracted extensive attention. This is because changing the number of hidden neurons and connection parameters greatly affect overall performance of the SLFN and increase the training complexity. In this article, the training error, validation error, and network complexity are treated as three conflicting objectives of multiobjective model for getting a compact network with good generalization ability. For solving the multiobjective model, a hybrid coding scheme is designed for network structure and connection parameters of a SLFN, and then a multiobjective optimization based extreme learning machine (MOELM) is proposed for structure learning and parameter optimization simultaneously. To improve recognition accuracy, a selective ensemble classifier with three base classifiers according to the selection strategy is utilized to make final decision. Experimental results and comparison with other classifiers on several benchmark classification problems indicate the effectiveness and superiority of the proposed MOELM.",10.1109/CIS54983.2021.00017,feedforward neural network;extreme learning machine;multiobjective optimization;ensemble learning;classification,,
SDN-Enabled Adaptive and Reliable Communication in IoT-Fog Environment Using Machine Learning and Multiobjective Optimization,A. Akbar; M. Ibrar; M. A. Jan; A. K. Bashir; L. Wang,IEEE Internet of Things Journal,2021.0,"The Internet-of-Things (IoT) devices, backed by resourceful fog computing, are capable of meeting the requirements of computationally-intensive tasks. However, many existing IoT applications are unable to perform well, due to different Quality-of-Service (QoS) requirements, while communicating with the fog server. Besides, constantly changing traffic demands of applications is another challenge. For example, the demand for real-time applications includes communicating over a path that is less prone to delay, and applications that offload computationally intensive tasks to the fog server need a reliable path that has a lower probability of link failure. This results in a tradeoff between conflicting objectives that are constantly evolving, i.e., minimizing end-to-end delay and maximizing the reliability of paths between IoT devices and the fog server. We propose a novel approach that takes advantage of machine learning (ML) and multiobjective optimization (MOO)-based techniques. The reliability of links is evaluated using an ML-based algorithm in an software-defined network (SDN)-enabled multihop scenario for the IoT-fog environment. By considering the two conflicting objectives, the MOO algorithm is used to find the Pareto-optimal paths. Our experimental evaluation considers two applications with different QoS requirements-a real-time application (App-1) using UDP sockets and a task offloading application (App-2) using TCP sockets. Our results show that: 1) the tradeoff between the two objectives can be optimized and 2) the SDN controller was able to make adaptive decision on-the-fly to choose the best path from the Pareto-optimal set. The App-1 communicating over the selected path finished its execution in 13% less time than communicating over the shortest path. The App-2 had 41% less packet loss using the selected path compared to using the shortest path.",10.1109/JIOT.2020.3038768,Fog computing;Internet of Things (IoT);machine learning (ML);multiobjective optimization (MOO);software-defined networks (SDNs),11.0,
Multiobjective Design of 2-D-Material-Based Field-Effect Transistors With Machine Learning Methods,T. Wu; J. Guo,IEEE Transactions on Electron Devices,2021.0,"Design optimization of emerging nanoscale transistor technologies often requires careful design tradeoff between many objectives, including speed, power, variability, and so on. By leveraging machine learning (ML) methods, we develop a multiobjective optimization (MOO) framework for 2-D-material-based field-effect transistors (FETs) near the scaling limit. The MOO design framework performs gradient-free efficient global optimization and offers the option of using active learning. Optimum designs with a tradeoff between transistor speed, power, and variability are identified automatically for transition metal dichalcogenide (TMDC) and black phosphorene FETs by applying the MOO design framework that couples ML methods to quantum transport device simulations. The design optimization results show that the International Roadmap of Devices and Systems (IRDS) target of 2025 and 2028 technology nodes can be met by 2-D FETs.",10.1109/TED.2021.3085701,2-D field-effect transistors (FETs);active learning;global optimization;machine learning (ML);multiobjective optimization (MOO);nanoscale transistors;transistor simulation,,
Multiobjective fuzzy genetics-based machine learning with a reject option,Y. Nojima; H. Ishibuchi,2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),2016.0,"Classifier design for a classification problem with M classes can be viewed as finding an optimal partition of its pattern space into M disjoint subspaces. However, this is not always a good strategy especially when training patterns from different classes are heavily overlapping in the pattern space. A simple but practically useful idea is the use of a reject option. In this case, the pattern space is partitioned into (M+1) disjoint subspace where the classification of new patterns is rejected in the (M+1)th subspace. In this paper, we discuss the design of fuzzy rule-based classifiers with a reject option. The rejection subspace is specified by a threshold value for the difference of a kind of matching degrees between the best matching class and the second best matching class. The important research question is how to specify the threshold value. We examine the following two approaches: One is manual specification after designing a fuzzy rule-based classifier, and the other is simultaneous multiobjective optimization of a threshold value and a fuzzy rule-based classifier. In the latter approach, we use three objectives: maximization of the correct classification, and minimization of the rejection and the complexity of the classifier.",10.1109/FUZZ-IEEE.2016.7737854,Fuzzy genetics-based machine learning;reject option;evolutionary multiobjective optimization,1.0,
Determination of Weights for Multiobjective Decision Making or Machine Learning,P. Wang; H. Zhu; M. Wilamowska-Korsak; Z. Bi; L. Li,IEEE Systems Journal,2014.0,"Decision-making processes in complex systems generally require the mechanisms to make the tradeoff among contradicting design criteria. When multiple objectives are involved in decision making or machine learning, a crucial step is to determine the weights of individual objectives to the system-level performance. Determining the weights of multiobjectives is an evaluation process, and it has been often treated as an optimization problem. However, our preliminary investigation has shown that existing methodologies in dealing with the weights of multiobjectives have some obvious limitations in the sense that the determination of weights is tackled as a single optimization problem, a result based on such an optimization is incomprehensive, and it can even be unreliable when the information about multiple objectives is incomplete such as an incompleteness caused by poor data. The constraints of weights are also discussed. Variable weights are natural in decision-making processes. Therefore, we are motivated to develop a systematic methodology in determining variable weights of multiobjectives. The roles of weights in an original multiobjective decision-making or machine-learning problem are analyzed, and the weights are determined with the aid of a modular neural network. The inconsistency issue of weights is particularly discussed.",10.1109/JSYST.2013.2265663,Consistency;multidisciplinary design optimization (MDO);multifunctional machine learning (MFML);multiobjective decision making (MODM);neural network;tradeoff;variable weights,12.0,
An Entropy Driven Multiobjective Particle Swarm Optimization Algorithm for Feature Selection,J. Luo; D. Zhou; L. Jiang; H. Ma,2021 IEEE Congress on Evolutionary Computation (CEC),2021.0,"Feature selection is an important research field in machine learning since high-dimensionality is a common characteristic of real-world data. It has two main objectives, which are to maximize the classification accuracy while minimizing the number of selected features. As the two objectives are usually in conflict with each other, it makes feature selection a multi-objective problem. However, the large search space and discrete Pareto front makes it not easy for existing evolutionary multi-objective algorithms. In order to deal with the above mentioned difficulties in feature selection, an entropy driven multiobjective particle swarm optimization algorithm is proposed to remove redundant feature and decrease computational complexity. First, its basic idea is to model feature selection as a multiobjective optimization problem by optimizing the number of features and the classification accuracy in supervised condition simultaneously. Second, a particle initialization strategy based on information entropy is designed to improve the quality of initial solutions, and an adaptive velocity update rule is used to swap between local search and global search. Besides, a specified discrete nondominated sorting is designed. These strategies enable the proposed algorithm to gain better performance on both the quality and size of feature subset. The experimental results show that the proposed algorithm can maintain or improve the quality of Pareto fronts evolved by the state-of-the-art algorithms for feature selection.",10.1109/CEC45853.2021.9504837,feature selection;multiobjective optimization;particle swarm optimization,1.0,
Model-based multiobjective fuzzy control using a new multiobjective dynamic programming approach,Dong-Oh Kang; Zeungnam Bien,Proceedings Joint 9th IFSA World Congress and 20th NAFIPS International Conference (Cat. No. 01TH8569),2001.0,"The authors propose a model-based multiobjective fuzzy control method which is optimized online via a novel multiobjective dynamic programming. The new multiobjective dynamic programming is guaranteed to derive a Pareto optimal solution. To estimate the effect of each candidate for control input in the dynamic programming procedure, we use state-value predictors of multiple objectives based on the plant model. Temporal difference learning and supervised learning are used for update of the predictors and the plant model. As the learning proceeds, the proposed method derives the compromised solution among multiple objectives. To show the effectiveness of the proposed method, some simulation results are given.",10.1109/NAFIPS.2001.943752,,1.0,
Effects of the Use of Multiple Fuzzy Partitions on the Search Ability of Multiobjective Fuzzy Genetics-Based Machine Learning,Y. Nojima; Y. Nakashima; H. Ishibuchi,2009 International Conference of Soft Computing and Pattern Recognition,2009.0,"An important issue in the design of fuzzy rule-based systems is to find a good accuracy-complexity tradeoff. While simple fuzzy systems with high interpretability are usually not accurate, complicated fuzzy systems with high accuracy are usually not interpretable. Recently evolutionary multiobjective optimization (EMO) algorithms have been used to search for simple and accurate fuzzy systems. The main advantage of EMO-based approaches over single-objective techniques is that a number of alternative fuzzy systems with different accuracy-complexity tradeoffs can be obtained by their single run. We have already proposed a multiobjective fuzzy genetics-based machine learning (GBML) algorithm for pattern classification problems. In our GBML algorithm, multiple fuzzy partitions with different granularities are simultaneously used. This is because we usually do not know an appropriate fuzzy partition for each input variable. However, the use of multiple fuzzy partitions significantly increases the size of the search space. In this paper, we examine the effect of the use of multiple fuzzy partitions on the search ability of our multiobjective fuzzy GBML algorithms through computational experiments.",10.1109/SoCPaR.2009.74,Fuzzy genetics-based machine learning;Evolutionary multiobjective optimization;Fuzzy rules;Fuzzy partitions;Pattern classification problems,,
Multiobjective model-based optimization of diesel injection rate profile by machine learning methods,E. Immonen; M. Lauren; L. Roininen; S. Särkkä,2020 IEEE International Systems Conference (SysCon),2020.0,"The contribution of this article is to present a model-based machine learning methodology for automatic and simultaneous optimization of the power output and exhaust emissions of diesel internal combustion (IC) engines. We carry out parametric optimization of the rate profile at which fuel is injected into the cylinder for producing minimal nitrogen oxide (NOx) emissions and maximal cylinder power (nIMEP) output, on a computational simulation model of an Agco Power 44 AWI engine calibrated by measurements. Our results display the tradeoffs in reaching these two contradictory optimization objectives on the Pareto frontiers. We show that the so-called boot injection profile, which is commonly used in practice, also emerges through mathematical optimization as a reasonable compromise of the objectives.",10.1109/SysCon47679.2020.9349028,Machine learning;multiobjective optimization;diesel engine;fuel injection;NOx emissions;modeling and simulation,,
Attitudinal Choquet Integral-Based Stochastic Multicriteria Acceptability Analysis,X. Mi; H. Liao; X. -J. Zeng,2020 International Joint Conference on Neural Networks (IJCNN),2020.0,"Preference learning is a subfield of machine learning. In the preference learning-fused decision analysis, the utility values of alternatives are inducted from human decision behavior. The commonly used utility model is the weighted summation. The model assumes the independency between criteria and the same attitude towards each performance value of alternatives, which is often unrealistic in practice. To solve this problem, this study presents an ACI-SMAA (Attitudinal Choquet Integral-based Stochastic Multicriteria Acceptability Analysis) model to consider the criteria interaction and attitudinal parameter of human decision behavior in decision analysis. The ACI-SMAA model is helpful to learn human preferences and analyze latent correlations. An application example about household energy selection is used to show the applicability and validity of the proposed model.",10.1109/IJCNN48605.2020.9207443,Preference learning;Attitudinal Choquet integral;Human decision behavior;Criteria interaction;Stochastic multicriteria acceptability analysis (SMAA),,
Seasonal-trend and multiobjective ensemble learning model for water consumption forecasting,M. H. Dal Molin Ribeiro; R. G. Da Silva; J. H. K. Larcher; J. D. De Lima; V. C. Mariani; L. Dos Santos Coelho,2021 International Joint Conference on Neural Networks (IJCNN),2021.0,"Water consumption forecasting is essential for the development of efficient cities planning. Due to the non-linearities and relations of the water consumption with different factors the developing of an accurate forecasting system is challenging. This paper proposes a seasonal, trend and multiobjective ensemble learning model to forecast multi-step-ahead (one, two, and three-month-ahead) water consumption for two cities of Paraná state in Brazil. The proposed data analysis uses seasonal and trend decomposition using Loess (STL) to split the original data into the seasonal, trend, and residual components. In the next stage, the machine learning models named Support Vector Regression and Ridge Regression as well as the stochastic approach Gaussian Processes model are employed to train and predict the STL components. The previous components are weighted integrated to compose a heterogeneous ensemble learning of components obtaining the final forecasts. The elitist Non-Dominated Sorting Genetic Algorithm – version II (NSGA-II) is adopted to obtain the weights assigned to the components. The best model has better generalization out-of-sample considering the root mean squared error, mean absolute error, and mean absolute percentage error criteria in respect to minimization problem. Through developed comparisons, results showed that combining STL and multi-objective optimization with a heterogeneous ensemble learning can achieve high forecasting accuracy in comparison with some models. The framework proposed in this paper is effective to obtain reliable water consumption forecasting and can support and help future decision.",10.1109/IJCNN52387.2021.9534104,Decomposition;water consumption;ensemble learning;forecasting;multiobjective optimization,,
Multiobjective Fuzzy Genetics-Based Machine Learning for Multi-Label Classification,Y. Omozaki; N. Masuyama; Y. Nojima; H. Ishibuchi,2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),2020.0,"In multi-label classification problems, multiple class labels are assigned to each instance. Two approaches have been studied in the literature. One is a data transformation approach, which transforms a multi-label dataset into a number of singlelabel datasets. However, this approach often loses the correlation information among classes in the multi-class assignment. The other is a method adaptation approach where a conventional classification method is extended to multi-label classification. Recently, some explainable classification models for multi-label classification have been proposed. Their high interpretability has also been discussed with respect to the transparency of the classification process. Although the explainability is a well-known advantage of fuzzy systems, their applications to multi-label classification have not been well studied. Since multi-label classification problems often have vague class boundaries, fuzzy systems seem to be a promising approach to multi-label classification. In this paper, we propose a new multiobjective evolutionary fuzzy system, which can be categorized as a method adaptation approach. The proposed algorithm produces nondominated classifiers with different tradeoffs between accuracy and complexity. We examine the behavior of the proposed algorithm using synthetic multi-label datasets. We also compare the proposed algorithm with five representative algorithms. Our experimental results on real-world datasets show that the obtained fuzzy classifiers with a small number of fuzzy rules have high transparency and comparable generalization ability to the other examined multi-label classification algorithms.",10.1109/FUZZ48607.2020.9177804,multi-label classification;multiobjective fuzzy genetics-based machine learning;fuzzy rule-based classification system;method adaptation approach,1.0,
Effects of heuristic rule generation from multiple patterns in multiobjective fuzzy genetics-based machine learning,Y. Nojima; K. Watanabe; H. Ishibuchi,2015 IEEE Congress on Evolutionary Computation (CEC),2015.0,"Fuzzy genetics-based machine learning (FGBML) has frequently been used for fuzzy classifier design. It is one of the promising evolutionary machine learning (EML) techniques from the viewpoint of data mining. This is because FGBML can generate accurate classifiers with linguistically interpretable fuzzy if-then rules. Of course, a classifier with tens of thousands of if-then rules is not linguistically understandable. Thus, the complexity minimization of fuzzy classifiers should be considered together with the accuracy maximization. In previous studies, we proposed hybrid FGBML and its multiobjective formulation (MoFGBML) to handle both the accuracy maximization and the complexity minimization simultaneously. MoFGBML can obtain a number of non-dominated classifiers with different tradeoffs between accuracy and complexity. In this paper, we focus on heuristic rule generation in MoFGBML to improve the search performance. In the original heuristic rule generation, each if-then rule is generated from a randomly-selected training pattern in a heuristic manner. This operation is performed at population initialization and during evolution. To generate more generalized rules according to the training data, we propose new heuristic rule generation where each rule is generated from multiple training patterns. Through computational experiments using some benchmark data sets, we discuss the effects of the proposed operation on the search performance of our MoFGBML.",10.1109/CEC.2015.7257262,Fuzzy genetics-based machine learning;heuristic rule generation;evolutionary multiobjective optimization,5.0,
Multiobjective Particle Swarm Optimization for Feature Selection With Fuzzy Cost,Y. Hu; Y. Zhang; D. Gong,IEEE Transactions on Cybernetics,2021.0,"Feature selection (FS) is an important data processing technique in the field of machine learning. There have been various FS methods, but all assume that the cost associated with a feature is precise, which restricts their real applications. Focusing on the FS problem with fuzzy cost, a fuzzy multiobjective FS method with particle swarm optimization, called PSOMOFS, is studied in this article. The proposed method develops a fuzzy dominance relationship to compare the goodness of candidate particles and defines a fuzzy crowding distance measure to prune the elitist archive and determine the global leader of particles. Also, a tolerance coefficient is introduced into the proposed method to ensure that the Pareto-optimal solutions obtained satisfy decision makers' preferences. The developed method is used to tackle a series of the UCI datasets and is compared with three fuzzy multiobjective evolutionary methods and three typical multiobjective FS methods. Experimental results show that the proposed method can achieve feature sets with superior performances in approximation, diversity, and feature cost.",10.1109/TCYB.2020.3015756,Feature selection (FS);fuzzy cost;multiobjective optimization;particle swarm optimization (PSO),37.0,
Evolutionary Algorithm-Based and Network Architecture Search-Enabled Multiobjective Traffic Classification,X. Wang; X. Wang; L. Jin; R. Lv; B. Dai; M. He; T. Lv,IEEE Access,2021.0,"Network traffic classification technology plays an important role in network security management. However, the inherent limitations of traditional methods have become increasingly obvious, and they cannot address existing traffic classification tasks. Very recently, neural architecture search (NAS) has aroused widespread interest as a tool to automate the manual architecture construction process. To this end, this paper proposes NAS based on multiobjective evolutionary algorithms (MOEAs) to classify malicious network traffic. The main purpose is to simplify the search space by reducing the spatial ratio and number of channels of the model. In addition, the search strategy is changed in the effective search space, and the utilized strategies include EAs with the nondominated sorting genetic algorithm with the elite retention strategy (NSGA-II), strength Pareto evolutionary algorithm (SPEA-II) and multiobjective particle swarm optimization (MOPSO) to solve the formulated multiobjective NAS. Through comprehensive comparison of the population convergence times, model accuracies, Pareto optimality sets, model complexities and running speeds of the strategies, it is concluded that the model based on NSGA-II search has the best performance. The experimental results of the current machine learning algorithms and artificial learning methods based on the network are compared, showing that our method achieved better classification performance on two public datasets with a lower computational complexity, as mainly measured by FLOPs. Our approach is able to achieve 99.806% and 99.369% F1-score with 11.501 MB and 4.718 MB FLOPs on both IDS2012 and ISCX VPN dataset respectively.",10.1109/ACCESS.2021.3068267,Deep learning;multiobjective;neural architecture search;traffic classification,3.0,
Multiobjective Semisupervised Classifier Ensemble,Z. Yu; Y. Zhang; C. L. P. Chen; J. You; H. Wong; D. Dai; S. Wu; J. Zhang,IEEE Transactions on Cybernetics,2019.0,"Classification of high-dimensional data with very limited labels is a challenging task in the field of data mining and machine learning. In this paper, we propose the multiobjective semisupervised classifier ensemble (MOSSCE) approach to address this challenge. Specifically, a multiobjective subspace selection process (MOSSP) in MOSSCE is first designed to generate the optimal combination of feature subspaces. Three objective functions are then proposed for MOSSP, which include the relevance of features, the redundancy between features, and the data reconstruction error. Then, MOSSCE generates an auxiliary training set based on the sample confidence to improve the performance of the classifier ensemble. Finally, the training set, combined with the auxiliary training set, is used to select the optimal combination of basic classifiers in the ensemble, train the classifier ensemble, and generate the final result. In addition, diversity analysis of the ensemble learning process is applied, and a set of nonparametric statistical tests is adopted for the comparison of semisupervised classification approaches on multiple datasets. The experiments on 12 gene expression datasets and two large image datasets show that MOSSCE has a better performance than other state-of-the-art semisupervised classifiers on high-dimensional data.",10.1109/TCYB.2018.2824299,Ensemble learning;feature selection;multiobjective optimization;semisupervised learning,13.0,
Convex Hull-Based Multiobjective Genetic Programming for Maximizing Receiver Operating Characteristic Performance,P. Wang; M. Emmerich; R. Li; K. Tang; T. Bäck; X. Yao,IEEE Transactions on Evolutionary Computation,2015.0,"The receiver operating characteristic (ROC) is commonly used to analyze the performance of classifiers in data mining. An important topic in ROC analysis is the ROC convex hull (ROCCH), which is the least convex majorant (LCM) of the empirical ROC curve and covers potential optima for a given set of classifiers. ROCCH maximization problems have been taken as multiobjective optimization problem (MOPs) in some previous work. However, the special characteristics of ROCCH maximization problem makes it different from traditional MOPs. In this paper, the difference will be discussed in detail and a new convex hull-based multiobjective genetic programming (CH-MOGP) is proposed to solve ROCCH maximization problems. Specifically, convex hull-based without redundancy sorting (CWR-sorting) is introduced, which is an indicator-based selection scheme that aims to maximize the area under the convex hull. A novel selection procedure is also proposed based on the proposed sorting scheme. It is hypothesized that by using a tailored indicator-based selection, CH-MOGP becomes more efficient for ROC convex hull approximation than algorithms that compute all Pareto optimal points. Empirical studies are conducted to compare CH-MOGP to both existing machine learning approaches and multiobjective genetic programming (MOGP) methods with classical selection schemes. Experimental results show that CH-MOGP outperforms the other approaches significantly.",10.1109/TEVC.2014.2305671,Classification;evolutionary multiobjective algorithm;genetic programming;memetic algorithm;receiver operating characteristic (ROC) convex hull,26.0,
Multiobjectivization from two objectives to four objectives in evolutionary multi-objective optimization algorithms,H. Ishibuchi; Y. Hitotsuyanagi; Y. Nakashima; Y. Nojima,2010 Second World Congress on Nature and Biologically Inspired Computing (NaBIC),2010.0,"Multiobjectivization is an interesting idea to solve a difficult single-objective optimization problem through its reformulation as a multiobjective problem. The reformulation is performed by introducing an additional objective function or decomposing the original objective function into multiple ones. Evolutionary multiobjective optimization (EMO) algorithms are often used to solve the reformulated problem. Such an optimization approach, which is called multiobjectivization, has been used to solve difficult single-objective problems in many studies. In this paper, we discuss the use of multiobjectivization to solve two-objective problems. That is, we discuss the idea of solving a two-objective optimization problem by reformulating it as a four-objective one. In general, the increase in the number of objectives usually makes the problem more difficult for EMO algorithms. Thus the handling of two-objective problems as four-objective ones may simply lead to the deterioration in the quality of obtained non-dominated solutions. However, in this paper, we demonstrate through computational experiments that better results are obtained for some two-objective test problems by increasing the number of objectives from two to four.",10.1109/NABIC.2010.5716359,multiobjectivization;single-objective optimization;evolutionary multiobjective optimization;knapsack problems;fuzzy genetics-based machine learning,5.0,
Transfer Learning-Based Dynamic Multiobjective Optimization Algorithms,M. Jiang; Z. Huang; L. Qiu; W. Huang; G. G. Yen,IEEE Transactions on Evolutionary Computation,2018.0,"One of the major distinguishing features of the dynamic multiobjective optimization problems (DMOPs) is that optimization objectives will change over time, thus tracking the varying Pareto-optimal front becomes a challenge. One of the promising solutions is reusing “experiences” to construct a prediction model via statistical machine learning approaches. However, most existing methods neglect the nonindependent and identically distributed nature of data to construct the prediction model. In this paper, we propose an algorithmic framework, called transfer learning-based dynamic multiobjective evolutionary algorithm (EA), which integrates transfer learning and population-based EAs to solve the DMOPs. This approach exploits the transfer learning technique as a tool to generate an effective initial population pool via reusing past experience to speed up the evolutionary process, and at the same time any population-based multiobjective algorithms can benefit from this integration without any extensive modifications. To verify this idea, we incorporate the proposed approach into the development of three well-known EAs, nondominated sorting genetic algorithm II, multiobjective particle swarm optimization, and the regularity model-based multiobjective estimation of distribution algorithm. We employ 12 benchmark functions to test these algorithms as well as compare them with some chosen state-of-the-art designs. The experimental results confirm the effectiveness of the proposed design for DMOPs.",10.1109/TEVC.2017.2771451,Dimensionality reduction;domain adaption;dynamic multiobjective optimization;evolutionary algorithm (EA);transfer learning,100.0,
Multiobjective Deep Belief Networks Ensemble for Remaining Useful Life Estimation in Prognostics,C. Zhang; P. Lim; A. K. Qin; K. C. Tan,IEEE Transactions on Neural Networks and Learning Systems,2017.0,"In numerous industrial applications where safety, efficiency, and reliability are among primary concerns, condition-based maintenance (CBM) is often the most effective and reliable maintenance policy. Prognostics, as one of the key enablers of CBM, involves the core task of estimating the remaining useful life (RUL) of the system. Neural networks-based approaches have produced promising results on RUL estimation, although their performances are influenced by handcrafted features and manually specified parameters. In this paper, we propose a multiobjective deep belief networks ensemble (MODBNE) method. MODBNE employs a multiobjective evolutionary algorithm integrated with the traditional DBN training technique to evolve multiple DBNs simultaneously subject to accuracy and diversity as two conflicting objectives. The eventually evolved DBNs are combined to establish an ensemble model used for RUL estimation, where combination weights are optimized via a single-objective differential evolution algorithm using a task-oriented objective function. We evaluate the proposed method on several prognostic benchmarking data sets and also compare it with some existing approaches. Experimental results demonstrate the superiority of our proposed method.",10.1109/TNNLS.2016.2582798,Deep belief network (DBN);ensemble learning;evolutionary algorithm (EA);multiobjective;prognostics,290.0,
Learning From a Stream of Nonstationary and Dependent Data in Multiobjective Evolutionary Optimization,J. Sun; H. Zhang; A. Zhou; Q. Zhang; K. Zhang; Z. Tu; K. Ye,IEEE Transactions on Evolutionary Computation,2019.0,"Combining machine learning techniques has shown great potentials in evolutionary optimization since the domain knowledge of an optimization problem, if well learned, can be a great help for creating high-quality solutions. However, existing learning-based multiobjective evolutionary algorithms (MOEAs) spend too much computational overhead on learning. To address this problem, we propose a learning-based MOEA where an online learning algorithm is embedded within the evolutionary search procedure. The online learning algorithm takes the stream of sequentially generated solutions along the evolution as its training data. It is noted that the stream of solutions are temporal, dependent, nonstationary, and nonstatic. These data characteristics make existing online learning algorithm not suitable for the evolution data. We hence modify an existing online agglomerative clustering algorithm to accommodate these characteristics. The modified online clustering algorithm is applied to adaptively discover the structure of the Pareto optimal set; and the learned structure is used to guide new solution creation. Experimental results have shown significant improvement over four state-of-the-art MOEAs on a variety of benchmark problems.",10.1109/TEVC.2018.2865495,Evolutionary algorithms (EAs);machine learning (ML);multiobjective optimization;online agglomerative clustering,12.0,
Semi-supervised clustering using multiobjective optimization,S. Saha; A. Ekbal; A. K. Alok,2012 12th International Conference on Hybrid Intelligent Systems (HIS),2012.0,"Semi-supervised clustering uses the information of unsupervised and supervised learning to overcome the problems associated with them. Extracted information are given in the form of class labels and data distribution during clustering process. In this paper the problem of semi-supervised clustering is formulated under the framework of multiobjective optimization (MOO). Thereafter, a multiobjective based clustering technique is extended to solve the semi-supervised clustering problem. The newly developed semi-supervised multiobjective clustering algorithm (Semi-GenClustMOO), is used for appropriate partitioning of data into appropriate number of clusters. Four objective functions are optimized, out of which first three use some unsupervised information and the last one uses supervised information. These four objective functions represent, respectively, the, total compactness of the partitioning, total symmetry present in the clusters, cluster connectedness and Adjust Rand Index. These four objective functions are optimized simultaneously using AMOSA, a newly developed simulated annealing based multiobjective optimization method. Results show that it can easily detect the appropriate number of clusters as well as the appropriate partitioning from data sets having either well-separated clusters of any shape or symmetrical clusters with or without overlaps. Seven artificial and four real-life data sets have been used for evaluation to show the effectiveness of the Semi-GenClustMOO technique. In each case class information of 10% randomly chosen data point is known to us <sup>1</sup>.",10.1109/HIS.2012.6421361,Semi-supervised clustering;Multiobjective optimization;Cluster validity index;Adjusted Rand Index (ARI);Sym-index;Con-index;I-Index;AMOSA,11.0,
Survey of Multiobjective Evolutionary Algorithms for Data Mining: Part II,A. Mukhopadhyay; U. Maulik; S. Bandyopadhyay; C. A. C. Coello,IEEE Transactions on Evolutionary Computation,2014.0,"This paper is the second part of a two-part paper, which is a survey of multiobjective evolutionary algorithms for data mining problems. In Part I , multiobjective evolutionary algorithms used for feature selection and classification have been reviewed. In this part, different multiobjective evolutionary algorithms used for clustering, association rule mining, and other data mining tasks are surveyed. Moreover, a general discussion is provided along with scopes for future research in the domain of multiobjective evolutionary algorithms for data mining.",10.1109/TEVC.2013.2290082,Association rule mining;biclustering;clustering;ensemble learning;multiobjective evolutionary algorithms,114.0,
Decomposition-Based Evolutionary Multiobjective Optimization to Self-Paced Learning,M. Gong; H. Li; D. Meng; Q. Miao; J. Liu,IEEE Transactions on Evolutionary Computation,2019.0,"Self-paced learning (SPL) is a recently proposed paradigm to imitate the learning process of humans/animals. SPL involves easier samples into training at first and then gradually takes more complex ones into consideration. Current SPL regimes incorporate a self-paced (SP) regularizer into the learning objective with a gradually increasing pace parameter. Therefore, it is difficult to obtain the solution path of the SPL regime and determine where to optimally stop this increasing process. In this paper, a multiobjective SPL method is proposed to optimize the loss function and the SP regularizer simultaneously. A decomposition-based multiobjective particle swarm optimization algorithm is used to simultaneously optimize the two objectives for obtaining the solutions. In the proposed method, a polynomial soft weighting regularizer is proposed to penalize the loss. Theoretical studies are conducted to show that the previous regularizers are roughly particular cases of the proposed polynomial soft weighting regularizer family. Then an implicit decomposition method is proposed to search the solutions with respect to the sample number involved into training. A set of solutions can be obtained by the proposed method and naturally constitute the solution path of the SPL regime. Then a satisfactory solution can be naturally obtained from these solutions by utilizing some effective tools in evolutionary multiobjective optimization. Experiments on matrix factorization and classification problems demonstrate the effectiveness of the proposed technique.",10.1109/TEVC.2018.2850769,Decomposition;machine learning;multiobjective optimization;self-paced learning (SPL),13.0,
An Efficient Multiobjective Design Optimization Method for a PMSLM Based on an Extreme Learning Machine,J. Song; F. Dong; J. Zhao; H. Wang; Z. He; L. Wang,IEEE Transactions on Industrial Electronics,2019.0,"This paper focuses on the multiobjective design optimization of the permanent magnet synchronous linear motors (PMSLMs), which are applied to a high-precision laser engraving machine. A novel efficient multiobjective design optimization method for a PMSLM is proposed to achieve optimal performances as indicated by high average thrust, low thrust ripple, and low total harmonic distortion at different running speeds. First, based on the finite-element analysis (FEA) data, a regression machine learning algorithm, called an extreme learning machine (ELM), is introduced to solve the calculation modeling problem by mapping out the nonlinear and complex relationship between input structural factors and output motor performances. Comparative simulation experiments conducted using the traditional analytical modeling method and another machine learning modeling method, i.e., support vector machine, confirm the superiority of the ELM. Then, a new bionic intelligent optimization algorithm, called the gray wolf optimizer algorithm, is used to search the best optimization performances and structural parameters by performing iteration optimization calculation for multiobjective functions. Finally, FEA and prototype motor experiments prove the effectiveness and validity of the proposed method.",10.1109/TIE.2018.2835413,Extreme learning machine (ELM);finite-element analysis (FEA);gray wolf optimizer algorithm (GWOA);multiobjective design optimization;permanent magnet synchronous linear motors (PMSLMs);support vector machine (SVM),50.0,
On a Multiobjective Training Algorithm for RBF Networks Using Particle Swarm Optimization,G. R. L. Silva; D. A. G. Vieira; A. C. Lisboa; V. Palade,2010 22nd IEEE International Conference on Tools with Artificial Intelligence,2010.0,"This paper presents a novel algorithm for multiobjective training of Radial Basis Function (RBF) networks based on least-squares and Particle Swarm Optimization methods. The formulation is based on the fundamental concept that supervised learning is a bi-objective optimization problem, in which two conflicting objectives should be minimized. The objectives are related to the empirical training error and the machine complexity. The training is done in three steps: i) a conventional minimization of the training error, ii) multiobjective least-squares optimization for the linear parameters and, iii) particle swarm optimization for the nonlinear parameters. Some results are presented and they show the effectiveness of the proposed approach.",10.1109/ICTAI.2010.112,radial basis network;multiobjective least squares;particle swarm optimization,,
Multicriteria-Based Active Discriminative Dictionary Learning for Scene Recognition,C. Zheng; Y. Yi; M. Qi; F. Liu; C. Bi; J. Wang; J. Kong,IEEE Access,2018.0,"Scene recognition is a significant and challenging problem in the field of computer vision. One of the principal bottlenecks in applying machine learning techniques to scene recognition tasks is the requirement of a large number of labeled training data. However, labeling massive training data manually (especially labeling images and videos) is very expensive in terms of human time and effort. In this paper, we present a novel multicriteria-based active discriminative dictionary learning (M-ADDL) algorithm to reduce the human annotation effort and create a robust scene recognition model. The M-ADDL algorithm possesses three advantages. First, M-ADDL introduces an active learning strategy into the discriminative dictionary learning model so that the performance of discriminative dictionary learning can be improved when the number of labeled samples is small. Second, different from most existing active learning methods that measure either the informativeness or representativeness of unlabeled samples to select useful samples for expanding the training dataset, M-ADDL employs both informativeness and representativeness to query useful unlabeled samples and utilizes the manifold-preserving ability of unlabeled samples as an additional sample selection criterion. Finally, a more effective representativeness criterion is presented based on the reconstruction coefficients of the samples. The experimental results of four standard scene recognition databases demonstrate the feasibility and validity of the proposed M-ADDL algorithm.",10.1109/ACCESS.2017.2786672,Active learning;dictionary learning;multicriteria of sample selection;scene recognition,9.0,
A reinforcement neuro-fuzzy combiner for multiobjective control,Chin-Teng Lin; I-Fang Chung,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",1999.0,"This paper proposes a neuro-fuzzy combiner (NFC) with reinforcement learning capability for solving multiobjective control problems. The proposed NFC can combine n existing low-level controllers in a hierarchical way to form a multiobjective fuzzy controller. It is assumed that each low-level (fuzzy or nonfuzzy) controller has been well designed to serve a particular objective. The role of the NFC is to fuse the n actions decided by the n low-level controllers and determine a proper action acting on the environment (plant) at each time step. Hence, the NFC can combine low-level controllers and achieve multiple objectives (goals) at once. The NFC acts like a switch that chooses a proper action from the actions of low-level controllers according to the feedback information from the environment. In fact, the NFC is a soft switch; it allows more than one low-level actions to be active with different degrees through fuzzy combination at each time step. An NFC can be designed by the trial-and-error approach if enough a priori knowledge is available, or it can be obtained by supervised learning if precise input/output training data are available. In the more practical cases when there is no instructive teaching information available, the NFC can learn by itself using the proposed reinforcement learning scheme. Adopted with reinforcement learning capability, the NFC can learn to achieve desired multiobjectives simultaneously through the rough reinforcement feedback from the environment, which contains only critic information such as ""success (good)"" or ""failure (bad)"" for each desired objective. Computer simulations have been conducted to illustrate the performance and applicability of the proposed architecture and learning scheme.",10.1109/3477.809028,,10.0,
Multiobjective Operation Optimization of Continuous Annealing Based on Data Analytics,B. Zhang; Z. Wang; X. Wang,IEEE Access,2019.0,"Continuous annealing production process generally consists of multiple complex processes that are coupled to each other, and each process contains many control variables. It is difficult to establish a precise mechanism model of the production process. The operators mainly set these control variables based on past production experience, which often result in great fluctuations of product quality (even unqualified products) and high energy consumption. This in turn significantly affected production cost and economic benefits of the cold rolling mill. To efficiently handle this problem, an ensemble learning modeling method based on production data is first proposed for this production process, and then, a multiobjective operation optimization model is established to optimize the operation of continuous annealing production process. Finally, an improved multiobjective differential evolution algorithm based on search process memory is developed to solve this model and achieve the optimal setting of control variables. The computational results on both benchmark problems and practical problems illustrate that the proposed algorithm is superior to some powerful multiobjective evolutionary algorithms in the literature and it can effectively achieve good setting of control variables for the continuous annealing production process.",10.1109/ACCESS.2019.2911087,Multiobjective operation optimization;differential evolutions;data analytics,5.0,
Deep Learning for Generalized Multiobjective Optimization of Metamaterials,R. P. Jenkins; P. J. O’Connor; S. D. Campbell; P. L. Werner; D. H. Werner,2020 Fourteenth International Congress on Artificial Materials for Novel Wave Phenomena (Metamaterials),2020.0,"The inverse-design of metamaterials often requires full-wave evaluation of an enormous number of different candidate designs, which can be extremely time consuming and, in many cases of practical interest, even intractable. By introducing deep learning at an early stage in the design process, a generalized network can be paired with multiobjective optimization to rapidly solve a variety of complex electromagnetic metamaterial design problems.",10.1109/Metamaterials49557.2020.9285137,,,
Evolutionary Multiobjective Feature Selection for Sentiment Analysis,A. Deniz; M. Angin; P. Angin,IEEE Access,2021.0,"Sentiment analysis is one of the prominent research areas in data mining and knowledge discovery, which has proven to be an effective technique for monitoring public opinion. The big data era with a high volume of data generated by a variety of sources has provided enhanced opportunities for utilizing sentiment analysis in various domains. In order to take best advantage of the high volume of data for accurate sentiment analysis, it is essential to clean the data before the analysis, as irrelevant or redundant data will hinder extracting valuable information. In this paper, we propose a hybrid feature selection algorithm to improve the performance of sentiment analysis tasks. Our proposed sentiment analysis approach builds a binary classification model based on two feature selection techniques: an entropy-based metric and an evolutionary algorithm. We have performed comprehensive experiments in two different domains using a benchmark dataset, Stanford Sentiment Treebank, and a real-world dataset we have created based on World Health Organization (WHO) public speeches regarding COVID-19. The proposed feature selection model is shown to achieve significant performance improvements in both datasets, increasing classification accuracy for all utilized machine learning and text representation technique combinations. Moreover, it achieves over 70% reduction in feature size, which provides efficiency in computation time and space.",10.1109/ACCESS.2021.3118961,Binary classification;evolutionary computation;feature selection;multiobjective optimization;sentiment analysis,,
Classification rule mining approach based on multiobjective optimization,T. Sağ; H. Kahramanli,2017 International Artificial Intelligence and Data Processing Symposium (IDAP),2017.0,"In this paper, a novel approach for classification rule mining is presented. The remarkable relationship between the rule extraction procedure and the concept of multiobjective optimization is emphasized. The range values of features composing the rules are handled as decision variables in the modelled multiobjective optimization problem. The proposed method is applied to three well-known datasets in literature. These are Iris, Haberman's Survival Data and Pima Indians Diabetes Datasets obtained from machine learning repository of University of California at Irvine (UCI). The classification rules are extracted with 100% accuracy for all datasets. These experimental results are the best outcomes found in literature so far.",10.1109/IDAP.2017.8090264,Rule extraction;multiobjective optimization;genetic algorithms,,
A Multiobjective Approach to Classification in Drug Discovery,P. Echtenbruck; M. Emmerich; B. Naujoks,2019 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB),2019.0,"Classification based on machine learning algorithms is a widely used technique in contemporary in silico methods for drug discovery. However, typically the performance of the classification tool is evaluated based on a scalar performance score and essential information, such as the balance between false positive rates (FPRs)and false negative rates (FNRs)is not directly assessed. Moreover, there might be a large number of molecular features that are not relevant for the classification task and merely slow down the computations or add noise to the learning process. In this paper we adopt an approach that previously was used for the classification of text messages (spam/no-spam)to the classification of drug compounds (active/inactive). By considering the minimization of the classification costs (FPR, FNR)and the minimization of the number of features as separate optimization tasks, we demonstrate that it is possible to develop a more informative and versatile tool for drug discovery. We show, how to derive and evaluate 2-D and 3-D Pareto fronts for the classification of small compounds in active and non-active (similar studies could be conducted for toxic/non-toxic classification, and on other chemically relevant properties). We demonstrate the applicability of the method on a small data set for bio-activity prediction of ligands.",10.1109/CIBCB.2019.8791463,Bioactivity Prediction;Machine Learning;Multiobjective Optimization;Hyperparameter Tuning;In-Silico Drug Discovery,3.0,
Application of Multiobjective Genetic Programming to the Design of Robot Failure Recognition Systems,Y. Zhang; P. I. Rockett,IEEE Transactions on Automation Science and Engineering,2009.0,"We present an evolutionary approach using multiobjective genetic programming (MOGP) to derive optimal feature extraction preprocessing stages for robot failure detection. This data-driven machine learning method is compared both with conventional (nonevolutionary) classifiers and a set of domain-dependent feature extraction methods. We conclude MOGP is an effective and practical design method for failure recognition systems with enhanced recognition accuracy over conventional classifiers, independent of domain knowledge.",10.1109/TASE.2008.2004414,Autonomous robots;failure recognition;feature extraction;multiobjective genetic programming (MOGP),4.0,
Multiobjective evolutionary optimization of quadratic Takagi-Sugeno fuzzy rules for remote bathymetry estimation,M. Cococcioni; B. Lazzerini,OCEANS 2015 - Genova,2015.0,"In this work we tackle the problem of bathymetry estimation using: i) a multispectral optical image of the region of interest, and ii) a set of in situ measurements. The idea is to learn the relation that between the reflectances and the depth using a supervised learning approach. In particular, quadratic Takagi-Sugeno fuzzy rules are used to model this relation. The rule base is optimized by means of a multiobjective evolutionary algorithm. To the best of our knowledge this work represents the first use of a quadratic Takagi-Sugeno fuzzy system optimized by a multiobjective evolutionary algorithm with bounded complexity, i.e., able to control the complexity of the consequent part of second-order fuzzy rules. This model has an outstanding modeling power, without inheriting the drawback of complexity due to the use of quadratic functions (which have complexity that scales quadratically with the number of inputs). This opens the way to the use of the proposed approach even for medium/high dimensional problems, like in the case of hyper-spectral images.",10.1109/OCEANS-Genova.2015.7271447,Bathymetry estimation;quadratic Takagi-Sugeno fuzzy rules;remotely sensed optical images;multiobjective evolutionary algorithms,1.0,
FedMCCS: Multicriteria Client Selection Model for Optimal IoT Federated Learning,S. Abdulrahman; H. Tout; A. Mourad; C. Talhi,IEEE Internet of Things Journal,2021.0,"As an alternative centralized systems, which may prevent data to be stored in a central repository due to its privacy and/or abundance, federated learning (FL) is nowadays a game changer addressing both privacy and cooperative learning. It succeeds in keeping training data on the devices, while sharing locally computed then globally aggregated models throughout several communication rounds. The selection of clients participating in FL process is currently at complete/quasi randomness. However, the heterogeneity of the client devices within Internet-of-Things environment and their limited communication and computation resources might fail to complete the training task, which may lead to many discarded learning rounds affecting the model accuracy. In this article, we propose FedMCCS, a multicriteria-based approach for client selection in FL. All of the CPU, memory, energy, and time are considered for the clients resources to predict whether they are able to perform the FL task. Particularly, in each round, the number of clients in FedMCCS is maximized to the utmost, while considering each client resources and its capability to successfully train and send the needed updates. The conducted experiments show that FedMCCS outperforms the other approaches by: 1) reducing the number of communication rounds to reach the intended accuracy; 2) maximizing the number of clients; 3) handling the least number of discarded rounds; and 4) optimizing the network traffic.",10.1109/JIOT.2020.3028742,Bilevel optimization;cooperative learning;federated learning (FL);Internet of Things (IoT);linear regression;machine learning;multicriteria selection;privacy;resource management;resource utilization prediction,7.0,
A Robo-Advisor Design using Multiobjective RankNets with Gated Neural Network Structure,P. Wang; C. Liu; Y. Yang; S. Huang,2019 IEEE International Conference on Agents (ICA),2019.0,"With rapid developments in deep learning and financial technology, a customized robo-advisory service based on novel artificial intelligence techniques has been widely adopted to realize financial inclusion. This study proposes a novel robo-advisor system that integrates trend prediction, portfolio management, and a recommendation mechanism. A gated neural network structure combining three multiobjective RankNet kernels could rank target financial products and recommend the top-n securities to investors. The gated neural network learns to choose or weigh each RankNet for incorporating the most important partial network inputs, such as earnings per share, market index, and hidden information from the time series. Experimental results indicate that the recommendation results of our proposed robo-advisor based on a gated neural network and multiobjective RankNets can outperform existing models.",10.1109/AGENTS.2019.8929188,learning preferences;rankings;deep learning,2.0,
Multiobjective Intelligent Energy Management for a Microgrid,A. Chaouachi; R. M. Kamel; R. Andoulsi; K. Nagasaka,IEEE Transactions on Industrial Electronics,2013.0,"In this paper, a generalized formulation for intelligent energy management of a microgrid is proposed using artificial intelligence techniques jointly with linear-programming-based multiobjective optimization. The proposed multiobjective intelligent energy management aims to minimize the operation cost and the environmental impact of a microgrid, taking into account its preoperational variables as future availability of renewable energies and load demand (LD). An artificial neural network ensemble is developed to predict 24-h-ahead photovoltaic generation and 1-h-ahead wind power generation and LD. The proposed machine learning is characterized by enhanced learning model and generalization capability. The efficiency of the microgrid operation strongly depends on the battery scheduling process, which cannot be achieved through conventional optimization formulation. In this paper, a fuzzy logic expert system is used for battery scheduling. The proposed approach can handle uncertainties regarding to the fuzzy environment of the overall microgrid operation and the uncertainty related to the forecasted parameters. The results show considerable minimization on operation cost and emission level compared to literature microgrid energy management approaches based on opportunity charging and Heuristic Flowchart (HF) battery management.",10.1109/TIE.2012.2188873,Fuzzy logic (FL);microgrid;multiobjective intelligent energy management (MIEM);neural network ensemble (NNE);short-term forecasting,439.0,
Predicting Fault Proneness of Classes Trough a Multiobjective Particle Swarm Optimization Algorithm,A. B. de Carvalho; A. Pozo; S. Vergilio; A. Lenz,2008 20th IEEE International Conference on Tools with Artificial Intelligence,2008.0,"Software testing is a fundamental software engineering activity for quality assurance that is also traditionally very expensive. To reduce efforts of testing strategies, some design metrics have been used to predict the fault-proneness of a software class or module. Recent works have explored the use of machine learning (ML) techniques for fault prediction. However most used ML techniques can not deal with unbalanced data and their results usually have a difficult interpretation. Because of this, this paper introduces a multi-objective particle swarm optimization (MOPSO) algorithm for fault prediction. It allows the creation of classifiers composed by rules with specific properties by exploring Pareto dominance concepts. These rules are more intuitive and easier to understand because they can be interpreted independently one of each other. Furthermore, an experiment using the approach is presented and the results are compared to the other techniques explored in the area.",10.1109/ICTAI.2008.76,Particle Swarm Optimization;Multiobjective Optimization;Fault prediction;Software mining,12.0,
Design and Development of a Benchmark for Dynamic Multi-objective Optimisation Problem in the Context of Deep Reinforcement Learning,M. M. Hasan; K. Lwin; A. Shabut; M. A. Hossain,2019 22nd International Conference on Computer and Information Technology (ICCIT),2019.0,"Different benchmarks have played an important role in analysing algorithms for dynamic multi-objective optimisation problems. According to the literature, there are several benchmarks to deal with the dynamic multi-objective optimisation problems, especially in the evolutionary approaches. In this study, a comprehensive review has been done regarding the existing benchmarks in the single objective and multi-objective reinforcement learning (MORL) settings. To the best of our knowledge, there is no benchmark in the context of dynamic multiobjective reinforcement learning (DMORL). Therefore, this study has addressed this gap by applying the existing knowledge to propose a benchmark which may help to investigate the performance of different algorithms. It can also support to understand the dynamics while objectives are conflicting with each other and deal with the constraints and problem parameters that change over time. The proposed benchmark is the modified version of the deep-sea treasure hunt problem where several features such as changing parameters and objectives have been integrated to support the dynamics in a multi-objective environment. This paper highlights the methodology part of designing and developing a benchmark.",10.1109/ICCIT48885.2019.9038529,machine learning;deep reinforcement learning;multiobjective reinforcement learning;artificial intelligence;Markov decision process;dynamic multi-objective optimisation,,
Designing Fuzzy Ensemble Classifiers by Evolutionary Multiobjective Optimization with an Entropy-Based Diversity Criterion,Y. Nojima; H. Ishibuchi,2006 Sixth International Conference on Hybrid Intelligent Systems (HIS'06),2006.0,"In this paper, we propose a multi-classifier coding scheme and an entropy-based diversity criterion in evolutionary multiobjective optimization algorithms for the design of fuzzy ensemble classifiers. In a multi-classifier coding scheme, an ensemble classifier is coded as an integer string. Each string is evaluated by using its accuracy and diversity. We use two accuracy criteria. One is the overall classification rate of the string as an ensemble classifier. The other is the average classification rate of component classifiers in the ensemble classifier. As a diversity criterion, we use the entropy of outputs from component classifiers in the ensemble classifier. We examine four formulations based on the above criteria through computational experiments on benchmark data sets in the UCI machine learning repository. The experimental results show the effectiveness of the multi-classifier coding scheme and the entropy-based diversity criterion.",10.1109/HIS.2006.264942,,9.0,
An Adaptive Multiobjective Approach to Evolving ART Architectures,A. Kaylani; M. Georgiopoulos; M. Mollaghasemi; G. C. Anagnostopoulos; C. Sentelle; M. Zhong,IEEE Transactions on Neural Networks,2010.0,"In this paper, we present the evolution of adaptive resonance theory (ART) neural network architectures (classifiers) using a multiobjective optimization approach. In particular, we propose the use of a multiobjective evolutionary approach to simultaneously evolve the weights and the topology of three well-known ART architectures; fuzzy ARTMAP (FAM), ellipsoidal ARTMAP (EAM), and Gaussian ARTMAP (GAM). We refer to the resulting architectures as MO-GFAM, MO-GEAM, and MO-GGAM, and collectively as MO-GART. The major advantage of MO-GART is that it produces a number of solutions for the classification problem at hand that have different levels of merit [accuracy on unseen data (generalization) and size (number of categories created)]. MO-GART is shown to be more elegant (does not require user intervention to define the network parameters), more effective (of better accuracy and smaller size), and more efficient (faster to produce the solution networks) than other ART neural network architectures that have appeared in the literature. Furthermore, MO-GART is shown to be competitive with other popular classifiers, such as classification and regression tree (CART) and support vector machines (SVMs).",10.1109/TNN.2009.2037813,ARTMAP;category proliferation;classification;genetic algorithms (GAs);genetic operators;machine learning,31.0,
Design Space Exploration based on multiobjective genetic algorithms and clustering-based high-level estimation,L. G. A. Martins; E. Marques,2013 23rd International Conference on Field programmable Logic and Applications,2013.0,"A desirable characteristic in high-level synthesis (HLS) is fast search and analysis of implementation alternatives with low or none intervention. This process is known as Design Space Exploration (DSE) and it requires an efficient search method. The employment of intelligent techniques like evolutionary algorithms has been investigated as an alternative to DSE. They turn possible to reduce the search time through selection of higher potential regions of the solution space. We propose here the development of a DSE approach based on a multiobjective evolutionary algorithm (MOEA) and machine learning techniques. It must be employed to indicate the code transformations and architectural parameters adopted in design solution. Furthermore, DSE will use a high-level estimator model to evaluate candidate solutions. Such model must be able to provide a good estimation of energy consumption and execution time at early stages of design.",10.1109/FPL.2013.6645608,,1.0,
An Advanced Home Energy Management System Facilitated by Nonintrusive Load Monitoring With Automated Multiobjective Power Scheduling,Y. Lin; M. Tsai,IEEE Transactions on Smart Grid,2015.0,"Nowadays, electricity energy demands requested from down-stream sectors in a smart grid constantly increase. One way to meet those demands is use of home energy management systems (HEMS). By effectively scheduling major household appliances in response to demand response (DR) schemes, residents can save their electricity bills. In this paper, an advanced HEMS facilitated by a nonintrusive load monitoring (NILM) technique with an automated nondominated sorting genetic algorithm-II (NSGA-II)-based multiobjective in-home power scheduling mechanism is proposed. The NILM as an electricity audit is able to nonintrusively estimate power consumed by each of monitored major household appliances at a certain period of time. Data identified by the NILM are very useful for DR implementation. For DR implementation, the NSGA-II-based multiobjective in-home power scheduling mechanism autonomously and meta-heuristically schedules monitored and enrolled major household appliances without user intervention. It is based on an analysis of the NILM with historical data with past trends. The experimental results reported in this paper reveal that the proposed advanced HEMS with the NILM assessed in a real-house environment with uncertainties is workable and feasible.",10.1109/TSG.2015.2388492,Data fusion;demand response (DR);energy management system;ensemble learning;nonintrusive load monitoring (NILM);power scheduling;smart grid;smart house;Data fusion;demand response (DR);energy management system;ensemble learning;nonintrusive load monitoring (NILM);power scheduling;smart grid;smart house,107.0,
Multicriteria approaches based on a new discrimination criterions for feature selection,H. Chamlal; T. Ouaderhman; B. E. Mourtji,2021 Fifth International Conference On Intelligent Computing in Data Sciences (ICDS),2021.0,"In machine learning, the presence of a large number of explanatory features leads to a greater complexity of the algorithms and to a strong degradation in the performance of the prediction models. For this, a selection of an optimal discriminating subset is necessary. Feature selection can be viewed as a multi-objective optimization problem, since, in the simplest case, it involves feature subset size minimization and performance maximization. In our work, we introduced approaches based on the theory of preordering that use association techniques between two heterogeneous features, introduced by S.Chah, H. Chamla1 [14], and between several ones, the first approach consists in selecting at each step a variable by maximizing two criteria, eliminating the effect of the variable in question, and moving on to the next step, the procedure stops when the second criterion is no longer significant. In the second approach, we adapted SFFS (Sequential Floating Forward Selection) and SFBS (Sequential Floating Backward Selection) to our problem. These methods consist of using the SFS (Sequential Forward Selection) algorithm once so as to add 1 features, then using the SBS (Sequential Backward Selection) algorithm for r times in order to remove r features. These steps are then repeated until the stop criterion is obtained. As the optimal values of these parameters cannot be determined theoretically, we proposed to leave them floating during the selection process in order to get as close as possible to the optimal solution. The proposed approaches were tested on several datasets and the experimental results were very satisfactory.",10.1109/ICDS53782.2021.9626744,Heterogeneous variables;preordonnance;concordance measures;feature selection;discrimination;TOPSIS,,
Multiobjective fuzzy genetics-based machine learning based on MOEA/D with its modifications,Y. Nojima; K. Arahari; S. Takemura; H. Ishibuchi,2017 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),2017.0,"Various evolutionary multiobjective optimization (EMO) algorithms have been used in the field of evolutionary fuzzy systems (EFS), because EMO algorithms can easily handle multiple objective functions such as the accuracy maximization and complexity minimization for fuzzy system design. Most EMO algorithms used in EFS are Pareto dominance-based algorithms such as NSGA-II, SPEA2, and PAES. There are a few studies where other types of EMO algorithms are used in EFS. In this paper, we apply a multiobjective evolutionary algorithm based on decomposition called MOEA/D to EFS for fuzzy classifier design. MOEA/D is one of the most well-known decomposition-based EMO algorithms. The key idea is to divide a multiobjective optimization problem into a number of single-objective problems using a set of uniformly distributed weight vectors in a scalarizing function. We propose a new scalarizing function called an accuracy-oriented function (AOF) which is specialized for classifier design. We examine the effects of using AOF in MOEA/D on the search ability of our multiobjective fuzzy genetics-based machine learning (GBML). We also examine the synergy effect of MOEA/D with AOF and parallel distributed implementation of fuzzy GBML on the generalization ability.",10.1109/FUZZ-IEEE.2017.8015749,Fuzzy classifier design;evolutionary fuzzy systems;MOEA/D;accuracy-oriented scalarizingfunction,3.0,
Multiobjective Optimization for Model Selection in Kernel Methods in Regression,D. You; C. F. Benitez-Quiroz; A. M. Martinez,IEEE Transactions on Neural Networks and Learning Systems,2014.0,"Regression plays a major role in many scientific and engineering problems. The goal of regression is to learn the unknown underlying function from a set of sample vectors with known outcomes. In recent years, kernel methods in regression have facilitated the estimation of nonlinear functions. However, two major (interconnected) problems remain open. The first problem is given by the bias-versus-variance tradeoff. If the model used to estimate the underlying function is too flexible (i.e., high model complexity), the variance will be very large. If the model is fixed (i.e., low complexity), the bias will be large. The second problem is to define an approach for selecting the appropriate parameters of the kernel function. To address these two problems, this paper derives a new smoothing kernel criterion, which measures the roughness of the estimated function as a measure of model complexity. Then, we use multiobjective optimization to derive a criterion for selecting the parameters of that kernel. The goal of this criterion is to find a tradeoff between the bias and the variance of the learned function. That is, the goal is to increase the model fit while keeping the model complexity in check. We provide extensive experimental evaluations using a variety of problems in machine learning, pattern recognition, and computer vision. The results demonstrate that the proposed approach yields smaller estimation errors as compared with methods in the state of the art.",10.1109/TNNLS.2013.2297686,Kernel methods;kernel optimization;optimization;Pareto optimality;regression.;Kernel methods;kernel optimization;optimization;Pareto optimality;regression,12.0,
Evolutionary Multiobjective Ensemble Learning Based on Bayesian Feature Selection,Huanhuan Chen; Xin Yao,2006 IEEE International Conference on Evolutionary Computation,2006.0,"This paper proposes to incorporate evolutionary multiobjective algorithm and Bayesian Automatic Relevance Determination (ARD) to automatically design and train ensemble. The algorithm determines almost all the parameters of ensemble automatically. Our algorithm adopts different feature subsets, selected by Bayesian ARD, to maintain accuracy and promote diversity among individual NNs in an ensemble. The multiobjective evaluation of the fitness of the networks encourages the networks with lower error rate and fewer features. The proposed algorithm is applied to several real-world classification problems and in all cases the performance of the method is better than the performance of other ensemble construction algorithms.",10.1109/CEC.2006.1688318,,4.0,
Multiobjective Reinforcement Learning for Cognitive Satellite Communications Using Deep Neural Network Ensembles,P. V. R. Ferreira; R. Paffenroth; A. M. Wyglinski; T. M. Hackett; S. G. Bilén; R. C. Reinhart; D. J. Mortensen,IEEE Journal on Selected Areas in Communications,2018.0,"Future spacecraft communication subsystems will potentially benefit from software-defined radios controlled by artificial intelligence algorithms. In this paper, we propose a novel radio resource allocation algorithm leveraging multiobjective reinforcement learning and artificial neural network ensembles able to manage available resources and conflicting mission-based goals. The uncertainty in the performance of thousands of possible radio parameter combinations and the dynamic behavior of the radio channel over time producing a continuous multidimensional state–action space requires a fixed-size memory continuous state–action mapping instead of the traditional discrete mapping. In addition, actions need to be decoupled from states in order to allow for online learning, performance monitoring, and resource allocation prediction. The proposed approach leverages the authors’ previous research on constraining decisions predicted to have poor performance through ”virtual environment exploration.” The simulation results show the performance for different communication mission profiles, and accuracy benchmarks are provided for the future research reference. The proposed approach constitutes part of the core cognitive engine proof-of-concept delivered to the NASA John H. Glenn Research Center’s SCaN Testbed radios on-board the International Space Station.",10.1109/JSAC.2018.2832820,Satellite communication;machine learning;artificial intelligence;reinforcement learning;neural networks;cognitive radio;space communication;SCaN Testbed;NASA GRC,20.0,
Difficulties in choosing a single final classifier from non-dominated solutions in multiobjective fuzzy genetics-based machine learning,H. Ishibuchi; Y. Nojima,2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS),2013.0,"A large number of non-dominated fuzzy rule-based classifiers are often obtained by applying a multiobjective fuzzy genetics-based machine learning (MoFGBML) algorithm to a pattern classification problem. The obtained set of non-dominated classifiers can be used to analyze their accuracy-interpretability tradeoff relation. One important issue, which has not been discussed in many studies on MoFGBML, is the choice of a single final classifier from a large number of non-dominated classifiers. The selected classifier is used for the classification of new input patterns. In this paper, we focus on this important research issue: classifier selection from a large number of non-dominated fuzzy rule-based classifiers. In general, it is not easy to choose a single final solution from non-dominated solutions in multiobjective optimization. This is because further information on the decision maker's preference is needed to choose the single final solution. In addition to this general difficulty in multiobjective optimization, MoFGBML has its own difficulty in classifier selection, which is the difference between training data accuracy and test data accuracy. While our true objective is to maximize the test data accuracy (i.e., classifier's generalization ability), only the training data accuracy is available for fitness evaluation and classifier selection. In this paper, we discuss why classifier selection is difficult in MoFGBML.",10.1109/IFSA-NAFIPS.2013.6608572,,1.0,
Active Learning of Pareto Fronts,P. Campigotto; A. Passerini; R. Battiti,IEEE Transactions on Neural Networks and Learning Systems,2014.0,"This paper introduces the active learning of Pareto fronts (ALP) algorithm, a novel approach to recover the Pareto front of a multiobjective optimization problem. ALP casts the identification of the Pareto front into a supervised machine learning task. This approach enables an analytical model of the Pareto front to be built. The computational effort in generating the supervised information is reduced by an active learning strategy. In particular, the model is learned from a set of informative training objective vectors. The training objective vectors are approximated Pareto-optimal vectors obtained by solving different scalarized problem instances. The experimental results show that ALP achieves an accurate Pareto front approximation with a lower computational effort than state-of-the-art estimation of distribution algorithms and widely known genetic techniques.",10.1109/TNNLS.2013.2275918,Active learning;Gaussian process regression;multiobjective optimization;uncertainty sampling,23.0,
Distributed Pareto Optimization for Large-Scale Noisy Subset Selection,C. Qian,IEEE Transactions on Evolutionary Computation,2020.0,"Subset selection, aiming to select the best subset from a ground set with respect to some objective function, is a fundamental problem with applications in many areas, such as combinatorial optimization, machine learning, data mining, computer vision, information retrieval, etc. Along with the development of data collection and storage, the size of the ground set grows larger. Furthermore, in many subset selection applications, the objective function evaluation is subject to noise. We thus study the large-scale noisy subset selection problem in this paper. The recently proposed DPOSS algorithm based on multiobjective evolutionary optimization is a powerful distributed solver for large-scale subset selection. Its performance, however, has been only validated in the noise-free environment. In this paper, we first prove its approximation guarantee under two common noise models, i.e., multiplicative noise and additive noise, disclosing that the presence of noise degrades the performance of DPOSS largely. Next, we propose a new distributed multiobjective evolutionary algorithm called DPONSS for large-scale noisy subset selection. We prove that the approximation guarantee of DPONSS under noise is significantly better than that of DPOSS. We also conduct experiments on the application of sparse regression, where the objective evaluation is often estimated using a sample data, bringing noise. The results on various real-world data sets, whose size can reach millions, clearly show the excellent performance of DPONSS.",10.1109/TEVC.2019.2929555,Distributed algorithms;experimental studies;large-scale;multiobjective evolutionary algorithms (MOEAs);noise;Pareto optimization;subset selection;theoretical analyses,8.0,
Unsupervised texture image segmentation using multiobjective evolutionary clustering ensemble algorithm,Xiaoxue Qian; Xiangrong Zhang; Licheng Jiao; Wenping Ma,2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence),2008.0,"Multiobjective evolutionary clustering approach has been successfully utilized in data clustering. In this paper, we propose a novel unsupervised machine learning algorithm namely multiobjective evolutionary clustering ensemble algorithm (MECEA) to perform the texture image segmentation. MECEA comprises two main phases. In the first phase, MECEA uses a multiobjective evolutionary clustering algorithm to optimize two complementary clustering objectives: one based on compactness in the same cluster, and the other based on connectedness of different clusters. The output of the first phase is a set of Pareto solutions, which correspond to different tradeoffs between two clustering objectives, and different numbers of clusters. In the second phase, we make use of the meta-clustering algorithm (MCLA) to combine all the Pareto solutions to get the final segmentation. The segmentation results are evaluated by comparing with three known algorithms: K-means, fuzzy K-means (FCM), and evolutionary clustering algorithm (ECA). It is shown that MECEA is an adaptive clustering algorithm, which outperforms the three algorithms in the experiments we carried out.",10.1109/CEC.2008.4631279,,4.0,
Automated ischemic beat classification using genetic algorithms and multicriteria decision analysis,Y. Goletsis; C. Papaloukas; D. I. Fotiadis; A. Likas; L. K. Michalis,IEEE Transactions on Biomedical Engineering,2004.0,"Cardiac beat classification is a key process in the detection of myocardial ischemic episodes in the electrocardiographic signal. In the present study, we propose a multicriteria sorting method for classifying the cardiac beats as ischemic or not. Through a supervised learning procedure, each beat is compared to preclassified category prototypes under five criteria. These criteria refer to ST segment changes, T wave alterations, and the patient's age. The difficulty in applying the above criteria is the determination of the required method parameters, namely the thresholds and weight values. To overcome this problem, we employed a genetic algorithm, which, after proper training, automatically calculates the optimum values for the above parameters. A task-specific cardiac beat database was developed for training and testing the proposed method using data from the European Society of Cardiology ST-T database. Various experimental tests were carried out in order to adjust each module of the classification system. The obtained performance was 91% in terms of both sensitivity and specificity and compares favorably to other beat classification approaches proposed in the literature.",10.1109/TBME.2004.828033,,74.0,
Algorithms for Speeding-Up the Deep Neural Networks For Detecting Plant Disease,L. Kouhalvandi; E. O. Gunes; S. Ozoguz,2019 8th International Conference on Agro-Geoinformatics (Agro-Geoinformatics),2019.0,"In designing an artificial network, different parameters such as activation functions, hyper-parameters, etc. are considered. Dealing with large number of parameters and also the functions that are expensive for evalualtion are very hard tasks. In this case, it is logical to find methods that results in smaller number of evaluations and improvements in performance. There are various techniques for multiobjective Bayesian optimization in deep learning structure. S-metric selection efficient global optimization (SMS-EGO) and DIRECT are one of the many techniques for multiobjective Bayesian optimization. In this paper, SMS-EGO and DIRECT techniques are applied to deep learning model and the average number of evaluations of each objective including time and error are investigated. For training and validating the deep network, a number of images present various diseases in leaves are provided from Plant Village data set. The simulation results show that by using SMSEGO technique, performance is improved and average time per iteration is faster.",10.1109/Agro-Geoinformatics.2019.8820541,Agriculture;Bayesian optimization;deep learning (DL);multiobjective optimization;planet disease,1.0,
Multiobjective Optimization in Bioinformatics and Computational Biology,J. Handl; D. B. Kell; J. Knowles,IEEE/ACM Transactions on Computational Biology and Bioinformatics,2007.0,"This paper reviews the application of multiobjective optimization in the fields of bioinformatics and computational biology. A survey of existing work, organized by application area, forms the main body of the review, following an introduction to the key concepts in multiobjective optimization. An original contribution of the review is the identification of five distinct ""contexts,"" giving rise to multiple objectives: These are used to explain the reasons behind the use of multiobjective optimization in each application area and also to point the way to potential future uses of the technique",10.1109/TCBB.2007.070203,Global optimization;clustering;classification and association rules;interactive data exploration and discovery;experimental design;machine learning;bioinformatics (genome or protein) databases.,212.0,
Ranking Machine Learning Classifiers Using Multicriteria Approach,F. de Moura Rezende dos Santos; F. Guedes de Oliveira Almeida; A. C. Pereira Rocha Martins; A. C. Bittencourt Reis; M. Holanda,2018 11th International Conference on the Quality of Information and Communications Technology (QUATIC),2018.0,"Classification algorithms are widely used as data mining tools for knowledge extraction. The literature presents several classifiers, but none of them applies to all problems. encountered in the various context in which they are used. Faced with this situation, the present article proposes a multicriteria approach to help practitioners to select the classifiers that will generate the best quality results by observing their performance measures. An empirical study was performed using a baseline of fetal examination from an UCI database using five classification algorithms (C4.5, Naive Bayes, SMO, KNN and Bayesnet), and each classifier was measured using five performance indicators (accuracy, true positive rate, precision, ROC curve and f-measure). Once implemented, a classifier ranking was conducted based on MCDA PROMETHEE II method, and the results show that SMO, C4.5 and Naive Bayes achieved the highest overall ranking.",10.1109/QUATIC.2018.00034,MCDA;Promethee;Machine Learning;Algorithms Selection;Classification Algorithms,,
Sensitivity Versus Accuracy in Multiclass Problems Using Memetic Pareto Evolutionary Neural Networks,J. C. Fernandez Caballero; F. J. Martinez; C. Hervas; P. A. Gutierrez,IEEE Transactions on Neural Networks,2010.0,"This paper proposes a multiclassification algorithm using multilayer perceptron neural network models. It tries to boost two conflicting main objectives of multiclassifiers: a high correct classification rate level and a high classification rate for each class. This last objective is not usually optimized in classification, but is considered here given the need to obtain high precision in each class in real problems. To solve this machine learning problem, we use a Pareto-based multiobjective optimization methodology based on a memetic evolutionary algorithm. We consider a memetic Pareto evolutionary approach based on the NSGA2 evolutionary algorithm (MPENSGA2). Once the Pareto front is built, two strategies or automatic individual selection are used: the best model in accuracy and the best model in sensitivity (extremes in the Pareto front). These methodologies are applied to solve 17 classification benchmark problems obtained from the University of California at Irvine (UCI) repository and one complex real classification problem. The models obtained show high accuracy and a high classification rate for each class.",10.1109/TNN.2010.2041468,Accuracy;local search;multiclassification;multiobjective evolutionary algorithms;neural networks;sensitivity,113.0,
Comparision of Classification Algorithims for Survival of Breast Cancer Patients,G. Y. Özkan; S. Y. Gündüz,2020 Innovations in Intelligent Systems and Applications Conference (ASYU),2020.0,"Breast cancer has become one of the most common diseases, especially among women, increasing the importance of predicting survival. In this study, the successes of machine learning algorithms on survival prediction were compared using the Surveillance, Epidemiology, and End Results (SEER) breast cancer data set. Used machine learning algorithms are: Naive Bayes, J48, Multiobjective Evolutionary Fuzzy Classifier (MEFC), Support Vector Machines (SVM). The most successful algorithm is J48 algorithm. The most successful algorithm is J48 algorithm according to the tests.",10.1109/ASYU50717.2020.9259846,Breast cancer;SEER Dataset;Naive Bayes;J48;Multiobjective Evolutionary Fuzzy Classifier;Support Vector Machine,,
Brain–Computer Evolutionary Multiobjective Optimization: A Genetic Algorithm Adapting to the Decision Maker,R. Battiti; A. Passerini,IEEE Transactions on Evolutionary Computation,2010.0,"The centrality of the decision maker (DM) is widely recognized in the multiple criteria decision-making community. This translates into emphasis on seamless human-computer interaction, and adaptation of the solution technique to the knowledge which is progressively acquired from the DM. This paper adopts the methodology of reactive search optimization (RSO) for evolutionary interactive multiobjective optimization. RSO follows to the paradigm of “learning while optimizing,” through the use of online machine learning techniques as an integral part of a self-tuning optimization scheme. User judgments of couples of solutions are used to build robust incremental models of the user utility function, with the objective to reduce the cognitive burden required from the DM to identify a satisficing solution. The technique of support vector ranking is used together with a k-fold cross-validation procedure to select the best kernel for the problem at hand, during the utility function training procedure. Experimental results are presented for a series of benchmark problems.",10.1109/TEVC.2010.2058118,Interactive decision making;machine learning;reactive search optimization;support vector ranking,64.0,
A neuro-fuzzy combiner for multiobjective control,I-Fang Chung; Chin-Teng Lin,Proceedings Joint 9th IFSA World Congress and 20th NAFIPS International Conference (Cat. No. 01TH8569),2001.0,"The paper proposes a neuro-fuzzy combiner (NFC) with supervised learning capability for solving multiobjective control problems. The proposed NFC can combine n existing low-level controllers in a hierarchical way to form a multiobjective fuzzy controller. It is assumed that each low-level (fuzzy or nonfuzzy) controller has been well designed to serve a particular objective. The role of the NFC is to fuse the n actions decided by the n low-level controllers and determine a proper action acting on the environment (plant) at each time step. Hence, the NFC can combine low-level controllers and achieve multiple objectives (goals) at once. A NFC can be designed by the proposed architecture and supervised learning scheme. Computer simulations have been conducted to illustrate the performance and applicability of the proposed architecture and learning scheme.",10.1109/NAFIPS.2001.943751,,,
Semi-supervised training of Least Squares Support Vector Machine using a multiobjective evolutionary algorithm,C. Silva; J. S. Santos; E. F. Wanner; E. G. Carrano; R. H. C. Takahashi,2009 IEEE Congress on Evolutionary Computation,2009.0,"Support Vector Machines (SVMs) are considered state-of-the-art learning machines techniques for classification problems. This paper studies the training of SVMs in the special case of problems in which the raw data to be used for training purposes is composed of both labeled and unlabeled data - the semi-supervised learning problem. This paper proposes the definition of an intermediate problem of attributing labels to the unlabeled data as a multiobjective optimization problem, with the conflicting objectives of minimizing the classification error over the training data set and maximizing the regularity of the resulting classifier. This intermediate problem is solved using an evolutionary multiobjective algorithm, the SPEA2. Simulation results are presented in order to illustrate the suitability of the proposed technique.",10.1109/CEC.2009.4983321,,1.0,
Machine Learning Enabled Design Automation and Multi-Objective Optimization for Electric Transportation Power Systems,D. Jackson; S. Belakaria; Y. Cao; J. R. Doppa; X. Lu,IEEE Transactions on Transportation Electrification,2022.0,"This article presents an automated design and optimization framework for electric transportation power systems (ETPS) enabled by machine learning (ML). The use of physical models, simulations, and optimization methods can greatly aid the engineering design process. However, when considering the optimal co-design of multiple interdependent subsystems that span multiple physical domains, such model-based simulations can be computationally expensive, and traditional metaheuristic optimization methods can be unreliable. Bayesian optimization (BO), an ML framework, paves one feasible pathway to realize an efficient design process practically. However, current state-of-the-art BO algorithms are non-compatible or perform poorly when applied to system-level ETPS design with multiple objectives and constraints. This article proposes a novel BO algorithm referred to as max-value entropy search for multiobjective optimization with constraints (MESMOC) to solve multiobjective optimization (MOO) problems with black-box constraints that can only be evaluated through design simulations. After a full presentation of the algorithm, MESMOC is applied to a realistic ETPS design case using a heavy-duty electric vertical-takeoff-landing (eVTOL) urban aerial vehicle (UAV) power system. Two MOO experimental trials show a drastic reduction in the number of design simulations to discover a high-quality Pareto front. In Trial 1, MESMOC uncovered the entire Pareto front while only requiring to explore ~4% of the design space. With expanded design parameters and larger design space in Trial 2, a near complete but high-quality Pareto front was uncovered. Both trials compared MESMOC to the popular genetic algorithm NSGA-II and another BO algorithm predictive entropy search for multi-objective Bayesian optimization with constraints (PESMOC), showing superior performance.",10.1109/TTE.2021.3113958,Aviation;Bayesian optimization (BO);design automation;electric transportation;machine learning (ML);model-based design;multiobjective optimization (MOO);power system design;urban aerial vehicle (UAV),,
Ensemble of Classifiers Based on Multiobjective Genetic Sampling for Imbalanced Data,E. R. Q. Fernandes; A. C. P. L. F. de Carvalho; X. Yao,IEEE Transactions on Knowledge and Data Engineering,2020.0,"Imbalanced datasets may negatively impact the predictive performance of most classical classification algorithms. This problem, commonly found in real-world, is known in machine learning domain as imbalanced learning. Most techniques proposed to deal with imbalanced learning have been proposed and applied only to binary classification. When applied to multiclass tasks, their efficiency usually decreases and negative side effects may appear. This paper addresses these limitations by presenting a novel adaptive approach, E-MOSAIC (Ensemble of Classifiers based on MultiObjective Genetic Sampling for Imbalanced Classification). E-MOSAIC evolves a selection of samples extracted from training dataset, which are treated as individuals of a MOEA. The multiobjective process looks for the best combinations of instances capable of producing classifiers with high predictive accuracy in all classes. E-MOSAIC also incorporates two mechanisms to promote the diversity of these classifiers, which are combined into an ensemble specifically designed for imbalanced learning. Experiments using twenty imbalanced multi-class datasets were carried out. In these experiments, the predictive performance of E-MOSAIC is compared with state-of-the-art methods, including methods based on presampling, active-learning, cost-sensitive, and boosting. According to the experimental results, the proposed method obtained the best predictive performance for the multiclass accuracy measures mAUC and G-mean.",10.1109/TKDE.2019.2898861,Imbalanced datasets;ensemble of classifiers;evolutionary algorithm,18.0,
Pareto-Optimal Adaptive Loss Residual Shrinkage Network for Imbalanced Fault Diagnostics of Machines,Y. Yu; L. Guo; H. Gao; Y. Liu; T. Feng,IEEE Transactions on Industrial Informatics,2022.0,"In the industrial applications of mechanical fault diagnosis, machines work in normal condition at most time. In other words, most of the collected datasets are highly imbalanced. Although deep learning has been widely applied in intelligent diagnosis, it is unsuitable for such imbalanced situation. In addition, few studies attempted to determine the parameters in the diagnosis models. For solving such problems, Pareto-optimal adaptive loss residual shrinkage network (PALRSN) is proposed. First, a fixed length-based encoding method is implemented to represent the candidate architectures of PALRSN. Then, multiply accumulate operations and Gmean value representing the model complexity and identification performance, respectively, on imbalanced datasets are selected as the optimization targets to search for the optimal PALRSN architecture. In the training process, an adaptive loss function assigns different misclassification costs on all categories according to their number discrepancy to highlight the minority samples. The proposed method is validated by bearing data and milling cutter data with different imbalanced ratio. The experimental results demonstrate that such approach outperforms the state-of-the-art methods in imbalanced classification.",10.1109/TII.2021.3094186,Adaptive loss function;imbalanced data;multiobjective genetic algorithm (GA);Pareto front;residual shrinkage network,,
Multistage Collaborative Machine Learning and its Application to Antenna Modeling and Optimization,Q. Wu; H. Wang; W. Hong,IEEE Transactions on Antennas and Propagation,2020.0,"A multistage collaborative machine learning (MS-CoML) method that can be applied to efficient multiobjective antenna modeling and optimization is proposed. Machine learning methods, including single-output Gaussian process regression (SOGPR) and symmetric and asymmetric multioutput GPR (MOGPR) methods, are introduced to collaboratively build highly accurate multitask surrogate models for antennas. Variable-fidelity electromagnetic (EM) models are simulated, with their responses utilized to build separate MOGPR surrogate models. By combining the three machine-learning methods in a multistage framework, mappings between the same and different responses of the EM models with variable fidelity are learned, therein helping to substantially reduce the computational effort under a negligible loss of predictive power. Three antenna designs aiming at single-band, broadband, and multiband applications are selected as examples. And, for illustrating the applicability and superiority of the proposed MS-CoML method, a reference point-based multiobjective antenna optimization algorithm is used to optimize these three antennas. Simulation results show that using the MS-CoML method can significantly reduce the total optimization time without compromising modeling accuracy and optimized performance.",10.1109/TAP.2019.2963570,Antenna modeling;machine learning;multiobjective optimization;multioutput Gaussian process regression;optimization methods,17.0,
Multicriteria Classifier Ensemble Learning for Imbalanced Data,W. Węgier; M. Koziarski; M. Woźniak,IEEE Access,2022.0,"One of the vital problems with the imbalanced data classifier training is the definition of an optimization criterion. Typically, since the exact cost of misclassification of the individual classes is unknown, combined metrics and loss functions that roughly balance the cost for each class are used. However, this approach can lead to a loss of information, since different trade-offs between class misclassification rates can produce similar combined metric values. To address this issue, this paper discusses a multi-criteria ensemble training method for the imbalanced data. The proposed method jointly optimizes <italic>precision</italic> and <italic>recall</italic>, and provides the end-user with a set of Pareto optimal solutions, from which the final one can be chosen according to the user’s preference. The proposed approach was evaluated on a number of benchmark datasets and compared with the single-criterion approach (where the selected criterion was one of the chosen metrics). The results of the experiments confirmed the usefulness of the obtained method, which on the one hand guarantees good quality, i.e., not worse than the one obtained with the use of single-criterion optimization, and on the other hand, offers the user the opportunity to choose the solution that best meets their expectations regarding the trade-off between errors on the minority and the majority class.",10.1109/ACCESS.2022.3149914,Classifier ensemble;imbalanced data;multi-objective optimization;pattern classification,,
Multiobjective Optimization of Fully Autonomous Evolving Fuzzy Granular Models,D. Leite; F. Gomide; I. Škrjanc,2019 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),2019.0,"We introduce an incremental learning method for the optimal construction of rule-based granular models from numerical data streams. We take into account a multiobjective function, the specificity of information, model compactness, and variability and coverage of the data. We use α-level sets over Gaussian membership functions to set model granularity and operate with hyper-rectangular forms of granules in nonstationary environment. Rule-based models are formed in a systematic fashion and can be used for time series prediction and nonlinear function approximation. Precise estimates and enclosures are given by linear piecewise and inclusion functions related to optimal granular mappings. An application example on early detection and monitoring of the severity of the Parkinson's disease shows the usefulness of the method.",10.1109/FUZZ-IEEE.2019.8858964,Evolving system;fuzzy system;machine learning;online data stream;granular computing,,
Benchmarking evolutionary multiobjective optimization algorithms,O. Mersmann; H. Trautmann; B. Naujoks; C. Weihs,IEEE Congress on Evolutionary Computation,2010.0,"Choosing and tuning an optimization procedure for a given class of nonlinear optimization problems is not an easy task. One way to proceed is to consider this as a tournament, where each procedure will compete in different `disciplines'. Here, disciplines could either be different functions, which we want to optimize, or specific performance measures of the optimization procedure. We would then be interested in the algorithm that performs best in a majority of cases or whose average performance is maximal. We will focus on evolutionary multiobjective optimization algorithms (EMOA), and will present a novel approach to the design and analysis of evolutionary multiobjective benchmark experiments based on similar work from the context of machine learning. We focus on deriving a consensus among several benchmarks over different test problems and illustrate the methodology by reanalyzing the results of the CEC 2007 EMOA competition.",10.1109/CEC.2010.5586241,,5.0,
Feature Selection with Dynamic Classifier Ensembles,H. E. Kiziloz; A. Deniz,"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",2020.0,"With the advance in technology, the volume of available data grows massively. Therefore, feature selection has become an essential preprocessing step to extract valuable information. Feature selection is the task of reducing the number of features by removing redundant features from data while preserving the classification accuracy. It is a multiobjective problem as there are two objectives. In general, multiobjective selection algorithms with machine learning techniques are utilized to find the most promising feature subsets; however, classification performances of these machine learning techniques are analyzed separately. In this study, we propose a new multiobjective selection model that dynamically searches for the best ensemble of five classifiers to extract the best representative feature subsets. We present the experiment results on 12 well-known datasets. The results show that the proposed method performs significantly better than all the machine learning techniques when they are executed separately. Moreover, the proposed method outperforms two existing ensemble algorithms, namely AdaBoost and Gradient Boosting.",10.1109/SMC42975.2020.9282969,feature selection;multiobjective optimization;machine learning;classifier ensemble,1.0,
A two-phase evolutionary algorithm for multiobjective mining of classification rules,Y. -H. Chan; T. -C. Chiang; L. -C. Fu,IEEE Congress on Evolutionary Computation,2010.0,"Classification rule mining, addressed a lot in machine learning and statistics communities, is an important task to extract knowledge from data. Most existing approaches do not particularly deal with data instances matched by more than one rule, which results in restricted performance. We present a two-phase multiobjective evolutionary algorithm which first aims at searching decent rules and then takes the rule interaction into account to produce the final rule sets. The algorithm incorporates the concept of Pareto dominance to deal with trade-off relations in both phases. Through computational experiments, the proposed algorithm shows competitive to the state-of-the-art. We also study the effect of a niching mechanism.",10.1109/CEC.2010.5586523,,12.0,
Model-building algorithms for multiobjective EDAs: Directions for improvement,L. Marti; J. Garcia; A. Berlanga; J. M. Molina,2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence),2008.0,In order to comprehend the advantages and short-comings of each model-building algorithm they should be tested under similar conditions and isolated from the MOEDA it takes part of. In this work we will assess some of the main machine learning algorithms used or suitable for model-building in a controlled environment and under equal conditions. They are analyzed in terms of solution accuracy and computational complexity. To the best of our knowledge a study like this has not been put forward before and it is essential for the understanding of the nature of the model-building problem of MOEDAs and how they should be improved to achieve a quantum leap in their problem solving capacity.,10.1109/CEC.2008.4631179,,2.0,
The Power Quality Forecasting Model for Off-Grid System Supported by Multiobjective Optimization,T. Vantuch; S. Mišák; T. Ježowicz; T. Buriánek; V. Snášel,IEEE Transactions on Industrial Electronics,2017.0,"Measurement and control of electric power quality (PQ) parameters in off-grid systems has played an important role in recent years. The purpose is to detect or forecast the presence of PQ parameter disturbances to be able to suppress or to avoid their negative effects on the power grid and appliances. This paper focuses on several PQ parameters in off-grid systems and it defines three evaluation criteria that are supposed to estimate the performance of a new forecasting model combining all the involved PQ parameters. These criteria are based on common statistical evaluations of computational models from the machine learning field of study. The studied PQ parameters are voltage, power frequency, total harmonic distortion, and flicker severity. The approach presented in this paper also applies a machine learning based model of random decision forest for PQ forecasting. The database applied in this task contains real off-grid data from long-term one-minute measurements. The hyperparameters of the model are optimized by multiobjective optimization toward the defined evaluation criteria.",10.1109/TIE.2017.2711540,Forecasting;off-grid system;power quality,23.0,
A Data-Driven Method for IGBT Open-Circuit Fault Diagnosis Based on Hybrid Ensemble Learning and Sliding-Window Classification,Y. Xia; Y. Xu; B. Gou,IEEE Transactions on Industrial Informatics,2020.0,"In this article, a novel data-driven method is proposed for open-circuit fault diagnosis of insulated gate bipolar transistor used in three-phase pulsewidth modulation converter. Based on the sampled three-phase current signals, fast Fourier transform and ReliefF algorithm are used to select most correlated features. Then, based on two randomized learning technologies named extreme learning machine and random vector functional link network, a hybrid ensemble learning scheme is proposed for extracting mapping relationship between fault modes and the selected features. Furthermore, in order to achieve an accurate and fast diagnostic performance, a sliding-window classification framework is designed. Finally, parameters in the diagnostic model are optimized by a multiobjective optimization programming model to achieve optimal balance between diagnosis accuracy and speed. At offline testing stage, the overall average diagnostic accuracy can be as high as 99% with the diagnostic time of around one-cycle sampling time. Furthermore, real-time experiments verify its effectiveness and reliability under different operation conditions.",10.1109/TII.2019.2949344,Hybrid ensemble learning;insulated gate bipolar transistor (IGBT) open-circuit fault;multiobjective optimization programming (MOP);sliding-window classifier,18.0,
On Balancing Neighborhood and Global Replacement Strategies in MOEA/D,X. Chen; C. Shi; A. Zhou; B. Wu; P. Sheng,IEEE Access,2019.0,"In recent years, the multiobjective evolutionary algorithm based on decomposition (MOEA/D) has shown superior performance in solving multiobjective optimization problems (MOPs). In MOEA/D, the adaptive replacement strategy (ARS) plays a key role in balancing convergence and diversity. However, existing ARSs do not effectively balance convergence and diversity. To overcome this disadvantage, we propose a mechanism for adapting neighborhood and global replacement. This mechanism determines whether a neighborhood or global replacement strategy should be employed in the search process. Furthermore, we design an offspring generation strategy to generate high-quality solutions. We call this new algorithm framework MOEA/D-ARS. The experimental results suggest that the proposed algorithm performs better than certain state-of-the-art MOEAs.",10.1109/ACCESS.2019.2909290,Evolutionary algorithm;multiobjective optimization;convergence;adaptive replacement strategy;supervised learning;upper bound,,
Two Machine Learning Approaches for Short-Term Wind Speed Time-Series Prediction,R. Ak; O. Fink; E. Zio,IEEE Transactions on Neural Networks and Learning Systems,2016.0,"The increasing liberalization of European electricity markets, the growing proportion of intermittent renewable energy being fed into the energy grids, and also new challenges in the patterns of energy consumption (such as electric mobility) require flexible and intelligent power grids capable of providing efficient, reliable, economical, and sustainable energy production and distribution. From the supplier side, particularly, the integration of renewable energy sources (e.g., wind and solar) into the grid imposes an engineering and economic challenge because of the limited ability to control and dispatch these energy sources due to their intermittent characteristics. Time-series prediction of wind speed for wind power production is a particularly important and challenging task, wherein prediction intervals (PIs) are preferable results of the prediction, rather than point estimates, because they provide information on the confidence in the prediction. In this paper, two different machine learning approaches to assess PIs of time-series predictions are considered and compared: 1) multilayer perceptron neural networks trained with a multiobjective genetic algorithm and 2) extreme learning machines combined with the nearest neighbors approach. The proposed approaches are applied for short-term wind speed prediction from a real data set of hourly wind speed measurements for the region of Regina in Saskatchewan, Canada. Both approaches demonstrate good prediction precision and provide complementary advantages with respect to different evaluation criteria.",10.1109/TNNLS.2015.2418739,Extreme learning machines (ELMs);multilayer perceptron (MLP);multiobjective genetic algorithms (MOGAs);prediction intervals (PIs);short-term wind speed prediction;wind power production,85.0,
A Neurobiologically-inspired Deep Learning Framework for Autonomous Context Learning,D. W. Ludwig; L. W. Remedios; J. L. Phillips,2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI),2021.0,"Neurobiologically-inspired working memory models demonstrate human/animal capabilities to rapidly adapt and alter responses to the environment via context-switching and error monitoring. However, the application of these models outside of reinforcement learning problems has been relatively unexplored. We present a new framework compatible with Tensorflow/Keras enabling the integration of working memory-inspired mechanisms into typical neural network architectures. These mechanisms allow models to autonomously learn multiple tasks, statically or dynamically allocated. We also examine the generalization of the framework across a variety of multi-context supervised learning and reinforcement learning tasks. The resulting experiments successfully integrate these mechanisms with multi-layer and convolutional neural network architectures and the diversity of problems solved demonstrates the framework's generalizability across a variety of architectures and tasks.",10.1109/ICTAI52525.2021.00022,cognitive architectures;machine learning;taskswitching;multiobjective;working memory,,
Multicriteria approaches for predictive model generation: A comparative experimental study,B. Al-Jubouri; B. Gabrys,2014 IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making (MCDM),2014.0,"This study investigates the evaluation of machine learning models based on multiple criteria. The criteria included are: predictive model accuracy, model complexity, and algorithmic complexity (related to the learning/adaptation algorithm and prediction delivery) captured by monitoring the execution time. Furthermore, it compares the models generated from optimising the criteria using two approaches. The first approach is a scalarized multi objective optimisation, where the models are generated from optimising a single cost function that combines the criteria. On the other hand the second approach uses a Pareto-based multi objective optimisation to trade-off the three criteria and to generate a set of non-dominated models. This study shows that defining universal measures for the three criteria is not always feasible. Furthermore, it was shown that, the models generated from Pareto-based multi objective optimisation approach can be more accurate and more diverse than the models generated from scalarized multi objective optimisation approach.",10.1109/MCDM.2014.7007189,,3.0,
A Survey of Evolutionary Algorithms for Multi-Objective Optimization Problems With Irregular Pareto Fronts,Y. Hua; Q. Liu; K. Hao; Y. Jin,IEEE/CAA Journal of Automatica Sinica,2021.0,"Evolutionary algorithms have been shown to be very successful in solving multi-objective optimization problems (MOPs). However, their performance often deteriorates when solving MOPs with irregular Pareto fronts. To remedy this issue, a large body of research has been performed in recent years and many new algorithms have been proposed. This paper provides a comprehensive survey of the research on MOPs with irregular Pareto fronts. We start with a brief introduction to the basic concepts, followed by a summary of the benchmark test problems with irregular problems, an analysis of the causes of the irregularity, and real-world optimization problems with irregular Pareto fronts. Then, a taxonomy of the existing methodologies for handling irregular problems is given and representative algorithms are reviewed with a discussion of their strengths and weaknesses. Finally, open challenges are pointed out and a few promising future directions are suggested.",10.1109/JAS.2021.1003817,Evolutionary algorithm;machine learning;multiobjective optimization problems (MOPs);irregular Pareto fronts,9.0,
A first study on bagging fuzzy rule-based classification systems with multicriteria genetic selection of the component classifiers,O. Cordon; A. Quirin; L. Sanchez,2008 3rd International Workshop on Genetic and Evolving Systems,2008.0,"Fuzzy rule-based classification systems (FRBCSs) are able to design interpretable classifiers but suffer from the curse of dimensionality when dealing with complex problems with a large number of features. In this contribution we explore the use of popular approaches for designing ensembles of classifiers in the machine learning field, bagging and random subspace, to design FRBCS multiclassifiers from a basic, heuristic fuzzy classification rule generation method, aiming to both improve their accuracy and to make them able to deal with high dimensional classification problems. Besides, a multicriteria genetic algorithm is proposed to select the component classifiers in the ensemble guided by the cumulative likelihood in order to look for an appropriate accuracy-complexity trade-off.",10.1109/GEFS.2008.4484560,,8.0,
Cooperative coevolution of artificial neural network ensembles for pattern classification,N. Garcia-Pedrajas; C. Hervas-Martinez; D. Ortiz-Boyer,IEEE Transactions on Evolutionary Computation,2005.0,"This paper presents a cooperative coevolutive approach for designing neural network ensembles. Cooperative coevolution is a recent paradigm in evolutionary computation that allows the effective modeling of cooperative environments. Although theoretically, a single neural network with a sufficient number of neurons in the hidden layer would suffice to solve any problem, in practice many real-world problems are too hard to construct the appropriate network that solve them. In such problems, neural network ensembles are a successful alternative. Nevertheless, the design of neural network ensembles is a complex task. In this paper, we propose a general framework for designing neural network ensembles by means of cooperative coevolution. The proposed model has two main objectives: first, the improvement of the combination of the trained individual networks; second, the cooperative evolution of such networks, encouraging collaboration among them, instead of a separate training of each network. In order to favor the cooperation of the networks, each network is evaluated throughout the evolutionary process using a multiobjective method. For each network, different objectives are defined, considering not only its performance in the given problem, but also its cooperation with the rest of the networks. In addition, a population of ensembles is evolved, improving the combination of networks and obtaining subsets of networks to form ensembles that perform better than the combination of all the evolved networks. The proposed model is applied to ten real-world classification problems of a very different nature from the UCI machine learning repository and proben1 benchmark set. In all of them the performance of the model is better than the performance of standard ensembles in terms of generalization error. Moreover, the size of the obtained ensembles is also smaller.",10.1109/TEVC.2005.844158,Classification;cooperative coevolution;multiobjective optimization;neural network ensembles,150.0,
Plausible Counterfactuals: Auditing Deep Learning Classifiers with Realistic Adversarial Examples,A. Barredo-Arrieta; J. Del Ser,2020 International Joint Conference on Neural Networks (IJCNN),2020.0,"The last decade has witnessed the proliferation of Deep Learning models in many applications, achieving unrivaled levels of predictive performance. Unfortunately, the black-box nature of Deep Learning models has posed unanswered questions about what they learn from data. Certain application scenarios have highlighted the importance of assessing the bounds under which Deep Learning models operate, a problem addressed by using assorted approaches aimed at audiences from different domains. However, as the focus of the application is placed more on non-expert users, it results mandatory to provide the means for him/her to trust the model, just like a human gets familiar with a system or process: by understanding the hypothetical circumstances under which it fails. This is indeed the angular stone for this research work: to undertake an adversarial analysis of a Deep Learning model. The proposed framework constructs counterfactual examples by ensuring their plausibility, e.g. there is a reasonable probability that a human could generate them without resorting to a computer program. Therefore, this work must be regarded as valuable auditing exercise of the usable bounds a certain model is constrained within, thereby allowing for a much greater understanding of the capabilities and pitfalls of a model used in a real application. To this end, a Generative Adversarial Network (GAN) and multi-objective heuristics are used to furnish a plausible attack to the audited model, efficiently trading between the confusion of this model, the intensity and plausibility of the generated counterfactual. Its utility is showcased within a human face classification task, unveiling the enormous potential of the proposed framework.",10.1109/IJCNN48605.2020.9206728,Explainable Artificial Intelligence;Deep Learning;Counterfactuals;Generative Adversarial Networks;Multiobjective Optimization;Meta-heuristics,2.0,
Analysis of Multiobjective Algorithms for the Classification of Multi-Label Video Datasets,G. N. Karagoz; A. Yazici; T. Dokeroglu; A. Cosar,IEEE Access,2020.0,"It is of great importance to extract and validate an optimal subset of non-dominated features for effective multi-label classification. However, deciding on the best subset of features is an NP-Hard problem and plays a key role in improving the prediction accuracy and the processing time of video datasets. In this study, we propose autoencoders for dimensionality reduction of video data sets and ensemble the features extracted by the multi-objective evolutionary Non-dominated Sorting Genetic Algorithm and the autoencoder. We explore the performance of well-known multi-label classification algorithms for video datasets in terms of prediction accuracy and the number of features used. More specifically, we evaluate Non-dominated Sorting Genetic Algorithm-II, autoencoders, ensemble learning algorithms, Principal Component Analysis, Information Gain, and Correlation Based Feature Selection. Some of these algorithms use feature selection techniques to improve the accuracy of the classification. Experiments are carried out with local feature descriptors extracted from two multi-label datasets, the MIR-Flickr dataset which consists of images and the Wireless Multimedia Sensor dataset that we have generated from our video recordings. Significant improvements in the accuracy performance of the algorithms are observed while the number of features is being reduced.",10.1109/ACCESS.2020.3022317,Feature selection;multi-label;multi-objective optimization;autoencoder;ensemble;classification,1.0,
A Novel Deep Learning Approach: Stacked Evolutionary Auto-encoder,Y. Cai; Z. Cai; M. Zeng; X. Liu; J. Wu; G. Wang,2018 International Joint Conference on Neural Networks (IJCNN),2018.0,"Deep neural networks have been successfully applied to many data mining problems in recent works. The training of deep neural networks relies heavily upon gradient descent methods, however, which may lead to the failure of training due to the vanishing gradient (or exploding gradient) and local optima problems. In this paper, we present SEvoAE method based on using Evolutionary Multiobjective optimization (EMO) algorithm to train single layer auto-encoder, and sequentially learning deeper representation in a stacking way. SEvoAE is able to achieve accurate feature representation with good sparseness by globally simultaneously optimizing two conflicting objective functions and allows users to flexibly design objective functions and evolutionary optimizers. We compare results of the proposed method with existing architectures for seven classification problems, showing that the proposed method is able to outperform existing methods with a reduced risk of overfitting the training data.",10.1109/IJCNN.2018.8489138,Deep learning;Auto-encoder;Evolutionary Multiobjective Optimization,4.0,
Differential evolution based multiobjective optimization for biomedical entity extraction,U. K. Sikdar; A. Ekbal; S. Saha,"2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)",2014.0,"In this paper, we propose multi-objective differential evolution (DE) based feature selection and ensemble learning techniques for biomedical entity extraction. The algorithm operates in two layers, first step of which concerns with the problem of automatic feature selection for a machine learning algorithm, namely Conditional Random Field (CRF). The solutions of the final best population provides different feature combinations. The classifiers generated with these feature representations are combined together using a multi-objective differential based ensemble technique. We evaluate the proposed algorithm for named entity (NE) extraction in biomedical text. Experiments on the benchmark setup yield recall, precision and F-measure values of 73.50%, 77.02% and 75.22%, respectively.",10.1109/ICACCI.2014.6968390,,1.0,
Multiobjective Evolutionary Design of Deep Convolutional Neural Networks for Image Classification,Z. Lu; I. Whalen; Y. Dhebar; K. Deb; E. D. Goodman; W. Banzhaf; V. N. Boddeti,IEEE Transactions on Evolutionary Computation,2021.0,"Convolutional neural networks (CNNs) are the backbones of deep learning paradigms for numerous vision tasks. Early advancements in CNN architectures are primarily driven by human expertise and by elaborate design processes. Recently, neural architecture search was proposed with the aim of automating the network design process and generating task-dependent architectures. While existing approaches have achieved competitive performance in image classification, they are not well suited to problems where the computational budget is limited for two reasons: 1) the obtained architectures are either solely optimized for classification performance, or only for one deployment scenario and 2) the search process requires vast computational resources in most approaches. To overcome these limitations, we propose an evolutionary algorithm for searching neural architectures under multiple objectives, such as classification performance and floating point operations (FLOPs). The proposed method addresses the first shortcoming by populating a set of architectures to approximate the entire Pareto frontier through genetic operations that recombine and modify architectural components progressively. Our approach improves computational efficiency by carefully down-scaling the architectures during the search as well as reinforcing the patterns commonly shared among past successful architectures through Bayesian model learning. The integration of these two main contributions allows an efficient design of architectures that are competitive and in most cases outperform both manually and automatically designed architectures on benchmark image classification datasets: CIFAR, ImageNet, and human chest X-ray. The flexibility provided from simultaneously obtaining multiple architecture choices for different compute requirements further differentiates our approach from other methods in the literature.",10.1109/TEVC.2020.3024708,Convolutional neural networks (CNNs);evolutionary deep learning;genetic algorithms (GAs);neural architecture search (NAS),17.0,
An Information Fusion-Based Multiobjective Security System With a Multiple-Input/ Single-Output Sensor,T. Ishigaki; T. Higuchi; K. Watanabe,IEEE Sensors Journal,2007.0,"In the framework of sensor fusion, multiple sensors corresponding to the number of physical variables that must be measured are used. In this paper, we propose a novel sensing approach that simultaneously deals with heterogeneous physical variables with a sensor. It is fundamentally different from sensor fusion. The proposed approach takes into consideration the fact that any sensor that detects a certain physical variable is influenced to a degree by other physical variables, which are designated as noise. The objective in conventional sensor design has been the minimization of noise. In contrast, the proposed approach takes advantage of sensors that are easily influenced by many physical variables and makes full use of the multisensing characteristics of these sensors. The system designed using this concept has advantages in terms of cost performance and system simplification compared to existing approaches. This concept can be realized by developing a novel multiple-input/single-output sensor that can detect various variables, including pressure, acceleration, temperature and incandescent light emission, by a single device. We apply the sensor to monitor the symptoms of fire, earthquakes, and break-ins for the purpose of home security. The proposed security system is realized through statistical signal processing and machine learning techniques",10.1109/JSEN.2007.894887,Autoregressive model;home security;information fusion;Kalman filter;multiple-input/single-output sensor;support vector machine,10.0,
Transfer Clustering Ensemble Selection,Y. Shi; Z. Yu; C. L. P. Chen; J. You; H. -S. Wong; Y. Wang; J. Zhang,IEEE Transactions on Cybernetics,2020.0,"Clustering ensemble (CE) takes multiple clustering solutions into consideration in order to effectively improve the accuracy and robustness of the final result. To reduce redundancy as well as noise, a CE selection (CES) step is added to further enhance performance. Quality and diversity are two important metrics of CES. However, most of the CES strategies adopt heuristic selection methods or a threshold parameter setting to achieve tradeoff between quality and diversity. In this paper, we propose a transfer CES (TCES) algorithm which makes use of the relationship between quality and diversity in a source dataset, and transfers it into a target dataset based on three objective functions. Furthermore, a multiobjective self-evolutionary process is designed to optimize these three objective functions. Finally, we construct a transfer CE framework (TCE-TCES) based on TCES to obtain better clustering results. The experimental results on 12 transfer clustering tasks obtained from the 20newsgroups dataset show that TCE-TCES can find a better tradeoff between quality and diversity, as well as obtaining more desirable clustering results.",10.1109/TCYB.2018.2885585,Clustering ensemble selection (CES);machine learning;multiobjective;transfer learning,7.0,
Understanding the Interplay of Model Complexity and Fidelity in Multiagent Systems via an Evolutionary Framework,E. Lakshika; M. Barlow; A. Easton,IEEE Transactions on Computational Intelligence and AI in Games,2017.0,"Modern video games come with highly realistic graphics enabling the players to interact with visually rich virtual worlds. Realistic (life-like) animation of nonplayer characters (NPCs) in such virtual worlds is particularly important to enhance the gaming experience. Multiagent systems are one effective approach to synthesize life-like behaviors and interactions by codifying simple rules into NPCs (each NPC as an autonomous agent). However, such behaviors generally come at the cost of increasing computational expense and complexity in terms of aspects such as number of rules and parameters. Therefore, the desire for high fidelity (highly realistic) behaviors is often in conflict with the drive for low complexity. Multiobjective evolutionary algorithms provide a sophisticated mechanism to optimize two or more conflicting objectives simultaneously. However, evolutionary computing techniques need an appropriate objective function to drive the exploration in the correct direction. Pairing of evolutionary techniques and multiagent systems is challenging in the classes of problems in which the fitness is evaluated based on human aesthetic judgment rather than on objective forms of measurements. In this study, we present a multiobjective evolutionary framework to evolve low complexity and high fidelity multiagent systems by utilizing a machine learning system trained by bootstrapping human aesthetic judgment. We have gathered empirical data in three problem areas-simulation of conversational group dynamics, sheepdog herding behaviors, and traffic dynamics, and show the effectiveness of our approach in deriving low complexity and high fidelity multiagent systems. Further, we have identified common properties of the Pareto-optimal frontiers in the three problem areas that can ultimately lead to an understanding of a relationship between simulation model complexity and behavior fidelity. This understanding will be useful in deciding which level of behavioral fidelity is required for the characters in video games based on the distance to the camera, importance to the scene, and available computational resources.",10.1109/TCIAIG.2016.2560882,Complexity;fidelity;level of detail artificial intelligence (LOD AI);multiagent systems;multiobjective optimization,10.0,
Privacy-Preserving Multiobjective Sanitization Model in 6G IoT Environments,J. C. -W. Lin; G. Srivastava; Y. Zhang; Y. Djenouri; M. Aloqaily,IEEE Internet of Things Journal,2021.0,"The next revolution of the smart industry relies on the emergence of the Industrial Internet of Things (IoT) and 5G/6G technology. The properties of such sophisticated communication technologies will change our perspective of information and communication by enabling seamless connectivity and bring closer entities, data, and “things.” Terahertz-based 6G networks promise the best speed and reliability, but they will face new man-in-the-middle attacks. In such critical and high-sensitive environments, the security of data and privacy of information still a big challenge. Without privacy-preserving considerations, the configuration state may be attacked or modified, thus causing security problems and damage to data. In this article, motivated by the need to secure 6G IoT networks, an ant colony optimization (ACO) approach is presented by adopting multiple objectives as well as using transaction deletion to secure confidential and sensitive information. Each ant in the population is represented as a set of possible deletion transactions for hiding sensitive information. We utilize the use of a prelarge concept to assist in the reduction of multiple database scans in the evaluation progress. We then also adopt external solutions to maintain discovered Pareto solutions, thus improving effectiveness to find optimized solutions. Experiments are conducted comparing our methodology to state-of-the-art bioinspired particle swarm optimization (PSO) as well as genetic algorithm (GA). Our strong results clearly show that the designed approach achieves fewer side effects while maintaining low computational cost overall (Chen et al., 2020).",10.1109/JIOT.2020.3032896,5G/6G;ant colony;decomposition;deep learning;IIoT;object detection;particle swarm optimization (PSO);smart factory,43.0,
Multiobjective Optimization for Stiffness and Position Control in a Soft Robot Arm Module,Y. Ansari; M. Manti; E. Falotico; M. Cianchetti; C. Laschi,IEEE Robotics and Automation Letters,2018.0,"The central concept of this letter is to develop an assistive manipulator that can automate the bathing task for elderly citizens. We propose to exploit principles of soft robotic technologies to design and control a compliant system to ensure safe human-robot interaction, a primary requirement for the task. The overall system is intended to be modular with a proximal segment that provides structural integrity to overcome gravitational challenges and a distal segment to perform the main bathing activities. The focus of this letter is on the design and control of the latter module. The design comprises of alternating tendons and pneumatics in a radial arrangement, which enables elongation, contraction, and omnidirectional bending. Additionally, a synergetic coactivation of cables and tendons in a given configuration allows for stiffness modulation, which is necessary to facilitate washing and scrubbing. The novelty of the work is twofold: 1) Three base cases of antagonistic actuation are identified that enable stiffness variation. Each category is then experimentally characterized by the application of an external force that imposes a linear displacement at the tip in both axial and lateral directions. 2) The development of a novel algorithm based on cooperative multiagent reinforcement learning that simultaneously optimizes stiffness and position. The results highlight the effectiveness of the design and control to contribute toward the development of the assistive device.",10.1109/LRA.2017.2734247,Assistive robotics;machine learning;robot control;soft robotics,45.0,
A Review and Classification of Multi-Criteria Recommender Systems,S. Gupta; V. Kant,2020 4th International Conference on Intelligent Computing and Control Systems (ICICCS),2020.0,"Recommender systems (RSs) are personalization tools that gives recommendations for items to users by exploiting various methods. Conventional collaborative filtering (CF) based RSs provide suggestions to users based on overall rating of items which is not an efficient procedure as users in system may have different choices on different criteria. So, multicriteria recommender systems (MCRS) came into existence as an extension of traditional CF based RSs. MCRS recommends items to users based on number of criteria. Recommending products to users from the vast catalog is still a challenge for researchers. This paper presents a review of some significant work in the area of multi-criteria recommender system. After a brief introduction, we present review of existing methods categories according to heuristic and model based approach, and some of the popular approaches are classified into different sets such as recommendation fields, research problem, data mining and machine learning techniques. Insights and possible future work in the area of MCRSs are also discussed.",10.1109/ICICCS48265.2020.9120983,classification;collaborative filtering;multicriteria ratings;recommender system,,
Cognitive Analytics of Social Media Services for Edge Resource Pre-Allocation in Industrial Manufacturing,D. Zhu; Z. Xu; X. Xu; Q. Zhao; L. Qi; G. Srivastava,IEEE Transactions on Computational Social Systems,2021.0,"With the development of industrial intelligence, the resource requests of various social media services in smart cities are expanding rapidly. For hosting services, the edge computing (EC) platform for its low-latency resource provisioning is fully explored. However, the mapping between edge servers (ESs) and services affects the service latency. Meanwhile, the real-time dynamic distribution of resource requirements also impairs the load balance. Therefore, how to optimize the load balance of ESs while meeting the latency-critical requests remains challenging. To deal with the above challenge, in this article, we propose a resource pre-allocation (RPA) method for the social media services with cognitive analytics. Technically, the deep spatiotemporal residual network (ST-ResNet) is employed to complete the cognitive analytics of resource requests. Then based on the analysis results, the optimal resource allocation (ORA) scheme is designed with multiobjective optimization. Finally, the performance of RPA is evaluated by a real-world resource request data set.",10.1109/TCSS.2021.3052231,Cognitive analytics;deep learning;multiobjective optimization;optimal resource allocation (ORA);social media,1.0,
DCBRTS: A Classification-Summarization Approach for Evolving Tweet Streams in Multiobjective Optimization Framework,D. Bansal; N. Saini; S. Saha,IEEE Access,2021.0,"The emergence of social media platforms like Twitter has become a prominent communication source in disaster outbreak. NGOs, Government agencies leverage twitter’s open and public features to provide immediate relief. Nevertheless, situational information gets immersed in millions of tweets with varying characteristics. Examining each tweet can be cumbersome and time-consuming. Thus, the efficient extraction of disaster-related situational tweets and getting information from all the extracted tweets is required. In the current paper, we have developed a novel framework that uses a deep learning-based classification model to separate the situational tweets from others and summarize them in real-time. Our system is a three-phase process: (a) Creating tweet clusters using a representative set of tweets from the initial set of extracted tweets using a multi-objective optimization concept; (b) When a new tweet arrives, the clusters are updated. The new tweet is classified as situational vs. non-situational. If situational, it is assigned to the closest cluster or new cluster. This assignment is based on its weighted average of syntactic and semantic distances and relevancy to the cluster; (c) Summary is formulated by extracting tweets from each cluster. The proposed approach’s superior performance on four datasets related to different disaster-related events indicates the developed framework’s efficiency over the state-of-the-art techniques.",10.1109/ACCESS.2021.3120112,Evolving tweet-stream;summarization;classification;convolution neural network;clustering;multi-objective optimization,,
Pareto-Path Multitask Multiple Kernel Learning,C. Li; M. Georgiopoulos; G. C. Anagnostopoulos,IEEE Transactions on Neural Networks and Learning Systems,2015.0,"A traditional and intuitively appealing Multitask Multiple Kernel Learning (MT-MKL) method is to optimize the sum (thus, the average) of objective functions with (partially) shared kernel function, which allows information sharing among the tasks. We point out that the obtained solution corresponds to a single point on the Pareto Front (PF) of a multiobjective optimization problem, which considers the concurrent optimization of all task objectives involved in the Multitask Learning (MTL) problem. Motivated by this last observation and arguing that the former approach is heuristic, we propose a novel support vector machine MT-MKL framework that considers an implicitly defined set of conic combinations of task objectives. We show that solving our framework produces solutions along a path on the aforementioned PF and that it subsumes the optimization of the average of objective functions as a special case. Using the algorithms we derived, we demonstrate through a series of experimental results that the framework is capable of achieving a better classification performance, when compared with other similar MTL approaches.",10.1109/TNNLS.2014.2309939,Machine learning;optimization methods;pattern recognition;supervised learning;support vector machines (SVM).;Machine learning;optimization methods;pattern recognition;supervised learning;support vector machines (SVM),14.0,
Investigating The Influential Factors On Firefighter Injuries Using Statistical Machine Learning,Z. Yang; Y. Liu,2018 International Conference on Machine Learning and Cybernetics (ICMLC),2018.0,"Firefighters are the most important resources in protecting the public and responding to emergencies. Canada's first-ever national fire information database (NFID) was implemented in 2017, which enables effective big data analytics to investigate fire and firefighter related issues. This paper proposes principal component analysis and deep neural networks to investigate the influential factors that affect firefighter injuries. The methods have been validated using the data available in NFID. The results are valuable in supporting multicriteria decision making and decision support systems.",10.1109/ICMLC.2018.8527021,Statistical machine learning;Big data analytics;Ensemble method;Principal component analysis;Deep neural networks;Multicriteria decision making;Decision support systems,,
Evolving Diverse Ensembles Using Genetic Programming for Classification With Unbalanced Data,U. Bhowan; M. Johnston; M. Zhang; X. Yao,IEEE Transactions on Evolutionary Computation,2013.0,"In classification, machine learning algorithms can suffer a performance bias when data sets are unbalanced. Data sets are unbalanced when at least one class is represented by only a small number of training examples (called the minority class), while the other class(es) make up the majority. In this scenario, classifiers can have good accuracy on the majority class, but very poor accuracy on the minority class(es). This paper proposes a multiobjective genetic programming (MOGP) approach to evolving accurate and diverse ensembles of genetic program classifiers with good performance on both the minority and majority of classes. The evolved ensembles comprise of nondominated solutions in the population where individual members vote on class membership. This paper evaluates the effectiveness of two popular Pareto-based fitness strategies in the MOGP algorithm (SPEA2 and NSGAII), and investigates techniques to encourage diversity between solutions in the evolved ensembles. Experimental results on six (binary) class imbalance problems show that the evolved ensembles outperform their individual members, as well as single-predictor methods such as canonical GP, naive Bayes, and support vector machines, on highly unbalanced tasks. This highlights the importance of developing an effective fitness evaluation strategy in the underlying MOGP algorithm to evolve good ensemble members.",10.1109/TEVC.2012.2199119,Classification;class imbalance learning;genetic programming (GP);multiobjective machine learning (ML),140.0,
A multi-objective competitive co-evolutionary approach for classification problems,V. T. VU; L. T. BUI; T. T. NGUYEN,2019 6th NAFOSTED Conference on Information and Computer Science (NICS),2019.0,"This paper proposes a multi-objective competitive co-evolutionary algorithm (MOCPCEA) based on the PreyPredator model to solve classification problems. In the MOCPCEA, a data population acts as preys. To be specific, each prey represents a selected subset of the training dataset. Another population is ANN classifiers which play as Predators. The task of the Predators is to try to classify the data sets as correctly as possible, whereas the Preys try to find the data sets that are difficult to be classified. Through this interaction process, MOCPCEA generates a set of classifiers that are able to classify difficult data sets. The final classification result is given by the ensemble voting mechanism among these sets of classifiers. The performance of the proposed algorithm is performed on seven benchmark problems. Through comparison with other algorithms, the proposed algorithm indicates that it could create an ensemble of ANN networks that give high and stable classification results.",10.1109/NICS48868.2019.9023887,competitive co-evolutionary;Prey-Predator;multiobjective optimization;classification;ensemble learning.,,
Optimal Energy Operation Strategy for We-Energy of Energy Internet Based on Hybrid Reinforcement Learning With Human-in-the-Loop,L. Yang; Q. Sun; N. Zhang; Z. Liu,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",2022.0,"This article investigates the energy operation problem based on We-Energy (WE), a novel full-duplex model in Energy Internet (EI). A dual-objective optimal energy operation model of WE is formulated with the consideration of economical benefit and security operation under different time scenarios. Due to the inaccurate model of distributed generation devices and loads, a multipolicy convex hull reinforcement learning (MCRL) algorithm is proposed. It can find the multiobjective strategy set with model-free feature. Moreover, considering the limitations of artificial intelligence technology and the human advantages in information processing for complex task, a two-channel Human-in-the-loop (HITL) method is designed to combine with MCRL to avoid decision-making risks. The one channel of HITL can evaluate the operation strategy by human under normal conditions so that the understanding of human for complex operating conditions can be incorporated into the machine learning algorithms to improve the confidence of intelligent systems. The other channel of HITL can allow human to participate in real-time adjustment under abnormal conditions to avoid system out of control. Simulation studies of modified EI are confirmed that the proposed algorithm can improve system performance effectively.",10.1109/TSMC.2020.3035406,Human-in-the-loop (HITL);multiobjective optimization model;multipolicy convex hull reinforcement learning (MCRL);optimal energy operation;We-Energy (WE),,
A Hierarchical Self-Adaptive Data-Analytics Method for Real-Time Power System Short-Term Voltage Stability Assessment,Y. Zhang; Y. Xu; Z. Y. Dong; R. Zhang,IEEE Transactions on Industrial Informatics,2019.0,"As one of the most complex and largest dynamic industrial systems, a modern power grid envisages the wide-area measurement protection and control (WAMPAC) system as the grid sensing backbone to enhance security, reliability, and resiliency. However, based on the massive wide-area measurement data, how to realize real-time short-term voltage stability (STVS) assessment is an essential yet challenging problem. This paper proposes a hierarchical and self-adaptive data-analytics method for real-time STVS assessment covering both the voltage instability and the fault-induced delayed voltage recovery phenomenon. Based on a strategically designed ensemble-based randomized learning model, the STVS assessment is achieved sequentially and self-adaptively. Besides, the assessment accuracy and the earliness are simultaneously optimized through the multiobjective programming. The proposed method has been tested on a benchmark power system, and its exceptional assessment accuracy, speed, and comprehensiveness are demonstrated by comparing with existing methods.",10.1109/TII.2018.2829818,Data-analytics;ensemble learning;extreme learning machine;multiobjective programming;short-term voltage stability (STVS);smart grid,56.0,
Learning of a Decision-Maker’s Preference Zone With an Evolutionary Approach,M. Aggarwal,IEEE Transactions on Neural Networks and Learning Systems,2019.0,"A new evolutionary-learning algorithm is proposed to learn a decision maker (DM)'s best solution on a conflicting multiobjective space. Given the exemplary pairwise comparisons of solutions by a DM, we learn an ideal point (for the DM) that is used to evolve toward a better set of solutions. The process is repeated to get the DM's best solution. The comparison of solutions in pairs facilitates the process of eliciting training information for the proposed learning model. Experimental study on standard multiobjective data sets shows that the proposed method accurately identifies a DM's preferred zone in relatively a few generations and with a small number of preferences. Besides, it is found to be robust to inconsistencies in the preference statements. The results obtained are validated through a variant of the established NSGA-2 algorithm.",10.1109/TNNLS.2018.2847412,Evolutionary;machine learning;multicriteria decision making;preference information;utility preference,4.0,
"The <formula formulatype=""inline""><tex Notation=""TeX"">$Q$</tex> </formula>-Norm Complexity Measure and the Minimum Gradient Method: A Novel Approach to the Machine Learning Structural Risk Minimization Problem",D. A. G. Vieira; R. H. C. Takahashi; V. Palade; J. A. Vasconcelos; W. M. Caminhas,IEEE Transactions on Neural Networks,2008.0,"This paper presents a novel approach for dealing with the structural risk minimization (SRM) applied to a general setting of the machine learning problem. The formulation is based on the fundamental concept that supervised learning is a bi-objective optimization problem in which two conflicting objectives should be minimized. The objectives are related to the empirical training error and the machine complexity. In this paper, one general Q-norm method to compute the machine complexity is presented, and, as a particular practical case, the minimum gradient method (MGM) is derived relying on the definition of the fat-shattering dimension. A practical mechanism for parallel layer perceptron (PLP) network training, involving only quasi-convex functions, is generated using the aforementioned definitions. Experimental results on 15 different benchmarks are presented, which show the potential of the proposed ideas.",10.1109/TNN.2008.2000442,Complexity measure;multiobjective training algorithms;neural networks;parallel layer perceptron (PLP);regularization methods;structural risk minimization (SRM),26.0,
Multi-Query Video Retrieval Based on Deep Learning and Pareto Optimality,C. Vural; E. Akbacak,2020 28th Signal Processing and Communications Applications Conference (SIU),2020.0,"Existing video retrieval studies support single query. To the best of our knowledge, there is no multi-query video retrieval method. In this study, an efficient and fast multi-query video retrieval method is proposed for queries having different semantics. The metod supports unlimited number of queries. Real valued features representing a video are extracted by a deep network and are converted into binary codes. Database items that simultaneously most closely resemble multiple queries are retrieved by Pareto front method. Efficiency of the method is determined by means of a designed graphical user interface.",10.1109/SIU49456.2020.9302123,Hash codes;Pareto optimization;multi-query video retrieval,,
Multi-Query Image Retrieval Based on Deep Learning and Pareto Optimality,C. Vural; E. Akbacak,2020 28th Signal Processing and Communications Applications Conference (SIU),2020.0,"In this study, a method for fast and efficient multiquery image retrieval from large scale databases is introduced. Images used as queries are semantically different from each other. In order to obtain similarity between multiple queries and each item in the database, image features are extracted from a deep networks and then they are converted into binary codes. The database items that simultaneously most closely resemble multiple queries are obtained by the Pareto front method. Furthermore, the method is tested on a designed graphical user interface.",10.1109/SIU49456.2020.9302140,Hash codes;Pareto optimization;multi-query image retrieval,,
Automated Deep Neural Learning-Based Optimization for High Performance High Power Amplifier Designs,L. Kouhalvandi; O. Ceylan; S. Ozoguz,IEEE Transactions on Circuits and Systems I: Regular Papers,2020.0,"This study presents an automated optimization-oriented strategy for designing high power amplifiers (HPAs) using deep neural networks (DNNs). The proposed strategy consists of two optimization phases that are applied sequentially. In the first phase, the circuit topology is optimized by determining the number of passive components in the input and output matching networks using deep learning classification network. In the second optimization phase, component values are estimated using a deep learning regression network with electromagnetic-based Thompson Sampling Efficient Multiobjective Optimization (TSEMO). The proposed approach is compact, in the sense that the optimum solution is automatically generated by the process, opposite to the conventional approaches where manual post-processing is required to prune the process outcomes. It addresses the problem of heavy reliance of the system performance on the designer's experience and automatically generates valid layouts. In the demanding HPA design problem, uses of DNNs have been shown to provide much more accuracy than conventional shallow neural networks. The effectiveness of the proposed method is verified by implementing two designed HPAs, including GaN HEMTs. The efficiency-oriented optimized amplifier reveals higher than 60% drain efficiency, and the gain-oriented optimized amplifier has 17.6-18 dB linear gain in the frequency band of 1.8-2.2 GHz.",10.1109/TCSI.2020.3008947,Automated design;deep neural network (DNN);high efficiency;high gain;multiobjective optimization;power amplifiers,9.0,
Pareto-based Multi-Objective Machine Learning,Y. Jin,7th International Conference on Hybrid Intelligent Systems (HIS 2007),2007.0,"Machine learning is inherently a multi-objective task. Traditionally, however, either only one of the objectives is adopted as the cost function or multiple objectives are aggregated to a scalar cost function. This can mainly attributed to the fact that most conventional learning algorithms can only deal with a scalar cost function. Over the last decade, efforts on solving machine learning problems using the Pareto-based multi-objective optimization methodology have gained increasing impetus, particularly thanks to the great success of multi-objective optimization using evolutionary algorithms and other population-based stochastic search methods. It has been shown that Pareto-based multi-objective learning approaches are more powerful compared to learning algorithms with a scalar cost functions in addressing various topics of machine learning, such as clustering, feature selection, improvement of generalization ability, knowledge extraction, and ensemble generation. This talk provides first a brief overview of Pareto-based multi-objective machine learning techniques. In addition, a number of case studies are provided to illustrate the major benefits of the Pareto-based approach to machine learning, e.g., how to identify interpretable models and models that can generalize on unseen data from the obtained Pareto-optimal solutions. Three approaches to Pareto-based multi-objective ensemble generation are compared and discussed in detail. Most recent results on multi-objective optimization of spiking neural networks will be presented.",10.1109/HIS.2007.73,,2.0,
Intelligent Early Warning of Power System Dynamic Insecurity Risk: Toward Optimal Accuracy-Earliness Tradeoff,Y. Zhang; Y. Xu; Z. Y. Dong; Z. Xu; K. P. Wong,IEEE Transactions on Industrial Informatics,2017.0,"Dynamic insecurity risk of a power system has been increasingly concerned due to the integration of stochastic renewable power sources (such as wind and solar power) and complicated demand response. In this paper, an intelligent early-warning system to achieve reliable online detection of risky operating conditions is proposed. The proposed intelligent system (IS) consists of an ensemble learning model based on extreme learning machine (ELM) and a decision-making process under a multiobjective programming framework. Taking an ensemble form, the randomness existing in individual ELM training is generalized and reliable classification results can be obtained. The decision making is designed for ELM ensemble whose parameters are optimized to search for the optimal tradeoff between the warning accuracy and the warning earliness of the proposed IS. The compromise solution turns out to significantly speed up the overall computation with an acceptable sacrifice in the accuracy (e.g., from 100% to 99.9%). More importantly, the proposed IS can provide multiple and switchable performances to the operators in order to satisfy different local dynamic security assessment requirements.",10.1109/TII.2017.2676879,Dynamic insecurity risk;early warning;extreme learning machine (ELM);intelligent system (IS);multiobjective programming (MOP),58.0,
Enhancing utility and privacy with noisy minimax filters,J. Hamm,"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",2017.0,"Preserving privacy of continuous and/or high-dimensional data such as images, videos and audios is challenging. Syntactic anonymization methods were proposed typically for discrete data types and can be unsuitable. Differential privacy, which provides a stricter type of privacy, has shown more success in sanitizing continuous data. However, both syntactic and differential privacy are susceptible to inference attacks, i.e., an adversary can accurately guess sensitive attributes from insensitive attributes. On the other hand, minimax filters were proposed previously to minimize the accuracy of inference while maximizing utility at the same time. The paper presents noisy minimax filter that combines minimax filter and differentially private mechanism, which can attain high average utility and protection against inference attacks and a formal worst-case privacy guarantee. The proposed algorithm is demonstrated with real databases of faces, voices, and motion data.",10.1109/ICASSP.2017.7953386,syntactic anonymity;differential privacy;minimax optimization;postprocessing;machine learning,8.0,
"Pareto-optimality is everywhere: From engineering design, machine learning, to biological systems",Yaochu Jin,2008 3rd International Workshop on Genetic and Evolving Systems,2008.0,"This talk attempts to argue that almost all adaptive systems have multiple objectives to achieve. Very often, there is no single solution that can optimize all objectives, in which case, the concept of Pareto-optimization plays an important rule. Examples will be given ranging from engineering design, machine learning, to biological systems to show how Pareto-optimality can make a difference in analyzing these systems. The first example we will discuss is the aerodynamic design optimization of turbine blades, where energy efficiency in terms of pressure loss as well as the variation of pressure distribution must be minimized. One additional difficulty in aerodynamic design optimization is that the quality of candidate designs must be assessed by performing computational fluid dynamics analysis, which is very time consuming. To reduce computation time, computational techniques like parallel computation, and machine learning techniques, such as meta-modeling can be employed. Surprisingly interesting results will also be achieved when the concept of Pareto-optimality is applied to machine learning. Two cases will be provided to illustrate this idea. In the first case, we show how Pareto-based approach can address neural network regularization more elegantly, through which deeper insights into the problem can be gained. In the second case, we show that analysis of the Pareto-optimal solutions will help determine the optimal number of clusters in data clustering, which again shown how the Pareto front can disclose additional knowledge about the problem at hand. The final example is concerned with tradeoffs in simulated evolution of genetic representation. It has been argued that robustness is critical for biological evolution, because without certain degree of robustness to mutations, it is impossible for evolution to create new functionalities. Therefore, evolution must find representations that are sufficiently robust yet have the potential to innovate. Examples will be given to show that such tradeoff does exist in evolving both a stationary genotype-phenotype mapping, and also a gene regulatory network described by a random Boolean network.",10.1109/GEFS.2008.4484555,,2.0,
Resource-Aware Pareto-Optimal Automated Machine Learning Platform,Y. Yang; A. Nam; M. Nasr-Azadani; T. Tung,2020 3rd International Seminar on Research of Information Technology and Intelligent Systems (ISRITI),2020.0,"In this study, we introduce a novel platform Resource-Aware AutoML (RA-AutoML) which enables flexible and generalized algorithms to build machine learning models subjected to multiple objectives, as well as resource and hardware constraints. RA-AutoML intelligently conducts Hyper-Parameter Search (HPS) as well as Neural Architecture Search (NAS) to build models optimizing predefined objectives. RA-AutoML is a versatile framework that allows user to prescribe many resource/hardware constraints along with objectives demanded by the problem or even business requirements. At its core, RA-AutoML relies on our in-house search-engine algorithm, MOBOGA, which combines a modified constraint-aware Bayesian Osptimization and Genetic Algorithm to construct Pareto optimal candidates. Our experiments on CIFAR-10 dataset shows very good accuracy compared to results obtained by state-of-art neural network models, while subjected to resource constraints in the form of model size.",10.1109/ISRITI51436.2020.9315336,Automatic Machine Learning;Resource-aware optimization;Hardware-aware Machine;Learning Resource constraints;Bayesian optimization;Pareto optimal;Constraint-aware AutoML Platform,,
Decision Variable Learning,M. Santos; J. Alves de Oliveira; A. Britto,2019 8th Brazilian Conference on Intelligent Systems (BRACIS),2019.0,"Many alternatives to traditional Multi-Objective Optimization Algorithms are emerging due to the increasing number of Multi-Objective Problems with a high degree of complexity, such as Many-Objective Problems. Among these alternatives, the methods known as surrogate stand out. These methods seek to construct new models for the objective functions based on the data obtained previously from the actual objective functions. The input of these models are a set of vectors in decision variable space and the output are the values of the objective functions. In this work, the Decision Variable Learning (DVL) algorithm is proposed, which presents an inverse idea to traditional surrogates. In the DVL, machine learning models will be used to learn the behavior of the decision variables in Many-Objective Optimization Problems. In this context, we have enough information to learn from the objective vectors, to predict near optimal solution in decision variable space. DVL algorithm will be evaluated using benchmark problems and its results will be compared with the NSGA-III algorithm.",10.1109/BRACIS.2019.00093,"multiobjective-optimization, machine-learning, complex-problems",,
RSSM-Net: Remote Sensing Image Scene Classification Based on Multi-Objective Neural Architecture Search,Y. Wan; Y. Zhong; A. Ma; J. Wang; R. Feng,IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium,2020.0,"The deep learning (DL)-based scene classification methods have been obtained the remarkable attention for the high spatial resolution remote sensing (HRS) imagery. However, from one aspect, the existing DL methods in HRS image scene classification are usually the variations of the natural image processing methods and often the inherent network structures; from another aspect, the strenuous and significant efforts have been devoted to the design of relevant network structures by human experts. In this paper, learning from the natural evolution, the deep neural network is expected to be globally evolved by the machine for automatically adapting the structure of the HRS imagery, a multi-objective neural architecture search based HRS image scene classification method is proposed (RSSM-Net). The two objectives of minimizing a classification error and the computational complexity have been simultaneously optimized through the evolutionary multi-objective method, the competitive neural architectures in a Pareto solution set are then obtained. The effectiveness is proved by the experiment of the UC Merced dataset with several networks designed by human experts.",10.1109/IGARSS39084.2020.9323429,Remote sensing;scene classification;neural architecture search;evolutionary algorithm;multiobjective optimization,,
Supervised learning for feed-forward neural networks: a new minimax approach for fast convergence,A. Chella; A. Gentile; F. Sorbello; A. Tarantino,IEEE International Conference on Neural Networks,1993.0,"An approach to the problem of the learning process for feedforward neural networks, based on an optimization point of view, is proposed. The developed algorithm is a minimax method based on a configuration of the quasi-Newton and steepest-descent methods. The optimum point is reached by minimizing the maximum of the error functions of the network without requiring any tuning of internal parameters. The algorithm is tested on several widespread benchmarks and shows superior convergence properties when compared with other algorithms available in the literature. Significant experimental results are included.<<ETX>></ETX>",10.1109/ICNN.1993.298626,,3.0,
A Multi-Criteria Multi-Modal Predictive Trip Planner: Application on Paris Metropolitan Network,P. Benchimol; A. Amrani; M. Khouadjia,2021 IEEE International Smart Cities Conference (ISC2),2021.0,"Public transport route planning is of growing interest in smart cities and especially in metropolitan areas where congestions and traffic jams are frequently recorded. The availability of multiple data sources, such as passenger load in trains or ticketing logs, provides an interesting opportunity to develop decision support tools to help passengers better plan their trips around the city and to enhance their travel experience. We present, in this paper, a multi-criteria journey planner that incorporates train load predictions as criteria. To this end, on the one hand, we enrich the proposed routes with predictive indicators of passenger flow such as the load on board the trains. These indicators are computed for each section of the itinerary using machine learning algorithms. On the other hand, we design a journey planner that incorporates the predicted load in its search criteria.",10.1109/ISC253183.2021.9562921,journey planner;machine learning;multicriteria;transport;train load,,
Cross-Layer Optimization for High Speed Adders: A Pareto Driven Machine Learning Approach,Y. Ma; S. Roy; J. Miao; J. Chen; B. Yu,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,2019.0,"In spite of maturity to the modern electronic design automation (EDA) tools, optimized designs at architectural stage may become suboptimal after going through physical design flow. Adder design has been such a long studied fundamental problem in very large-scale integration industry yet designers cannot achieve optimal solutions by running EDA tools on the set of available prefix adder architectures. In this paper, we enhance a state-of-the-art prefix adder synthesis algorithm to obtain a much wider solution space in architectural domain. On top of that, a machine learning-based design space exploration methodology is applied to predict the Pareto frontier of the adders in physical domain, which is infeasible by exhaustively running EDA tools for innumerable architectural solutions. Considering the high cost of obtaining the true values for learning, an active learning algorithm is proposed to select the representative data during learning process, which uses less labeled data while achieving better quality of Pareto frontier. Experimental results demonstrate that our framework can achieve Pareto frontier of high quality over a wide design space, bridging the gap between architectural and physical designs. Source code and data are available at <uri>https://github.com/yuzhe630/adder-DSE</uri>.",10.1109/TCAD.2018.2878129,Active learning;design space exploration;machine learning;Pareto optimality;prefix adder,6.0,
Feature Learning in Feature-Sample Networks Using Multi-Objective Optimization,F. A. Neto Verri; R. Tinós; L. Zhao,2018 IEEE Congress on Evolutionary Computation (CEC),2018.0,"Data and knowledge representation are fundamental concepts in machine learning. The quality of the representation impacts the performance of a learning model directly. Feature learning transforms or enhances raw data to structures that are effectively exploited by those methods. In recent years, several works have been using complex networks for data representation and analysis. However, no feature learning method has been proposed to enhance such category of representation. Here, we present an unsupervised feature learning mechanism that works on datasets with binary features. First, the dataset is mapped into a feature-sample network. Then, a multi-objective optimization process selects a set of new vertices to produce an enhanced version of the network. The new features depend on a nonlinear function of a combination of preexisting features. Effectively, the process projects the input data into a higher-dimensional space. To solve the optimization problem, we design two metaheuristics based on the lexicographic genetic algorithm and the improved strength Pareto evolutionary algorithm (SPEA2). We show that the enhanced network contains more useful information and can be exploited to improve the performance of machine learning methods. The advantages and disadvantages of each optimization strategy are discussed.",10.1109/CEC.2018.8477891,Feature learning;complex networks;multiobjective optimization;genetic algorithm,,
Machine Learning in Adversarial Game Using Flight Chess,Y. Liu; D. Li; Y. Hu,2011 Third International Conference on Multimedia Information Networking and Security,2011.0,"Game playing is a perfect domain of the study of machine learning for its simplicity that allows the researchers to focus on the learning problems themselves and ignore marginal factors. Many learning techniques derived from games have been applied successfully in other learning problems. In this paper, we introduce a Minimax Recurrence Learning algorithm to reinforce the intelligence of a game agent and a supervised learning technique to train the agent. It proves that our intelligent flight chess agent defeat human players in the flight chess game with high probability. Theory deduction proves that combination of the reinforcement learning and supervised learning techniques used in our agent can learn the essential knowledge in an adversarial game. The infrastructure and the algorithm of our agent can be extended in other learning problems also.",10.1109/MINES.2011.124,reinforcement learning;supervised learning;machine learning;feature characterization,,
Supervised Learning Model Predictive Control Trained by ABC Algorithm for Common-Mode Voltage Suppression in NPC Inverter,M. Babaie; M. Sharifzadeh; M. Mehrasa; G. Chouinard; K. Al-Haddad,IEEE Journal of Emerging and Selected Topics in Power Electronics,2021.0,"Training the weighting factors of model predictive control in multiobjective problems is a time consuming and sophisticated process. In this article, conventional model predictive control (CMPC) has been developed as supervised learning model predictive control (SLMPC) to cancel common-mode voltage (CMV) in a three-phase neutral-point-clamped (NPC) inverter, while other control objectives are desirably tracked. SLMPC is accurately and quickly trained through the artificial bee colony (ABC) algorithm to optimize the controller weighting factors. Using the optimized weighting factors, transient response is minimized and CMV is surpassed. After training the weighting factors, SLMPC containing the optimized waiting factors is applied to the three-phase NPC inverter without considering the ABC algorithm in the control loop. By applying the optimized weighting factors to the cost function, SLMPC has been evaluated under several experimental and simulation tests to show that desired control objectives, particularly CMV suppression, have been attained. The proposed training process can be generalized and used for MPC cost functions with more control objectives to obtain the best possible performance.",10.1109/JESTPE.2020.2984674,Artificial bee colony (ABC) algorithm;common-mode voltage (CMV) suppression;optimized weighting factors;supervised learning model predictive control (SLMPC);three-phase neutral-point-clamped (NPC) inverter,23.0,
Selective ensemble learning method for belief-rule-base classification system based on PAES,W. Liu; W. Wu; Y. Wang; Y. Fu; Y. Lin,Big Data Mining and Analytics,2019.0,"Traditional Belief-Rule-Based (BRB) ensemble learning methods integrate all of the trained sub-BRB systems to obtain better results than a single belief-rule-based system. However, as the number of BRB systems participating in ensemble learning increases, a large amount of redundant sub-BRB systems are generated because of the diminishing difference between subsystems. This drastically decreases the prediction speed and increases the storage requirements for BRB systems. In order to solve these problems, this paper proposes BRBCS-PAES: a selective ensemble learning approach for BRB Classification Systems (BRBCS) based on Pareto-Archived Evolutionary Strategy (PAES) multi-objective optimization. This system employs the improved Bagging algorithm to train the base classifier. For the purpose of increasing the degree of difference in the integration of the base classifier, the training set is constructed by the repeated sampling of data. In the base classifier selection stage, the trained base classifier is binary coded, and the number of base classifiers participating in integration and generalization error of the base classifier is used as the objective function for multi-objective optimization. Finally, the elite retention strategy and the adaptive mesh algorithm are adopted to produce the PAES optimal solution set. Three experimental studies on classification problems are performed to verify the effectiveness of the proposed method. The comparison results demonstrate that the proposed method can effectively reduce the number of base classifiers participating in the integration and improve the accuracy of BRBCS.",10.26599/BDMA.2019.9020008,belief-rule-base;pareto-archived evolutionary strategy;selective ensemble;classification,1.0,
Classification as Clustering: A Pareto Cooperative-Competitive GP Approach,A. R. McIntyre; M. I. Heywood,Evolutionary Computation,2011.0,"Intuitively population based algorithms such as genetic programming provide a natural environment for supporting solutions that learn to decompose the overall task between multiple individuals, or a team. This work presents a framework for evolving teams without recourse to prespecifying the number of cooperating individuals. To do so, each individual evolves a mapping to a distribution of outcomes that, following clustering, establishes the parameterization of a (Gaussian) local membership function. This gives individuals the opportunity to represent <italic>subsets</italic> of tasks, where the overall task is that of classification under the supervised learning domain. Thus, rather than each team member representing an entire class, individuals are free to identify unique subsets of the overall classification task. The framework is supported by techniques from evolutionary multiobjective optimization (EMO) and Pareto competitive coevolution. EMO establishes the basis for encouraging individuals to provide accurate yet nonoverlaping behaviors; whereas competitive coevolution provides the mechanism for scaling to potentially large unbalanced datasets. Benchmarking is performed against recent examples of nonlinear SVM classifiers over 12 UCI datasets with between 150 and 200,000 training instances. Solutions from the proposed coevolutionary multiobjective GP framework appear to provide a good balance between classification performance and model complexity, especially as the dataset instance count increases.",10.1162/EVCO_a_00016,Genetic programming;Pareto multi-objective optimization;coevolution;problem decomposition;classification,5.0,
Thirty Years of Machine Learning: The Road to Pareto-Optimal Wireless Networks,J. Wang; C. Jiang; H. Zhang; Y. Ren; K. -C. Chen; L. Hanzo,IEEE Communications Surveys & Tutorials,2020.0,"Future wireless networks have a substantial potential in terms of supporting a broad range of complex compelling applications both in military and civilian fields, where the users are able to enjoy high-rate, low-latency, low-cost and reliable information services. Achieving this ambitious goal requires new radio techniques for adaptive learning and intelligent decision making because of the complex heterogeneous nature of the network structures and wireless services. Machine learning (ML) algorithms have great success in supporting big data analytics, efficient parameter estimation and interactive decision making. Hence, in this article, we review the thirty-year history of ML by elaborating on supervised learning, unsupervised learning, reinforcement learning and deep learning. Furthermore, we investigate their employment in the compelling applications of wireless networks, including heterogeneous networks (HetNets), cognitive radios (CR), Internet of Things (IoT), machine to machine networks (M2M), and so on. This article aims for assisting the readers in clarifying the motivation and methodology of the various ML algorithms, so as to invoke them for hitherto unexplored services as well as scenarios of future wireless networks.",10.1109/COMST.2020.2965856,Machine learning (ML);future wireless network;deep learning;regression;classification;clustering;network association;resource allocation,189.0,
Regression Models and Ranking Method for p53 Inhibitor Candidates Using Machine Learning,H. Motohashi; T. Teraoka; S. Aoki; H. Ohwada,2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),2018.0,"Radiation therapy is one of the main treatments for cancer. However, it may cause various side effects owing to the apoptosis activity of the p53 protein in normal cells. Therefore, to avoid the side effects, it is important to protect normal cells against radiation by using p53 inhibitors. It is also expected that p53 inhibitors have low toxicity against patients' bodies. However, the design of p53 inhibitors is not easy because drug discovery requires enormous costs and long time. In this paper, we propose a new method for ranking candidate p53 inhibitors, considering both their radioprotective function and cytotoxicity. We use features of the two- and three-dimensional structures of the compounds, including fingerprints, some machine learning methods such as random forest and SVR (Support Vector Machine), and one method for ranking, i.e., the Pareto ranking method. Therefore, we present the regression models of the cytotoxicity and radioprotective functions of the candidates to determine their ranking. Our proposed methods yield useful rankings for drug discovery.",10.1109/BIBM.2018.8621142,SVR;p53;Pareto ranking;machine learning,3.0,
Using Manifold Learning and Minimax Probability Machine for Face Recognition,X. Sun; L. Li; Z. Wang,"2010 Second International Conference on Modeling, Simulation and Visualization Methods",2010.0,"Face recognition has become one of the most important research areas of pattern recognition and machine learning due to its potential applications in many fields. To effectively cope with this problem, a novel face recognition algorithm is proposed by using manifold learning and minimax probability machine. Comprehensive comparisons and extensive experiments show that the proposed algorithm achieves much higher recognition rates than the ordinary face recognition algorithms.",10.1109/WMSVM.2010.46,face recognition;manifold learning;minimax probability machine;pattern recognition,,
Investigating the Creation of a Surrogate Model for Adaptive Control of Amplifier Operating Point Using Machine Learning,C. J. A. Bastos-Filho; L. M. de Freitas; E. de A. Barboza; J. F. Martins-Filho,2020 22nd International Conference on Transparent Optical Networks (ICTON),2020.0,"Dynamic operation is one of the current challenges in optical communication and networks, and the adaptive control of optical amplifier (ACOP) is one of the problems in this challenge. The ACOP approaches aim to define the gains of the optical amplifiers dynamically to increase the quality of the transmission after a cascade of amplifiers. The most recent ACOP approach uses a multiobjective evolutionary optimization algorithm to define the gains of the amplifiers to maximize the optical signal to noise ratio (OSNR) and to minimize OSNR ripple. Despite the promising results regarding Quality of Transmission, it is not desirable to rely on an evolutionary algorithm to make decisions in real-time. In this work, we investigate the creation of a surrogate model that can obtain solutions as good as the multiobjective algorithm, but in real-time. We show the results for a machine learning (ML) regression technique, trained with the optimization algorithm solutions, can return configurations with OSNR less than 1 dB close to the best OSNR returned by the optimization algorithm. Moreover, the ML solution answers in milliseconds, whereas the optimization-based approach needs several minutes to find a proper configuration.",10.1109/ICTON51198.2020.9203524,optical communication;optical amplifier;dynamic operation;adaptive control;machine learning,1.0,
Ensembling of Gene Clusters Utilizing Deep Learning and Protein-Protein Interaction Information,P. Dutta; S. Saha; S. Chopra; V. Miglani,IEEE/ACM Transactions on Computational Biology and Bioinformatics,2020.0,"Cluster ensemble techniques aim to combine the outputs of multiple clustering algorithms to obtain a single consensus partitioning. The current paper reports about the development of a cluster ensemble based technique combining the concepts of multiobjective optimization and deep-learning models for gene clustering where some additional protein-protein interaction information are utilized for generating the consensus partitioning. The proposed ensemble based framework works in four phases: (i) filtering out the irrelevant genes from the microarray dataset: only the statistically significant genes are considered for further data analysis; (ii) generation of diverse base partitionings: a multi-objective optimization-based clustering technique is proposed which simultaneously optimizes three different cluster quality measures and generates a set of partitioning solutions on the Pareto optimal front; (iii) generation of a consensus partitioning: mentha scores, calculated by accessing a highly enriched protein-protein interaction archive named <italic>mentha</italic>, of different clustering solutions are considered for generating a weighted incidence matrix; (iv) finally, two approaches are used to generate a consensus partitioning from the obtained incidence matrix. The first approach is based on a traditional machine learning method, and another approach exploits the graph partitioning algorithm and two deep neural models to generate the final clustering. To validate the efficacy of the proposed ensemble framework, it is applied on five gene expression datasets. We present a comparative analysis of the proposed technique over different clustering algorithms in terms of biological homogeneity index (BHI) and biological stability index (BSI). The traditional approach attains an average 3 and 2 percent improvements over the best non-dominated solution with respect to BHI and BSI, respectively, whereas deep learning models illustrate an average 6.8 and 1.5 percent improvements over the proposed traditional approach with respect to BHI and BSI, respectively. Subsequently, Welch's t-test is executed to prove that the results obtained by the proposed methods are statistically significant. <italic>Availability of data and materials:</italic> <uri>https://github.com/sduttap16/DeepEnsm</uri>.",10.1109/TCBB.2019.2918523,Protein-protein interactions;deep learning;clustering;ensemble technique,4.0,
The U-Net model application for retinal vessels segmentation using minimax approach,V. Martsenyuk; R. Milian; N. Milian,2021 International Conference on Information and Digital Technologies (IDT),2021.0,"In this article the implementation of neural network architecture based on a dense U-Net network is proposed. It is noted that retinal blood vessels are the basis for clinical diagnosis of some diseases. A review of the convolutional networks use for classification tasks and generalizion retinal vessel segmentation algorithms is performed. The general process of the neural network is presented. The differences between the real and the obtained results were evaluated. Evaluation of the neural network is carried out on several parameters. Indicators of binary cross-entropy and learning time when using different tile sizes are presented, based on these data determined by the solution to the problem of minimax ML for binary cross-entropy and learning time. The Figure with the recognized blood vessels as a result of the model is presented.",10.1109/IDT52577.2021.9532495,machine learning;neural network;machine learning library;retinal vessels segmentation;minimax,,
The Blessing of Dimensionality in Many-Objective Search: An Inverse Machine Learning Insight,A. Gupta; Y. -S. Ong; M. Shakeri; X. Chi; A. Z. NengSheng,2019 IEEE International Conference on Big Data (Big Data),2019.0,"Sample-based evolutionary algorithms (EAs) are widely used for optimizing problems with multi (greater than one but less than four) or even many (greater than or equal to four) objectives of interest. In general, the difficulty of a problem exponentially increases with the number of objectives, serving as a clear example of the curse of dimensionality. The exploratory approach an EA takes in these cases has led to it being thought of as a big data generator, progressively sampling and evaluating solutions in high performing regions of a decision space to guide the search towards optimal solutions. Notably, in both multi- and many-objective EAs, the sampled data can be further utilized for building inverse generative models, mapping points in objective space back to solutions in the decision space. Such models offer immense flexibility to a decision maker in generating new target solutions on the fly, thereby facilitating real-time a posteriori preference incorporation into the search. In this paper, we show that the data distribution resulting from a many-objective formulation is in fact more conducive to building accurate inverse models than its multiobjective counterpart. Given the potential utility of these models, we in turn shed light on a rare blessing of dimensionality that is yet to be explored in the context of optimization. We first present simple theoretical arguments supporting our claim. Thereafter, experimental studies of Gaussian process-based inverse modeling for a synthetic and a real-world example are carried out to further confirm the theory.",10.1109/BigData47090.2019.9005525,Blessing of dimensionality;inverse modeling;many-objective optimization;Gaussian processes,,
Minimax Approach for Semivariogram Fitting in Ordinary Kriging,A. Setiyoko; T. Basaruddin; A. M. Arymurthy,IEEE Access,2020.0,"This research paper aims to analyze the minimax approach used in the semivariogram fitting process that forms one stage of the kriging operation performed for interpolation. The conventional method uses the weighted least squares fit for various theoretical functions such as stable, exponential, spherical. However, several recent approaches have been developed using machine learning regression techniques. This research employs the ordinary kriging technique where the proposed minimax approach is expected to increase the accuracy of the interpolation resulted by reducing the error of the final result. Kriging, which is based on the stochastic method, is widely used for spatial values and has been proven to be a better predicting process than deterministic methods. The novel approach to ordinary kriging discussed here, the minimax approach, is able to increase result accuracy based on the experiments performed. Minimax can predict the weights of the semivariogram values better than the weighted least-squares method and performs faster than machine learning approaches.",10.1109/ACCESS.2020.2991428,Minimax techniques;interpolation;approximation methods,2.0,
On the performance of classification algorithms for learning Pareto-dominance relations,S. Bandaru; A. H. C. Ng; K. Deb,2014 IEEE Congress on Evolutionary Computation (CEC),2014.0,"Multi-objective evolutionary algorithms (MOEAs) are often criticized for their high-computational costs. This becomes especially relevant in simulation-based optimization where the objectives lack a closed form and are expensive to evaluate. Over the years, meta-modeling or surrogate modeling techniques have been used to build inexpensive approximations of the objective functions which reduce the overall number of function evaluations (simulations). Some recent studies however, have pointed out that accurate models of the objective functions may not be required at all since evolutionary algorithms only rely on the relative ranking of candidate solutions. Extending this notion to MOEAs, algorithms which can ‘learn’ Pareto-dominance relations can be used to compare candidate solutions under multiple objectives. With this goal in mind, in this paper, we study the performance of ten different off-the-shelf classification algorithms for learning Pareto-dominance relations in the ZDT test suite of benchmark problems. We consider prediction accuracy and training time as performance measures with respect to dimensionality and skewness of the training data. Being a preliminary study, this paper does not include results of integrating the classifiers into the search process of MOEAs.",10.1109/CEC.2014.6900641,Meta-modeling;Multi-objective optimization;Classification algorithms;Pareto-dominance;Machine learning,22.0,
Machine-Learning-Aided Optimization Framework for Design of Medium-Voltage Grid-Connected Solid-State Transformers,J. Saha; D. Hazarika; N. B. Y. Gorla; S. K. Panda,IEEE Journal of Emerging and Selected Topics in Power Electronics,2021.0,"Due to the lack of a comprehensive multiobjective solid-state transformer (SST) design framework, SST designs are mostly obtained through trial/experience. In this article, a machine-learning (ML)-aided optimal SST design framework is proposed, which involves the objectives of maximizing efficiency (<inline-formula> <tex-math notation=""LaTeX"">$\eta $ </tex-math></inline-formula>) and power density (<inline-formula> <tex-math notation=""LaTeX"">$\rho $ </tex-math></inline-formula>). The challenges of computationally expensive magnetics design, coupled with the correlation between magnetics design and performance of semiconductor devices, are tackled by developing a hybrid local optimization algorithm. This local optimization is subsequently learned through ML techniques, using a limited number of optimal design data sets, and, thus, assists in the genesis of optimal SST design limits for several combinations of semiconductor devices and switching frequencies. The proposed framework is implemented for a cascaded matrix-based dual-active-bridge (CMB-DAB) SST comprising of SiC MOSFETs to demonstrate the optimization routine. The optimization results exhibit low-error fits of the selected ML models and the <inline-formula> <tex-math notation=""LaTeX"">$\eta $ </tex-math></inline-formula>–<inline-formula> <tex-math notation=""LaTeX"">$\rho $ </tex-math></inline-formula> limits in different categories of optimal SST designs. The SiC-based SST designs are also observed to offer better <inline-formula> <tex-math notation=""LaTeX"">$\eta $ </tex-math></inline-formula>–<inline-formula> <tex-math notation=""LaTeX"">$\rho $ </tex-math></inline-formula> optimal designs compared to Si-based SSTs. A laboratory-scale CMB-DAB prototype with experimental measurements is also presented to validate the proposed design optimization framework at a scaled-down level.",10.1109/JESTPE.2021.3074408,Design optimization;machine learning (ML);silicon carbide (SiC);solid-state transformer (SST),3.0,
"Local Minimax Learning of Functions With Best Finite Sample Estimation Error Bounds: Applications to Ridge and Lasso Regression, Boosting, Tree Learning, Kernel Machines, and Inverse Problems",L. K. Jones,IEEE Transactions on Information Theory,2009.0,"Optimal local estimation is formulated in the minimax sense for inverse problems and nonlinear regression. This theory provides best mean squared finite sample error bounds for some popular statistical learning algorithms and also for several optimal improvements of other existing learning algorithms such as smoothing splines and kernel regularization. The bounds and improved algorithms are not based on asymptotics or Bayesian assumptions and are truly local for each query, not depending on cross validating estimates at other queries to optimize modeling parameters. Results are given for optimal local learning of approximately linear functions with side information (context) using real algebraic geometry. In particular, finite sample error bounds are given for ridge regression and for a local version of lasso regression. The new regression methods require only quadratic programming with linear or quadratic inequality constraints for implementation. Greedy additive expansions are then combined with local minimax learning via a change in metric. An optimal strategy is presented for fusing the local minimax estimators of a class of experts-providing optimal finite sample prediction error bounds from (random) forests. Local minimax learning is extended to kernel machines. Best local prediction error bounds for finite samples are given for Tikhonov regularization. The geometry of reproducing kernel Hilbert space is used to derive improved estimators with finite sample mean squared error (MSE) bounds for class membership probability in two class pattern classification problems. A purely local, cross validation free algorithm is proposed which uses Fisher information with these bounds to determine best local kernel shape in vector machine learning. Finally, a locally quadratic solution to the finite Fourier moments problem is presented. After reading the first three sections the reader may proceed directly to any of the subsequent applications sections.",10.1109/TIT.2009.2027479,Fusion;inverse problem;minimax;reproducing kernel;ridge regression,6.0,
A multi-objective parallel detection algorithm for images of power transmission line corridors,W. Zheng; C. Li; X. Cui; P. Shang,2020 IEEE 5th Information Technology and Mechatronics Engineering Conference (ITOEC),2020.0,"The application of deep learning-based image recognition technology in remote monitoring of power transmission lines has improved the protection level of power transmission lines. However, since the current hidden danger detection is based on a single general algorithm, the accuracy of the detection results needs to be improved. In this paper, we proposed a multi-objective parallel detection algorithm for images of power transmission line corridors based on basic feature sharing. Firstly, we tested the detection effects of different detection algorithms on various types of hidden danger images, based on which benchmark algorithms are selected and optimized for various types of hidden dangers. After that, we designed a multi-objective parallel detection framework to implement the parallel detection of the above three detection algorithms. The experimental results show that the multiobjective parallel detection algorithm proposed in this paper can improve the detection accuracy, as well the detection speed.",10.1109/ITOEC49072.2020.9141566,power transmission line;hidden danger;deep learning;image recognition;parallel detection,1.0,
Machine Learning Enabled Fast Multi-Objective Optimization for Electrified Aviation Power System Design,D. Jackson; S. Belakaria; Y. Cao; J. Rao Doppa; X. Lu,2020 IEEE Energy Conversion Congress and Exposition (ECCE),2020.0,"With the rise of more electric and all-electric aviation power systems, engineering efforts of system optimization shift to the electrical domain. A substantial amount of time and resources are dedicated to finding the best system architecture and design specifications to meet energy efficiency goals and physical constraints. Current processes utilize models of power system components to determine the optimal designs. However, such modeling is computationally expensive as numerous iterations are required to settle on an optimal design. This paper proposes a machine learning (ML) enabled constrained multi-objective optimization solver to drastically reduce the amount of design iterations required for Pareto set discovery for power systems. The process contributes significantly to design automation. A heavy-duty vertical-takeoff-landing (VTOL) unmanned aerial vehicle (UAV) power system is selected to demonstrate the efficacy and limitation of ML enabled optimization. Two extreme trials were run: 1) a search throughout the entire design space with only 9% valid designs within constraints; 2) a search throughout the valid design space. While Trial 1 was unsuccessful in discovering the Pareto front, Trial 2 uncovered all Pareto optimal designs with a 99% reduction of iterations compared to a brute force method.",10.1109/ECCE44975.2020.9235599,Power Electronics;Power System Design;Machine Learning;Multi-Objective Optimization;Design Automation;Pareto Front;Aviation;UAV;VTOL,1.0,
Minimax Learning for Distributed Inference,C. T. Li; X. Wu; A. Özgür; A. El Gamal,IEEE Transactions on Information Theory,2020.0,"The classical problem of supervised learning is to infer an accurate estimate of a target variable Y from a measured variable X using a set of labeled training samples. Motivated by the increasingly distributed nature of data and decision making, this paper considers a variation of this classical problem in which the inference is distributed between two nodes, e.g., a mobile device and a cloud, with a rate constraint on the communication between them. The mobile device observes X and sends a description M of X to the cloud, which computes an estimate Y̑ of Y. We follow the recent minimax learning approach to study this inference problem and show that it corresponds to a one-shot minimax noisy lossy source coding problem. We then establish information theoretic bounds on the risk-rate Lagrangian cost, leading to a general method for designing a near-optimal descriptor-estimator pair. A key ingredient in the proof of our result is a refined version of the strong functional representation lemma previously used to establish several one-shot source coding theorems. Our results show that a naive estimate-compress scheme for rate-constrained inference is not optimal in general. When the distribution of (X, Y) is known and the error is measured by the logarithmic loss, our bounds on the risk-rate Lagrangian cost provide a new one-shot operational interpretation of the information bottleneck. We also demonstrate a way to bound the excess risk of the descriptor-estimator pair obtained by our method.",10.1109/TIT.2020.3029182,Minimax learning;distributionally robust learning;information bottleneck;one-shot source coding;functional representation,1.0,
"Energy-Efficient and Delay Sensitive Routing Paths Using Mobility Prediction in Mobile WSN: Mathematical Optimization, Markov Chains, and Deep Learning Approaches",G. A. Montoya; C. Lozano-Garzon; Y. Donoso,IEEE Access,2021.0,"In Mobile Wireless Sensor Networks there could be scenarios where absolutely all network nodes (including the base station) are mobile, becoming a very hard task to find a communication path between a sensor node and the base station due to many network variables are changing at each moment. In addition, there are delay-sensitive applications that require establishing communication paths as soon as possible to mitigate low network performance in terms of end-to-end delay, reducing, at the same time, the energy consumption of the network. For this reason, we propose a multiobjective mathematical optimization model for finding the optimal communication path between a source node and a sink (base station) considering hard scenarios where all network nodes are mobile and minimizing end-to-end delay and energy consumption. This mathematical model would offer significant advantages to evaluate new algorithms due to we could know how far or close are the algorithm results from the optimal values given by the mathematical model. In addition, we propose a prediction distributed routing algorithm based on Markov Chains that takes into account the network mobility in order to find as fast as possible a communication path between a source node and a sink with minimal energy consumption. We also propose a deep learning approach to predict future nodes’ distances in a mobile network to determine if future movements of nodes will cause communication disruptions in paths. Significant findings were obtained when the Markov Chains and Deep Learning approaches were compared in terms of predicting nodes mobility and reducing the delay and the energy consumption in the network. The performance of our prediction algorithms (Markov Chains and Deep Learning approaches) is evaluated against the mathematical model to determine how good it is. Finally, to analyze our prediction algorithms considering real online scenarios, we compared it against typical routing algorithms, obtaining promising results in terms of delay and energy consumption in all mobile node scenarios.",10.1109/ACCESS.2021.3124737,Mathematical optimization model for time-varying graphs;delays;energy consumption;prediction algorithm;Markov chains;deep learning,,
Rapid Multi-Criterial Antenna Optimization by Means of Pareto Front Triangulation and Interpolative Design Predictors,S. Koziel; A. Pietrenko-Dabrowska,IEEE Access,2021.0,"Modern antenna systems are designed to meet stringent performance requirements pertinent to both their electrical and field properties. The objectives typically stay in conflict with each other. As the simultaneous improvement of all performance parameters is rarely possible, compromise solutions have to be sought. The most comprehensive information about available design trade-offs can be obtained through multi-objective optimization (MO), typically in the form of a Pareto set. Notwithstanding, MO is a numerically challenging task, in a large part due to high CPU cost of evaluating the antenna properties, normally carried out through full-wave electromagnetic (EM) analysis. Surrogate-assisted procedures can mitigate the cost issue to a certain extent but construction of reliable metamodels is hindered by the curse of dimensionality, and often highly nonlinear antenna characteristics. This work proposes an alternative approach to MO of antennas. The major contribution of our work consists in establishing a deterministic machine learning procedure, which involves sequential generation of Pareto-optimal designs based on the knowledge gathered so far in the process (specifically, by triangulation of the already obtained Pareto set), and local surrogate-assisted refinement procedures. Our methodology allows for rendering uniformly-distributed Pareto designs at the cost of a few hundreds of antenna EM simulations, as demonstrated by means of three verification case studies. Benchmarking against state-of-the-art MO techniques is provided as well.",10.1109/ACCESS.2021.3062449,Antenna optimization;EM-driven design;multi-criterial design;Pareto front triangulation;surrogate modeling,1.0,
On generalisation of machine learning with neural-evolutionary computations,R. Kumar,Proceedings Third International Conference on Computational Intelligence and Multimedia Applications. ICCIMA'99 (Cat. No.PR00300),1999.0,"Generalisation is a non-trivial problem in machine learning and more so with neural networks which have the capabilities of inducing varying degrees of freedom. It is influenced by many factors in network design, such as network size, initial conditions, learning rate, weight decay factor, pruning algorithms, and many more. In spite of continuous research efforts, we could not arrive at a practical solution which can offer a superior generalisation. We present a novel approach for handling complex problems of machine learning. A multiobjective genetic algorithm is used for identifying (near-) optimal subspaces for hierarchical learning. This strategy of explicitly partitioning the data for subsequent mapping onto a hierarchical classifier is found both to reduce the learning complexity and the classification time. The classification performance of various algorithms is compared and it is argued that the neural modules are superior for learning the localised decision surfaces of such partitions and offer better generalisation.",10.1109/ICCIMA.1999.798512,,,
Fine-Grained Powercap Allocation for Power-Constrained Systems Based on Multi-Objective Machine Learning,M. Hao; W. Zhang; Y. Wang; G. Lu; F. Wang; A. V. Vasilakos,IEEE Transactions on Parallel and Distributed Systems,2021.0,"Power capping is an important solution to keep the system within a fixed power constraint. However, for the over-provisioned and power-constrained systems, especially the future exascale supercomputers, powercap needs to be reasonably allocated according to the workloads of compute nodes to achieve trade-offs among performance, energy and powercap. Thus it is necessary to model performance and energy and to predict the optimal powercap allocation strategies. Existing power allocation approaches have insufficient granularity within nodes. Modeling approaches usually model performance and energy separately, ignoring the correlation between objectives, and do not expose the Pareto-optimal powercap configurations. Therefore, this article combines the powercap with uncore frequency scaling and proposes an approach to predict the Pareto-optimal powercap configurations on the power-constrained system for input MPI and OpenMP parallel applications. Our approach first uses the elaborately designed micro-benchmarks and a small number of existing benchmarks to build the training set, and then applies a multi-objective machine learning algorithm which combines the stacked single-target method with extreme gradient boosting to build multi-objective models of performance and energy. The models can be used to predict the optimal processor and memory powercap settings, helping compute nodes perform fine-grained powercap allocation. When the optimal powercap configuration is determined, the uncore frequency scaling is used to further optimize the energy consumption. Compared with the reference powercap configuration, the predicted optimal configurations predicted by our method can achieve an average powercap reduction of 31.35 percent, an average energy reduction of 12.32 percent, and average performance degradation of only 2.43 percent.",10.1109/TPDS.2020.3045983,Power capping;performance and energy modeling;pareto front;multi-objective machine learning,4.0,
Design-Space Exploration of Pareto-Optimal Architectures for Deep Learning with DVFS,G. Santoro; M. R. Casu; V. Peluso; A. Calimera; M. Alioto,2018 IEEE International Symposium on Circuits and Systems (ISCAS),2018.0,"Specialized computing engines are required to accelerate the execution of Deep Learning (DL) algorithms in an energy-efficient way. To adapt the processing throughput of these accelerators to the workload requirements while saving power, Dynamic Voltage and Frequency Scaling (DVFS) seems the natural solution. However, DL workloads need to frequently access the off-chip memory, which tends to make the performance of these accelerators memory-bound rather than computation-bound, hence reducing the effectiveness of DVFS. In this work we use a performance-power analytical model fitted on a parametrized implementation of a DL accelerator in a 28-nm FDSOI technology to explore a large design space and to obtain the Pareto points that maximize the effectiveness of DVFS in the sub-space of throughput and energy efficiency. In our model we consider the impact on performance and power of the off-chip memory using real data of a commercial low-power DRAM.",10.1109/ISCAS.2018.8351685,,4.0,
Approximating Pareto Optimal Set by An Incremental Learning Model,T. Liu; S. Song; X. Li; L. Tan,2021 IEEE Congress on Evolutionary Computation (CEC),2021.0,"Combining a machine learning model within the search procedure has shown great potentials in evolutionary multiobjective optimization (EMO). The priori knowledge obtained from the property of Pareto optimal set (PS) is a great help for reproducing high-quality offspring solutions. However, the existing learning model in the framework of EMO is also accompanied with a high computational cost resulted from its iterative strategy or repetitive learning. To overcome this shortcoming, the paper proposes to approximate the PS by an incremental learning model. Specifically, it consists of two interdependent parts, i.e., a learning module and a forgetting module. The basic idea is to take the all new high-quality offspring solutions at the current evolution iteration as a data stream, and incrementally train a model based on Gaussian mixture models with the data stream to discover the manifold structure of the PS and guide the evolutionary search. The learning module is used to obtain the knowledge from the data stream in a batch manner, while the forgetting module is applied to delete the information from the relatively poor solution as is removed incrementally. The proposed algorithm is employed to test suites, and the numerical experiments demonstrates that the incremental learning model can help to improve the algorithm performance with less computational cost compared with the representative algorithms.",10.1109/CEC45853.2021.9504996,,,
MLComp: A Methodology for Machine Learning-based Performance Estimation and Adaptive Selection of Pareto-Optimal Compiler Optimization Sequences,A. Colucci; D. Juhász; M. Mosbeck; A. Marchisio; S. Rehman; M. Kreutzer; G. Nadbath; A. Jantsch; M. Shafique,"2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)",2021.0,"Embedded systems have proliferated in various consumer and industrial applications with the evolution of Cyber-Physical Systems and the Internet of Things. These systems are subjected to stringent constraints so that embedded software must be optimized for multiple objectives simultaneously, namely reduced energy consumption, execution time, and code size. Compilers offer optimization phases to improve these metrics. However, proper selection and ordering of them depends on multiple factors and typically requires expert knowledge. State-of-the-art optimizers facilitate different platforms and applications case by case, and they are limited by optimizing one metric at a time, as well as requiring a time-consuming adaptation for different targets through dynamic profiling. To address these problems, we propose the novel MLComp methodology, in which optimization phases are sequenced by a Reinforcement Learning-based policy. Training of the policy is supported by Machine Learning-based analytical models for quick performance estimation, thereby drastically reducing the time spent for dynamic profiling. In our framework, different Machine Learning models are automatically tested to choose the best-fitting one. The trained Performance Estimator model is leveraged to efficiently devise Reinforcement Learning-based multi-objective policies for creating quasi-optimal phase sequences. Compared to state-of-the-art estimation models, our Performance Estimator model achieves lower relative error (< 2%) with up to 50 × faster training time over multiple platforms and application domains. Our Phase Selection Policy improves execution time and energy consumption of a given code by up to 12% and 6%, respectively. The Performance Estimator and the Phase Selection Policy can be trained efficiently for any target platform and application domain.",10.23919/DATE51398.2021.9474158,,,
Multiview Synthetic Aperture Radar Automatic Target Recognition Optimization: Modeling and Implementation,J. Pei; Y. Huang; Z. Sun; Y. Zhang; J. Yang; T. -S. Yeo,IEEE Transactions on Geoscience and Remote Sensing,2018.0,"Multiview synthetic aperture radar (SAR) images could provide much richer information for automatic target recognition (ATR) than from a single-view image. It is desirable to find optimal SAR platform flight paths and acquire a sequence of SAR images from appropriate views, so that multiview SAR ATR can be carried out accurately and efficiently. In this paper, a novel optimization framework for multiview SAR ATR is proposed and implemented. The geometry of the multiview SAR ATR is modeled according to the recognition mission and flight environment. Then, the multiview SAR ATR is abstracted and transformed into a constrained multiobjective optimization problem with objective functions considering the tradeoffs between recognition performance and efficiency and security. A specific approach based on convolutional neural network ensemble and constrained nondominated sorting genetic algorithm II is employed to solve the multiobjective optimization, and optimal flight paths and corresponding imaging viewpoints are obtained. The SAR sensor can thus choose an applicable flight path to acquire the multiview SAR images from different tradeoff solutions according to application requirements. Finally, accurate recognition results can be obtained based on those multiview SAR images. Extensive experiments have shown the validity and superiority of the proposed optimization framework of multiview SAR ATR.",10.1109/TGRS.2018.2838593,Automatic target recognition (ATR);convolutional neural network (CNN);ensemble learning;multiview;optimization;synthetic aperture radar (SAR),7.0,
SPIRIT: Spectral-Aware Pareto Iterative Refinement Optimization for Supervised High-Level Synthesis,S. Xydis; G. Palermo; V. Zaccaria; C. Silvano,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,2015.0,"Supervised high-level synthesis (HLS) is a new class of design problems where exploration strategies play the role of supervisor for tuning an HLS engine. The complexity of the problem is increased due to the large set of tunable parameters exposed by the “new wave” of HLS tools that include not only architectural alternatives but also compiler transformations. In this paper, we developed a novel exploration approach, called spectral-aware Pareto iterative refinement, that exploits response surface models (RSMs) and spectral analysis for predicting the quality of the design points without resorting to costly architectural synthesis procedures. We show that the target solution space can be accurately modeled through RSMs, thus enabling a speedup of the overall exploration without compromising the quality of results. Furthermore, we introduce the usage of spectral techniques to find high variance regions of the design space that require analysis for improving the RSMs prediction accuracy.",10.1109/TCAD.2014.2363392,system level design;high level synthesis;design space exploration;machine learning;spectral analysis;Design space exploration (DSE);high-level synthesis (HLS);machine learning;spectral analysis;system level design,29.0,
PDM: Privacy-Aware Deployment of Machine-Learning Applications for Industrial Cyber–Physical Cloud Systems,X. Xu; R. Mo; X. Yin; M. R. Khosravi; F. Aghaei; V. Chang; G. Li,IEEE Transactions on Industrial Informatics,2021.0,"The cyber-physical cloud systems (CPCSs) release powerful capability in provisioning the complicated industrial services. Due to the advances of machine learning (ML) in attack detection, a wide range of ML applications are involved in industrial CPCSs. However, how to ensure the implementation efficiency of these applications, and meanwhile avoid the privacy disclosure of the datasets due to data acquisition by different operators, remain challenging for the design of the CPCSs. To fill this gap, in this article a privacy-aware deployment method (PDM), named PDM, is devised for hosting the ML applications in the industrial CPCSs. In PDM, the ML applications are partitioned as multiple computing tasks with certain execution order, like workflows. Specifically, the deployment problem is formulated as a multiobjective problem for improving the implementation performance and resource utility. Then, the most balanced and optimal strategy is selected by leveraging an improved differential evolution technique. Finally, through comprehensive experiments and comparison analysis, PDM is fully evaluated.",10.1109/TII.2020.3031440,Cyber–physical cloud systems (CPCSs);machine learning (ML);nondominated sorting differential evolution (NSDE);privacy-aware deployment,13.0,
NeuroTrajectory: A Neuroevolutionary Approach to Local State Trajectory Learning for Autonomous Vehicles,S. M. Grigorescu; B. Trasnea; L. Marina; A. Vasilcoi; T. Cocias,IEEE Robotics and Automation Letters,2019.0,"Autonomous vehicles are controlled today either based on sequences of decoupled perception-planning-action operations, either based on End2End or deep reinforcement learning (DRL) systems. Current deep learning solutions for autonomous driving are subject to several limitations (e.g., they estimate driving actions through a direct mapping of sensors to actuators, or require complex reward shaping methods). Although the cost function used for training can aggregate multiple weighted objectives, the gradient descent step is computed by the backpropagation algorithm using a single-objective loss. To address these issues, we introduce NeuroTrajectory, which is a multiobjective neuroevolutionary approach to local state trajectory learning for autonomous driving, where the desired state trajectory of the ego-vehicle is estimated over a finite prediction horizon by a perception-planning deep neural network. In comparison to DRL methods, which predict optimal actions for the upcoming sampling time, we estimate a sequence of optimal states that can be used for motion control. We propose an approach which uses genetic algorithms for training a population of deep neural networks, where each network individual is evaluated based on a multi-objective fitness vector, with the purpose of establishing a so-called Pareto front of optimal deep neural networks. The performance of an individual is given by a fitness vector composed of three elements. Each element describes the vehicle's travel path, lateral velocity and longitudinal speed, respectively. The same network structure can be trained on synthetic, as well as on real-world data sequences. We have benchmarked our system against a baseline Dynamic Window Approach (DWA), as well as against an End2End supervised learning method.",10.1109/LRA.2019.2926224,,17.0,
Maximizing sensitivity in medical diagnosis using biased minimax probability Machine,Kaizhu Huang; Haiqin Yang; Irwin King; M. R. Lyu,IEEE Transactions on Biomedical Engineering,2006.0,"The challenging task of medical diagnosis based on machine learning techniques requires an inherent bias, i.e., the diagnosis should favor the ""ill"" class over the ""healthy"" class, since misdiagnosing a patient as a healthy person may delay the therapy and aggravate the illness. Therefore,the objective in this task is not to improve the overall accuracy of the classification,but to focus on improving the sensitivity (the accuracy of the ""ill"" class) while maintaining an acceptable specificity (the accuracy of the ""healthy"" class). Some current methods adopt roundabout ways to impose a certain bias toward the important class, i.e., they try to utilize some intermediate factors to influence the classification. However, it remains uncertain whether these methods can improve the classification performance systematically. In this paper, by engaging a novel learning tool, the biased minimax probability machine(BMPM), we deal with the issue in a more elegant way and directly achieve the objective of appropriate medical diagnosis. More specifically, the BMPM directly controls the worst case accuracies to incorporate a bias toward the ""ill"" class. Moreover, in a distribution-free way, the BMPM derives the decision rule in such a way as to maximize the worst case sensitivity while maintaining an acceptable worst case specificity. By directly controlling the accuracies,the BMPM provides a more rigorous way to handle medical diagnosis; by deriving a distribution-free decision rule, the BMPM distinguishes itself from a large family of classifiers, namely, the generative classifiers, where an assumption on the data distribution is necessary. We evaluate the performance of the model and compare it with three traditional classifiers: the k-nearest neighbor, the naive Bayesian, and the C4.5. The test results on two medical datasets, the breast-cancer dataset and the heart disease dataset, show that the BMPM outperforms the other three models.",10.1109/TBME.2006.872819,Biased classification;medical diagnosis;minimax probability machine;worst case accuracy,28.0,
"A minimax theorem with applications to machine learning, signal processing, and finance",Seung-Jean Kim; S. Boyd,2007 46th IEEE Conference on Decision and Control,2007.0,"This paper concerns a fractional function of the form x<sup>T</sup>a/√x<sup>T</sup>Bx, where B is positive definite. We consider the game of choosing χ from a convex set, to maximize the function, and choosing (a, B) from a convex set, to minimize it. We prove the existence of a saddle point and describe an efficient method, based on convex optimization, for computing it. We describe applications in machine learning (robust Fisher linear discriminant analysis), signal processing (robust beamforming, robust matched filtering), and finance (robust portfolio selection). In these applications, χ corresponds to some design variables to be chosen, and the pair (a, B) corresponds to the statistical model, which is uncertain.",10.1109/CDC.2007.4434853,,1.0,
Transfer Learning Through Deep Learning: Application to Topology Optimization of Electric Motor,J. Asanuma; S. Doi; H. Igarashi,IEEE Transactions on Magnetics,2020.0,"This article proposes the use of transfer learning for the deep neural network to reduce the computing cost of the topology optimization of electric motors based on a genetic algorithm (GA). The average torque and torque ripple values are shown to be accurately inferred by the transfer learning with small learning data. The individuals on the Pareto front are only evaluated by the finite-element method, while others are fast evaluated only by convolutional neural networks (CNNs). The proposed method makes it possible to reduce the computing cost to less than 15% of the conventional topology optimization method.",10.1109/TMAG.2019.2956849,Deep learning;electric motor;regression;topology optimization;transfer learning,13.0,
Scalable Pareto Front Approximation for Deep Multi-Objective Learning,M. Ruchte; J. Grabocka,2021 IEEE International Conference on Data Mining (ICDM),2021.0,"Multi-objective optimization is important for various Deep Learning applications, however, no prior multi-objective method suits very deep networks. Existing approaches either require training a new network for every solution on the Pareto front or add a considerable overhead to the number of parameters by introducing hyper-networks conditioned on modifiable preferences. In this paper, we present a novel method that contextualizes the network directly on the preferences by adding them to the input space. In addition, we ensure a well-spread Pareto front by forcing the solutions to preserve a small angle to the preference vector. Through extensive experiments, we demonstrate that our Pareto fronts achieve state-of-the-art quality despite being computed significantly faster. Furthermore, we demonstrate the scalability as our method approximates the full Pareto front on the CelebA dataset with an EfficientNet network at a marginal training time overhead of 7% compared to a single-objective optimization. We make the code publicly available at https://github.com/ruchtem/cosmos.",10.1109/ICDM51629.2021.00162,Multi-objective optimization;Deep Learning;Fairness,,
Incremental Cross-Domain Adaptation for Robust Retinopathy Screening via Bayesian Deep Learning,T. Hassan; B. Hassan; M. U. Akram; S. Hashmi; A. H. Taguri; N. Werghi,IEEE Transactions on Instrumentation and Measurement,2021.0,"Retinopathy represents a group of retinal diseases that, if not treated timely, can cause severe visual impairments or even blindness. Many researchers have developed autonomous systems to recognize retinopathy via fundus and optical coherence tomography (OCT) imagery. However, most of these frameworks employ conventional transfer learning and fine-tuning approaches, requiring a decent amount of well-annotated training data to produce accurate diagnostic performance. This article presents a novel incremental cross-domain adaptation instrument that allows any deep classification model to progressively learn abnormal retinal pathologies in OCT and fundus imagery via few-shot training. Furthermore, unlike its competitors, the proposed instrument is driven via a Bayesian multiobjective function that not only enforces the candidate classification network to retain its prior learned knowledge during incremental training, but also ensures that the network understands the structural and semantic relationships between previously learned pathologies and newly added disease categories to effectively recognize them at the inference stage. The proposed framework, evaluated on six public datasets acquired with three different scanners to screen 13 retinal pathologies, outperforms the state-of-the-art competitors by achieving an overall accuracy and F1 score of 0.9826 and 0.9846, respectively.",10.1109/TIM.2021.3122172,Bayesian deep learning;fundus photography;incremental domain adaptation (DA);optical coherence tomography;retinopathy,,
A study of Pareto-based methods for ensemble pool generation and aggregation,V. H. Alves Ribeiro; G. Reynoso-Meza,2019 IEEE Congress on Evolutionary Computation (CEC),2019.0,"In the field of machine learning, the application of ensemble methods is one of the most successful techniques in order to achieve a good performance in classification tasks. The combination of multiple classifiers is able to achieve better results than a single model, and much effort has been put into applying multi-objective optimisation for improving results with diverse ensemble generation and classifier aggregation. Most recently, dynamic classifier selection and weighting has acquired relevance in the field of multiple-classifier systems. However, to the authors knowledge, there has not yet been a comparison study of Pareto based techniques and dynamic ensemble methods. Thus, this paper proposes a comparison of two ensemble member generation techniques (Pareto-based diverse ensemble generation and bootstrap aggregating) and five aggregation methods (selection of the best classifier, majority voting with all members, majority voting with members selected with multi-objective optimisation, dynamic classifier selection and dynamic classifier weighting), performed on six binary classification benchmark data sets. Results indicate that the combination of bootstrap aggregating and majority voting with multi-objective ensemble member selection achieves the best performance.",10.1109/CEC.2019.8790291,Ensemble methods;multi-objective optimisation;supervised learning.,,
Application of Bayesian Machine Learning To Create A Low-Cost Silicon Failure Mechanism Pareto,C. Schuermyer; S. Palosh; P. Babighian; Y. Pan,2019 30th Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC),2019.0,"The increasing challenges with relying on Physical Failure Analysis and inline inspection for ramping the yield are the reason that Volume Scan Diagnostics Analysis (VSDA) has become a mainstream methodology that supplements traditional yield learning. Because scan diagnostics are inherently noisy, the results often require expert knowledge to manually select the location that has the highest likelihood of being correct. In this paper, Failure Mechanism Analysis (FMA) applies the technique of Bayesian Machine Learning in a yield analysis system that can empirically estimate sources of yield loss using physical diagnostic information.",10.1109/ASMC.2019.8791833,,1.0,
Robust transfer learning in multi-robot systems by using sparse autoencoder,L. V. Utkin; S. G. Popov; Y. A. Zhuk,2016 XIX IEEE International Conference on Soft Computing and Measurements (SCM),2016.0,Robust algorithms for transfer learning in multirobot systems based on elements of the deep learning are proposed in the paper. The algorithms are based on using the sparse autoencoder. The main ideas underlying the algorithms are to extend the set of set-valued observations by training examples having uncertain weights and to apply the robust minimax strategy in order to find an optimal autoencoder for dealing with set-valued observations. An interesting scheme for transfer learning is considered for which source learning set is reconstructed by means of the sparse autoencoder trained on the target learning set.,10.1109/SCM.2016.7519735,multi-robot system;deep learning;transfer learning;sparse autoencoder;extreme points;minimax strategy,2.0,
sEMG Signal Classification Using Ensemble Learning Classification Approach and DWT,N. Thakur; L. Mathew,2018 International Conference on Current Trends towards Converging Technologies (ICCTCT),2018.0,"Nowadays surface electromyography (sEMG) signals play a very authoritative role in facilitating a neuromuscular disordered person and disabled person to live a smooth life. This offline study mainly focuses on the denoising, feature extraction and classification of the sEMG signals with discrete wavelet packet transform (DWT) with ensemble support vector machine (SVM) classification approach. In this work DWT (db 2, 4<sup>th</sup> level) is selected for denoising and TFD feature extraction to form feature vectors and soft thresholding method (minimax) was utilized and the threshold value was taken as 2.991. The classification accuracy is 98% is achieved at the better precision and speed of response for elbow movement. feature vectors and soft thresholding method (minimax) was utilized and the threshold value was taken as 2.991. The classification accuracy is 98% is achieved at the better precision and speed of response for elbow movement.",10.1109/ICCTCT.2018.8551098,sEMG- Suface Electromyography Signal;DWT- discrete Wavelet Transform;WGN- White Gaussian Noise;SVM- Support Vector Machine,2.0,
Study of the approximation of the fitness landscape and the ranking process of scalarizing functions for many-objective problems,G. Toscano; K. Deb,2016 IEEE Congress on Evolutionary Computation (CEC),2016.0,"Although surrogate models have been successfully adopted by evolutionary algorithms to solve time-consuming multiobjective problems, their use has been confined to solving problems with a low number of objectives. On the other hand, scalarizing functions have proved to work well with many-objective problems. This paper presents a novel study on many-objective optimization concerning the use of surrogate models to approximate both (1) the fitness landscape of traditional multiobjective approaches and (2) the ranking relation imposed by such approaches. Our methodology involves a thorough comparison of four popular surrogate modeling techniques in order to approximate the fitness landscape and the ranking relations of three different scalarizing functions. Additionally, we explored the interactions of these methods through four well-known scalable test problems with four, six, eight, and ten objectives. Besides finding that Tchebycheff scalarizing function and Gaussian processes for machine learning are accurate methods to handle many-objective problems, one of our most important findings involves the capabilities of metamodeling techniques to approximate the ranking procedure from the information gathered from the parameter space. Such a capability can be effectively used for pre-screening purposes on MOEAs.",10.1109/CEC.2016.7744344,,3.0,
"Combining Machine Learning and Multi Criteria Decision Analysis Modeling Regulatory, Economic and Social Influences on Wind Turbine Allocation",L. Lück; A. Moser,2018 15th International Conference on the European Energy Market (EEM),2018.0,"Knowledge about the future allocation of wind turbines is relevant for assessments of energy markets or necessary grid expansions. In Germany, political decisions drive the allocation together with investment decisions, social rejections, land use planning, regional development and ecological aspects. Taking all influences into account, a standardized multi-criteria optimization problem combining economic suitability, residential burden and site suitability calculates the regional distribution of wind turbines as input for further assessments. By considering the political framework as boundary conditions for the optimization and detailed geographic area suitability factors using a machine learning approach as input parameters, it is possible to assess effects of regulatory restrictions on regional developments. We use a backtesting for validation and weighting of the objectives. Sensitivities of changing regulatory frameworks modeled as different boundary conditions show effects of changing political decisions.",10.1109/EEM.2018.8470016,Wind Energy Integration;Power System Planning;Distributed Power Generation;Pareto Optimization;Machine Learning,1.0,
Minimax Modifications of Linear Discriminant Analysis for Classification with Rare Classes,K. Bratanova; I. Kareev; R. Salimov,2020 IEEE East-West Design & Test Symposium (EWDTS),2020.0,"We consider the problem of classification for imbalanced samples with rare classes. A common problem for machine learning methods in such setting is that a rare class would have extremely high classification error compared to more widespread classes. In general, this problem could be mitigated with re-sampling or fitting additional weights to control the classification errors in classes, though those methods are computationally expensive for large datasets and sometimes fail to attain appropriate results. It this paper we present cost-efficient modifications of Linear Discriminant Analysis allowing to mitigate the problem by minimizing maximal classification error among the classes. For example, this allows achieving more robust machinery malfunction detection algorithms where our expectations on recall would be more consistent among different malfunction types.",10.1109/EWDTS50664.2020.9224895,classification;imbalanced sample dataset;rare class;linear discriminant analysis;minimax error,,
A Robust Machine Learning Method for Cell-Load Approximation in Wireless Networks,D. A. Awan; R. L. G. Cavalcante; S. Stanczak,"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",2018.0,"We propose a learning algorithm for cell-load approximation in wireless networks. The proposed algorithm is robust in the sense that it is designed to cope with the uncertainty arising from a small number of training samples. This scenario is highly relevant in wireless networks where training has to be performed on short time scales because of a fast time-varying communication environment. The first part of this work studies the set of feasible rates and shows that this set is compact. We then prove that the mapping relating a feasible rate vector to the unique fixed point of the non-linear cell-load mapping is monotone and uniformly continuous. Utilizing these properties, we apply an approximation framework that achieves the best worst-case performance. Furthermore, the approximation preserves the monotonicity and continuity properties. Simulations show that the proposed method exhibits better robustness and accuracy for small training sets in comparison with standard approximation techniques for multivariate data.",10.1109/ICASSP.2018.8462320,machine learning;5G;multivariate scattered data;data interpolation;minimax approximation,5.0,
Constrained Multi-Objective Optimization for Automated Machine Learning,S. Gardner; O. Golovidov; J. Griffin; P. Koch; W. Thompson; B. Wujek; Y. Xu,2019 IEEE International Conference on Data Science and Advanced Analytics (DSAA),2019.0,"Automated machine learning has gained a lot of attention recently. Building and selecting the right machine learning models is often a multi-objective optimization problem. General purpose machine learning software that simultaneously supports multiple objectives and constraints is scant, though the potential benefits are great. In this work, we present a framework called Autotune that effectively handles multiple objectives and constraints that arise in machine learning problems. Autotune is built on a suite of derivative-free optimization methods, and utilizes multi-level parallelism in a distributed computing environment for automatically training, scoring, and selecting good models. Incorporation of multiple objectives and constraints in the model exploration and selection process provides the flexibility needed to satisfy trade-offs necessary in practical machine learning applications. Experimental results from standard multi-objective optimization benchmark problems show that Autotune is very efficient in capturing Pareto fronts. These benchmark results also show how adding constraints can guide the search to more promising regions of the solution space, ultimately producing more desirable Pareto fronts. Results from two real-world case studies demonstrate the effectiveness of the constrained multi-objective optimization capability offered by Autotune.",10.1109/DSAA.2019.00051,Multi-objective Optimization;Automated Machine Learning;Distributed Computing System,4.0,
HyperASPO: Fusion of Model and Hyper Parameter Optimization for Multi-objective Machine Learning,A. Kannan; A. Roy Choudhury; V. Saxena; S. Raje; P. Ram; A. Verma; Y. Sabharwal,2021 IEEE International Conference on Big Data (Big Data),2021.0,"Current state of the art methods for generating Pareto-optimal solutions for multi-objective optimization problems mostly rely on optimizing the hyper-parameters of the models (HPO - hyper-parameter Optimization). Few recent, less studied methods focus on optimizing over the space of model parameters, leveraging the problem specific knowledge. We present a generic first-of-a-kind method, referred to as HyperASPO, that combines optimization over the spaces of both hyper-parameters and model parameters for multi-objective optimization of learning problems. HyperASPO consists of two stages. First, we perform a coarse HPO to determine a set of favorable hyper-parameter configurations. In the second step, for each of these configurations, we solve a sequence of weighted single objective optimization problems for estimating Pareto-optimal solutions. We generate the weights in the second step using an adaptive mesh constructed iteratively based on the metrics of interest, resulting in further refinement of Pareto frontier efficiently. We consider the widely used XGBoost (Gradient Boosted Trees) model and validate our method on multiple classification datasets. Our proposed method shows up to 20% improvement over the hypervolumes of Pareto fronts obtained through state of the art HPO based methods with up to 2× reduction in computational time.",10.1109/BigData52589.2021.9671604,Hyperparameter optimization;Model parameters;XGBoost;HyperASPO;Pareto Optimization,,
Pareto-Optimal Bit Allocation for Collaborative Intelligence,S. R. Alvar; I. V. Bajić,IEEE Transactions on Image Processing,2021.0,"In recent studies, collaborative intelligence (CI) has emerged as a promising framework for deployment of Artificial Intelligence (AI)-based services on mobile/edge devices. In CI, the AI model (a deep neural network) is split between the edge and the cloud, and intermediate features are sent from the edge sub-model to the cloud sub-model. In this article, we study bit allocation for feature coding in multi-stream CI systems. We model task distortion as a function of rate using convex surfaces similar to those found in distortion-rate theory. Using such models, we are able to provide closed-form bit allocation solutions for single-task systems and scalarized multi-task systems. Moreover, we provide analytical characterization of the full Pareto set for 2-stream k-task systems, and bounds on the Pareto set for 3-stream 2-task systems. Analytical results are examined on a variety of DNN models from the literature to demonstrate wide applicability of the results.",10.1109/TIP.2021.3060875,Bit allocation;rate distortion optimization;collaborative intelligence;multi objective optimization;deep learning;multi-task learning,2.0,
A Deep Learning Model Based on Multi-Objective Particle Swarm Optimization for Scene Classification in Unmanned Aerial Vehicles,A. Rajagopal; G. P. Joshi; A. Ramachandran; R. T. Subhalakshmi; M. Khari; S. Jha; K. Shankar; J. You,IEEE Access,2020.0,"Recently, the increase in inexpensive and compact unmanned aerial vehicles (UAVs) and light-weight imaging sensors has led to an interest in using them in various remote sensing applications. The processes of collecting, calibrating, registering, and processing data from miniature UAVs and interpreting the data semantically are time-consuming. In UAV aerial imagery, learning effective image representations is central to the scene classification process. Earlier approaches to the scene classification process depended on feature coding methods with low-level hand-engineered features or unsupervised feature learning. These methods could produce mid-level image features with restricted representational abilities, which generally yielded mediocre results. The development of convolutional neural networks (CNNs) has made image classification more efficient. Due to the limited resources in UAVs, it is hard to fine-tune the hyperparameters and the trade-offs between classifier results and computation complexity. This paper introduces a new multi-objective optimization model for evolving state-of-the-art deep CNNs for scene classification, which generates the non-dominant solutions in an automated way at the Pareto front. We use a set of two benchmark datasets to test the performance of the scene classification model and make a detailed comparative study. The proposed method attains a very low computational time of 80 sec and maximum accuracy of 97.88% compared to all other methods. The proposed method is found to be appropriate for the effective scene classification of images captured by UAVs.",10.1109/ACCESS.2020.3011502,Unmanned aerial vehicle;particle swarm optimization;deep learning;convolutional neural networks;machine learning;internet of everything;aerial images;smart environment,22.0,
A Multiple Gradient Descent Design for Multi-Task Learning on Edge Computing: Multi-Objective Machine Learning Approach,X. Zhou; Y. Gao; C. Li; Z. Huang,IEEE Transactions on Network Science and Engineering,2022.0,"Multi-task learning technique is widely utilized in machine learning modeling where commonalities and differences across multiple tasks are exploited. However, multiple conflicting objectives often occur in multi-task learning. Conventionally, a common compromise is to minimize the weighted sum of multiple objectives which may be invalid if the objectives are competing. In this paper, a novel multi-objective machine learning approach is proposed to solve this challenging issue, which reformulates the multi-task learning as multi-objective optimization. To address the issues contributed by existing multi-objective optimization algorithms, a multi-gradient descent algorithm is introduced for the multi-objective machine learning problem by which an innovative gradient-based optimization is leveraged to converge to an optimal solution of the Pareto set. Moreover, the gradient surgery for the multi-gradient descent algorithm is proposed to obtain a stable Pareto optimal solution. As most of the edge computing devices are computational resource-constrained, the proposed method is implemented for optimizing the edge device's memory, computation and communication demands. The proposed method is applied to the multiple license plate recognition problem. The experimental results show that the proposed method outperforms state-of-the-art learning methods and can successfully find solutions that balance multiple objectives of the learning task over different datasets.",10.1109/TNSE.2021.3067454,Deep neural network;edge computing;multi-objective machine learning;multi-task learning;multiple gradient descent,1.0,
Interval-based algorithms to extract fuzzy measures for Software Quality Assessment,X. Wang; A. F. G. Contreras; M. Ceberio; C. Del Hoyo; L. C. Gutierrez; S. Virane,2012 Annual Meeting of the North American Fuzzy Information Processing Society (NAFIPS),2012.0,"In this paper, we consider the problem of automatically assessing sofware quality. We show that we can look at this problem, called Software Quality Assessment (SQA), as a multicriteria decision-making problem. Indeed, just like software is assessed along different criteria, Multi-Criteria Decision Making (MCDM) is about decisions that are based on several criteria that are usually conflicting and non-homogenously satisfied. Nonadditive (fuzzy) measures along with the Choquet integral can be used to model and aggregate the levels of satisfaction of these criteria by considering their relationships. However, in practice, fuzzy measures are difficult to identify. An automated process is necessary and possible when sample data is available. Several optimization approaches have been proposed to extract fuzzy measures from sample data; e.g., genetic algorithms, gradient descent algorithms, and the Bees algorithm, all local search techniques. In this article, we propose a hybrid approach, combining the Bees algorithm and an interval constraint solver, resulting in a focused search expected to be less prone to falling into local results. Our approach, when tested on SQA decision data, shows promise and compares well to previous approaches to SQA that were using machine learning techniques.",10.1109/NAFIPS.2012.6291044,,2.0,
Autonomic Management of a Building’s Multi-HVAC System Start-Up,J. Aguilar; A. Garcés-Jiménez; J. M. Gómez-Pulido; M. D. R. Moreno; J. A. G. De Mesa; N. Gallego-Salvador,IEEE Access,2021.0,"Most studies about the control, automation, optimization and supervision of building HVAC systems concentrate on the steady-state regime, i.e., when the equipment is already working at its setpoints. The originality of the current work consists of proposing the optimization of building multi-HVAC systems from start-up until they reach the setpoint, making the transition to steady state-based strategies smooth. The proposed approach works on the transient regime of multi-HVAC systems optimizing contradictory objectives, such as the desired comfort and energy costs, based on the “Autonomic Cycle of Data Analysis Tasks” concept. In this case, the autonomic cycle is composed of two data analysis tasks: one for determining if the system is going towards the defined operational setpoint, and if that is not the case, another task for reconfiguring the operational mode of the multi-HVAC system to redirect it. The first task uses machine learning techniques to build detection and prediction models, and the second task defines a reconfiguration model using multiobjective evolutionary algorithms. This proposal is proven in a real case study that characterizes a particular multi-HVAC system and its operational setpoints. The performance obtained from the experiments in diverse situations is impressive since there is a high level of conformity for the multi-HVAC system to reach the setpoint and deliver the operation to the steady-state smoothly, avoiding overshooting and other non-desirable transitional effects.",10.1109/ACCESS.2021.3078550,Energy management;heating;ventilation and air conditioning systems;autonomic computing;machine learning;multi-objective optimization;smart building,2.0,
Minimax lower bounds for ridge combinations including neural nets,J. M. Klusowski; A. R. Barron,2017 IEEE International Symposium on Information Theory (ISIT),2017.0,"Estimation of functions of d variables is considered using ridge combinations of the form Σ<sub>k=1</sub><sup>m</sup> c<sub>1, k</sub>Φ(Σ<sub>j=1</sub><sup>d</sup>c<sub>0, j, k</sub>x<sub>j</sub>-b<sub>k</sub>) where the activation function Φ is a function with bounded value and derivative. These include single-hidden layer neural networks, polynomials, and sinusoidal models. From a sample of size n of possibly noisy values at random sites X ϵ B = [-1, 1]<sup>d</sup>, the minimax mean square error is examined for functions in the closure of the ℓ<sub>1</sub> hull of ridge functions with activation Φ. It is shown to be of order d/n to a fractional power (when d is of smaller order than n), and to be of order (log d)/n to a fractional power (when d is of larger order than n). Dependence on constraints v<sub>0</sub> and v<sub>1</sub> on the ℓ<sub>1</sub> norms of inner parameter co and outer parameter c<sub>1</sub>, respectively, is also examined. Also, lower and upper bounds on the fractional power are given. The heart of the analysis is development of information-theoretic packing numbers for these classes of functions.",10.1109/ISIT.2017.8006754,Nonparametric regression;nonlinear regression;neural nets;penalization;machine learning;high-dimensional data analysis;learning theory;generalization error;greedy algorithms;metric entropy;packing sets;polynomial nets;sinusoidal nets;constant weight codes,6.0,
Sequential Minimax Search for Multi-Layer Gene Grouping,W. Wang; X. Zhou; F. Chen; B. Cao,IEEE Access,2019.0,"Many areas of exploratory data analysis need to deal with high-dimensional data sets. Some real life data like human gene have an inherent structure of hierarchy, which embeds multi-layer feature groups. In this paper, we propose an algorithm to search for the number of feature groups in high-dimensional data by sequential minimax method and detect the hierarchical structure of high-dimensional data. Several proper numbers of feature grouping can be discovered. The feature grouping and group weights are investigated for each group number. After the comparison of feature groupings, the multi-layer structure of feature groups is detected. The latent feature group learning (LFGL) algorithm is proposed to evaluate the effectiveness of the number of feature groups and provide a method of subspace clustering. In the experiments on several gene data sets, the proposed algorithm outstands several representative algorithms.",10.1109/ACCESS.2019.2924491,Machine learning;evolutionary computing;feature grouping;high-dimensional data analysis;gene grouping;knowledge transfer,,
Deep Belief Network Enabled Surrogate Modeling for Fast Preventive Control of Power System Transient Stability,T. Su; Y. Liu; J. Zhao; J. Liu,IEEE Transactions on Industrial Informatics,2022.0,"The widely used transient stability-constrained optimal power flow (TSC-OPF) method for power system preventive control is very time-consuming and thus not applicable for large-scale systems. This article proposes a new deep learning-enabled surrogate model that can significantly improve computational efficiency while maintaining high accuracy. To achieve that, the deep belief network (DBN) is strategically integrated with the reference-point-based nondominated sorting genetic algorithm (NSGA-III) to develop a new preventive control framework. The DBN allows us to identify the mapping relationship between the transient stability index and system operational features. The identified functional mapping relationship is further used as the surrogate to connect the DBN results with TSC-OPF for preventive control. The integrated NSGA-III and surrogate model enable the multiobjective optimization to consider various constraints and objectives, such as minimization of costs of generation dispatch cost and load shedding while maintaining the system stability. Extensive simulation results on several IEEE test systems show that the proposed method can achieve highly efficient control solutions and outperform other alternatives in terms of computational efficiency and economic benefits.",10.1109/TII.2021.3072594,Deep belief network (DBN);deep learning;nondominated sorting genetic algorithm (NSGA-III);surrogate model;transient stability preventive control,,
Feature selection for facilitation of evolutionary multi-objective design optimization: Application to IPM motor design problems,A. Salimi; D. A. Lowther,2016 IEEE Conference on Electromagnetic Field Computation (CEFC),2016.0,"This paper discusses the application of statistical analysis and machine learning techniques, more specifically Correlation Feature Selection (CFS), in multi-objective design optimization of problems where the computational cost (of optimization) is dominated by the cost of solution evaluations, e.g. electromagnetic shape design problems. Here, CFS is used in order to reduce the dimensionality of the design space. As demonstrated through an Internal Permanent Magnet (IPM) motor design problem, the reduction information can be useful in decreasing the cost of optimization as well as detecting the dependencies of different variables in the vicinity of the optima.",10.1109/CEFC.2016.7816205,Design Optimization;Dimensionality Reduction;Feature Selection;Machine Learning;Pareto Optimization,,
Wrapper Framework for Test-Cost-Sensitive Feature Selection,L. Jiang; G. Kong; C. Li,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",2021.0,"Feature selection is an optional preprocessing procedure and is frequently used to improve the classification accuracy of a machine learning algorithm by removing irrelevant and/or redundant features. However, in many real-world applications, the test cost is also required for making optimal decisions, in addition to the classification accuracy. To the best of our knowledge, thus far, few studies have been conducted on test-cost-sensitive feature selection (TCSFS). In TCSFS, the objectives are twofold: 1) to improve the classification accuracy and 2) to decrease the test cost. Therefore, in fact, it constitutes a multiobjective optimization problem. In this paper, we transformed this multiobjective optimization problem into a single-objective optimization problem by utilizing a new evaluation function and in this paper, we propose a new general wrapper framework for TCSFS. Specifically, in our proposed framework, we add a new term to the evaluation function of a wrapper feature selection method so that the test cost of measuring features is taken into account. We experimentally tested our proposed framework, using 36 classification problems from the University of California at Irvine (UCI) repository, and compared it to some other state-of-the-art feature selection frameworks. The experimental results showed that our framework allows users to select an optimal feature subset with the minimal test cost, while simultaneously maintaining a high classification accuracy.",10.1109/TSMC.2019.2904662,Classification accuracy;decision making;feature selection;test cost;test-cost-sensitive learning,18.0,
Training Confidence-Calibrated Classifier via Distributionally Robust Learning,H. Wu; M. D. Wang,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",2020.0,"Supervised learning via empirical risk minimization, despite its solid theoretical foundations, faces a major challenge in generalization capability, which limits its application in real-world data science problems. In particular, current models fail to distinguish in-distribution and out-of-distribution and give over confident predictions for out-of-distribution samples. In this paper, we propose an distributionally robust learning method to train classifiers via solving an unconstrained minimax game between an adversary test distribution and a hypothesis. We showed the theoretical generalization performance guarantees, and empirically, our learned classifier when coupled with thresholded detectors, can efficiently detect out-of-distribution samples.",10.1109/COMPSAC48688.2020.0-230,supervised learning;adversarial machine learning;robust machine learning;distributionally robust optimization,,
Deep Learning Empowered Traffic Offloading in Intelligent Software Defined Cellular V2X Networks,B. Fan; Z. He; Y. Wu; J. He; Y. Chen; L. Jiang,IEEE Transactions on Vehicular Technology,2020.0,"The ever-increasing and unbalanced traffic load in cellular vehicle-to-everything (C-V2X) networks have increased the network congestion and led to user dissatisfaction. To relieve the network congestion and improve the traffic load balance, in this paper, we propose an intelligent software defined C-V2X network framework to enable flexible and low-complexity traffic offloading by decoupling the network data plane from the control plane. In the data plane, the cellular traffic offloading and the vehicle assisted traffic offloading are jointly performed. In the control plane, deep learning is deployed to reduce the software defined network (SDN) control complexity and improve the traffic offloading efficiency. Under the proposed framework, we investigate the traffic offloading problem, which can be formulated as a multi-objective optimization problem. Specifically, the first objective maximizes the cellular access point (AP) throughput with consideration of the load balance by associating the users with the APs. The second objective maximizes the vehicle throughput with consideration of the vehicle trajectory by associating the delay-insensitive users with the vehicles. The two objectives are coupled by the association between the cellular APs and the vehicles. A deep learning based online-offline approach is proposed to solve the multi-objective optimization problem. The online stage decouples the optimization problem into two sub-problems and utilizes the `Pareto optimal' to find the solutions. The offline stage utilizes deep learning to learn from the historical optimization information of the online stage and helps predict the optimal solutions with reduced complexity. Numerical results are provided to validate the advantages of our proposed traffic offloading approach via deep learning in C-V2X networks.",10.1109/TVT.2020.3023194,C-V2X networks;deep learning;software-defined-networking;traffic offloading,3.0,
An impossibility result for high dimensional supervised learning,M. H. Rohban; P. Ishwar; B. Orten; W. C. Karl; V. Saligrama,2013 IEEE Information Theory Workshop (ITW),2013.0,"We study high-dimensional asymptotic performance limits of binary supervised classification problems where the class conditional densities are Gaussian with unknown means and covariances and the number of signal dimensions scales faster than the number of labeled training samples. We show that the Bayes error, namely the minimum attainable error probability with complete distributional knowledge and equally likely classes, can be arbitrarily close to zero and yet the limiting minimax error probability of every supervised learning algorithm is no better than a random coin toss. In contrast to related studies where the classification difficulty (Bayes error) is made to vanish, we hold it constant when taking high-dimensional limits. In contrast to VC-dimension based minimax lower bounds that consider the worst case error probability over all distributions that have a fixed Bayes error, our worst case is over the family of Gaussian distributions with constant Bayes error. We also show that a nontrivial asymptotic minimax error probability can only be attained for parametric subsets of zero measure (in a suitable measure space). These results expose the fundamental importance of prior knowledge and suggest that unless we impose strong structural constraints, such as sparsity, on the parametric space, supervised learning may be ineffective in high dimensional small sample settings.",10.1109/ITW.2013.6691252,,1.0,
Scheduling of Deep Learning Applications Onto Heterogeneous Processors in an Embedded Device,D. Kang; J. Oh; J. Choi; Y. Yi; S. Ha,IEEE Access,2020.0,"As the need for on-device machine learning is increasing recently, embedded devices tend to be equipped with heterogeneous processors that include a multi-core CPU, a GPU, and/or a DNN accelerator called a Neural Processing Unit (NPU). In the scheduling of multiple deep learning (DL) applications in such embedded devices, there are several technical challenges. First, a task can be mapped onto a single core or any number of available cores. So we need to consider various possible configurations of CPU cores. Second, embedded devices usually apply Dynamic Voltage and Frequency Scaling (DVFS) to reduce energy consumption at run-time. We need to consider the effect of DVFS in the profiling of task execution times. Third, to avoid overheat condition, it is recommended to limit the core utilization. Lastly, some cores will be shut-down at run-time if core utilization is not high enough, in case the hot-plugging option is turned on. In this paper, we propose a scheduling technique based on Genetic Algorithm to run DL applications on heterogeneous processors, considering all those issues. First, we aim to optimize the throughput of a single deep learning application. Next, we aim to find the Pareto optimal scheduling of multiple DL applications in terms of the response time of each DL application and overall energy consumption under the given throughput constraints of DL applications. The proposed technique is verified with real DL networks running on two embedded devices, Galaxy S9 and HiKey970.",10.1109/ACCESS.2020.2977496,Deep learning scheduling;genetic algorithm;heterogeneous processor;mobile device,5.0,
Minimax Learning for Remote Prediction,C. T. Li; X. Wu; A. Ozgur; A. El Gamal,2018 IEEE International Symposium on Information Theory (ISIT),2018.0,"The classical problem of supervised learning is to infer an accurate predictor of a target variable Y from a measured variable X by using a finite number of labeled training samples. Motivated by the increasingly distributed nature of data and decision making, in this paper we consider a variation of this classical problem in which the prediction is performed remotely based on a rate-constrained description M of X. Upon receiving M, the remote node computes an estimate Y of Y. We follow the recent minimax approach to study this learning problem and show that it corresponds to a one-shot minimax noisy source coding problem. We then establish information theoretic bounds on the risk-rate Lagrangian cost and a general method to design a near-optimal descriptor-estimator pair, which can be viewed as a rate-constrained analog to the maximum conditional entropy principle used in the classical minimax learning problem. Our results show that a naive estimate-compress scheme for rate-constrained prediction is not in general optimal.",10.1109/ISIT.2018.8437318,,4.0,
A Multi-objective Deep Reinforcement Learning Approach for Stock Index Future’s Intraday Trading,W. Si; J. Li; P. Ding; R. Rao,2017 10th International Symposium on Computational Intelligence and Design (ISCID),2017.0,"Modern artificial intelligence has been widely discussed to practice in automated financial asserts trading. Automated intraday trading means that the agent can react to the market conditions automatically, while simultaneously make the right decisions. Besides, the profits will be made within a day considering transaction cost charged by the brokerage company. In this paper, we introduce a multiobjective deep reinforcement learning approach for intraday financial signal representation and trading. We design the deep neural networks to automatically discover the dynamic market features, then a reinforcement learning method implemented by a special kind of recurrent neural network (LSTM) is applied to make continuous trading decisions. In terms of balancing the profit and risk, we implement a multi-objective structure which includes two objectives with different weights. We conduct experiments on stock index futures data, and our analysis and experiments not only offer insights into financial market features mining, but also provide a straightforward and reliable method to make profits, which sheds light on its wide application on automated financial trading.",10.1109/ISCID.2017.210,Deep learning;Reinforcement learning;Intraday trading;Financial signal processing,9.0,
A New Multi-Layer Classification Method Based on Logistic Regression,K. Kang; F. Gao; J. Feng,2018 13th International Conference on Computer Science & Education (ICCSE),2018.0,"To improve the effect of logistic regression in multiobjective classification and explore its greatest potential, a set of training and classification algorithms is constructed, by using the high accuracy of two-class classification. Multi-layer predictions are made under the premise of ensuring clear structure of the model. The method of outlier detection is introduced to choose a proper number of two-class classifiers for categories that are prone to be confused. Then further predictions are made with these two-class classifiers. The evaluation on MNIST dataset show that this method can effectively improve the classification accuracy of multi-class datasets with limited increase of running time.",10.1109/ICCSE.2018.8468725,logistic regression;multi-objective classification;outlier;machine learning;MNIST,5.0,
A statistical machine learning based modeling and exploration framework for run-time cross-stack energy optimization,C. Zhang; A. Ravindran,2013 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),2013.0,"As the complexity of many-core processors grow, meeting performance, energy, temperature, reliability, and noise requirements under dynamically changing operating conditions requires run-time optimization of all parts of the computing stack - architecture, system software, and applications. Unfortunately, the combination of design parameters for the entire computing stack results in an operating space of millions of points that must be explored and evaluated at run-time. In this paper, we present a statistical machine learning (SML) based modeling framework that can be used to rapidly explore such vast operating spaces. We construct a multivariate adaptive regression spline (MARS) based model that uses a number of architecture and application parameters as predictor variables to predict performance and power. We then use a Pareto-front exploring evolutionary algorithm to determine operating points for optimal power and performance. The operating points constituting the Pareto front are stored in look-up tables for runtime use. The proposed framework is applied to an ×264 video encoding application executing on a quad core processor. The microarchitectural predictor variables include core and cache parameters. The application predictor variables include the video resolution, and visual quality determined by the choice of the motion estimation algorithm. The model outputs the average frames per second (FPS) and the average power consumption. The MARS model has an R<sup>2</sup> of 0.9657 and 0.9467 respectively for FPS and power. For a video frame resolution of 480x320, and FPS of 20, a power saving of 55% can be obtained by jointly tuning the microarchitectural parameters and the visual quality.",10.1109/ISPASS.2013.6557161,modeling;energy;optimization;run-time;cross stack,,
Multi-Objective Convolutional Neural Networks for Robot Localisation and 3D Position Estimation in 2D Camera Images,J. Miseikis; I. Brijacak; S. Yahyanejad; K. Glette; O. J. Elle; J. Torresen,2018 15th International Conference on Ubiquitous Robots (UR),2018.0,"The field of collaborative robotics and human-robot interaction often focuses on the prediction of human behaviour, while assuming the information about the robot setup and configuration being known. This is often the case with fixed setups, which have all the sensors fixed and calibrated in relation to the rest of the system. However, it becomes a limiting factor when the system needs to be reconfigured or moved. We present a deep learning approach, which aims to solve this issue. Our method learns to identify and precisely localise the robot in 2D camera images, so having a fixed setup is no longer a requirement and a camera can be moved. In addition, our approach identifies the robot type and estimates the 3D position of the robot base in the camera image as well as 3D positions of each of the robot joints. Learning is done by using a multiobjective convolutional neural network with four previously mentioned objectives simultaneously using a combined loss function. The multi-objective approach makes the system more flexible and efficient by reusing some of the same features and diversifying for each objective in lower layers. A fully trained system shows promising results in providing an accurate mask of where the robot is located and an estimate of its base and joint positions in 3D. We compare the results to our previous approach of using cascaded convolutional neural networks.",10.1109/URAI.2018.8441813,,8.0,
Investigation of the Noise Sensitivity of Machine Learning Algorithms on Credit Card Fraud Detection,İ. Aytutuldu; M. A. Aydin,2021 29th Signal Processing and Communications Applications Conference (SIU),2021.0,"The misleading of machine learning based credit card fraud detection systems, due to various cyber attacks and information transfer-related distortions, is highly critical for the financial sector and its effects globally. In this study, the noise sensitivity and reliability of the machine learning algorithms on the credit card transactions database, which was balanced by over sampling method, were investigated. For this purpose, the noise generated in different distributions was added from 5% to 100 percent level and applied on different algorithms. Common noise distributions such as Normal, Poisson, Pareto, Exponential, Power and Uniform have been used. Logistic regression, K nearest neighbor, Decision trees, Random Forest, Extreme Gradient Boosting (XGB) and Gradient Boosting (GB) machine learning algorithms have been used in this study. Results were evaluated by complexity matrix and f1 score. The results include evaluation and comparison of classification criteria for each algorithm and noise level for the noise sensitivity study.",10.1109/SIU53274.2021.9477832,Credit Card Fraud Detection;Machine Learning;Noise;Noise Sensitivity,,
Machine Learning-Assisted Tolerance Analysis and Its Application to Antennas,Q. Wu; W. Chen; H. Wang; W. Hong,2020 IEEE International Symposium on Antennas and Propagation and North American Radio Science Meeting,2020.0,"An efficient machine learning-assisted tolerance analysis (MLATA) method is proposed by applying machine learning (ML) methods into multiple layers of the antenna tolerance analysis. The computational time for operations including worst case performance searching, maximum input tolerance hypervolume searching and robust optimization is greatly reduced while maintaining high reliability due to the introduction of the ML methods. The surrogate models which are built using ML methods have been introduced to predict both antenna performance and tolerance of parameters at given design points. The Pareto front combining antenna performance, robustness and size has been obtained to guide trade-offs for antenna robust design. A planar inverted-L antenna for mobile terminals is simulated to validate the proposed MLATA method.",10.1109/IEEECONF35879.2020.9330387,,1.0,
Deep-Learning-Based Resource Allocation for Multi-Band Communications in CubeSat Networks,S. Nie; J. M. Jornet; I. F. Akyildiz,2019 IEEE International Conference on Communications Workshops (ICC Workshops),2019.0,"CubeSats, a type of miniaturized satellites with the benefits of low cost and short deployment cycle, are envisioned as a promising solution for future satellite communication networks. Currently, CubeSats communicate only with ground stations under limited spectrum resources and at low data rates, whereas with growing launches of CubeSats and more diverse services expected every year, novel communication techniques and resource allocation schemes should be investigated. In this paper, a multiobjective resource allocation strategy is designed based on deep learning algorithms for autonomous operation in CubeSats across millimeter wave (60-300 GHz) and Terahertz band (300 GHz-1 THz) frequencies with the utilization of reconfigurable plasmonic reflectarrays. Simulation results demonstrate the intersatellite links can achieve multi-gigabits-per-second throughput and ground-to-satellite links with more than 10 times of capacity enhancements in realistic channel conditions.",10.1109/ICCW.2019.8757157,,5.0,
Feature selection for event extraction in biomedical text,A. Majumder; M. Hasanuzzaman; A. Ekbal,2015 Eighth International Conference on Advances in Pattern Recognition (ICAPR),2015.0,"In this paper we report our work on multiobjective optimization (MOO) based feature selection approach for event extraction in biomedical texts. Event extraction deals with the detection and classification of expressions that represent complex biological phenomenon involving genes and proteins. We perform feature selection within the framework of a robust machine learning algorithm, namely Conditional Random Field (CRF). We implement a set of diverse features that exploit lexical, shallow syntactic and contextual information. At first we develop a single objective optimization (SOO) based feature selection technique where we optimize F-measure function. Thereafter we develop two different models of MOO based feature selection by optimizing different pairs of objective functions, i.e. recall and precision; and feature count and F-measure. We carried out experiments on the benchmark setup of BioNLP-2013 shared task. We obtain the best performance with the overall average recall, precision and F-measure values of 57.04%, 75.08% and 64.77%, respectively. Evaluation shows that the classifier can achieve good performance level when trained with an effective feature set. We also observe that MOO can indeed performs better than the SOO based approach.",10.1109/ICAPR.2015.7050708,Text Mining;Event Extraction;Feature Selection;Conditional Random Field,1.0,
Adaptive strategy optimization with multi-agent machine learning in the game of radar countermeasure,D. Zhang; Y. Li; Z. Tian; Z. Jiang,IET International Radar Conference (IET IRC 2020),2020.0,"Traditional radar countermeasure usually acts with some pre-defined strategies, ignoring the dynamic changes of both sides. In this paper, the scenario of radar countermeasure is represented as a two-player zero-sum dynamic game, where each player adaptively optimizes its own strategy. Specifically, according to game theory, two effective multi-agent machine learning methods, i.e. multi-stage minimax backward induction and deep counterfactual regret minimization are utilized to obtain the final strategies for both players. The experimental results demonstrate that the learned strategies for both players are more effective and reasonable than some simple strategies.",10.1049/icp.2021.0527,,,
Weather-Driven Predictive Control of a Battery Storage for Improved Microgrid Resilience,D. Gutierrez-Rojas; A. Mashlakov; C. Brester; H. Niska; M. Kolehmainen; A. Narayanan; S. Honkapuro; P. H. J. Nardelli,IEEE Access,2021.0,"This paper aims to introduce a predictive weather-based control policy for the microgrid energy management to improve the resilience of the microgrid. This policy relies on the application of machine learning models for the prediction of microgrid load demand and solar production and supply interruption in the upstream distribution network. The predictions serve as an input to multiobjective chance constraint optimization that balances the microgrid resilience and economic objectives based on the probability of a supply interruption. The interruption predictions are made with a decision-tree-based model that can predict an upcoming interruption in the distribution network with 78% of the maximum accuracy. The case study microgrid consisting of several customers, solar photovoltaic generation, and battery storage is applied to cluster areas located in Finland. Overall, the developed control policy shows an improvement in the daily resilience of the microgrid in regard to an interruption in the main grid when compared with economic dispatch only.",10.1109/ACCESS.2021.3133490,Microgrid resilience;weather prediction;machine learning;battery storage;chance constraint optimization,,
From minimax value to low-regret algorithms for online Markov decision processes,P. Guan; M. Raginsky; R. Willett,2014 American Control Conference,2014.0,"The standard Markov Decision Process (MDP) framework assumes a stationary (or at least predictable) environment. Online learning algorithms can deal with non-stationary or unpredictable environments, but there is no notion of a state that might be changing throughout the learning process as a function of past actions. In recent years, there has been a growing interest in combining the above two frameworks and considering an MDP setting, where the cost function is allowed to change arbitrarily after each time step. However, most of the work in this area has been algorithmic: given a problem, one would design an algorithm from scratch and analyze its performance on a case-by-case basis. Moreover, the presence of the state and the assumption of an arbitrarily varying environment complicate both the theoretical analysis and the development of computationally efficient methods. This paper builds on recent results of Rakhlin et al. to give a general framework for deriving algorithms in an MDP setting with arbitrarily changing costs. This framework leads to a unifying view of existing methods and provides a general procedure for constructing new ones.",10.1109/ACC.2014.6858844,Markov processes;Machine learning,2.0,
Minimax strategies for training classifiers under unknown priors,R. Alaiz-Rodriguez; J. Cid-Sueiro,Proceedings of the 12th IEEE Workshop on Neural Networks for Signal Processing,2002.0,"Most supervised learning algorithms are based on the assumption that the training data set reflects the underlying statistical model of the real data. However, this stationarity assumption is not always satisfied in practice: quite frequently, class prior probabilities are not in accordance with the class proportions in the training data set. The minimax approach is based on selecting the classifier that minimize the error probability under the worst case conditions. We propose a two-step learning algorithm to train a neural network in order to estimate the minimax classifier that is robust to changes in the class priors. During the first step, posterior probabilities based on training data priors are estimated. During the second step, class priors are modified in order to minimize a cost function that is asymptotically equivalent to the worst-case error rate. This procedure is illustrated on a softmax-based neural network. Several experimental results show the advantages of the proposed method with respect to other approaches.",10.1109/NNSP.2002.1030036,,,
The SMART Framework: Selection of Machine Learning Algorithms With ReplicaTions—A Case Study on the Microvascular Complications of Diabetes,B. P. Swan; M. E. Mayorga; J. S. Ivy,IEEE Journal of Biomedical and Health Informatics,2022.0,"Over 34 million people in the US have diabetes, a major cause of blindness, renal failure, and amputations. Machine learning (ML) models can predict high-risk patients to help prevent adverse outcomes. Selecting the ‘best’ prediction model for a given disease, population, and clinical application is challenging due to the hundreds of health-related ML models in the literature and the increasing availability of ML methodologies. To support this decision process, we developed the Selection of Machine-learning Algorithms with ReplicaTions (SMART) Framework that integrates building and selecting ML models with decision theory. We build ML models and estimate performance for multiple plausible future populations with a replicated nested cross-validation technique. We rank ML models by simulating decision-maker priorities, using a range of accuracy measures (e.g., AUC) and robustness metrics from decision theory (e.g., minimax Regret). We present the SMART Framework through a case study on the microvascular complications of diabetes using data from the ACCORD clinical trial. We compare selections made by risk-averse, -neutral, and -seeking decision-makers, finding agreement in 80% of the risk-averse and risk-neutral selections, with the risk-averse selections showing consistency for a given complication. We also found that the models that best predicted outcomes in the validation set were those with low performance variance on the testing set, indicating a risk-averse approach in model selection is ideal when there is a potential for high population feature variability. The SMART Framework is a powerful, interactive tool that incorporates various ML algorithms and stakeholder preferences, generalizable to new data and technological advancements.",10.1109/JBHI.2021.3094777,Data-driven modeling;decision theory;diabetes;machine learning,,
Reusing Genetic Programming for Ensemble Selection in Classification of Unbalanced Data,U. Bhowan; M. Johnston; M. Zhang; X. Yao,IEEE Transactions on Evolutionary Computation,2014.0,"Classification algorithms can suffer from performance degradation when the class distribution is unbalanced. This paper develops a two-step approach to evolving ensembles using genetic programming (GP) for unbalanced data. The first step uses multiobjective (MO) GP to evolve a Pareto-approximated front of GP classifiers to form the ensemble by trading-off the minority and the majority class against each other during learning. The MO component alleviates the reliance on sampling to artificially rebalance the data. The second step, which is the focus this paper, proposes a novel ensemble selection approach using GP to automatically find/choose the best individuals for the ensemble. This new GP approach combines multiple Pareto-approximated front members into a single composite genetic program solution to represent the (optimized) ensemble. This ensemble representation has two main advantages/novelties over traditional genetic algorithm (GA) approaches. First, by limiting the depth of the composite solution trees, we use selection pressure during evolution to find small highly-cooperative groups of individuals for the ensemble. This means that ensemble sizes are not fixed a priori (as in GA), but vary depending on the strength of the base learners. Second, we compare different function set operators in the composite solution trees to explore new ways to aggregate the member outputs and thus, control how the ensemble computes its output. We show that the proposed GP approach evolves smaller more diverse ensembles compared to an established ensemble selection algorithm, while still performing as well as, or better than the established approach. The evolved GP ensembles also perform well compared to other bagging and boosting approaches, particularly on tasks with high levels of class imbalance.",10.1109/TEVC.2013.2293393,Classification;ensemble machine learning;genetic programming;unbalanced data,57.0,
Advanced Design Optimization Technique for Torque Profile Improvement in Six-Phase PMSM Using Supervised Machine Learning for Direct-Drive EV,H. Dhulipati; E. Ghosh; S. Mukundan; P. Korta; J. Tjong; N. C. Kar,IEEE Transactions on Energy Conversion,2019.0,"Few of the challenges with development of a single on-board motor for direct-drive electric vehicles include high torque density and low torque ripple. Therefore, in this paper, a 36-slot, 34-pole consequent pole six-phase permanent magnet synchronous machine (PMSM) has been optimized to address the aforementioned challenges for direct-drive application. Existing literature on optimization processes that rely solely on finite element models are restricted to three-phase machines only and also take longer computation time. Therefore, this paper proposes a novel optimization approach based on supervised machine learning for six-phase PMSM. In this approach, a non-conventional extended dual dq-frame model that accounts for higher order space harmonics in inductances and flux linkages has been developed and used for accurate computation of average torque and torque ripple of six-phase PMSM. Using the performance characteristics obtained from the extended dual dq-frame model for a set of initial design candidates, support vector regression algorithm is employed for supervised machine learning and increasing solutions in the design space. Furthermore, pareto front is used for selecting optimal machine models with maximum torque density and reduced torque ripple. Multi-objective trade-offs and comparison of initial and optimized designs based on average torque, torque ripple, efficiency and cost are performed.",10.1109/TEC.2019.2933619,Direct–drive;  $dq$  –frame model;electric vehicle;machine learning;permanent magnet synchronous machine;support vector regression;time–step finite element analysis,20.0,
Wind Turbine Fault Diagnosis and Predictive Maintenance Through Statistical Process Control and Machine Learning,J. -Y. Hsu; Y. -F. Wang; K. -C. Lin; M. -Y. Chen; J. H. -Y. Hsu,IEEE Access,2020.0,"This study applies statistical process control and machine learning techniques to diagnose wind turbine faults and predict maintenance needs by analyzing 2.8 million sensor data collected from 31 wind turbines from 2015 to 2017 in Taiwan. Unlike previous studies that only relied on historical wind turbine data, this study analyzed the sensor data with practitioners' insight by incorporating maintenance check list items into the data mining processes. We used Pareto analyses, scatter plots, and the cause and effect diagram to cluster and classify the failure types of wind turbines. In addition, control charts were used to establish a monitoring mechanism to track whether operation data are deviated from the controls (i.e., standard deviations) as a mean to detect wind turbine abnormalities. While statistical process control was applied to fault diagnosis, machine learning algorithms were used to predict maintenance needs of wind turbines. First, the density-based spatial clustering of applications with noise algorithm was used to classify abnormal-state wind turbine data from normal-state data. Then, random forest and decision tree algorithms were employed to construct the predictive models for wind turbine anomalies and tested with K-fold cross-validation. The results indicate a high level of accuracy: 92.68% for the decision tree model, and 91.98% for the random forest model. The study demonstrates that, by data mining and modeling, the failures of wind turbines can be detected, and the maintenance needs of parts can be predicted. Model results may provide technicians early warnings, improve equipment efficient, and decrease system downtime of wind turbine operation.",10.1109/ACCESS.2020.2968615,Decision trees;fault diagnosis;machine learning;predictive maintenance;random forest;statistical process control;wind energy,24.0,
Multilevel Sensor Fusion With Deep Learning,V. Vielzeuf; A. Lechervy; S. Pateux; F. Jurie,IEEE Sensors Letters,2019.0,"In the context of deep learning, this article presents an original deep network, namely CentralNet, for the fusion of information coming from different sensors. This approach is designed to efficiently and automatically balance the tradeoff between early and late fusion (i.e., between the fusion of low-level versus high-level information). More specifically, at each level of abstraction—the different levels of deep networks—unimodal representations of the data are fed to a central neural network which combines them into a common embedding. In addition, a multiobjective regularization is also introduced, helping to both optimize the central network and the unimodal networks. Experiments on four multimodal datasets not only show the state-of-the-art performance but also demonstrate that CentralNet can actually choose the best possible fusion strategy for a given problem.",10.1109/LSENS.2018.2878908,Sensor data fusion;multimodal fusion;neural networks,8.0,
Pareto-Optimal Model Selection via SPRINT-Race,T. Zhang; M. Georgiopoulos; G. C. Anagnostopoulos,IEEE Transactions on Cybernetics,2018.0,"In machine learning, the notion of multi-objective model selection (MOMS) refers to the problem of identifying the set of Pareto-optimal models that optimize by compromising more than one predefined objectives simultaneously. This paper introduces SPRINT-Race, the first multi-objective racing algorithm in a fixed-confidence setting, which is based on the sequential probability ratio with indifference zone test. SPRINT-Race addresses the problem of MOMS with multiple stochastic optimization objectives in the proper Pareto-optimality sense. In SPRINT-Race, a pairwise dominance or non-dominance relationship is statistically inferred via a non-parametric, ternary-decision, dual-sequential probability ratio test. The overall probability of falsely eliminating any Pareto-optimal models or mistakenly returning any clearly dominated models is strictly controlled by a sequential Holm's step-down family-wise error rate control method. As a fixed-confidence model selection algorithm, the objective of SPRINT-Race is to minimize the computational effort required to achieve a prescribed confidence level about the quality of the returned models. The performance of SPRINT-Race is first examined via an artificially constructed MOMS problem with known ground truth. Subsequently, SPRINT-Race is applied on two real-world applications: 1) hybrid recommender system design and 2) multi-criteria stock selection. The experimental results verify that SPRINT-Race is an effective and efficient tool for such MOMS problems.<sup>11</sup>MATLAB code of SPRINT-Race is available at https://github.com/watera427/SPRINT-Race.",10.1109/TCYB.2017.2647821,Model selection (MS);multi-objective optimization;racing algorithm;sequential probability ratio test (SPRT),3.0,
Design of Artificial Neural Networks Using a Memetic Pareto Evolutionary Algorithm Using as Objectives Entropy versus Variation Coefficient,J. C. Fernández; C. Hervás; F. J. Martínez; M. Cruz,2009 Ninth International Conference on Intelligent Systems Design and Applications,2009.0,"This paper proposes a multi-classification pattern algorithm using multilayer perceptron neural network models which try to boost two conflicting main objectives of a classifier, a high correct classification rate and a high classification rate for each class. To solve this machine learning problem, we consider a Memetic Pareto Evolutionary approach based on the NSGA2 algorithm (MPENSGA2), where we defined two objectives for determining the goodness of a classifier: the cross-entropy error function and the variation coefficient of its sensitivities, because both measures are continuous functions, making the convergence more robust. Once the Pareto front is built, we use an automatic selection methodology of individuals: the best model in accuracy (upper extreme in the Pareto front). This methodology is applied to solve six benchmark classification problems, obtaining promising results and achieving a high classification rate in the generalization set with an acceptable level of accuracy for each class.",10.1109/ISDA.2009.153,Classification;Neural Networks;Multi-objective;Entropy;Variation Coefficient,,
A Fast Design Space Exploration Framework for the Deep Learning Accelerators: Work-in-Progress,A. Colucci; A. Marchisio; B. Bussolino; V. Mrazek; M. Martina; G. Masera; M. Shafique,2020 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS),2020.0,"The Capsule Networks (CapsNets) is an advanced form of Convolutional Neural Network (CNN), capable of learning spatial relations and being invariant to transformations. CapsNets requires complex matrix operations which current accelerators are not optimized for, concerning both <sub>training</sub> and <sub>inference</sub> passes. Current state-of-the-art simulators and design space exploration (DSE) tools for DNN hardware neglect the modeling of training operations, while requiring long exploration times that slow down the complete design flow. These impediments restrict the real-world applications of CapsNets (e.g., autonomous driving and robotics) as well as the further development of DNNs in life-long learning scenarios that require training on low-power embedded devices. Towards this, we present <sub>XploreDL</sub>, a novel framework to perform fast yet high-fidelity DSE for both inference and training accelerators, supporting both CNNs and CapsNets operations. <sub>XploreDL</sub> enables a resource-efficient DSE for accelerators, focusing on power, area, and latency, highlighting Pareto-optimal solutions which can be a green-lit to expedite the design flow. <sub>XploreDL</sub> can reach the same fidelity as ARM's SCALE-sim, while providing 600x speedup and having a 50x lower memory-footprint. Preliminary results with a deep CapsNet model on MNIST for training accelerators show promising Pareto-optimal architectures with up to 0.4 TOPS/squared-mm and 800 fJ/op efficiency. With inference accelerators for AlexNet the Pareto-optimal solutions reach up to 1.8 TOPS/squared-mm and 200 fJ/op efficiency.",10.1109/CODESISSS51650.2020.9244038,Design Space Exploration;Hardware Accelerator;Capsule Networks;Convolutional Neural Networks;Training,,
Learning-Enabled NoC Design for Heterogeneous Manycore Systems,R. G. Kim,2020 21st International Symposium on Quality Electronic Design (ISQED),2020.0,"As systems grow in specialization (e.g., domain specific architectures), we need the tools to handle the growing design space from increased heterogeneity and system sizes. In this paper, we investigate the specific challenges posed by heterogeneous systems on the NoC in two separate contexts: wireless- and 3D-enabled, formulate each as a separate multiobjective optimization problem, and present a machine learning based design space exploration technique, MOO-STAGE, to intelligently explore this growing design space.",10.1109/ISQED48828.2020.9137000,NoC;Wireless NoC;3D NoC;Multi-objective optimization;Machine learning;CPU-GPU systems,,
Computational Intelligence-Based Methodology for Antenna Development,M. C. D. Melo; P. B. Santos; E. Faustino; C. J. A. Bastos-Filho; A. Cerqueira Sodré,IEEE Access,2022.0,"The antenna design is a challenging task, which might be time-consuming using conventional computational methods that typically require high computational capability, due to the need for several sweeps and re-running processes. This work proposes an efficient and accurate computational intelligence-based methodology for the antenna design and optimization. The computational technical solution consists of a surrogate model application, composed of a Multilayer Perceptron (MLP) artificial neural network with backpropagation for the regression process. Combined with the surrogate model, two multiobjective optimization meta-heuristic strategies, Non-dominated Sorting Genetic Algorithm (NSGA-II) and Multiobjective Evolutionary Algorithm based on Decomposition (MOEA/D), are used to overcome the mentioned issues from the traditional antenna design method. A study of case considering a dipole antenna for the 3.5 GHz 5G band is reported, as proof of the proposed methodology concept. Comparisons of antenna impedance matching obtained by the proposed methodology, numerical full-wave results from ANSYS HFSS and experimental result from the antenna prototype are performed for demonstrating its applicability and effectiveness for antenna development.",10.1109/ACCESS.2021.3137198,Antennas design;computational intelligence;machine learning;multi-objective optimization,,
Minimax Active Learning Via Minimal Model Capacity,S. Shayovitz; M. Feder,2019 IEEE 29th International Workshop on Machine Learning for Signal Processing (MLSP),2019.0,"Active learning is a form of machine learning which combines supervised learning and feedback to minimize the training set size, subject to low generalization errors. Since direct optimization of the generalization error is difficult, many heuristics have been developed which lack a firm theoretical foundation. In this paper, a new information theoretic criterion is proposed based on a minimax log-loss regret formulation of the active learning problem. In the first part of this paper, a Redundancy Capacity theorem for active learning is derived along with an optimal learner. Building on this, a new active learning criterion is proposed which naturally induces an exploration - exploitation trade-off in feature selection. In the second part, the linear separator hypotheses class with additive label noise is considered and a low complexity algorithm is proposed which optimizes the active learning criterion from the first part. This greedy algorithm is based on the Posterior Matching scheme for communication with feedback and is shown that for BSC and BEC label noise, the proposed information theoretic criterion decays at an exponential rate.",10.1109/MLSP.2019.8918907,Active Learning;Linear Separator;Posterior Matching,1.0,
Evolutionary Multi-objective Ensemble Learning for Multivariate Electricity Consumption Prediction,H. Song; A. K. Qin; F. D. Salim,2018 International Joint Conference on Neural Networks (IJCNN),2018.0,"Energy consumption prediction typically corresponds to a multivariate time series prediction task where different channels in the multivariate time series represent energy consumption data and various auxiliary data related to energy consumption such as environmental factors. It is non-trivial to resolve this task, which requires finding the most appropriate prediction model and the most useful features (extracted from the raw data) to be used by the model. This work proposes an evolutionary multi-objective ensemble learning (EMOEL) technique which uses extreme learning machines (ELMs) as base predictors due to its highly recognized efficacy. EMOEL employs evolutionary multi-objective optimization to search for the optimal parameters of the model as well as the optimal features fed into the model subjected to two conflicting criteria, i.e., accuracy and diversity. It leads to a Pareto front composed of non-dominated optimal solutions where each solution depicts the number of hidden neurons in the ELM, the selected channels in the multivariate time series, the selected feature extraction methods and the selected time windows applied to the selected channels. The optimal solutions in the Pareto front stand for different end-to-end prediction models which may lead to different prediction results. To boost ultimate prediction accuracy, the models with respect to these optimal solutions are linearly combined with combination coefficients being optimized via an evolutionary algorithm. We evaluate the proposed method in comparison to some existing prediction techniques on an Australian University based dataset, which demonstrates the superiority of the proposed method.",10.1109/IJCNN.2018.8489261,,2.0,
Software to Predict the Process Parameters of Electron Beam Welding,V. S. Tynchenko; S. O. Kurashkin; V. V. Tynchenko; V. V. Bukhtoyarov; V. V. Kukartsev; R. B. Sergienko; S. V. Tynchenko; K. A. Bashmur,IEEE Access,2021.0,"This paper discusses the problem of choosing the effective process parameters of electron beam welding (EBW). To that end, the research team has developed a mathematical model that applies machine learning to predict the effective process parameters. Since predicting process parameters requires a regression model, this research uses regression analysis algorithms such as the ridge regression and the random forest regressor. The paper analyzes whether these algorithms are applicable to the problem and tests the accuracy of their predictions. To generalize the approach and strengthen the justification of choosing the hyperparameters of the regression algorithms studied herein and considering the high variability of these hyperparameters, the multiobjective optimization technique applicable for this combinatorial problem - an (evolutionary) genetic algorithm - is proposed to determine effective sets of hyperparameters. All the models successfully addressed the task, achieving a forecasting accuracy of at least 89%. The article presents the final form of the ridge regression model describing the dependence of the weld’s depth and width: for the weld depth, there is a 2nd degree polynomial dependence with a regularization of 10<sup>−5</sup>, and for the weld width, there is a 3rd degree polynomial dependence with a regularization of 10<sup>−4</sup>. An automated system based on this approach that accurately predicts the process parameters is proposed herein. In addition to performing basic modeling functions, the proposed system allows the visualization of the model-predicted data in the form of an interactive plot. This function could be useful for technologists by allowing them to determine the process parameters that ensure the required weld dimensions. Adopting the proposed EBW parameter prediction method in practice will provide decision support for cases when engineers need to test the EBW process or to start making new products.",10.1109/ACCESS.2021.3092221,Electron beam welding;choice of process parameters;software;decision support;prediction;ridge regression;random forest;genetic algorithm;algorithm ensembles;machine learning,5.0,
Smart Multi-Objective Evolutionary GAN,M. Baioletti; G. D. Bari; V. Poggioni; C. A. C. Coello,2021 IEEE Congress on Evolutionary Computation (CEC),2021.0,"Generative Adversarial Network (GAN) is a family of machine learning algorithms designed to train neural networks able to imitate real data distributions. Unfortunately, GAN suffers from problems such as gradient vanishing and mode collapse. In Multi-Objective Evolutionary Generative Adversarial Network (MO-EGAN) these problems were addressed using an evolutionary technique combined with Multi-Objective selection, obtaining better results on synthetic datasets at the expense of larger computation times. In this works, we present the Smart MultiObjective Evolutionary Generative Adversarial Network (SMO-EGAN) algorithm, which reduces the computational cost of MO-EGAN and achieves better results on real data distributions.",10.1109/CEC45853.2021.9504858,,,
Assembly Line Anomaly Detection and Root Cause Analysis Using Machine Learning,O. Abdelrahman; P. Keikhosrokiani,IEEE Access,2020.0,"Anomaly detection is becoming widely used in Manufacturing Industry to enhance product quality. At the same time, it plays a great role in several other domains due to the fact that anomaly may reveal rare but represent an important phenomenon. The objective of this paper is to detect anomalies and identify the possible variables that caused these anomalies on historical assembly data for two series of products. Multiple anomaly detection techniques were performed; HBOS, IForest, KNN, CBLOF, OCSVM, LOF, and ABOD. Moreover, we used AUROC and Rank Power as performance metrics, followed by Boosting ensemble learning method to ensure the best anomaly detectors robustness. The techniques that gave the highest performance are KNN, ABOD for both product series datasets with 0.95 and 0.99 AUROC respectively. Finally, we applied a statistical root cause analysis on the detected anomalies with the use of Pareto chart to visualize the frequency of the possible causes and its cumulative occurrence. The results showed that there are seven rejection causes for both product series, whereas the first three causes are responsible for 85% of the rejection rates. Besides, assembly machines engineers reported a significant reduction in the rejection rates in both assembly machines after tuning the specification limits of the rejection causes identified by this research results.",10.1109/ACCESS.2020.3029826,Anomaly detection;assembly lines;big data;machine learning;manufacturing industries;root cause analysis;unsupervised learning,5.0,
Pareto Self-Supervised Training for Few-Shot Learning,Z. Chen; J. Ge; H. Zhan; S. Huang; D. Wang,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2021.0,"While few-shot learning (FSL) aims for rapid generalization to new concepts with little supervision, self-supervised learning (SSL) constructs supervisory signals directly computed from unlabeled data. Exploiting the complementarity of these two manners, few-shot auxiliary learning has recently drawn much attention to deal with few labeled data. Previous works benefit from sharing inductive bias between the main task (FSL) and auxiliary tasks (SSL), where the shared parameters of tasks are optimized by minimizing a linear combination of task losses. However, it is challenging to select a proper weight to balance tasks and reduce task conflict. To handle the problem as a whole, we propose a novel approach named as Pareto self-supervised training (PSST) for FSL. PSST explicitly decomposes the few-shot auxiliary problem into multiple constrained multi-objective subproblems with different trade-off preferences, and here a preference region in which the main task achieves the best performance is identified. Then, an effective preferred Pareto exploration is proposed to find a set of optimal solutions in such a preference region. Extensive experiments on several public benchmark datasets validate the effectiveness of our approach by achieving state-of-the-art performance.",10.1109/CVPR46437.2021.01345,,2.0,
The Potential of Sentinel Satellites for Large Area Aboveground Forest Biomass Mapping,A. Haywood; C. Stone; S. Jones,IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium,2018.0,"Estimation of aboveground forest biomass is critical for regional carbon policies and sustainable forest management. Both passive optical remote sensing and active microwave remote sensing can play an important role in the monitoring of forest biomass. In this study, the recently launched Sentinel-2 Multi Spectral Instrument satellite and Sentinel-1 SAR satellite systems were evaluated and integrated to investigate the relative strengths of each sensor for mapping aboveground forest biomass at a regional scale. The Australian state of Victoria, with its wide range of forest vegetation was chosen as the study area to demonstrate the scalability and transferability of the approach. In this study aboveground forest biomass (AGB) was defined as the tons of carbon per hectare for the aboveground components (stem, branches, leaves) of all live large trees greater than 10 cm in diameter at breast height (DBHOB). Sentinel-2 and Sentinel-1 data were fused within a machine learning framework using a boosted regression tree model and high-quality ground survey data. Multicriteria evaluations showed the use of the two independent and fundamentally different Sentinel satellite systems were able to provide robust estimates (R<sup>2</sup> of 0.62, RMSE of 32.2 t.C.ha<sup>-1</sup>) of aboveground forest biomass, with each sensor compensating for the weakness (cloud perturbations and spectral saturation for Sentinel 2, and sensitivity to ground moisture for Sentinel 1) of each other. As archives for Sentinel-2 and Sentinel-1 continue to grow, mapping aboveground forest biomass and dynamics at moderate resolution over large regions should become increasingly feasible.",10.1109/IGARSS.2018.8517597,biomass estimation;Sentinel-1;Sentinel-2;machine learning;boosted regression tree model;data fusion;Victoria;Australia,1.0,
Imbalanced learning with a biased minimax probability machine,Kaizhu Huang; Haiqin Yang; Irwin King; M. R. Lyu,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",2006.0,"Imbalanced learning is a challenged task in machine learning. In this context, the data associated with one class are far fewer than those associated with the other class. Traditional machine learning methods seeking classification accuracy over a full range of instances are not suitable to deal with this problem, since they tend to classify all the data into a majority class, usually the less important class. In this correspondence, the authors describe a new approach named the biased minimax probability machine (BMPM) to deal with the problem of imbalanced learning. This BMPM model is demonstrated to provide an elegant and systematic way for imbalanced learning. More specifically, by controlling the accuracy of the majority class under all possible choices of class-conditional densities with a given mean and covariance matrix, this model can quantitatively and systematically incorporate a bias for the minority class. By establishing an explicit connection between the classification accuracy and the bias, this approach distinguishes itself from the many current imbalanced-learning methods; these methods often impose a certain bias on the minority data by adapting intermediate factors via the trial-and-error procedure. The authors detail the theoretical foundation, prove its solvability, propose an efficient optimization algorithm, and perform a series of experiments to evaluate the novel model. The comparison with other competitive methods demonstrates the effectiveness of this new model.",10.1109/TSMCB.2006.870610,Fractional programming (FP);imbalanced learning;receiver operating characteristic (ROC) analysis;worst case accuracy,38.0,
Chess Moves Prediction using Deep Learning Neural Networks,H. Panchal; S. Mishra; V. Shrivastava,2021 International Conference on Advances in Computing and Communications (ICACC),2021.0,"Chess is a game that is popular for high intelligence and strategic thinking. There has been a lot of research on chess for predicting chess moves, applying chess game theory, and automating chess games. The art of playing chess using computer vision can be implemented using various learning algorithms. A class of Deep Learning has the ability to solve problems of predicting chess moves although facing the necessity of huge datasets. The traditional chess algorithm Minimax with the Convolutional Neural Network can perceive and learn the patterns and rules in chess i.e., identification of some small and native tactics of the game, and should be trained on this method with appropriate functions for smarter universal play. CNN when trained with appropriate architecture and validation data can learn to function based on the reasoning in complex logical tasks. Training on 15,00,000 board states in the dataset which is a board state represented as 8x8x14 dimensions. Each board state is given as an input to the input layer of the Convolutional Neural Network. The CNN model tested and validated against the stockfish chess engine achieved the best accuracy of 39.16% for board evaluation. However, this doesn’t reflect the actual accuracy of the model since the evaluation by the model is relative for two different board states. CNN learning the game of chess and based on the result of chess is essentially pre-computation on a given situation.",10.1109/ICACC-202152719.2021.9708405,Chess Moves Prediction;Convolution Neural Network;Deep Learning;Strategy Board Game,,
Pareto-Optimal Quantized ResNet Is Mostly 4-bit,A. Abdolrashidi; L. Wang; S. Agrawal; J. Malmaud; O. Rybakov; C. Leichner; L. Lew,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2021.0,"Quantization has become a popular technique to compress neural networks and reduce compute cost, but most prior work focuses on studying quantization without changing the network size. Many real-world applications of neural networks have compute cost and memory budgets, which can be traded off with model quality by changing the number of parameters. In this work, we use ResNet as a case study to systematically investigate the effects of quantization on inference compute cost-quality tradeoff curves. Our results suggest that for each bfloat16 ResNet model, there are quantized models with lower cost and higher ac-curacy; in other words, the bfloat16 compute cost-quality tradeoff curve is Pareto-dominated by the 4-bit and 8-bit curves, with models primarily quantized to 4-bit yielding the best Pareto curve. Furthermore, we achieve state-of-the-art results on ImageNet for 4-bit ResNet-50 with quantization-aware training, obtaining a top-1 eval accuracy of 77.09%. We demonstrate the regularizing effect of quantization by measuring the generalization gap. The quantization method we used is optimized for practicality: It requires little tuning and is designed with hardware capabilities in mind. Our work motivates further research into optimal numeric formats for quantization, as well as the development of machine learning accelerators supporting these formats. As part of this work, we contribute a quantization library written in JAX, which is open-sourced at https://github.com/google-research/google-research/tree/master/aqt.",10.1109/CVPRW53098.2021.00345,,,
An Integrated Expert System with a Supervised Machine Learning based Probabilistic Approach to Play Tic-Tac-Toe,M. S. K. Inan; R. Hasan; T. T. Prama,"2021 IEEE 12th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",2021.0,"Tic-Tac-Toe, also known as Noughts and Crosses, is a widely popular game among people of all ages. In recent times, due to the rapid development of Artificial Intelligence (AI) based algorithms, AI in Games has become an interesting topic for research in both academia and industry. Due to the complicated yet competent nature of AI algorithms, the design and implementation of such AI-driven approaches in games are challenging and time intensive. In this regard, we propose a supervised Machine Learning (ML)-based approach that contributes in designing an innovative and less complex Tic-Tac-Toc expert system. Integrating AI and ML in the solution process will lead the concerned community toward a more lightweight and computationally efficient systems for playing games. In this study, we propose a novel algorithmic solution by combining an ensemble-based boosting approach and rule-based inference to build a probabilistic expert system that strategically chooses the best optimal move for next possible state of the game. A benchmark dataset containing 255,168 unique game states of Tic Tac Toe was utilized at training stage. The proposed strategy is able to successfully settle a draw against never-loosing MiniMax algorithm in 18 standard test cases.",10.1109/UEMCON53757.2021.9666728,noughts and crosses;machine learning in games;xgboost;tic tac toe;ai in games,,
Pareto-Optimal Active Learning with Cost,S. Adams; T. Cody; P. A. Beling,"2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",2021.0,"Supervised learning algorithms require a set of labeled training data. In many engineering applications, acquiring and accurately labeling the training data can be time consuming, burdensome, and costly. Active learning is an area of machine learning that selects observations in an unlabeled set to be passed to an oracle to retrieve the ground truth label, thereby improving the efficiency of the labeling and training process. However, most active learning algorithms only consider model improvement and, therefore, ignore cost considerations. The active learning algorithms that do consider cost require the practitioner to specify the trade-off between model improvement and cost. We propose an active learning with cost method that does not require this trade-off to be specified by randomly sampling observations from the Pareto optimal frontier. Further, we propose an extension to this method that accounts for uncertainty in the cost estimate of labeling an observation. These methods are evaluated on publicly available data sets, and the numerical experiments demonstrate that the proposed methods can produce models that achieve similar performance to standard active learning algorithms while reducing the labeling cost.",10.1109/SMC52423.2021.9658761,,,
DyBatch: Efficient Batching and Fair Scheduling for Deep Learning Inference on Time-sharing Devices,S. Zhang; W. Li; C. Wang; Z. Tari; A. Y. Zomaya,"2020 20th IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing (CCGRID)",2020.0,"Recently, Deep Learning (DL) is widely applied to intelligent systems equipped with resource-constraint hardware accelerators. With multiple DL applications sharing the resource, the execution model can be divided into two stages: (i) batching independent inference tasks initiated by each application, and (ii) scheduling batches to run in a time-sharing manner. The state-of-the-art DL serving systems employ the execution model by organizing sequential tasks into batches and then scheduling batches concerning their targeting deep neural network (DNN) models in a round-robin manner. However, we demonstrated that these practices fail to alleviate the slowdown of tasks, and there is a need to re-visit batching and scheduling in terms of efficiency and fairness. To this end, we formulated batching as a resource allocation problem and investigated scheduling in terms of each application's utilization on the device. Then, we proposed the fine-grained batching scheme and fairness-driven scheduling scheme for DL serving and implemented a prototype system called DyBatch. To be exact, DyBatch accomplishes efficient batching by taking into account Pareto efficiency of and envy between batches. Besides, DyBatch's fair scheduler monitors the resource utilization of all applications and assigns a batch from the application with the lowest utilization for execution first. Evaluation under various benchmarks with comparison to the baseline system Tensorflow Serving (TFS) shows the superiority of DyBatch, which achieves up to 55% reduction of slowdown, and up to 12% improvement of throughput.",10.1109/CCGrid49817.2020.00-32,deep learning inference;batching;scheduling;efficiency;fairness;model serving,,
Data Mining-Based Model Simplification and Optimization of an Electrical Power Generation System,Z. Dai; L. Wang; S. Yang,IEEE Transactions on Transportation Electrification,2020.0,"To assess the performance of electrification in an aircraft, multiphysics modeling becomes a good choice for the design of more-electric equipment. However, the high computational cost and huge design space of this complex model lead to difficulties in the optimal design of the electrical power system, thus model simplification is mandatory. For this purpose, this article first proposes a novel model simplification approach based on data mining, and the design of a small electrical power generation system is investigated to demonstrate it. According to the formulated multiphysics model of the system, this article uses the optimal Latin Hypercube-based design of experiment to generate data for the analysis. Based on the generated data, a fusion algorithm integrating multiple feature selection methods is presented to facilitate the dimensionality reduction of the problem's design space. Also, machine learning algorithms are applied to the surrogate model establishment, allowing the reduction of computational time. The investigation of various optimization routines with various multiobjective genetic algorithms shows that the proposed practices improve the system-level optimization efficiency with low computational complexity, ease of search, and high accuracy, which is competitive compared with state-of-the-arts.",10.1109/TTE.2020.2995745,Classification;data mining (DM);design of experiment (DoE);electric machine;electrified aircraft;feature selection;machine learning;model simplification;system-level design,1.0,
Deep Neural Language-agnostic Multi-task Text Classifier,K. Gawron; M. Pogoda; N. Ropiak; M. Swędrowski; J. Kocoń,2021 International Conference on Data Mining Workshops (ICDMW),2021.0,"Many publications prove that the creation of a multiobjective machine learning model is possible and reasonable. Moreover, we can see significant gains in expanding the knowledge domain, increasing prediction quality, and reducing the inference time. New developments in cross-lingual knowledge transfer open up a range of possibilities, particularly in working with low-resource languages. With a motivation to explore the latest subfields of natural language processing and their interactions, we decided to create a multi-task multilingual model for the following text classification tasks: functional style, domain, readability, and sentiment. The paper discusses the effectiveness of particular language-agnostic approaches to Polish and English and the effectiveness and validity of the multi-task model.",10.1109/ICDMW53433.2021.00023,deep learning;language-agnostic;multi-task text classification,,
Machine Learning Meets Quantitative Planning: Enabling Self-Adaptation in Autonomous Robots,P. Jamshidi; J. Cámara; B. Schmerl; C. Käestner; D. Garlan,2019 IEEE/ACM 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS),2019.0,"Modern cyber-physical systems (e.g., robotics systems) are typically composed of physical and software components, the characteristics of which are likely to change over time. Assumptions about parts of the system made at design time may not hold at run time, especially when a system is deployed for long periods (e.g., over decades). Self-adaptation is designed to find reconfigurations of systems to handle such run-time inconsistencies. Planners can be used to find and enact optimal reconfigurations in such an evolving context. However, for systems that are highly configurable, such planning becomes intractable due to the size of the adaptation space. To overcome this challenge, in this paper we explore an approach that (a) uses machine learning to find Pareto-optimal configurations without needing to explore every configuration and (b) restricts the search space to such configurations to make planning tractable. We explore this in the context of robot missions that need to consider task timeliness and energy consumption. An independent evaluation shows that our approach results in high-quality adaptation plans in uncertain and adversarial environments.",10.1109/SEAMS.2019.00015,"Machine learning, artificial intelligence, quantitative planning, self-adaptive systems, robotics systems",19.0,
A semi-supervised learning-aided evolutionary approach to occupational safety improvement,M. Cococcioni; B. Lazzerini; F. Pistolesi,2016 IEEE Congress on Evolutionary Computation (CEC),2016.0,"Worldwide, four people die every minute as a consequence of illnesses and accidents at work. This considerable number makes occupational safety an important research area aimed at obtaining safer and safer workplaces. This paper presents a semi-supervised learning-aided evolutionary approach to improve occupational safety by classifying workers depending on their own risk perception for the task assigned. More in detail, a semi-supervised learning phase is carried out to initialize a good population of a non-dominated sorting genetic algorithm (NSGA-II). Each chromosome of the population represents a pair of classifiers: one determines a worker's risk perception with respect to a task, the other determines the level of caution of the same worker for the same task. Learning from constraints reinforces the initial training performance. The best Pareto-optimal solution to the problem is selected by means of the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS). The proposed framework was tested on real-world data gathered through a website purposely developed. Results showed a good performance of the obtained classifiers, thus validating the effectiveness of the proposed approach in supporting the decision-maker in critical job assignment problems, where risks are a serious threat to the workers' health.",10.1109/CEC.2016.7744257,,8.0,
A New MCDM Approach to Solve Public Sector Planning Problems,P. O. Kaplan; S. R. Ranjithan,2007 IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making,2007.0,"An interactive method is developed to aid decision makers in public sector planning and management. The method integrates machine learning algorithms along with multiobjective optimization and modeling-to-generate-alternatives procedures into decision analysis. The implicit preferences of the decision maker are elicited through screening of several alternatives. The alternatives are selected from Pareto front and near Pareto front regions that are identified first in the procedure. The decision maker's selections are input to the machine learning algorithms to generate decision rules, which are then incorporated into the analysis to generate more alternatives satisfying the decision rules. The method is illustrated using a municipal solid waste management planning problem",10.1109/MCDM.2007.369430,MCDM;interactive methods;association rule mining;preference elicitation methods,5.0,
Multilayer Value Metrics Using Lexical Link Analysis and Game Theory for Discovering Innovation from Big Data and Crowd-Sourcing,Y. Zhao; C. C. Zhou; J. K. Bellonio,2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM),2018.0,"We demonstrated a machine learning and artificial intelligence method, i.e., lexical link analysis (LLA) to discover different layers of semantic network that contribute to innovative ideas from big data. The LLA is an unsupervised machine learning paradigm that does not require manually labeled training data. Multilayer value metrics are defined based on game theory for LLA. We showed the following results: 1) the value metrics generated from LLA in a use case of an internet game and crowd-sourcing; 2) the results from LLA are validated and correlated with the ground truth; 3) the game-theoretic LLA can help an information provider to present the information in the most valuable way. The information presentation can solve a problem (e.g., a search request of innovation) that no other information providers can solve (i.e., expertise). In addition, it ties also to a broader context that the unique value can propagate through the consensus. Based on the game-theoretic LLA, an information provider should not always present expertise content or authoritative content but rather with a mixed strategy where each type of content is presented with certain probabilities for the best value overall.",10.1109/ASONAM.2018.8508498,lexical link analysis;crowd-sourcing;game theory;big data;unsupervised learning;Nash equilibrium;social welfare;Pareto superior;Pareto efficient,1.0,
Improving POF Quality in Multi Objective Optimization of Analog ICs via Deep Learning,T. O. Çakıcı; G. İslamoğlu; Ş. N. Güzelhan; E. Afacan; G. Dündar,2020 European Conference on Circuit Theory and Design (ECCTD),2020.0,"Multi-objective optimization (MOO) is commonly used in analog circuits to reveal the trade-offs among design specifications via Pareto optimal fronts (POF). Although the general trend of POF can be found in a reasonable time with MOO, a high-quality POF requires an excessive number of iterations, which results in extremely long synthesis times. In this paper, single-objective optimization (SOO) is utilized to increase the POF quality rather than running MOO algorithms for long duration. Moreover, deep neural networks (DNN) are used to replace SPICE, which reduces the synthesis time further. This approach provides up to 50.62% improvement in POF quality and DNNs speed up the process up to 29.6x.",10.1109/ECCTD49232.2020.9218272,,,
Genetic algorithm-based optimization of ELM for on-line hyperspectral image classification,J. Echanobe; I. del Campo; V. Martínez; K. Basterretxea,2017 International Joint Conference on Neural Networks (IJCNN),2017.0,"Hyperspectral remote sensing is becoming an active research field in the last decades thanks to the availability of efficient machine learning algorithms and also to the ever-increasing computation power. However, there exist application domains (e.g., embedded applications) in which the deployment of this kind of systems becomes unfeasible due to the high requirements related to the size, power consumption or processing speed. A way to overcome this trouble consists on using any method able to scale-down the dimensionality of the problem and/or to reduce the complexity of the machine learning models. In this paper, we propose the use of a multiobjective genetic algorithm to minimize both the dimension of the input space and the size of the machine learning model. In particular, we have developed a hyperspectral image classifier based on an Extreme Learning Machine (ELM) for which the number of system inputs (dimensionality) and the number of hidden neurons are minimized without decreasing its performance. The system is evaluated by using a known benchmark dataset.",10.1109/IJCNN.2017.7966387,Extreme Learning Machines (ELM);Multi-Objective Genetic Algorithm;Machine Learning;Hyperspectral remote sensing;Dimensionality reduction,2.0,
A Gaussian-Prioritized Approach for Deploying Additional Route on Existing Mass Transportation with Neural-Network-Based Passenger Flow Inference,F. Lin; J. -Y. Fang; H. -P. Hsieh,2020 IEEE Congress on Evolutionary Computation (CEC),2020.0,"Multi-criteria path planning is an important combinatorial optimization problem with broad real-world applications. Finding the Pareto-optimal set of paths ideal for all requiring features is time-consuming and unclear to obtain the subset of optimal paths efficiently for multiple origin states in the planning space. Meanwhile, due to the rise of deep learning, hybrid systems of computational intelligence thrive in recent years. When facing non-monotonic data or heuristics derived from pretrained neural networks, most of the existing methods for the oneto-all path problem fail to find an ideal solution. We employ Gaussian mixture model to propose a target-prioritized searching algorithm called Multi-Source Bidirectional Gaussian-Prioritized Spanning Tree (BiasSpan) in solving this non-monotonic multicriteria route planning problem given constraints including range, must-visit vertices, and the number of recommended vertices. Experimental results on mass transportation system in Tainan and Chicago cities show that BiasSpan outperforms comparative methods from 7% to 24% and runs in a reasonable time compared to state-of-art route-planning algorithms.",10.1109/CEC48606.2020.9185869,Constrained route planning;Bidirectional spanning tree;Gaussian mixture model (GMM);Non-monotonicity;Deep Neural Network (DNN),,
Dimensionality reduction based on minimax risk criterion for face recognition,L. Tang; Y. Lei; L. Zhu; D. Huang,The 2010 International Joint Conference on Neural Networks (IJCNN),2010.0,"In the field of pattern recognition and machine learning, many problems are involved in the tasks of dimensionality reduction and then classification. In this paper, we develop an efficient dimensionality reduction method named MiniRisk Supervised Discrimiant Projection (MRSDP), which extracts effective low-dimensional features for classification purpose. The proposed method utilizes discriminant information to guide the procedure of extracting intrinsic low-dimensional features and provides a linear projection matrix. Since MRSDP is based on minimax risk criterion, it can minimize the maximal probability of misclassification in the common borders of different classes of data by contracting within-class scatter and maximizing between-class scatter. The advantage of our method is borne out by comparison with other widely used methods. In the experiments on Yale face database and ORL face database, our method achieves constantly superior performance than those competing methods.",10.1109/IJCNN.2010.5596520,,1.0,
Learning classifiers from imbalanced data based on biased minimax probability machine,Kaizhu Huang; Haiqin Yang; I. King; M. R. Lyu,"Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.",2004.0,"We consider the problem of the binary classification on imbalanced data, in which nearly all the instances are labelled as one class, while far fewer instances are labelled as the other class, usually the more important class. Traditional machine learning methods seeking an accurate performance over a full range of instances are not suitable to deal with this problem, since they tend to classify all the data into the majority, usually the less important class. Moreover, some current methods have tried to utilize some intermediate factors, e.g., the distribution of the training set, the decision thresholds or the cost matrices, to influence the bias of the classification. However, it remains uncertain whether these methods can improve the performance in a systematic way. In this paper, we propose a novel model named biased minimax probability machine. Different from previous methods, this model directly controls the worst-case real accuracy of classification of the future data to build up biased classifier;. Hence, it provides a rigorous treatment on imbalanced data. The experimental results on the novel model comparing with those of three competitive methods, i.e., the naive Bayesian classifier, the k-nearest neighbor method, and the decision tree method C4.5, demonstrate the superiority of our novel model.",10.1109/CVPR.2004.1315213,,12.0,
Investigating the Robustness and Stability to Noisy Data of a Dynamic Feature Selection Method,J. Jesus; A. Canuto; D. Araújo,2019 8th Brazilian Conference on Intelligent Systems (BRACIS),2019.0,"The curse of dimensionality is one of the major problems faced by machine learning researchers. If we consider the fast growing of complex data in real world scenarios, feature selection (FS) becomes a imperative step for many application domains to reduce both data complexity and computing time. Based on that, several studies have been developed in order to create efficient FS methods that performs this task. However, a bad selection of one single criterion to evaluate the attribute importance and the arbitrary choice of the number of features usually leads to a poor analysis. On the other hand, recent studies have successfully created models to select features considering the particularities of the data, known as dynamic feature selection. In this paper, we evaluate one of this successful methods, called pareto front based dynamic feature selection (PF-DFS), to test its stability and robustness in noisy data. We used 15 artificial and real world data with additional noise data. Results shown that the PF-DFS is more stable to noisy scenarios than existing feature selection methods.",10.1109/BRACIS.2019.00040,feature selection;dynamic feature selection;data analysis;pareto front;supervised learning,,
A Game-Theoretic Lexical Link Analysis for Discovering High-Value Information from Big Data,Y. Zhao; C. C. Zhou,2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM),2018.0,"We demonstrate a machine learning and artificial intelligence method, i.e., lexical link analysis (LLA) to discover high-value information from big data. In this paper, high-value information refers to the information that has the potential to grow its value over time. LLA is a unsupervised learning method that does not require manually labeled training data. New value metrics are defined based on a game-theoretic framework for LLA. In this paper, we show the value metrics generated from LLA in a use case of analyzing business news. We show the results from LLA are validated and correlated with the ground truth. We show that by using game theory, the high-value information selected by LLA reaches a Nash equilibrium by superpositioning popular and anomalous information, and at the same time generates high social welfare, therefore, contains higher intrinsic value.",10.1109/ASONAM.2018.8508317,high-value;lexical link analysis;game theory;big data;unsupervised learning;Nash equilibrium;social welfare;Pareto superior;Pareto efficient,,
Customer Lifetime Value in Video Games Using Deep Learning and Parametric Models,P. P. Chen; A. Guitart; A. F. del Río; Á. Periáñez,2018 IEEE International Conference on Big Data (Big Data),2018.0,"Nowadays, video game developers record every virtual action performed by their players. As each player can remain in the game for years, this results in an exceptionally rich dataset that can be used to understand and predict player behavior. In particular, this information may serve to identify the most valuable players and foresee the amount of money they will spend in in-app purchases during their lifetime. This is crucial in free-to-play games, where up to 50% of the revenue is generated by just around 2% of the players, the so-called whales.To address this challenge, we explore how deep neural networks can be used to predict customer lifetime value in video games, and compare their performance to parametric models such as Pareto/NBD. Our results suggest that convolutional neural network structures are the most efficient in predicting the economic value of individual players. They not only perform better in terms of accuracy, but also scale to big data and significantly reduce computational time, as they can work directly with raw sequential data and thus do not require any feature engineering process. This becomes important when datasets are very large, as is often the case with video game logs.Moreover, convolutional neural networks are particularly well suited to identify potential whales. Such an early identification is of paramount importance for business purposes, as it would allow developers to implement in-game actions aimed at retaining big spenders and maximizing their lifetime, which would ultimately translate into increased revenue.",10.1109/BigData.2018.8622151,lifetime value;deep learning;big data;video games;user behavior;behavioral data,12.0,
Optimizing Coverage and Capacity in Cellular Networks using Machine Learning,R. M. Dreifuerst; S. Daulton; Y. Qian; P. Varkey; M. Balandat; S. Kasturia; A. Tomar; A. Yazdan; V. Ponnampalam; R. W. Heath,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",2021.0,"Wireless cellular networks have many parameters that are normally tuned upon deployment and re-tuned as the network changes. Many operational parameters affect reference signal received power (RSRP), reference signal received quality (RSRQ), signal-to-interference-plus-noise-ratio (SINR), and, ultimately, throughput. In this paper, we develop and compare two approaches for maximizing coverage and minimizing interference by jointly optimizing the transmit power and downtilt (elevation tilt) settings across sectors. To evaluate different parameter configurations offline, we construct a realistic simulation model that captures geographic correlations. Using this model, we evaluate two optimization methods: deep deterministic policy gradient (DDPG), a reinforcement learning (RL) algorithm, and multi-objective Bayesian optimization (BO). Our simulations show that both approaches significantly outperform random search and converge to comparable Pareto frontiers, but that BO converges with two orders of magnitude fewer evaluations than DDPG. Our results suggest that data-driven techniques can effectively self-optimize coverage and capacity in cellular networks.",10.1109/ICASSP39728.2021.9414155,coverage and capacity optimization;Bayesian optimization;machine learning;reinforcement learning,2.0,
Multi-objective Support Vector Machines Ensemble Generation for Water Quality Monitoring,V. H. Alves Ribeiro; G. Reynoso-Meza,2018 IEEE Congress on Evolutionary Computation (CEC),2018.0,"Real-world classification problems generally deal with imbalanced data, where one class represents the majority of the data set. The present work deals with event detection on a drinking-water quality time series, where the presence of a quality event is the minority class. In order to solve such problems, supervised learning algorithms are recommended. Researchers have also used multi-objective optimization (MOO) in order to generate diverse models to build ensembles of classifiers. Although MOO has been used for ensemble member generation, there is a lack on it's application for member selection, which is usually done by selecting a specific subset from the resulting models, or by using meta-algorithms, such as boosting. The proposed work comprises the application of MOO design in the whole process of ensemble generation. To do so, one multi-objective problem (MOP) is defined for the creation of a set of non-dominated solutions with Pareto-optimal support vector machines (SVM). After that, a second MOP is defined for the selection of such SVMs as members of an ensemble. Such methodology is compared to other member selection methods, such as: the single best classifier, an ensemble composed of the full set of non-dominated solutions, and the selection of a specific subset from the Pareto front. Results show that the proposed method is suitable for the creation of ensembles, achieving the highest classification scores.",10.1109/CEC.2018.8477745,machine learning;supervised learning;ensemble methods;support vector machines;genetic algorithms,1.0,
A Pareto Corner Search Evolutionary Algorithm and Principal Component Analysis for Objective Dimensionality Reduction,X. H. Nguyen; L. Thu Bui; C. T. Tran,2019 11th International Conference on Knowledge and Systems Engineering (KSE),2019.0,"Many-objective optimisation problems (MaOPs) cause serious difficulties for existing multi-objective evolutionary algorithms (MOEAs). One common way to alleviate these difficulties is to use objective dimensionality reduction. Most existing objective reduction methods are time-consuming because they require MOEAs to run numerous generations. Pareto corner search evolutionary algorithm (PCSEA) was proposed in [18] to speed up objective reduction methods by only seeking corner solutions instead of whole solutions. However, the PCSEA-based objective reduction method in [18] needs to predefine a threshold to select objectives which strongly depends on problems and is not straightforward to obtain. This paper proposes a new objective dimensionality reduction method by integrating PCSEA and principal component analysis (PCA). Thanks to combining advantages of PCSEA and PCA, the proposed method not only can be efficient to eliminate redundant objectives, but also not require to define any parameter in advanced. The experimental results also show that the proposed method can perform objective reduction more successfully than the PCSEA-based objective reduction method. The results further strengthen the links between evolutionary computation and machine learning to address optimization problems.",10.1109/KSE.2019.8919438,many-objective optimisation;objective dimensionality reduction;feature selection;evolutionary computation,2.0,
Dynamic Feature Selection Based on Pareto Front Optimization,J. Jesus; A. Canuto; D. Araújo,2018 International Joint Conference on Neural Networks (IJCNN),2018.0,"One of the main issues of machine learning algorithms is the curse of dimensionality. With the fast growing of complex data in real world scenarios, the feature selection becomes a mandatory preprocessing step in any application to reduce both the complexity of the data and the computing time. Based on that, several works have been produced in order to develop efficient methods to perform this task. Most feature selection methods select the best attributes based on some specific criteria. Additionally, recent studies have successfully constructed models to select features considering the particularities of the data, assuming that similar samples should be treated separately. Although some advance has been made, a bad choice of one single criteria to evaluate the importance of the attributes and the arbitrary choice of the number of features made by the user can lead to a poor analysis. In order to overcome some of these issues, this work brings an improvement of a dynamic feature selection algorithm (DFS) by using the idea of pareto front multi-objective optimization, which allow us to both consider distinct perspectives of the features relevance and automatically set the number of attributes to select. We tested our approach using 15 artificial and real world data and results have shown that when compared to the original DFS method, the performance of the proposed method is remarkable superior. In fact, the results are very promising since the proposed method also achieved better performance than well-established dimensionality reduction methods and when using the original datasets, showing that the reduction of noisy and/or redundant attributes can have a positive effect in the performance of a classification task.",10.1109/IJCNN.2018.8489680,,2.0,
Synthesizing Pareto-Optimal Interpretations for Black-Box Models,H. Torfah; S. Shah; S. Chakraborty; S. Akshay; S. A. Seshia,2021 Formal Methods in Computer Aided Design (FMCAD),2021.0,"We present a new multi-objective optimization approach for synthesizing interpretations that “explain” the behavior of black-box machine learning models. Constructing human-understandable interpretations for black-box models often requires balancing conflicting objectives. A simple interpretation may be easier to understand for humans while being less precise in its predictions vis-a-vis a complex interpretation. Existing methods for synthesizing interpretations use a single objective function and are often optimized for a single class of interpretations. In contrast, we provide a more general and multi-objective synthesis framework that allows users to choose (1) the class of syntactic templates from which an interpretation should be synthesized, and (2) quantitative measures on both the correctness and explainability of an interpretation. For a given black-box, our approach yields a set of Pareto-optimal interpretations with respect to the correctness and explainability measures. We show that the underlying multi-objective optimization problem can be solved via a reduction to quantitative constraint solving, such as weighted maximum satisfiability. To demonstrate the benefits of our approach, we have applied it to synthesize interpretations for black-box neural-network classifiers. Our experiments show that there often exists a rich and varied set of choices for interpretations that are missed by existing approaches.",10.34727/2021/isbn.978-3-85448-046-4_24,,,
Plenary Talks,M. Grabisch; S. G. Kong; K. Iwano; P. Sinčák,2016 Joint 8th International Conference on Soft Computing and Intelligent Systems (SCIS) and 17th International Symposium on Advanced Intelligent Systems (ISIS),2016.0,These plenary talks discuss the following: Multicriteria Decision Making with Interacting Criteria; Machine Learning for Computer Vision; Reality 2.0 and Wisdom Computing - our vision toward the future; Cloud Based Intelligent Robotics.,10.1109/SCIS-ISIS.2016.0009,,,
Hybridising rule induction and multi-objective evolutionary search for optimising water distribution systems,L. Jourdan; D. Corne; D. Savic; G. Walters,Fourth International Conference on Hybrid Intelligent Systems (HIS'04),2004.0,"In this article, we present our latest work with a hybrid multiobjective evolutionary algorithm called LEMMO (learnable evolution model for multiobjective optimization) which integrates machine learning into evolutionary search based on Michalski's ""LEM"" approach. The objective is to both improve the performance of the MOEA and to reduce the number of evaluations needed when used for optimising the design of water distribution networks (where evaluations are highly computationally costly). We compare LEMMO with NSGA-II and conclude that our approach is very promising for improved speed and quality in the water systems optimisation domain.",10.1109/ICHIS.2004.58,,5.0,
An Approach to Large Margin Design of Prototype-Based Pattern Classifiers,T. He; Y. Hu; Q. Huo,"2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07",2007.0,"In this paper, we propose a maximum separation margin (MSM) training method for multiple-prototype (MP)-based pattern classifiers in which a sample separation margin defined as the distance from the training sample to the classification boundary can be calculated precisely. Similar to support vector machine (SVM) methodology, MSM training is formulated as a multicriteria optimization problem which aims at maximizing the separation margin and minimizing the empirical error rate on training data simultaneously. By making certain relaxation assumptions, MSM training can be reformulated as a semidefinite programming (SDP) problem that can be solved efficiently by some standard optimization algorithms designed for SDP. Evaluation experiments are conducted on the task of the recognition of most confusable Kanji character pairs identified from popular Nakayosi and Kuchibue handwritten Japanese character databases. It is observed that the MSM-trained MP-based classifier achieves a similar character recognition accuracy as that of the state-of-the-art SVM-based classifier, yet requires much fewer classifier parameters.",10.1109/ICASSP.2007.366313,large margin;pattern classification;support vector machine;machine;machine learning;semidefinite programming,2.0,
Multi-objective parameter configuration of machine learning algorithms using model-based optimization,D. Horn; B. Bischl,2016 IEEE Symposium Series on Computational Intelligence (SSCI),2016.0,"The performance of many machine learning algorithms heavily depends on the setting of their respective hyperparameters. Many different tuning approaches exist, from simple grid or random search approaches to evolutionary algorithms and Bayesian optimization. Often, these algorithms are used to optimize a single performance criterion. But in practical applications, a single criterion may not be sufficient to adequately characterize the behavior of the machine learning method under consideration and the Pareto front of multiple criteria has to be considered. We propose to use model-based multi-objective optimization to efficiently approximate such Pareto fronts.",10.1109/SSCI.2016.7850221,,11.0,
Stochastic performance tuning of complex simulation applications using unsupervised machine learning,O. Shadura; F. Carminati,2016 IEEE Symposium Series on Computational Intelligence (SSCI),2016.0,"Machine learning for complex multi-objective problems (MOP) can substantially speedup the discovery of solutions belonging to Pareto landscapes and improve Pareto front accuracy. Studying convergence speedup of multi-objective search on well-known benchmarks is an important step in the development of algorithms to optimize complex problems such as High Energy Physics particle transport simulations. In this paper we will describe how we perform this optimization via a tuning based on genetic algorithms and machine learning for MOP. One of the approaches described is based on the introduction of a specific multivariate analysis operator that can be used in case of expensive fitness function evaluations, in order to speed-up the convergence of the “black-box” optimization problem.",10.1109/SSCI.2016.7850200,,,
Dynamic Collaborative Charging Algorithm for Mobile and Static Nodes in Industrial Internet of Things,G. Han; Z. Liao; M. Martínez-García; Y. Zhang; Y. Peng,IEEE Internet of Things Journal,2021.0,"Industrial Internet of Things inevitably leads to the implementation of highly data-intensive devices, where the associated sensing nodes accelerate the energy consumption rate, which ultimately produces an energy bottleneck. To address this issue, this article proposes a <italic>dynamic collaborative charging algorithm</italic> that acts on both the mobile nodes and the static nodes in a sensing node network. The proposed scheme is to design a collaborative group of charging robots that can rendezvous with the sensing nodes. The group includes aerial charging vehicles (ACVs)—able to charge the underpowered mobile nodes, and terrestrial charging vehicles (TCVs), which charge their targeted static nodes. The aim of this study is to optimize the charging effect and the energy cost in the rendezvous process. This approach consists of two subalgorithms: 1) a charging algorithm for mobile nodes (CAMNs) and 2) a charging algorithm for static nodes (CASNs). The CAMNs is designed so that each underpowered mobile node can be charged by a dedicated ACV. For this purpose, a deep learning model is trained to divide the underpowered mobile nodes into appropriate clusters, each of which is equipped with a mobile base station. The rendezvous process is then constructed as a mixed continuous/discrete optimization problem, which is solved by using the firefly algorithm. In addition, the CASNs ensures that the TCVs traverse their routes, charging static nodes as they proceed. This traversing process was formulated as a multiobjective optimization problem, solved by using genetic algorithm. Through various experiments and case studies, the results have demonstrated both the feasibility and the efficiency of the proposed algorithms.",10.1109/JIOT.2021.3082633,Collaborative artificial intelligence;deep learning;firefly algorithm;genetic algorithm,2.0,
A comparison of machine leaming methods using a two player board game,D. Draskovic; M. Brzakovic; B. Nikolic,IEEE EUROCON 2019 -18th International Conference on Smart Technologies,2019.0,"The board games are usually performed by game theory algorithms: minimax and minimax with alpha-beta pruning. Tic-tac-toe (X-O) is the best-known two-player board game. The game tic-tac-toe, based on machine learning algorithms, has been shown in this research. The neural network has been developed and trained to play the game utilizing three implemented agents: an agent based on deep Q-learning, an agent based on policy gradient method and a random agent. The agent can play the game perfectly in the 10-minute training interval, on an average graphics processing unit.",10.1109/EUROCON.2019.8861927,machine learning;reinforcement learning;neural network;Q-learning;policy gradient method;deep learning,1.0,
Design and Implementation of Fuzzy-PI Controllers for PMSM Based on Multi-Objective Optimization Algorithms,I. Kao; K. Lu; J. Perng,"2017 5th International Conference on Mechanical, Automotive and Materials Engineering (CMAME)",2017.0,"Various intelligent algorithms have been applied to our daily lives, such as fuzzy theory, neural networks, and machine learning. These methods are widely used for solving many real-world problems; however, these algorithms also exhibit deficiencies and limitations. This paper introduces the recently improved algorithm, known as multi-objective particle swarm optimization, based on decomposition and dominance (D^2 MOPSO) in order to design the permanent magnet synchronous motor (PMSM) fuzzy controller for different objects. This means that the user can easily change the customized controller, according to their requirements. Furthermore, this paper compares the final decision of the controller parameter with other algorithms: the multiobjective particle swarm optimization with crowding distance (MOPSO-CD), and nondominated sorting genetic algorithm II (NSGA-II). The simulation results of the three algorithms indicate the optimum PMSM controller parameter in the computing software MATLAB. Finally, we implement the fuzzy controller in an embedded system (DSP28069) to demonstrate that our design matches the reality system response and meets the user's demands with ease.",10.1109/CMAME.2017.8540108,fuzzy control;multi-objective optimization algorithm;multi-objective particle swarm optimization;MOPSO;Nondominated sorting genetic algorithm;NSGA-II,1.0,
A Pareto Front Based Evolutionary Model for Airfoil Self-Noise Prediction,A. Tahmassebi; A. H. Gandomi; A. Meyer-Baese,2018 IEEE Congress on Evolutionary Computation (CEC),2018.0,"According to NASA's report on the technologies that could reduce external aircraft noise by 10 dB, a challenge equally as important as finding approaches on airframe noise reduction is the demand to bring up strategies by which airframe noise can be predicted both accurately and rapidly. One of the components of the overall airframe noise is the self-noise of the airfoil itself. In this paper, an evolutionary symbolic implementation for airfoil self-noise prediction was proposed. Multi-objective genetic programming as a subset of evolutionary computation along with adaptive regression by mixing algorithm was used to create an executable fused model. The developed model was tested on the airfoil self-noise database and the performance of the developed model was compared to the previous works and benchmark machine learning algorithms. The reasonable results suggest that the proposed model can be applied to noise generation by low-Mach-number turbulent flows in aerospace, automobile, underwater, and wind turbine acoustic communities.",10.1109/CEC.2018.8477987,,4.0,
A Probabilistic Machine Learning Approach to Scheduling Parallel Loops With Bayesian Optimization,K. -R. Kim; Y. Kim; S. Park,IEEE Transactions on Parallel and Distributed Systems,2021.0,"This article proposes Bayesian optimization augmented factoring self-scheduling (BO FSS), a new parallel loop scheduling strategy. BO FSS is an automatic tuning variant of the factoring self-scheduling (FSS) algorithm and is based on Bayesian optimization (BO), a black-box optimization algorithm. Its core idea is to automatically tune the internal parameter of FSS by solving an optimization problem using BO. The tuning procedure only requires online execution time measurement of the target loop. In order to apply BO, we model the execution time using two Gaussian process (GP) probabilistic machine learning models. Notably, we propose a locality-aware GP model, which assumes that the temporal locality effect resembles an exponentially decreasing function. By accurately modeling the temporal locality effect, our locality-aware GP model accelerates the convergence of BO. We implemented BO FSS on the GCC implementation of the OpenMP standard and evaluated its performance against other scheduling algorithms. Also, to quantify our method's performance variation on different workloads, or workload-robustness in our terms, we measure the minimax regret. According to the minimax regret, BO FSS shows more consistent performance than other algorithms. Within the considered workloads, BO FSS improves the execution time of FSS by as much as 22% and 5% on average.",10.1109/TPDS.2020.3046461,Parallel loop scheduling;Bayesian optimization;parallel computing;OpenMP,1.0,
Multi-Criteria Evaluation of Publication Impacts: Deep Learning in Autonomous Vehicles,G. Ismayilov; C. D. Yilmaz,2021 29th Conference of Open Innovations Association (FRUCT),2021.0,"Deep learning is the state-of-the-art approach that has been extensively used in the recent years to variety of real-world problems in the literature. The autonomous vehicles are among the applications where their integration with deep learning techniques has potential to disruptively change our daily lives. In this work, we have proposed a multi-criteria framework to evaluate the relative impacts of both publications and authors for deep learning in autonomous vehicles. For the framework, we have considered several criteria extracted from the metadata of the publications and the authors. The conflicts among the criteria are also justified through Pearson correlation. For the experiments, two comprehensive datasets for the publication and the author impacts have been constructed. The resulting pareto-fronts of the datasets after ranking are presented. Moreover, top 30 most impactful publications and authors in the literature are identified. We hope that our findings will be useful for researchers to accelerate the further technological advancements.",10.23919/FRUCT52173.2021.9435554,,,
Filter Design and Optimization of Electromechanical Actuation Systems Using Search and Surrogate Algorithms for More-Electric Aircraft Applications,Y. Gao; T. Yang; S. Bozhko; P. Wheeler; T. Dragičević,IEEE Transactions on Transportation Electrification,2020.0,"In this article, the dc filter design and optimization problem is studied for dc electrical power distribution systems onboard more-electric aircraft. Component sizing models are built to serve as the basis of the optimization whose objectives are mass and power loss of this filter. A categorization strategy of search and surrogate algorithms is proposed and used for the target multiobjective optimization problem (MOOP). A genetic algorithm is utilized as a search algorithm to identify potential best solutions based on a set of filter sizing functions (subject to constraints). In addition, two machine learning (ML) algorithms are considered as surrogate algorithms to address the same optimization problem. In the ML training process, a constraint violation model is applied since there are various constraints in optimization, and this kind of classification model is relatively difficult to train. A support vector machine is applied for the constraint violation model; after that, two artificial neural networks are trained as the final surrogate model for mapping design variables to filter performance. To address these issues, a novel category of search and surrogate algorithms is proposed. Both algorithms are explored to solve the filter MOOP, and their optimization results are compared at the end.",10.1109/TTE.2020.3019729,Artificial neural network (ANN);filter design;genetic algorithm (GA);more-electric aircraft (MEA);optimization;search algorithm;support vector machine (SVM);surrogate algorithm,4.0,
A Global Bayesian Optimization Algorithm and Its Application to Integrated System Design,H. M. Torun; M. Swaminathan; A. Kavungal Davis; M. L. F. Bellaredj,IEEE Transactions on Very Large Scale Integration (VLSI) Systems,2018.0,"Increasing levels of system integration pose difficulties in meeting design specifications for high-performance systems. Oftentimes increased complexity, nonlinearity, and multiple tradeoffs need to be handled simultaneously during the design cycle. Since components in such systems are highly correlated with each other, codesign and co-optimization of the complete system are required. Machine learning (ML) provides opportunities for analyzing such systems with multiple control parameters, where techniques based on Bayesian optimization (BO) can be used to meet or exceed design specifications. In this paper, we propose a new BO-based global optimization algorithm titled Two-Stage BO (TSBO). TSBO can be applied to black box optimization problems where the computational time can be reduced through a reduction in the number of simulations required. Empirical analysis on a set of popular challenge functions with several local extrema and dimensions shows TSBO to have a faster convergence rate as compared with other optimization methods. In this paper, TSBO has been applied for clock skew minimization in 3-D integrated circuits and multiobjective co-optimization for maximizing efficiency in integrated voltage regulators. The results show that TSBO is between 2×-4× faster as compared with previously published BO algorithms and other non-ML-based techniques.",10.1109/TVLSI.2017.2784783,3-D integration;Bayesian optimization (BO);black box systems;integrated voltage regulator (IVR);machine learning (ML);magnetic core inductor;thermal management,52.0,
Practical Design Space Exploration,L. Nardi; D. Koeplinger; K. Olukotun,"2019 IEEE 27th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",2019.0,"Multi-objective optimization is a crucial matter in computer systems design space exploration because real-world applications often rely on a trade-off between several objectives. Derivatives are usually not available or impractical to compute and the feasibility of an experiment can not always be determined in advance. These problems are particularly difficult when the feasible region is relatively small, and it may be prohibitive to even find a feasible experiment, let alone an optimal one. We introduce a new methodology and corresponding software framework, HyperMapper 2.0, which handles multi-objective optimization, unknown feasibility constraints, and categorical/ordinal variables. This new methodology also supports injection of the user prior knowledge in the search when available. All of these features are common requirements in computer systems but rarely exposed in existing design space exploration systems. The proposed methodology follows a white-box model which is simple to understand and interpret (unlike, for example, neural networks) and can be used by the user to better understand the results of the automatic search. We apply and evaluate the new methodology to the automatic static tuning of hardware accelerators within the recently introduced Spatial programming language, with minimization of design run-time and compute logic under the constraint of the design fitting in a target field-programmable gate array chip. Our results show that HyperMapper 2.0 provides better Pareto fronts compared to state-of-the-art baselines, with better or competitive hypervolume indicator and with 8x improvement in sampling budget for most of the benchmarks explored.",10.1109/MASCOTS.2019.00045,"Pareto-optimal front;Design space exploration;Hardware design, Performance modeling, Optimizing compilers, Machine learning driven optimization",8.0,
Artificial Neural Network (ANN) Based Fast and Accurate Inductor Modeling and Design,T. Guillod; P. Papamanolis; J. W. Kolar,IEEE Open Journal of Power Electronics,2020.0,"This paper analyzes the potential of Artificial Neural Networks (ANNs) for the modeling and optimization of magnetic components and, specifically, inductors. After reviewing the basic properties of ANNs, several potential modeling and design workflows are presented. A hybrid method, which combines the accuracy of 3D Finite Element Method (FEM) and the low computational cost of ANNs, is selected and implemented. All relevant effects are considered (3D magnetic and thermal field patterns, detailed core loss data, winding proximity losses, coupled loss-thermal model, etc.) and the implemented model is extremely versatile (30 input and 40 output variables). The proposed ANN-based model can compute 50'000 designs per second with less than 3% deviation with respect to 3D FEM simulations. Finally, the inductor of a 2 kW DC-DC buck converter is optimized with the ANN-based workflow. From the Pareto fronts, a design is selected, measured, and successfully compared with the results obtained with the ANNs. The implementation (source code and data) of the proposed workflow is available under an open-source license.",10.1109/OJPEL.2020.3012777,Power converters;artificial neural networks;finite element analysis;inductors;machine learning;magnetic devices;open source software;pareto optimization,14.0,
Advanced RF and Microwave Design Optimization: A Journey and a Vision of Future Trends,J. E. Rayas-Sánchez; S. Koziel; J. W. Bandler,IEEE Journal of Microwaves,2021.0,"In this paper, we outline the historical evolution of RF and microwave design optimization and envisage imminent and future challenges that will be addressed by the next generation of optimization developments. Our journey starts in the 1960s, with the emergence of formal numerical optimization algorithms for circuit design. In our fast historical analysis, we emphasize the last two decades of documented microwave design optimization problems and solutions. From that retrospective, we identify a number of prominent scientific and engineering challenges: 1) the reliable and computationally efficient optimization of highly accurate system-level complex models subject to statistical uncertainty and varying operating or environmental conditions; 2) the computationally-efficient EM-driven multi-objective design optimization in high-dimensional design spaces including categorical, conditional, or combinatorial variables; and 3) the manufacturability assessment, statistical design, and yield optimization of high-frequency structures based on high-fidelity multi-physical representations. To address these major challenges, we venture into the development of sophisticated optimization approaches, exploiting confined and dimensionally reduced surrogate vehicles, automated feature-engineering-based optimization, and formal cognition-driven space mapping approaches, assisted by Bayesian and machine learning techniques.",10.1109/JMW.2020.3034263,ANN;Bayesian;Broyden;CAD;cognition;design automation;EDA;features;Gaussian process;Kriging;machine learning;multi-objective;multi-physics;optimization;Pareto;polynomial chaos;sensitivity;space mapping;statistical;surrogate;tolerances;uncertainty quantification;yield,6.0,
An Investigation of Pixel-Based and Object-Based Image Classification in Remote Sensing,M. C. Younis; E. Keedwell; D. Savic,2018 International Conference on Advanced Science and Engineering (ICOASE),2018.0,"This research evaluates pixel-based and object-based image classification techniques for extracting three land-use categories (buildings, roads, and vegetation areas) from six satellite images. The performance of eight supervised machine learning classifiers with 5-fold cross validation are also compared. Experimental validation found that using 'Bagged Tree' for object-based classification algorithms provides maximum overall accuracy when tested on 10,000 objects produced by the SLIC segmentation method, and improves upon an existing RGB-based approach. Our aforementioned proposed approach takes about 12 times less total runtime than the pixel-based method, demonstrating the power of the combined approach.",10.1109/ICOASE.2018.8548845,machine learning;image classification;image segmentation;Pareto Analysis;remote sensing,,
Coordinated Optimal Energy Management and Voyage Scheduling for All-Electric Ships Based on Predicted Shore-Side Electricity Price,S. Wen; T. Zhao; Y. Tang; Y. Xu; M. Zhu; S. Fang; Z. Ding,IEEE Transactions on Industry Applications,2021.0,"Unlike a land-based standalone microgrid, a shipboard microgrid of an all-electric ship (AES) needs to shut down generators during berthing at the port for examination and maintenance. Therefore, the cost of onshore power plays an important role in an economic operation for AESs. In order to fully exploit its potential, a two-stage joint scheduling model is proposed to optimally coordinate the power generation and voyage scheduling of an AES. Different from previous studies that only consider the operation cost of the ship itself, a novel coordinated framework is developed in this article to address the shore-side electricity price variations on the ship navigation route. A deep learning-based forecasting method is utilized to predict the electricity price in various harbors for ship operators. Then, a hybrid optimization algorithm is designed to solve the proposed multiobjective joint scheduling problem. A navigation route in Australia is adopted for case studies and simulation results demonstrate the high energy utilization efficiency of the proposed algorithm and the necessity of on-shore power influence on the AES voyage.",10.1109/TIA.2020.3034290,All-electric ship;deep learning;energy storage system;joint energy management and voyage scheduling;real-time electricity price prediction,6.0,
Generative Adversarial Network in the Air: Deep Adversarial Learning for Wireless Signal Spoofing,Y. Shi; K. Davaslioglu; Y. E. Sagduyu,IEEE Transactions on Cognitive Communications and Networking,2021.0,"The spoofing attack is critical to bypass physical-layer signal authentication. This paper presents a deep learning-based spoofing attack to generate synthetic wireless signals that cannot be statistically distinguished from intended transmissions. The adversary is modeled as a pair of a transmitter and a receiver that build the generator and discriminator of the generative adversarial network, respectively, by playing a minimax game over the air. The adversary transmitter trains a deep neural network to generate the best spoofing signals and fool the best defense trained as another deep neural network at the adversary receiver. Each node (defender or adversary) may have multiple transmitter or receiver antennas. Signals are spoofed by jointly capturing waveform, channel, and radio hardware effects that are inherent to wireless signals under attack. Compared with spoofing attacks using random or replayed signals, the proposed attack increases the probability of misclassifying spoofing signals as intended signals for different network topology and mobility patterns. The adversary transmitter can increase the spoofing attack success by using multiple antennas, while the attack success decreases when the defender receiver uses multiple antennas. For practical deployment, the attack implementation on embedded platforms demonstrates the low latency of generating or classifying spoofing signals.",10.1109/TCCN.2020.3010330,Adversarial machine learning;deep learning;generative adversarial network (GAN);spoofing attack,6.0,
Empirical Evaluation of Federated Learning with Local Privacy for Real-World Application,P. L. Li; X. Chai; W. D. Wadsworth; J. Liao; B. Paddock,2020 IEEE International Conference on Big Data (Big Data),2020.0,"As Machine Learning-based applications become increasingly pervasive, a growing concern is how to balance the need for large, representative data sets with the need to respect user data privacy. The increased compute and connectivity capabilities of edge devices (e.g. phones, PCs) presents us with new avenues for achieving this balance, including a promising approach known as federated learning with local privacy. However, today we have gaps in practical knowledge about applicability, trade-offs, and benefits for large-scale realworld implementation. In this paper, using large-scale data from a real-world Windows Update ML-driven application (as well as the publicly available CIFAR-10 data set to enhance reproducibility), we report empirical evaluations of four practical considerations: heterogeneity in device availability that may cause bias, resiliency of federated learning with local differential privacy, benefits of time-varying adaptive configurations, and data transmission/storage savings based on the Pareto principle. We discuss the implications of these findings for practitioners and researchers.",10.1109/BigData50022.2020.9378033,learning (artificial intelligence);machine learning;machine learning algorithms;prediction methods;predictive models;big data applications;federated learning;privacy;data privacy,,
Large margin hidden Markov models for speech recognition,Hui Jiang; Xinwei Li; Chaojun Liu,"IEEE Transactions on Audio, Speech, and Language Processing",2006.0,"In this paper, motivated by large margin classifiers in machine learning, we propose a novel method to estimate continuous-density hidden Markov model (CDHMM) for speech recognition according to the principle of maximizing the minimum multiclass separation margin. The approach is named large margin HMM. First, we show this type of large margin HMM estimation problem can be formulated as a constrained minimax optimization problem. Second, we propose to solve this constrained minimax optimization problem by using a penalized gradient descent algorithm, where the original objective function, i.e., minimum margin, is approximated by a differentiable function and the constraints are cast as penalty terms in the objective function. The new training method is evaluated in the speaker-independent isolated E-set recognition and the TIDIGITS connected digit string recognition tasks. Experimental results clearly show that the large margin HMMs consistently outperform the conventional HMM training methods. It has been consistently observed that the large margin training method yields significant recognition error rate reduction even on top of some popular discriminative training methods.",10.1109/TASL.2006.879805,Continuous-density hidden Markov models (CDHMMs);gradient descent search;large margin classifiers;minimax optimization;support vector machine,72.0,
"Agile Earth Observation Satellite Scheduling Over 20 Years: Formulations, Methods, and Future Directions",X. Wang; G. Wu; L. Xing; W. Pedrycz,IEEE Systems Journal,2021.0,"Agile satellites with advanced attitude maneuvering capability are the new generation of earth observation satellites (EOSs). The continuous improvement in satellite technology and decrease in launch cost have boosted the development of agile EOSs (AEOSs). To efficiently employ the increasing orbiting AEOSs, the AEOS scheduling problem (AEOSSP) aiming to maximize the entire observation profit while satisfying all complex operational constraints, has received much attention over the past 20 years. The objectives of this article are, thus, to summarize current research on AEOSSP, identify main accomplishments and highlight potential future research directions. To this end, general definitions of AEOSSP with operational constraints are described initially, followed by its three typical variations including different definitions of observation profit, multiobjective function and autonomous model. A detailed literature review from 1997-2019 is then presented in line with four different solution methods, i.e., exact method, heuristic, metaheuristic, and machine learning. Finally, we discuss a number of topics worth pursuing in the future.",10.1109/JSYST.2020.2997050,Agile earth observation satellite;aerospace engineering;review;earth observing system;scheduling;space systems,10.0,
Seeking Multiple Solutions: An Updated Survey on Niching Methods and Their Applications,X. Li; M. G. Epitropakis; K. Deb; A. Engelbrecht,IEEE Transactions on Evolutionary Computation,2017.0,"Multimodal optimization (MMO) aiming to locate multiple optimal (or near-optimal) solutions in a single simulation run has practical relevance to problem solving across many fields. Population-based meta-heuristics have been shown particularly effective in solving MMO problems, if equipped with specifically-designed diversity-preserving mechanisms, commonly known as niching methods. This paper provides an updated survey on niching methods. This paper first revisits the fundamental concepts about niching and its most representative schemes, then reviews the most recent development of niching methods, including novel and hybrid methods, performance measures, and benchmarks for their assessment. Furthermore, this paper surveys previous attempts at leveraging the capabilities of niching to facilitate various optimization tasks (e.g., multiobjective and dynamic optimization) and machine learning tasks (e.g., clustering, feature selection, and learning ensembles). A list of successful applications of niching methods to real-world problems is presented to demonstrate the capabilities of niching methods in providing solutions that are difficult for other optimization methods to offer. The significant practical value of niching methods is clearly exemplified through these applications. Finally, this paper poses challenges and research questions on niching that are yet to be appropriately addressed. Providing answers to these questions is crucial before we can bring more fruitful benefits of niching to real-world problem solving.",10.1109/TEVC.2016.2638437,Evolutionary computation;meta-heuristics;multimodal optimization (MMO);multisolution methods;niching methods;swarm intelligence,121.0,
Machine learning with incomplete datasets using multi-objective optimization models,H. A. Khorshidi; M. Kirley; U. Aickelin,2020 International Joint Conference on Neural Networks (IJCNN),2020.0,"Machine learning techniques have been developed to learn from complete data. When missing values exist in a dataset, the incomplete data should be preprocessed separately by removing data points with missing values or imputation. In this paper, we propose an online approach to handle missing values while a classification model is learnt. To reach this goal, we develop a multi-objective optimization model with two objective functions for imputation and model selection. We also propose three formulations for imputation objective function. We use an evolutionary algorithm based on NSGA II to find the optimal solutions as the Pareto solutions. We investigate the reliability and robustness of the proposed model using experiments by defining several scenarios in dealing with missing values and classification. We also describe how the proposed model can contribute to medical informatics. We compare the performance of three different formulations via experimental results. The proposed model results get validated by comparing with a comparable literature.",10.1109/IJCNN48605.2020.9206742,incomplete data;multi-objective model;uncertainty;model selection;classification,,
An Evolutionary Machine Learning Approach Towards Less Conservative Robust Optimization,P. D. Pantula; K. Mitra,2019 IEEE Congress on Evolutionary Computation (CEC),2019.0,"In the recent era, multi-criteria decision making under uncertainty is gaining importance due to its wide range of applicability. Among several types of uncertainty handling techniques, Robust Optimization (RO) is considered as an efficient and tractable approach provided one has accessibility to data in uncertain regions. However, solutions of RO may actually deviate from actual results in real scenarios, due to conservative sampling. This paper proposes a methodology to amalgamate unsupervised machine learning algorithms with RO which thereby makes it data-driven. A novel evolutionary fuzzy clustering mechanism is implemented to transcript the uncertain space such that the exact regions of uncertainty are identified. Subsequently, density based boundary point detection and Delaunay triangulation based boundary construction enables intelligent Sobol based sampling in these regions for use in RO. Results of two test cases with varying dimensions are presented along with a comprehensive comparison between conventional RO approach using box uncertainty set and proposed methodology. Considered case studies include highly nonlinear real life model for continuous casting from steelmaking industries, where a time expensive multi-objective optimization problem under uncertainty is formulated to resolve the conflict in productivity and energy consumption. Optimal Artificial Neural Network (ANN) surrogate assisted optimization under uncertainty for casting model is performed to obtain solutions in realistic time. The resulting RO problem being multi-objective in nature, the Pareto solutions are obtained by NSGA II.",10.1109/CEC.2019.8790094,Data Driven Robust Optimization;Evolutionary Algorithms;Fuzzy Clustering;ANN surrogate models;Multi objective Optimization,1.0,
A Novel Hybrid Machine Learning Algorithm for Limited and Big Data Modeling With Application in Industry 4.0,H. Khayyam; A. Jamali; A. Bab-Hadiashar; T. Esch; S. Ramakrishna; M. Jalili; M. Naebe,IEEE Access,2020.0,"To meet the challenges of manufacturing smart products, the manufacturing plants have been radically changed to become smart factories underpinned by industry 4.0 technologies. The transformation is assisted by employment of machine learning techniques that can deal with modeling both big or limited data. This manuscript reviews these concepts and present a case study that demonstrates the use of a novel intelligent hybrid algorithms for Industry 4.0 applications with limited data. In particular, an intelligent algorithm is proposed for robust data modeling of nonlinear systems based on input-output data. In our approach, a novel hybrid data-driven combining the Group-Method of Data-Handling and Singular-Value Decomposition is adapted to find an offline deterministic model combined with Pareto multi-objective optimization to overcome the overfitting issue. An Unscented-Kalman-Filter is also incorporated to update the coefficient of the deterministic model and increase its robustness against data uncertainties. The effectiveness of the proposed method is examined on a set of real industrial measurements.",10.1109/ACCESS.2020.2999898,Industry 40;big data modeling;limited data modeling;multi-objective optimization,6.0,
A Genetic Programming-Based Multi-Objective Optimization Approach to Data Replication Strategies for Distributed Systems,S. M. A. Bokhari; O. Theel,2020 IEEE Congress on Evolutionary Computation (CEC),2020.0,"Data replication is the core of distributed systems to enhance their fault tolerance and make services highly available to the end-users. Data replication masks run-time failures and hence, makes the system more reliable. There are many contemporary data replication strategies for this purpose, but the decision to choose an appropriate strategy for a certain environment and a specific scenario is a challenge and full of compromises. There exists a potentially indefinite number of scenarios that cannot be covered entirely by contemporary strategies. It demands designing new data replication strategies optimized for the given scenarios. The constraints of such scenarios are often conflicting in a sense that an increase in one objective could be sacrificial to the others, which implies there is no best solution to the problem but what serves the purpose. In this regard, this research provides a genetic programming-based multi-objective optimization approach that endeavors to not only identify, but also design new data replication strategies and optimize their conflicting objectives as a single-valued metric. The research provides an intelligent, automatic mechanism to generate new replication strategies as well as easing up the decision making so that relevant strategies with satisfactory trade-offs of constraints can easily be picked and used from the generated solutions at run-time. Moreover, it makes the notion of hybrid strategies easier to accomplish which otherwise would have been very cumbersome to achieve, therefore, to optimize.",10.1109/CEC48606.2020.9185598,Distributed Systems;Fault Tolerance;Data Replication;Quorum Protocols;Operation Availability;Operation Cost;Voting Structures;Optimization;Pareto Front;Machine Learning;Genetic Programming,2.0,
Meta-Optimization of Bias-Variance Trade-Off in Stochastic Model Learning,T. Aotani; T. Kobayashi; K. Sugimoto,IEEE Access,2021.0,"Model-based reinforcement learning is expected to be a method that can safely acquire the optimal policy under real-world conditions by using a stochastic dynamics model for planning. Since the stochastic dynamics model of the real world is generally unknown, a method for learning from state transition data is necessary. However, model learning suffers from the problem of bias-variance trade-off. Conventional model learning can be formulated as a minimization problem of expected loss. Failure to consider higher-order statistics for loss would lead to fatal errors in long-term model prediction. Although various methods have been proposed to explicitly handle bias and variance, this paper first formulates a new loss function, especially for sequential training of the deep neural networks. To explicitly consider the bias-variance trade-off, a new multi-objective optimization problem with the augmented weighted Tchebycheff scalarization, is proposed. In this problem, the bias-variance trade-off can be balanced by adjusting a weight hyperparameter, although its optimal value is task-dependent and unknown. We additionally propose a general-purpose and efficient meta-optimization method for hyperparameter(s). According to the validation result on each epoch, the proposed meta-optimization can adjust the hyperparameter(s) towards the preferred solution simultaneously with model learning. In our case, the proposed meta-optimization enables the bias-variance trade-off to be balanced for maximizing the long-term prediction ability. Actually, the proposed method was applied to two simulation environments with uncertainty, and the numerical results showed that the well-balanced bias and variance of the stochastic model suitable for the long-term prediction can be achieved.",10.1109/ACCESS.2021.3125000,Machine learning algorithms;systems modeling;Pareto optimization;bias-variance trade-off,,
Cost-Time Performance of Scaling Applications on the Cloud,S. Rathnayake; L. Ramapantulu; Y. M. Teo,2018 IEEE International Conference on Cloud Computing Technology and Science (CloudCom),2018.0,"Recent advancements in big data processing and machine learning, among others, increase the resource demand for running applications with larger problem sizes. Elastic cloud computing resources with pay-per-use pricing offers new opportunities where large application execution is constrained only by the cost budget. Given a cost budget and a time deadline, this paper introduces a measurement-driven analytical modeling approach to determine the largest Pareto-optimal problem size and its corresponding cloud configuration for execution. We evaluate our approach with a set of representative applications that exhibit a range of resource demand growth patterns on Amazon AWS cloud. We show the existence of cost-time-size Pareto-frontier with multiple sweet spots meeting user constraints. To characterize the cost-performance of cloud resources, we use Performance Cost Ratio (PCR) metric. We extend Gustafson's fixed-time scaling in the context of cloud, and, investigate fixed-cost-time scaling of applications and show that using resources with higher PCR yields better cost-time performance. We discuss a number of useful insights on the trade-off between the execution time and the largest Pareto-optimal problem size, and, show that time deadline could be tightened for a proportionately much smaller reduction of problem size.",10.1109/CloudCom2018.2018.00021,scaling;largest problem size;cloud;cost-time performance;Pareto-optimal configuration,,
A new approach on multi-agent Multi-Objective Reinforcement Learning based on agents' preferences,Z. Daavarani Asl; V. Derhami; M. Yazdian-Dehkordi,2017 Artificial Intelligence and Signal Processing Conference (AISP),2017.0,"Reinforcement Learning (RL) is a powerful machine learning paradigm for solving Markov Decision Process (MDP). Traditional RL algorithms aim to solve one-objective problems, but many real-world problems have more than one objective which conflict each other. In recent years, Multi-Objective Reinforcement Learning (MORL) algorithms, which employ a reward vector instead of a scalar reward signal, have been proposed to solve multi-objective problems. In MORL, because of conflicting objectives, there is no one optimal solution and a set of solutions named Pareto Front will be learned. In this paper, we proposed a new multi-agent method, which uses a shared Q-table for all agents to solve bi-objective problems. However, each agent selects actions based on its preference. These preferences are different with each other and the agents reach to Pareto Front solutions based on this preferences. The proposed method is simple in understanding and its computational cost is very low. Moreover, after finding the Pareto Front set, we can easily track the policy. Simulation results show that our proposed method outperforms the available methods in the term of learning speed.",10.1109/AISP.2017.8324111,reinforcement learning;multi-agent systems;multi-objective;Pareto Front,2.0,
Adaptive image segmentation using genetic and hybrid search methods,B. Bhanu; Sungkee Lee; S. Das,IEEE Transactions on Aerospace and Electronic Systems,1995.0,"This paper describes an adaptive approach for the important image processing problem of image segmentation that relies on learning from experience to adapt and improve the segmentation performance. The adaptive image segmentation system incorporates a feedback loop consisting of a machine learning subsystem, an image segmentation algorithm, and an evaluation component which determines segmentation quality. The machine learning component is based on genetic adaptation and uses (separately) a pure genetic algorithm (GA) and a hybrid of GA and hill climbing (HC). When the learning subsystem is based on pure genetics, the corresponding evaluation component is based on a vector of evaluation criteria. For the hybrid case, the system employs a scalar evaluation measure which is a weighted combination of the different criteria. Experimental results for pure genetic and hybrid search methods are presented using a representative database of outdoor TV imagery. The multiobjective optimization demonstrates the ability of the adaptive image segmentation system to provide high quality segmentation results in a minimal number of generations.<<ETX>></ETX>",10.1109/7.464350,,47.0,
GAN-Based One-Class Classification for Personalized Image Retrieval,S. H. Kim; H. -J. Kim; J. -Y. Kim,2018 IEEE International Conference on Big Data and Smart Computing (BigComp),2018.0,"One-class classification for a personalized image retrieval system is one of most important research issues in machine learning. However, the conventional one-class classification techniques can have an overfitting problem. Thus, in this paper, we propose a novel one-class classification technique using the framework of generative adversarial nets (GAN) for image data. First, the support model and one-class model are trained with only positive-class data by a minimax game. At the end of this learning process, the one-class model learns the features of positive-class data very well while reducing generation error. One of our important findings is that the negative-class data generated by the support model help the one-class model conceptually and experimentally reduce the generative error. Using CIFAR-10, we show that our proposed technique outperforms the conventional technique by ~10% in terms of F1 measure.",10.1109/BigComp.2018.00147,one class classification;generative adversarial net;deep learning;image retrieval;convolutional neural network,6.0,
Improving Network Slimming With Nonconvex Regularization,K. Bui; F. Park; S. Zhang; Y. Qi; J. Xin,IEEE Access,2021.0,"Convolutional neural networks (CNNs) have developed to become powerful models for various computer vision tasks ranging from object detection to semantic segmentation. However, most of the state-of-the-art CNNs cannot be deployed directly on edge devices such as smartphones and drones, which need low latency under limited power and memory bandwidth. One popular, straightforward approach to compressing CNNs is network slimming, which imposes <inline-formula> <tex-math notation=""LaTeX"">$\ell _{1}$ </tex-math></inline-formula> regularization on the channel-associated scaling factors via the batch normalization layers during training. Network slimming thereby identifies insignificant channels that can be pruned for inference. In this paper, we propose replacing the <inline-formula> <tex-math notation=""LaTeX"">$\ell _{1}$ </tex-math></inline-formula> penalty with an alternative nonconvex, sparsity-inducing penalty in order to yield a more compressed and/or accurate CNN architecture. We investigate <inline-formula> <tex-math notation=""LaTeX"">$\ell _{p} (0 < p < 1)$ </tex-math></inline-formula>, transformed <inline-formula> <tex-math notation=""LaTeX"">$\ell _{1}$ </tex-math></inline-formula> (<inline-formula> <tex-math notation=""LaTeX"">$\text{T}\ell _{1}$ </tex-math></inline-formula>), minimax concave penalty (MCP), and smoothly clipped absolute deviation (SCAD) due to their recent successes and popularity in solving sparse optimization problems, such as compressed sensing and variable selection. We demonstrate the effectiveness of network slimming with nonconvex penalties on three neural network architectures – VGG-19, DenseNet-40, and ResNet-164 – on standard image classification datasets. Based on the numerical experiments, <inline-formula> <tex-math notation=""LaTeX"">$\text{T}\ell _{1}$ </tex-math></inline-formula> preserves model accuracy against channel pruning, <inline-formula> <tex-math notation=""LaTeX"">$\ell _{1/2, 3/4}$ </tex-math></inline-formula> yield better compressed models with similar accuracies after retraining as <inline-formula> <tex-math notation=""LaTeX"">$\ell _{1}$ </tex-math></inline-formula>, and MCP and SCAD provide more accurate models after retraining with similar compression as <inline-formula> <tex-math notation=""LaTeX"">$\ell _{1}$ </tex-math></inline-formula>. Network slimming with <inline-formula> <tex-math notation=""LaTeX"">$\text{T}\ell _{1}$ </tex-math></inline-formula> regularization also outperforms the latest Bayesian modification of network slimming in compressing a CNN architecture in terms of memory storage while preserving its model accuracy after channel pruning.",10.1109/ACCESS.2021.3105366,Convolutional neural networks (CNN);machine learning;deep learning;network pruning;nonconvex optimization,,
Neural Architecture Search for Robust Networks in 6G-Enabled Massive IoT Domain,K. Wang; P. Xu; C. -M. Chen; S. Kumari; M. Shojafar; M. Alazab,IEEE Internet of Things Journal,2021.0,"6G technology enables artificial intelligence (AI)-based massive IoT to manage network resources and data with ultra high speed, responsive network, and wide coverage. However, many AI-enabled Internet-of-Things (AIoT) systems are vulnerable to adversarial example attacks. Therefore, designing robust deep learning models that can be deployed on resource-constrained devices has become an important research topic in the field of 6G-enabled AIoT. In this article, we propose a method for automatically searching for robust and efficient neural network structures for AIoT systems. By introducing a skip connection structure, a feature map with reduced front-end influence can be used for calculations during the classification process. Additionally, a novel type of densely connected search space is proposed. By relaxing this space, it is possible to search for network structures efficiently. In addition, combined with adversarial training and model delay constraints, we propose a multiobjective gradient optimization method to realize the automatic searching of network structures. Experimental results demonstrate that our method is effective for AIoT systems and superior to state-of-the-art neural architecture search algorithms.",10.1109/JIOT.2020.3040281,6G;adversarial example;artificial intelligence-enabled Internet-of-Things (AIoT);massive IoT;neural architecture search,3.0,
Exploration of data-driven methods for multiphysics electromagnetic partial differential equations,H. Fu; W. Cheng; Y. Qin,2020 IEEE MTT-S International Conference on Numerical Electromagnetic and Multiphysics Modeling and Optimization (NEMO),2020.0,"In a complex electromagnetic environment, numerical solution of partial differential equations (PDEs) and how to sample less data to invert spatio-temporal dynamics to discover potential physical laws or governing equations have been facing quite difficult challenges. With rapid progress of data-driven methods for artificial intelligence and computational physics, this paper would like to discuss our attempt on data-driven inversion and forward model for electromagnetic Maxwell's Equations.First of all, we propose a deep learning neural network in conjunction with sparse regression and pareto analysis to retrieve the hidden governing equations - Maxwell's wave equations of multiphysics electromagnetic systems. The JEC-FDTD algorithm is adopted for calculating interaction between electromagnetic wave with magnetized plasmas. Based on the concept of automatic differentiation, the differentiation operator is approximated by convolution. The neural network by tempo-rally sampling data of few spatial points can retrieve multiple physical electromagnetic and inhomogeneous magnetized plasma parameters. Also, the arctecture with learned PDEs is capable of predicting electric fields throughout the whole process even with noise. The data-driven method for discovery of coupled partial differential equations describing the electromagnetic fields in a complex system may also be applied to solve changeling problems that may not be solvable from first principles.Secondly, we construct the relationship between the Maxwell's wave equations and the recurrent neural network (RNN) for multiphysics electromagnetic systems. Numerical solution and inversion of Maxwell's wave equations are investigated for multiphysics electromagnetic systems. Numerical results by RNN agrees with the traditional JEC-FDTD algorithm. Parameter inversion can be easily achieved with backpropagation.Finally, we would like to compare two methods and then discuss advantages, difficulties and challenges for data-driven method for multiphysics electromagnetic systems.",10.1109/NEMO49486.2020.9343645,Microwave and plasma interaction;Maxwell’s wave equation;Data-driven discovery;Machine learning;Recur-rent neural network,,
ApproxFPGAs: Embracing ASIC-Based Approximate Arithmetic Components for FPGA-Based Systems,B. S. Prabakaran; V. Mrazek; Z. Vasicek; L. Sekanina; M. Shafique,2020 57th ACM/IEEE Design Automation Conference (DAC),2020.0,"There has been abundant research on the development of Approximate Circuits (ACs) for ASICs. However, previous studies have illustrated that ASIC-based ACs offer asymmetrical gains in FPGA-based accelerators. Therefore, an AC that might be pareto-optimal for ASICs might not be pareto-optimal for FPGAs. In this work, we present the ApproxFPGAs methodology that uses machine learning models to reduce the exploration time for analyzing the state-of-the-art ASIC-based ACs to determine the set of pareto-optimal FPGA-based ACs. We also perform a case-study to illustrate the benefits obtained by deploying these pareto-optimal FPGA-based ACs in a state-of-the-art automation framework to systematically generate pareto-optimal approximate accelerators that can be deployed in FPGA-based systems to achieve high performance or low-power consumption.",10.1109/DAC18072.2020.9218533,Approximate Computing;FPGA;ASIC;Adder;Multiplier;Arithmetic Units;Machine Learning;Statistics;Models;Synthesis,3.0,
Invited Talk Abstract: Introducing ReQuEST: An Open Platform for Reproducible and Quality-Efficient Systems-ML Tournaments,G. Fursin,2018 1st Workshop on Energy Efficient Machine Learning and Cognitive Computing for Embedded Applications (EMC2),2018.0,"Co-designing efficient machine learning based systems across the whole application/hardware/software stack to trade off speed, accuracy, energy and costs is becoming extremely complex and time consuming. Researchers often struggle to evaluate and compare different published works across rapidly evolving software frameworks, heterogeneous hardware platforms, compilers, libraries, algorithms, data sets, models, and environments. I will present our community effort to develop an open co-design tournament platform with an online public scoreboard based on Collective Knowledge workflow framework (CK). It gradually incorporates best research practices while providing a common way for multidisciplinary researchers to optimize and compare the quality vs. efficiency Pareto optimality of various workloads on diverse and complete hardware/software systems. All the winning solutions will be made available to the community as portable and customizable ""plug&play"" components with a common API to accelerate research and innovation! I will then discuss how our open competition and collaboration can help to achieve energy efficiency for cognitive workloads based on energy-efficient submissions from the 1st ReQuEST tournament co-located with ASPLOS'18. Further details: http://cKnowledge.org/request.",10.1109/EMC2.2018.00008,Tournament;open platform;machine learning,,
Multi-Dimensional Dynamic Model Compression for Efficient Image Super-Resolution,Z. Hou; S. -Y. Kung,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),2022.0,"Modern single image super-resolution (SR) system based on convolutional neural networks achieves substantial progress. However, most SR deep networks are computationally expensive and require excessively large activation memory footprints, impeding their effective deployment to resource-limited devices. Based on the observation that the activation patterns in SR networks exhibit high input-dependency, we propose Multi-Dimensional Dynamic Model Compression method that can reduce both spatial and channel wise redundancy in an SR deep network for different input images. To reduce the spatial-wise redundancy, we propose to perform convolution on scaled-down feature-maps where the down-scaling factor is made adaptive to different input images. To reduce the channel-wise redundancy, we introduce a low-cost channel saliency predictor for each convolution to dynamically skip the computation of unimportant channels based on the Gumbel-Softmax. To better capture the feature-maps information and facilitate input-adaptive decision, we employ classic image processing metrics, e.g., Spatial Information, to guide the saliency predictors. The proposed method can be readily applied to a variety of SR deep networks and trained end-to-end with standard super-resolution loss, in combination with a sparsity criterion. Experiments on several benchmarks demonstrate that our method can effectively reduce the FLOPs of both lightweight and non-compact SR models with negligible PSNR loss. Moreover, our compressed models achieve competitive PSNR-FLOPs Pareto frontier compared with SOTA NAS-based SR methods.",10.1109/WACV51458.2022.00355,Deep Learning Deep Learning -> Efficient Training and Inference Methods for Networks,,
The Trade-off Between Privacy and Utility in Local Differential Privacy,M. Li; Y. Tian; J. Zhang; D. Fan; D. Zhao,2021 International Conference on Networking and Network Applications (NaNA),2021.0,"In statistical queries work, such as frequency estimation, the untrusted data collector could as an honest-but-curious (HbC) or malicious adversary to learn true values. Local differential privacy(LDP) protocols have been applied against the untrusted third party in data collecting. Nevertheless, excessive noise of LDP will reduce data utility, thus affecting the results of statistical queries. Therefore, it is significant to research the trade-off between privacy and utility. In this paper, we first measure the privacy loss by observing the maximum posterior confidence of the adversary (data collector). Then, through theoretical analysis and comparison we obtain the most suitable utility measure that is Wasserstein distance. Based on these, we introduce an originality framework for privacy-utility tradeoff framework, finding that this system conforms to the Pareto optimality state and formalizing a payoff function to find optimal equilibrium point under Pareto efficiency. Finally, we illustrate the efficacy of our system model by the Adult dataset from the UCI machine learning repository.",10.1109/NaNA53684.2021.00071,data collecting;local differential privacy;privacy metric;utility metric;Pareto optimality,,
Driver Danger-Level Monitoring System Using Multi-Sourced Big Driving Data,J. -L. Yin; B. -H. Chen; K. -H. R. Lai,IEEE Transactions on Intelligent Transportation Systems,2020.0,"Danger-level analysis is widely used to prevent potential driving risks based on driving performance. Such analysis is essential for monitoring driver performance. Moreover, danger-level analysis is vital for automotive safety systems and driving assistance applications. However, danger-level analysis that simultaneously considers driver-, vehicle-, and road-related information from driving data has rarely been conducted. Such analysis is very challenging due to the issues associated with the high volume and high variety in multisourced driving data. In this paper, we propose a novel danger-level analysis framework for dealing with high variety and high volume problems of multisourced driving data. Built upon a feature extraction method in the proposed framework, we first profile multisourced driving features for overcoming the variety problem. Next, danger-level analysis is formulated as a multiobjective pursuit problem in a linear model. The problem is then solved using a semisupervised learning strategy to overcome the volume issue. Therefore, the danger level can be accurately estimated from multisourced driving data by using the proposed framework. The experimental results indicate that the proposed framework outperforms existing machine learning techniques for multisourced driving data.",10.1109/TITS.2019.2954183,Danger-level analysis;driving risks;multi-sourced driving data,6.0,
Testing Vision-Based Control Systems Using Learnable Evolutionary Algorithms,R. Ben Abdessalem; S. Nejati; L. C. Briand; T. Stifter,2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE),2018.0,"Vision-based control systems are key enablers of many autonomous vehicular systems, including self-driving cars. Testing such systems is complicated by complex and multidimensional input spaces. We propose an automated testing algorithm that builds on learnable evolutionary algorithms. These algorithms rely on machine learning or a combination of machine learning and Darwinian genetic operators to guide the generation of new solutions (test scenarios in our context). Our approach combines multiobjective population-based search algorithms and decision tree classification models to achieve the following goals: First, classification models guide the search-based generation of tests faster towards critical test scenarios (i.e., test scenarios leading to failures). Second, search algorithms refine classification models so that the models can accurately characterize critical regions (i.e., the regions of a test input space that are likely to contain most critical test scenarios). Our evaluation performed on an industrial automotive automotive system shows that: (1) Our algorithm outperforms a baseline evolutionary search algorithm and generates 78% more distinct, critical test scenarios compared to the baseline algorithm. (2) Our algorithm accurately characterizes critical regions of the system under test, thus identifying the conditions that are likely to lead to system failures.",10.1145/3180155.3180160,Search-based Software Engineering;Evolutionary algorithms;Software Testing;Automotive Software Systems,46.0,
Deception In The Game of Guarding Multiple Territories: A Machine Learning Approach,A. Asgharnia; H. M. Schwartz; M. Atia,"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",2020.0,"In this paper, a deceptive version of guarding a territory in a grid world is proposed. Like the original version, a defender tries to intercept an invader before it invades the targets. However, the discerning invader can deceive the defender about its real goal so that it can improve its performance. On the other hand, the defender tries to confront the invader by guessing its true goal. A two-level policy is obtained via reinforcement learning (RL). In the lower level, the invader and the defender learn their optimal policies to invade or defend a particular territory. In the higher level, the invader learns which territory it should pretend to invade in order to manipulate the defender's belief function. A multiagent reinforcement learning (MARL) algorithm is implemented for obtaining the optimal policies via the minimax Q-learning algorithm at the lower level. Whereas for the higher-level policy a single-agent Q-learning algorithm is utilized. Results of different reward functions are compared. The results show that the invader can improve its performance by taking advantage of deception.",10.1109/SMC42975.2020.9283173,Reinforcement Learning;Multiagent systems;Deception;Belief;Guarding a territory,,
A Deep Learning Approach To Multi-Context Socially-Aware Navigation,S. B. Banisetty; V. Rajamohan; F. Vega; D. Feil-Seifer,2021 30th IEEE International Conference on Robot & Human Interactive Communication (RO-MAN),2021.0,"We present a context classification pipeline to allow a robot to change its navigation strategy based on the observed social scenario. Socially-Aware Navigation considers social behavior in order to improve navigation around people. Most of the existing research uses different techniques to incorporate social norms into robot path planning for a single context. Methods that work for hallway behavior might not work for approaching people, and so on. We developed a high-level decision-making subsystem, a model-based context classifier, and a multi-objective optimization-based local planner to achieve socially-aware trajectories for autonomously sensed contexts. Using a context classification system, the robot can select social objectives that are later used by Pareto Concavity Elimination Transformation (PaCcET) based local planner to generate safe, comfortable, and socially appropriate trajectories for its environment. This was tested and validated in multiple environments on a Pioneer mobile robot platform; results show that the robot could select and account for social objectives related to navigation autonomously.",10.1109/RO-MAN50785.2021.9515424,,,
Energy-performance design exploration of a low-power microprogrammed deep-learning accelerator,G. Santoro; M. R. Casu; V. Peluso; A. Calimera; M. Alioto,"2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)",2018.0,"This paper presents the design space exploration of a novel microprogrammable accelerator in which PEs are connected with a Network-on-Chip and benefit from low-power features enabled through a practical implementation of a Dual-V<sub>dd</sub> assignment scheme. An analytical model, fitted with postlayout data obtained with a 28nm FDSOI design kit, returns implementations with optimal energy-performance tradeoff by taking into consideration all the key design-space variables. The obtained Pareto analysis helps us infer optimization rules aimed at improving quality of design.",10.23919/DATE.2018.8342185,,3.0,
Applying Machine Learning in Designing Distributed Auction for Multi-agent Task Allocation with Budget Constraints,C. Luo; Q. Huang; F. Kong; S. Khan; Q. Qiu,2021 20th International Conference on Advanced Robotics (ICAR),2021.0,"The multi-agent task allocation can be solved in a distributed manner using Consensus-Based Bundle Algorithm (CBBA). Under this distributed auction process, each agent greedily maximizes the global score, which is the difference of the reward and the cost, through an iterative bundle construction and conflict resolution procedure. The distributed algorithm has provable convergence and guarantees 50% optimality if the score function satisfies the condition of diminishing marginal gain (DMG). While the previous work focuses on unconstrained optimization of rewards, this paper aims at applying CBBA to task allocation with budget constraints. Several heuristics were proposed to build the bundle and calculate the bidding scores as improvements to the original CBBA algorithm. We then prove that some of the new score functions are DMG, and therefore guarantees the convergence of the distributed process. We also show that these heuristic extended CBBAs are Pareto efficient; using different heuristic extensions under different scenarios is more efficient than consistently using the same one. To decide which heuristic extension should be used for a given task allocation problem, a graph convolutional neural network (GCN) model is trained to extract and analyze the features of the constrained optimization problem as a graph, and predict the potential performance (i.e., global reward) of different heuristic extensions. Based on the prediction, the best heuristic extension will be selected. Experimental results show that the predicted reward has more than 0.98 correlation with the actual reward and for 70% of time the prediction guided selection picks the best heuristic extension for the budget constrained task allocation problem.",10.1109/ICAR53236.2021.9659364,multi-agent;task allocation;GCN;limited budget;graph embedding,,
Universal Active Learning via Conditional Mutual Information Minimization,S. Shayovitz; M. Feder,IEEE Journal on Selected Areas in Information Theory,2021.0,"Modern machine learning systems require massive amounts of labeled training data in order to achieve high accuracy rates which is very expensive in terms of time and cost. Active learning is an approach which uses feedback to only label the most informative data points and significantly reduce the labeling effort. Many heuristics for selecting data points have been developed in recent years which are usually tailored to a specific task and a general unified framework is lacking. In this work, a new information theoretic criterion is proposed based on a minimax log-loss regret formulation of the active learning problem. First, a Redundancy Capacity theorem for active learning is derived along with an optimal learner. This leads to a new active learning criterion which naturally induces an exploration - exploitation trade-off in feature selection and generalizes previously proposed heuristic criteria. The new criterion is compared analytically and via empirical simulation to other commonly used information theoretic active learning criteria. Next, the linear hyper-plane hypotheses class with possibly asymmetric label noise is considered. The achievable performance for the proposed criterion is analyzed using a new low complexity greedy algorithm based on the Posterior Matching scheme for communication with feedback. It is shown that for general label noise and bounded feature distribution, the new information theoretic criterion decays exponentially fast to zero.",10.1109/JSAIT.2021.3073842,Minimax learning;active learning;posterior matching;feedback,,
A Characterization of Stochastic Mirror Descent Algorithms and Their Convergence Properties,N. Azizan; B. Hassibi,"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",2019.0,"Stochastic mirror descent (SMD) algorithms have recently garnered a great deal of attention in optimization, signal processing, and machine learning. They are similar to stochastic gradient descent (SGD), in that they perform updates along the negative gradient of an instantaneous (or stochastically chosen) loss function. However, rather than update the parameter (or weight) vector directly, they update it in a ""mirrored"" domain whose transformation is given by the gradient of a strictly convex differentiable potential function. SMD was originally conceived to take advantage of the underlying geometry of the problem as a way to improve the convergence rate over SGD. In this paper, we study SMD, for linear models and convex loss functions, through the lens of H<sup>∞</sup> estimation theory and come up with a minimax interpretation of the SMD algorithm which is the counterpart of the H<sup>∞</sup>-optimality of the SGD algorithm for linear models and quadratic loss. In doing so, we identify a fundamental conservation law that SMD satisfies and use it to study the convergence properties of the algorithm. For constant step size SMD, when the linear model is over-parameterized, we give a deterministic proof of convergence for SMD and show that from any initial point, it converges to the closest point in the space of all parameter vectors that interpolate the data, where closest is in the sense of the Bregman divergence of the potential function. This property is referred to as implicit regularization: with an appropriate choice of the potential function one can guarantee convergence to the minimizer of any desired convex regularizer. For vanishing step size SMD, and in the standard stochastic optimization setting, we give a direct and elementary proof of convergence for SMD to the ""true"" parameter vector which avoids ergodic averaging or appealing to stochastic differential equations.",10.1109/ICASSP.2019.8682271,Stochastic gradient descent;mirror descent;minimax optimality;convergence;implicit regularization,2.0,
Information-Theoretic Lower Bounds on the Oracle Complexity of Stochastic Convex Optimization,A. Agarwal; P. L. Bartlett; P. Ravikumar; M. J. Wainwright,IEEE Transactions on Information Theory,2012.0,"Relative to the large literature on upper bounds on complexity of convex optimization, lesser attention has been paid to the fundamental hardn4516420ess of these problems. Given the extensive use of convex optimization in machine learning and statistics, gaining an understanding of these complexity-theoretic issues is important. In this paper, we study the complexity of stochastic convex optimization in an oracle model of computation. We introduce a new notion of discrepancy between functions, and use it to reduce problems of stochastic convex optimization to statistical parameter estimation, which can be lower bounded using information-theoretic methods. Using this approach, we improve upon known results and obtain tight minimax complexity estimates for various function classes.",10.1109/TIT.2011.2182178,Computational learning theory;convex optimization;Fano's inequality;information-based complexity;minimax analysis;oracle complexity,66.0,
Speech recognition based chess system for visually challanged,B. Bharathi; S. Kavitha; D. S. Shashaank; S. Priyanka; V. Sriram,"2017 International Conference on Energy, Communication, Data Analytics and Soft Computing (ICECDS)",2017.0,"To overcome the obstacle of visual chess simulators, a system is proposed in which visually impaired individuals can practice by using only voice commands. In addition, this system will be powered using machine learning algorithms with the help of a publicly available repository of over five million games. This will make the user feel like he/she is playing against another human and not a machine. The existing chess systems use a variety of algorithms in order to choose its move. Most of these use tree traversals and the most common one is the min-max algorithm with alpha-beta pruning. Min-max algorithm finds the best move, and alpha-beta pruning prevents it from going into branches of the game tree that cannot yield a better result than previously traversed branches. Since the tree generated in a chess game is very deep and have a lot nodes, these algorithms examine the depth only to a certain amount. Generally, these algorithms are accompanied by an opening repository which bolsters the algorithm's efficiency. Since these algorithms are too strong for a visually impaired individual, a new approach is suggested to choose the computer's move. A publicly available repository of over five million chess games played by humans will be used to train the machine, initially. When the user makes his/her move, it will be converted into text and given as input to both the Minimax and the k-NN algorithms. The moves given as output by both these algorithms are compared using an evaluation function. The move with a higher score is chosen. The game continues till a decisive result is obtained.",10.1109/ICECDS.2017.8389758,Speech Recognition;Hidden Markov Model;Minimax model;Alpha-beta pruning;k-nearest neighbour,,
Predicting biochemical interactions - human P450 2D6 enzyme inhibition,W. B. Langdon; S. J. Barrett; B. F. Buxton,"The 2003 Congress on Evolutionary Computation, 2003. CEC '03.",2003.0,"In silico screening of chemical libraries or virtual chemicals may reduce drug discovery and medicine optimisation lead times and increase the probability of success by directing search through chemical space. About a dozen intelligent pharmaceutical QSAR modelling techniques were used to predict IC50 concentration (three classes) of drug interaction with a cell wall enzyme (P450 CYC2D6). Genetic programming gave comprehensible cheminformatics models which generalised best. This was shown by a blind test on Glaxo Welcome molecules of machine learning knowledge nuggets mined from SmithKline Beecham compounds. Performance on similar chemicals (interpolation) and diverse chemicals (extrapolation) suggest generalisation is more difficult than avoiding over fitting. Two GP approaches, classification via regression using a multiobjective fitness measure and a direct winner takes all (WTA) or one versus all (OVA) classification, are described. Predictive rules were compressed by separate follow up GP runs seeded with the best program.",10.1109/CEC.2003.1299750,,3.0,
Real-Time Tracking of Packet-Pair Dispersion Nodes Using the Kernel-Density and Gaussian-Mixture Models,M. Hosseinpour; M. J. Tunnicliffe,2009 11th International Conference on Computer Modelling and Simulation,2009.0,"A brief simulation study of real-time packet dispersion mode-tracking using the Gaussian-mix model (originally devised for real-time background classification in moving pictures) and an adaptation of the kernel-density estimator is presented. The simulated environment consisted of two FIFO store-and-forward nodes where the probe packets interact with Poisson and Pareto-generated cross-traffic with a range of packet sizes. The two models produced broadly similar results, able to track node activity under the dynamically changing conditions associated with the Pareto cross-traffic. The Gaussian model sometimes replaced the primary mode with a double peak, which disappeared when some of the modelpsilas parameters were changed.",10.1109/UKSIM.2009.74,Bandwidth Measurement;Machine Learning,2.0,
Adversarially Robust Classification Based on GLRT,B. Puranik; U. Madhow; R. Pedarsani,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",2021.0,"Machine learning models are vulnerable to adversarial attacks that can often cause misclassification by introducing small but well designed perturbations. In this paper, we explore, in the setting of classical composite hypothesis testing, a defense strategy based on the generalized likelihood ratio test (GLRT), which jointly estimates the class of interest and the adversarial perturbation. We evaluate the GLRT approach for the special case of binary hypothesis testing in white Gaussian noise under ℓ<inf>∞</inf> norm-bounded adversarial perturbations, a setting for which a minimax strategy optimizing for the worst-case attack is known. We show that the GLRT approach yields performance competitive with that of the minimax approach under the worst-case attack, while yielding a better robustness-accuracy trade-off under weaker attacks. The GLRT defense is applicable in multi-class settings and generalizes naturally to more complex models for which optimal minimax classifiers are not known.",10.1109/ICASSP39728.2021.9413587,Adversarial machine learning;hypothesis testing;robust classification,,
Heuristic Edge Server Placement in Industrial Internet of Things and Cellular Networks,S. K. Kasi; M. K. Kasi; K. Ali; M. Raza; H. Afzal; A. Lasebae; B. Naeem; S. u. Islam; J. J. P. C. Rodrigues,IEEE Internet of Things Journal,2021.0,"Rapid developments in industry 4.0, machine learning, and digital twins have introduced new latency, reliability, and processing restrictions in Industrial Internet of Things (IIoT) and mobile devices. However, using current information and communications technology (ICT), it is difficult to optimally provide services that require high computing power and low latency. To meet these requirements, mobile-edge computing is emerging as a ubiquitous computing paradigm that enables the use of network infrastructure components such as cluster heads/sink nodes in IIoT and cellular network base stations to provide local data storage and computation servers at the edge of the network. However, optimal location selection for edge servers within a network out of a very large number of possibilities, such as to balance workload and minimize access delay, is a challenging problem. In this article, the edge server placement problem is addressed within an existing network infrastructure obtained from Shanghai Telecom's base station data set that includes a significant amount of call data records and locations of actual base stations. The problem of edge server placement is formulated as a multiobjective constraint optimization problem that places edge servers strategically to balance between the workloads of edge servers and reduce access delay between the industrial control center/cellular base stations and edge servers. To search randomly through a large number of possible solutions and selecting those that are most descriptive of optimal solution can be a very time-consuming process, therefore, we apply the genetic algorithm and local search algorithms (hill climbing and simulated annealing) to find the best solution in the least number of solution space explorations. Experimental results are obtained to compare the performance of the genetic algorithm against the above-mentioned local search algorithms. The results show that the genetic algorithm can quickly search through the large solution space as compared to local search optimization algorithms to find an edge placement strategy that minimizes the cost function.",10.1109/JIOT.2020.3041805,Data mining;edge server placement;genetic search;Industrial Internet of Things (IIoT);mobile-edge computing,4.0,
MoGFT-I: A Multi-objective Optimization approach for the Cart and Pole control problem,R. Ishibashi; C. Lúcio Nascimento Júnior,2015 Annual IEEE Systems Conference (SysCon) Proceedings,2015.0,"In this article a set of well-known computational intelligence techniques such as Decision Trees, Fuzzy Logic, and Multi-objective Optimization Genetic Algorithm are combined to generate a novel hybrid method which is called MoGFT-I: Multi-objective Genetic Fuzzy Rule Based System supported by a Decision Tree with Improved Interpretability. The output of the proposed supervised learning method is a set of Mamdani-type fuzzy systems which are optimized and distributed along a Pareto curve by considering two conflicting attributes: accuracy and interpretability. The MoGFT-I method is then applied to the Cart and Pole control problem such that a set of feedback controller are designed to control this unstable nonlinear dynamical system.",10.1109/SYSCON.2015.7116758,Multi-objective Optimization;Knowledge Extraction;Interpretability;Genetic Fuzzy Rule Based System;Fuzzy Controller;Pareto-optimal front,2.0,
Improving Robustness of DNNs against Common Corruptions via Gaussian Adversarial Training,C. Yi; H. Li; R. Wan; A. C. Kot,2020 IEEE International Conference on Visual Communications and Image Processing (VCIP),2020.0,"Deep neural networks have demonstrated tremendous success in image classification, but their performance sharply degrades when evaluated on slightly different test data (e.g., data with corruptions). To address these issues, we propose a minimax approach to improve common corruption robustness of deep neural networks via Gaussian Adversarial Training. To be specific, we propose to train neural networks with adversarial examples where the perturbations are Gaussian-distributed. Our experiments show that our proposed GAT can improve neural networks' robustness to noise corruptions more than other baseline methods. It also outperforms the state-of-the-art method in improving the overall robustness to common corruptions.",10.1109/VCIP49819.2020.9301856,Deep Learning;Robustness to Common Corruptions;Adversarial Training;Data Augmentation,1.0,
"Parallelization of <inline-formula><tex-math notation=""LaTeX"">$Top_{k}$</tex-math></inline-formula> Algorithm Through a New Hybrid Recommendation System for Big Data in Spark Cloud Computing Framework",K. El Handri; A. Idrissi,IEEE Systems Journal,2021.0,"In the era of big data, parallel <inline-formula><tex-math notation=""LaTeX"">$Top_{k}$</tex-math></inline-formula> query processing under information retrieval has received increasing attention from both the industry and academia. This query handling allows users to retrieve the most useful data objects in a set of choices. This problem is compounded by the use of <inline-formula><tex-math notation=""LaTeX"">$Top_{k}$</tex-math></inline-formula> in cases of multiple dimensions and extensive data analytics. In this article, we provide a novel parallel algorithm in a distributed recommender system based on the Apache Spark platform. The purpose of this approach was to implement the multicriteria decision aiding support and dominating query approach run by using matrix factorization and singular value decomposition (SVD)-based model as a sophisticated machine learning technique. Simultaneously, applying the resilient distributed datasets paradigm in cloud computing, which presents a favorable environment for big data management. Extensive experimental results in terms of accuracy, and scalability indicated the new algorithm’s advantage compared to other <inline-formula><tex-math notation=""LaTeX"">$Top_{k}$</tex-math></inline-formula> algorithms. Accordingly, our recommender system based on the conceived algorithm achieved high precision (62%–82%, depending on the data) to verify the profoundly positive effect of the use of the Spark framework and the SVD-based model while applying the commonly used evaluation metrics in the recommendation systems.",10.1109/JSYST.2020.3019368,Collaborative filtering (CF);Funk singular value decomposition (SVD);multiple criteria decision aiding (MCDA);Skyline;Spark; $Top_{k}$ ,3.0,
Study of Over-Sampling Methods Used in Distribution Transformer Connectivity Verification,Z. Tang; Y. Shen; L. Wan; H. Zhou; F. Yu; D. Qiu; W. Wang; X. Cao; T. Li,2020 IEEE 4th Conference on Energy Internet and Energy System Integration (EI2),2020.0,"Data-driven method has been used to carry out distribution transformer connectivity verification. The imbalanced dataset always results in bad performance of machine learning algorithms. In order to solve this problem, two kinds of over-sampling methods have been studied in this paper. Voltage curves of 3967 distribution transformer which belong to 197 10kV feeders have been collected. The performance of over-sampling methods under different sampling rate has also been studied. Pareto optimality has been carried out in order to obtain the best sampling rate. Results show that with the increase of sampling rate, the value of true negative (TN) increase and the value of true positive (TP) and accuracy decrease. The performance of synthetic minority over-sampling technique (SMOTE) is a little better compared with simple copy method (SCM). When the lower limits of TN, TP and accuracy are set as 0.93, the best sampling rate of SCM is 17 and the values for SMOTE are 16, 17 and 18.",10.1109/EI250167.2020.9347335,distribution transformer connectivity;imbalance dataset;over-sampling;SMOTE;Pareto optimality,,
Anomaly recognition in bursty IP traffic models,J. Smieško; M. Kontšek; R. Hajtmanek,2021 19th International Conference on Emerging eLearning Technologies and Applications (ICETA),2021.0,"In the first part of this article we deal with two models of IP traffic with bursty period, Poisson process and Pareto process. The paper deals with analysis of their probability structure and their comparison with each other. In the second part we deal with the use of one-parameter machine learning methods (autoregression coefficient and Hurst coefficient) for the recognition of anomalies in the traffic flow. We have created several scenarios for this task. These scenarios model changes of the observed flow structure. This article presents implementation of the Department of InfoComm Networks research, which focuses on computer security, into the curriculum of the master's study program Applied Network Engineering.",10.1109/ICETA54173.2021.9726543,IP traffic;detection;anomaly;DDoS attacks;autoregression coefficient;Hurst coefficient;bursty period;Poisson process;Pareto process,,
Solving Dynamic Multi-objective Optimization Problems Using Incremental Support Vector Machine,W. HU; M. JIANG; X. Gao; K. C. TAN; Y. -m. Cheung,2019 IEEE Congress on Evolutionary Computation (CEC),2019.0,"The main feature of the Dynamic Multi-objective Optimization Problems (DMOPs) is that optimization objective functions will change with times or environments. One of the promising approaches for solving the DMOPs is reusing the obtained Pareto optimal set (POS) to train prediction models via machine learning approaches. In this paper, we train an Incremental Support Vector Machine (ISVM) classifier with the past POS, and then the solutions of the DMOP we want to solve at the next moment are filtered through the trained ISVM classifier. A high-quality initial population will be generated by the ISVM classifier, and a variety of different types of population-based dynamic multi-objective optimization algorithms can benefit from the population. To verify this idea, we incorporate the proposed approach into three evolutionary algorithms, the multi-objective particle swarm optimization(MOPSO), Nondominated Sorting Genetic Algorithm II (NSGA-II), and the Regularity Model-based multi-objective estimation of distribution algorithm(RE-MEDA). We employ experiments to test these algorithms, and experimental results show the effectiveness.",10.1109/CEC.2019.8790005,Dynamic Multi-objective Optimization Problems;Incremental Support Vector Machine;Pareto Optimal Set,4.0,
A hybrid evolutionary approach for optimal fuzzy classifier design,A. S. Karthik Kannan; P. Thanapal,2010 INTERNATIONAL CONFERENCE ON COMMUNICATION CONTROL AND COMPUTING TECHNOLOGIES,2010.0,"One of the important issues in the design of fuzzy classifier is the formation of fuzzy if-then rules and the membership functions. This paper presents a Niched Pareto Genetic Algorithm (NPGA) approach to obtain the optimal rule-set and the membership function. To develop the fuzzy system the rule set and the membership functions are encoded into the chromosome and evolved simultaneously using NPGA. The performance of the proposed approach is demonstrated through development of fuzzy classifier for Iris data available in the UCI machine learning repository. From the simulation study, it is found that that NPGA produces a fuzzy classifier which has minimum number of rules and high classification accuracy compared with the existing methods.",10.1109/ICCCCT.2010.5670725,fuzzy classifier;if-then-rules;membership function;Niched Pareto Genetic Algorithm,2.0,
Adaptive Modulation Using Multi-Objective Reinforcement Learning for LEO Satellites,F. Pasquevich; A. F. Ramirez; J. M. Ayarde; G. C. Briones,2021 IEEE Cognitive Communications for Aerospace Applications Workshop (CCAAW),2021.0,"In this paper, an emerging Machine Learning technique, named Multi-Objective Reinforcement Learning (MORL), is applied and analyzed aiming to achieve a two-fold optimization in a Satellite-To-Ground communication. The objectives pursued in this work using MORL are to minimize the Bit Error Rate and keep simultaneously the best performance in terms of the maximum bit rate transmission. The scenario under test consists of a Low-Earth Orbit satellite moving in a circular orbit while establishing a line-of-sight communication with a Ground Station. Two approaches are evaluated, the Weighted Sum and the Thresholded Lexicographic Q-Learning. We show that these two approaches can not find all the solutions. To alleviate this situation, a novel proposal is considered based on an inverse scalarization function that allows to select any solution from the Pareto front. We show that the proposed algorithm is able to implement a suitable operation for many different digital modulation schemes to obtain the maximum throughput.",10.1109/CCAAW50069.2021.9527292,MORL;LEO;ML;Pareto,,
Early Prediction of Sepsis from Clinical Data Using Artificial Intelligence,R. M. Demirer; O. Demirer,2019 Scientific Meeting on Electrical-Electronics & Biomedical Engineering and Computer Science (EBBT),2019.0,"Sepsis is a major cause of death in the world. World Health Organization estimates 30 million people developing sepsis and 6 million people die from sepsis each year; an estimated 4.2 million newborns and children are affected. The mortality rate is highest in septic shock in poor and developing countries. Early prediction of sepsis is critical for improving sepsis outcomes. The late prediction of sepsis in non-sepsis patients is a challenging problem. The aim of this study is to develop an artificial intelligence-based early warning and therapeutic decision support system which reduces sepsis-associated hospital mortality. We propose two compatible Boolean switchable Partially Observable Markov Decision Processes (POMDP) under a general risk-sensitive optimization criterion with finite time horizon. It is based on Spectral analysis of unevenly sampled (missing) observations with Demographics, Vital Signs, and Laboratory values for the patient. The policy is a common mixture of sepsis and non-sepsis beliefs on own utility functions which favors to achieve Pareto Optimality from this high dimensional belief space.",10.1109/EBBT.2019.8741834,Sepsis;Artificial Intelligence;Decision Support;Lomb-Scargle Periyodogram;POMDP;Deep Learning,1.0,
Signal Recovery on Graphs: Fundamental Limits of Sampling Strategies,S. Chen; R. Varma; A. Singh; J. Kovačević,IEEE Transactions on Signal and Information Processing over Networks,2016.0,"This paper builds theoretical foundations for the recovery of a newly proposed class of smooth graph signals, approximately bandlimited graph signals, under three sampling strategies: uniform sampling, experimentally designed sampling, and active sampling. We then state minimax lower bounds on the maximum risk for the approximately bandlimited class under these three sampling strategies and show that active sampling cannot fundamentally outperform experimentally designed sampling. We propose a recovery strategy to compare uniform sampling with experimentally designed sampling. As the proposed recovery strategy lends itself well to statistical analysis, we derive the exact mean square error for each sampling strategy. To study convergence rates, we introduce two types of graphs and find that 1) the proposed recovery strategy achieves the optimal rates; and 2) the experimentally designed sampling fundamentally outperforms uniform sampling for Type-2 class of graphs. To validate our proposed recovery strategy, we test it on five specific graphs: a ring graph with k nearest neighbors, an Erdos-Rényi graph, a random geometric graph, a small-world graph, and a power-law graph and find that experimental results match the proposed theory well. This paper also presents a comprehensive explanation for when and why sampling for semi-supervised learning with graphs works.",10.1109/TSIPN.2016.2614903,Active sampling;experimentally designed sampling;semi-supervised learning;signal processing on graphs;signal recovery,53.0,
Neural Signature of Efficiency Relations,S. Basterrech; K. Ohnishi; M. Köppen,"2015 IEEE International Conference on Systems, Man, and Cybernetics",2015.0,"In last years -- especially due to the development of telecommunications -- fairness modelling has received a strong attention. This article presents an approach for categorizing unknown relations according to their ""closeness"" to known relations. We consider as reference relations, the well-known: Pareto dominance, Leximin and Proportional fairness relation. We simulate each relation generating a learning dataset that is used for learning Neural Networks. The learning performance evaluation is based in several metrics, which are used as a ""signature"" of each relation. Besides, we develop a new function that gives an estimation about the ""closeness"" between relations. This concept permits us to categorise a new dataset (generated by an unknown relation) according its ""closeness"" with the Pareto dominance, Leximin and Proportional fairness relations know relations. Our experimental results are coherent with the alpha fairness concept.",10.1109/SMC.2015.365,Fairness;Maxmin Fairness;Priority Fairness;Neural Networks;Supervised Learning,,
Simultaneously Structured Models With Application to Sparse and Low-Rank Matrices,S. Oymak; A. Jalali; M. Fazel; Y. C. Eldar; B. Hassibi,IEEE Transactions on Information Theory,2015.0,"Recovering structured models (e.g., sparse or group-sparse vectors, low-rank matrices) given a few linear observations have been well-studied recently. In various applications in signal processing and machine learning, the model of interest is structured in several ways, for example, a matrix that is simultaneously sparse and low rank. Often norms that promote the individual structures are known, and allow for recovery using an order-wise optimal number of measurements (e.g., 11 norm for sparsity, nuclear norm for matrix rank). Hence, it is reasonable to minimize a combination of such norms. We show that, surprisingly, using multiobjective optimization with these norms can do no better, orderwise, than exploiting only one of the structures, thus revealing a fundamental limitation in sample complexity. This result suggests that to fully exploit the multiple structures, we need an entirely new convex relaxation. Further, specializing our results to the case of sparse and low-rank matrices, we show that a nonconvex formulation recovers the model from very few measurements (on the order of the degrees of freedom), whereas the convex problem combining the 11 and nuclear norms requires many more measurements, illustrating a gap between the performance of the convex and nonconvex recovery problems. Our framework applies to arbitrary structure-inducing norms as well as to a wide range of measurement ensembles. This allows us to give sample complexity bounds for problems such as sparse phase retrieval and low-rank tensor completion.",10.1109/TIT.2015.2401574,compressed sensing;convex relaxation;regularization;sample complexity;Compressed sensing;convex relaxation;regularization;sample complexity,119.0,
Multi-Objective Optimization for Size and Resilience of Spiking Neural Networks,M. Dimovska; T. Johnston; C. D. Schuman; J. P. Mitchell; T. E. Potok,"2019 IEEE 10th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",2019.0,"Inspired by the connectivity mechanisms in the brain, neuromorphic computing architectures model Spiking Neural Networks (SNNs) in silicon. As such, neuromorphic architectures are designed and developed with the goal of having small, low power chips that can perform control and machine learning tasks. However, the power consumption of the developed hardware can greatly depend on the size of the network that is being evaluated on the chip. Furthermore, the accuracy of a trained SNN that is evaluated on chip can change due to voltage and current variations in the hardware that perturb the learned weights of the network. While efforts are made on the hardware side to minimize those perturbations, a software based strategy to make the deployed networks more resilient can help further alleviate that issue. In this work, we study Spiking Neural Networks in two neuromorphic architecture implementations with the goal of decreasing their size, while at the same time increasing their resiliency to hardware faults. We leverage an evolutionary algorithm to train the SNNs and propose a multiobjective fitness function to optimize the size and resiliency of the SNN. We demonstrate that this strategy leads to well-performing, small-sized networks that are more resilient to hardware faults.",10.1109/UEMCON47517.2019.8992983,Neuromorphic Computing;Spiking Neural Networks;Multi-objective;Fault Tolerance;Evolutionary Optimization,5.0,
Likelihood-Based Semi-Supervised Model Selection With Applications to Speech Processing,C. M. White; S. P. Khudanpur; P. J. Wolfe,IEEE Journal of Selected Topics in Signal Processing,2010.0,"In conventional supervised pattern recognition tasks, model selection is typically accomplished by minimizing the classification error rate on a set of so-called development data, subject to ground-truth labeling by human experts or some other means. In the context of speech processing systems and other large-scale practical applications, however, such labeled development data are typically costly and difficult to obtain. This paper investigates an alternative semi-supervised framework for likelihood-based model selection that leverages unlabeled data by using trained classifiers representing each model to automatically generate putative labels. The errors that result from this automatic labeling are shown to be amenable to results from robust statistics, which in turn provide for minimax-optimal censored likelihood ratio tests that recover the nonparametric sign test as a limiting case. This approach is then validated experimentally using a state-of-the-art automatic speech recognition system to select between candidate word pronunciations using unlabeled speech data that only potentially contain instances of the words under test. Results provide supporting evidence for the utility of this approach, and suggest that it may also find use in other applications of machine learning.",10.1109/JSTSP.2010.2076050,Likelihood ratio tests;pronunciation modeling;robust statistics;semi-supervised learning;sign test;speech recognition;spoken term detection,,
Learning Variable Importance to Guide Recombination on Many-Objective Optimization,M. Sagawa; H. Aguirre; F. Daolio; A. Liefooghe; B. Derbel; S. Verel; K. Tanaka,2017 6th IIAI International Congress on Advanced Applied Informatics (IIAI-AAI),2017.0,"There are numerous many-objective real-world problems in various application domains for which it is difficult or time-consuming to derive Pareto optimal solutions. In an evolutionary algorithm, variation operators such as recombination and mutation are extremely important to obtain an effective solution search. In this paper, we study a machine learning-enhanced recombination that incorporates an intelligent variable selection method. The method is based on the importance of variables with respect to convergence to the Pareto front. We verify the performance of the enhanced recombination on benchmark test problems with three or more objectives using the many-objective evolutionary algorithm AϵSϵH as a baseline algorithm. Results show that variable importance can enhance the performance of many-objective evolutionary algorithms.",10.1109/IIAI-AAI.2017.158,Multi-objective optimization;Many-objective optimization;Evolutionary algorithm;Machine learning;Random forest;Variable importance,,
Analytics of Heterogeneous Breast Cancer Data Using Neuroevolution,B. Abdikenov; Z. Iklassov; A. Sharipov; S. Hussain; P. K. Jamwal,IEEE Access,2019.0,"Breast cancer prognostic modeling is difficult since it is governed by many diverse factors. Given the low median survival and large scale breast cancer data, which comes from high throughput technology, the accurate and reliable prognosis of breast cancer is becoming increasingly difficult. While accurate and timely prognosis may save many patients from going through painful and expensive treatments, it may also help oncologists in managing the disease more efficiently and effectively. Data analytics augmented by machine-learning algorithms have been proposed in past for breast cancer prognosis; and however, most of these could not perform well owing to the heterogeneous nature of available data and model interpretability related issues. A robust prognostic modeling approach is proposed here whereby a Pareto optimal set of deep neural networks (DNNs) exhibiting equally good performance metrics is obtained. The set of DNNs is initialized and their hyperparameters are optimized using the evolutionary algorithm, NSGAIII. The final DNN model is selected from the Pareto optimal set of many DNNs using a fuzzy inferencing approach. Contrary to using DNNs as the black box, the proposed scheme allows understanding how various performance metrics (such as accuracy, sensitivity, F1, and so on) change with changes in hyper-parameters. This enhanced interpretability can be further used to improve or modify the behavior of DNNs. The heterogeneous breast cancer database requires preprocessing for better interpretation of categorical variables in order to improve prognosis from classifiers. Furthermore, we propose to use a neural network-based entity-embedding method for categorical features with high cardinality. This approach can provide a vector representation of categorical features in multidimensional space with enhanced interpretability. It is shown with evidence that DNNs optimized using evolutionary algorithms exhibit improved performance over other classifiers mentioned in this paper.",10.1109/ACCESS.2019.2897078,Breast cancer prognostic modelling;entity embedding;deep learning networks;evolutionary algorithms;fuzzy inferencing,11.0,
Mimicking the Human Approach in the Game of Hive,D. Kampert; A. -L. Varbanescu; M. Müller-Brockhausen; A. Plaat,2021 IEEE Symposium Series on Computational Intelligence (SSCI),2021.0,"While Deep Blue and AlphaGo make it seem like board games have been solved, there are still plenty of games for which no good game playing program exists. Hive is such a game. It is, combinatorically, of similar complexity as chess or Go, yet the rules of the game are such that current methods can barely beat a randomly playing agent. A major bottleneck for progress is the high branching factor of the game. We apply state of the art methods for which we develop new heuristics that are based on human domain-knowledge, attempting to improve upon the current dire state of Hive agents. Our methods have improved playing strength compared to the state of the art, although our AI still fails against actual Humans. We also find that, while in most board games, brute force or deep learning approaches work best, in Hive, an approach based on mimicking human knowledge outperforms these other approaches, including Monte Carlo Tree Search or Deep Reinforcement Learning. Future work will show if this anomalous situation is inherent to the game.",10.1109/SSCI50451.2021.9659999,heuristic;the game Hive;game-playing agents;artificial-intelligence (AI);minimax;Monte-Carlo Tree Search (MCTS),,
Network Intrusion Detection Using Multi-Criteria PROAFTN Classification,F. N. Al-Obeidat; E. M. El-Alfy,2014 International Conference on Information Science & Applications (ICISA),2014.0,"Network intrusion is recognized as a chronic and recurring problem. Hacking techniques continually change and several countermeasure methods have been suggested in the literature including statistical and machine learning approaches. However, no single solution can be claimed as a rule of thumb for the wide spectrum of attacks. In this paper, a novel methodology is proposed for network intrusion detection based on the multicriteria PROAFTN classification. The algorithm is evaluated and compared on a publicly available and widely used dataset. The results in this paper show that the proposed algorithm is promising in detecting various types of intrusions with high classification accuracy.",10.1109/ICISA.2014.6847436,,1.0,
Improving Dependability of Neuromorphic Computing With Non-Volatile Memory,S. Song; A. Das; N. Kandasamy,2020 16th European Dependable Computing Conference (EDCC),2020.0,"As process technology continues to scale aggressively, circuit aging in a neuromorphic hardware due to negative bias temperature instability (NBTI) and time-dependent dielectric breakdown (TDDB) is becoming a critical reliability issue and is expected to proliferate when using non-volatile memory (NVM) for synaptic storage. This is because NVM devices require high voltages and currents to access their synaptic weights, which further accelerate the circuit aging in neuromorphic hardware. Current methods for qualifying reliability are overly conservative, since they estimate circuit aging considering worst-case operating conditions and unnecessarily constrain performance. This paper proposes RENEU, a reliability-oriented approach to map machine learning applications to neuromorphic hardware, with the aim of improving system-wide reliability, without compromising key performance metrics such as execution time of these applications on the hardware. Fundamental to RENEU is a novel formulation of the aging of CMOS-based circuits in a neuromorphic hardware considering different failure mechanisms. Using this formulation, RENEU develops a system- wide reliability model which can be used inside a design-space exploration framework involving the mapping of neurons and synapses to the hardware. To this end, RENEU uses an instance of Particle Swarm Optimization (PSO) to generate mappings that are Pareto-optimal in terms of performance and reliability. We evaluate RENEU using different machine learning applications on a state-of-the-art neuromorphic hardware with NVM synapses. Our results demonstrate an average 38% reduction in circuit aging, leading to an average 18% improvement in the lifetime of the hardware compared to current practices. RENEU only introduces a marginal performance overhead of 5% compared to a performance-oriented state-of-the-art.",10.1109/EDCC51268.2020.00013,Neuromorphic Computing;Spiking Neural Network (SNN);Negative Bias Temperature Instability (NBTI);Time Dependent Dielectric Breakdown (TDDB);Hot Carrier Injection (HCI);Dependability;Machine Learning;Particle Swarm Optimization (PSO),9.0,
APENAS: An Asynchronous Parallel Evolution Based Multi-objective Neural Architecture Search,M. Hu; L. Liu; W. Wang; Y. Liu,"2020 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)",2020.0,"Machine learning is widely used in pattern classification, image processing and speech recognition. Neural architecture search (NAS) could reduce the dependence of human experts on machine learning effectively. Due to the high complexity of NAS, the tradeoff between time consumption and classification accuracy is vital. This paper presents APENAS, an asynchronous parallel evolution based multi-objective neural architecture search, using the classification accuracy and the number of parameters as objectives, encoding the network architectures as individuals. To make full use of computing resource, we propose a multi-generation undifferentiated fusion scheme to achieve asynchronous parallel evolution on multiple GPUs or CPUs, which speeds up the process of NAS. Accordingly, we propose an election pool and a buffer pool for two-layer filtration of individuals. The individuals are sorted in the election pool by non-dominated sorting and filtered in the buffer pool by the roulette algorithm to improve the elitism of the Pareto front. APENAS is evaluated on the CIFAR-10 and CIFAR-100 datasets [25]. The experimental results demonstrate that APENAS achieves 90.05% accuracy on CIFAR-10 with only 0.07 million parameters, which is comparable to state of the art. Especially, APENAS has high parallel scalability, achieving 92.5% parallel efficiency on 64 nodes.",10.1109/ISPA-BDCloud-SocialCom-SustainCom51426.2020.00045,automated machine learning;neural architecture search;multi-objective;asynchronous parallel evolution,1.0,
The Use of NSGA - II for Optimal Placement and Management of Renewable Energy Sources When Considering Network Uncertainty and Fault Current Limiters,A. A. Farahani; S. H. H. Sadeghi,2021 29th Iranian Conference on Electrical Engineering (ICEE),2021.0,"Due to the abundant benefits of renewable energy sources (RESs), their participation in distribution networks is booming. However, they could have adverse effects on the protection coordination schemes. This paper proposes a nondominated sorting genetic algorithm (NSGA-II) that is a multiobjective optimization procedure to obtain the best locations and sizes of renewable energy sources (RESs) with fault current limiters (FCLs), reducing the short-circuit level of buses. The support vector regression, a supervised time series prediction approach in machine learning, is introduced to consider the uncertainty of load demands, network bid changes, and the generated powers of some RESs based on probabilistic states. The efficiency of the proposed procedure is established on the IEEE 33-bus test network.",10.1109/ICEE52715.2021.9544336,Renewable energy sources;network uncertainty;support vector regression;multi-objective optimization;fault current limiters,,
Many-objective feature selection for motor imagery EEG signals using differential evolution and support vector machine,M. Pal; S. Bandyopadhyay,"2016 International Conference on Microelectronics, Computing and Communications (MicroCom)",2016.0,"Processing of the movement related task under planning by artificial means provides a means to those people whose natural modality of performing the task is bottlenecked by physical disability or neuro-motor disorders. Electroencephalography (EEG) based Brain-Computer Interfacing (BCI) systems can be defined to be a non-muscular pathway to operate rehabilitative devices using motor imagery signals captured from the motor activation areas in the brain. Supervised learning can help in prediction of motor imagery actions by processing raw EEG signals. However, dimension of the feature space plays a crucial role in this process. Large dimensional features not only increase the computational complexity but also the presence of redundant features causes reduction in classification accuracy. In this work, we intend to select the relevant features from the feature vector obtained by Power Spectrum Density estimation of the left/right motor imagery signals. BCI Competition 2008 - Graz dataset B has been used as the source of raw EEG data. To achieve this goal, we have used single-objective as well as many-objective version of Differential Evolution which optimizes the classifier's performance in terms of five metrics obtained from the Confusion Matrix. Support Vector Machine is used for fitness evaluation of the chosen feature subset as well as for classification of mental states. This work demonstrates the superiority of many-objective Differential Evolution in improving the accuracy due to reduction in feature dimension from an average of 60.56% to 82.60% while processing time of a test EEG sample reduces from 6.1 milliseconds to 5.6 milliseconds. The results obtained in this work are validated using Friedman Test.",10.1109/MicroCom.2016.7522574,Brain — Computer Interfacing;Electroencephalography;Differential Evolution;Many-Objective Optimization;Pareto-Optimality;Power Spectral Density;Support Vector Machine,10.0,
An Automatic Weak Learner Formulation for Lithium-Ion Battery State of Health Estimation,J. Meng; L. Cai; D. -I. Stroe; X. Huang; J. Peng; T. Liu; R. Teodorescu,IEEE Transactions on Industrial Electronics,2022.0,"Current pulses are convenient to be actively implemented by a battery management system. However, the short-term features (STF) from current pulses originate from various sensors with uneven qualities, which hinder one powerful and strong learner with STF for the battery state of health (SOH) estimation. This article, thus, proposes an optimized weak learner formulation procedure for lithium-ion battery SOH estimation, which further enables the automatic initialization and integration of the weak learners with STF into an efficient SOH estimation framework. A Pareto front-based selection strategy is designed to select the representative solutions from the nondominated solutions fed by a knee point driven evolutionary algorithm, which guarantees both the diversity and accuracy of the weak learners. Afterward, the weak learners, whose coefficients are obtained by self-adaptive differential evolution, are integrated by a weight-based structure. The proposed method utilizes the weak learners with STF to boost the overall performance of the SOH estimation. The validation of the proposed method is proved by LiFePO<inline-formula><tex-math notation=""LaTeX"">${_4}$</tex-math></inline-formula>/C batteries under accelerated cycling ageing test including one mission profile providing primary frequency regulation service to the grid and one constant current profile.",10.1109/TIE.2021.3065594,Automatic weak learner formulation;ensemble learning;lithium-ion (Li-ion) battery;state of health (SOH) estimation,8.0,
Apollo - A Hybrid Recommender for Museums and Cultural Tourism,G. Pavlidis,2018 International Conference on Intelligent Systems (IS),2018.0,"This paper introduces Apollo, a novel hybrid recommender for free-roaming or guided museum visits and cultural tourism. The recommender is based on a new conceptualisation of a visit and the adoption of a minimax (or `conservative') approach towards user satisfaction modelling. The approach is based on the integration of temporal, spatial and content dynamics captured during a visit and contribute to an estimate of the user satisfaction and a development of an optimal route as a sequence of points of interest. Apollo follows a minimax approach by targeting the minimisation of the highest possible user dissatisfaction or disengagement, rather than looking for a maximisation of the user satisfaction. A considerable effort has been dedicated to the creation of realistic simulation data for items (the exhibits), users (the visitors), and a small amount of ratings of the items by some of the users with specific characteristics selected to represent a realistic scenario. Extensive visit simulations have been conducted and results show a considerable decrease of the probable user dissatisfaction in relation to a baseline recommender.",10.1109/IS.2018.8710494,Recommender;recommender system;recommendation;cultural heritage;electronic guide;machine learning;artificial intelligence,3.0,
"Formal Analysis, Hardness, and Algorithms for Extracting Internal Structure of Test-Based Problems",W. Jaśkowski; K. Krawiec,Evolutionary Computation,2011.0,"Problems in which some elementary entities interact with each other are common in computational intelligence. This scenario, typical for coevolving artificial life agents, learning strategies for games, and machine learning from examples, can be formalized as a test-based problem and conveniently embedded in the common conceptual framework of coevolution. In test-based problems, candidate solutions are evaluated on a number of test cases (agents, opponents, examples). It has been recently shown that every test of such problem can be regarded as a separate objective, and the whole problem as multi-objective optimization. Research on reducing the number of such objectives while preserving the relations between candidate solutions and tests led to the notions of underlying objectives and internal problem structure, which can be formalized as a coordinate system that spatially arranges candidate solutions and tests. The coordinate system that spans the minimal number of axes determines the so-called dimension of a problem and, being an inherent property of every problem, is of particular interest. In this study, we investigate in-depth the formalism of a coordinate system and its properties, relate them to properties of partially ordered sets, and design an exact algorithm for finding a minimal coordinate system. We also prove that this problem is NP-hard and come up with a heuristic which is superior to the best algorithm proposed so far. Finally, we apply the algorithms to three abstract problems and demonstrate that the dimension of the problem is typically much lower than the number of tests, and for some problems converges to the intrinsic parameter of the problem–its a priori dimension.",10.1162/EVCO_a_00046,Coevolution;co-optimization;games;test-based problem;interactive domains;pareto coevolution;underlying objectives;internal problem structure;NP-hardness,,
Clustering by multi objective genetic algorithm,D. Dutta; P. Dutta; J. Sil,2012 1st International Conference on Recent Advances in Information Technology (RAIT),2012.0,"The aim of the paper is to study a real coded multi objective genetic algorithm based K-clustering, where K represents the number of clusters, may be known or unknown. If the value of K is known, it is called K-clustering algorithm. The searching power of Genetic Algorithm (GA) is exploited to get for proper clusters and centers of clusters in the feature space to optimize simultaneously intra-cluster distance (Homogeneity) (H) and inter-cluster distances (Separation) (S). Maximization of 1/H and S are the twin objectives of Multi Objective Genetic Algorithm (MOGA) achieved by measuring H and S using Euclidean distance metric, suitable for continuous features (attributes). We have selected 10 data sets from the UCI machine learning repository containing continuous features only to validate the proposed algorithms. All-important steps of algorithms are shown here. At the end, classification accuracies obtained by best chromosomes are shown.",10.1109/RAIT.2012.6194619,Clustering;homogeneity and separation;real coded multi objective genetic algorithm;Pareto optimal front,10.0,
Unreliable-to-Reliable Instance Translation for Semi-Supervised Pedestrian Detection,S. Lin; W. Wu; S. Wu; Y. Xu; H. -S. Wong,IEEE Transactions on Multimedia,2022.0,"Generating realistic pedestrian instances in a semi-supervised setting is promising but challenging due to the limited labeled data. We propose an unreliable-to-reliable instance translation model (Un2Reliab) conditioned on unreliable instances which poorly align with pedestrians. Un2Reliab mainly consists of an encoder-decoder-like generative network and a discriminative network, which are jointly trained in a minimax game. We adopt regularization to ensure that the synthesized instances are semantically similar to the corresponding ground truth. Furthermore, to preserve the identities of persons, we propose another regularization to ensure that the synthesized instances associated with the same person should be consistent in appearance. As a result, Un2Reliab learns to restore the missing parts of the original instances. As a side benefit, the synthesized instances are brought into better alignment. Inclusion of the synthesized data improves both the diversity and quality of training data, which eventually leads to better generalization performance. Extensive experiments indicate that Un2Reliab is able to synthesize high-fidelity pedestrian instances and improve the previous state-of-the-art results on multiple semi-supervised pedestrian detection benchmarks.",10.1109/TMM.2021.3058546,Generative adversarial network;image-to-image translation;pedestrian detection;semi-supervised learning,,
A Generative Adversarial Gated Recurrent Unit Model for Precipitation Nowcasting,L. Tian; X. Li; Y. Ye; P. Xie; Y. Li,IEEE Geoscience and Remote Sensing Letters,2020.0,"Precipitation nowcasting is an important task in operational weather forecasts. The key challenge of the task is the radar echo map extrapolation. The problem is mainly solved by an optical-flow method in existing systems. However, the method cannot model rapid and nonlinear movements. Recently, a convolutional gated recurrent unit (ConvGRU) method is developed, which aims to model such movements based on deep learning techniques. Despite the promising performance, ConvGRU tends to yield blurring extrapolation images and fails to multi-modal and skewed intensity distribution. To overcome the limitations, we propose in this letter a generative adversarial ConvGRU (GA-ConvGRU) model. The model is composed of two adversarial learning systems, which are a ConvGRU-based generator and a convolution neural network-based discriminator. The two systems are trained by playing a minimax game. With the adversarial learning scheme, GA-ConvGRU can yield more realistic and more accurate extrapolation. Experiments on real data sets have been conducted and the results demonstrate that the proposed GA-ConvGRU significantly outperforms state-of-the-art extrapolation methods ConvGRU and optical flow.",10.1109/LGRS.2019.2926776,Deep learning;image sequence prediction;nowcasting;radar echo extrapolation,18.0,
Worst-Case Prediction Performance Analysis of the Kalman Filter,S. Yasini; K. Pelckmans,IEEE Transactions on Automatic Control,2018.0,"In this paper, we study the prediction performance of the Kalman filter (KF) in a worst case minimax setting as studied in online machine learning, information, and game theory. The aim is to predict the sequence of observations almost as well as the best reference predictor (comparator) sequence in a comparison class. We prove worst-case bounds on the cumulative squared prediction errors using a priori knowledge about the complexity of reference predictor sequence. In fact, the performance of the KF is derived as a function of the performance of the best reference predictor and the total amount of drift that occurs in the schedule of the best comparator.",10.1109/TAC.2017.2757908,  $H_{\infty}$   estimation;Kalman filter (KF);online machine learning;tracking worst-case bounds,7.0,
REPAIR: Removing Representation Bias by Dataset Resampling,Y. Li; N. Vasconcelos,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2019.0,"Modern machine learning datasets can have biases for certain representations that are leveraged by algorithms to achieve high performance without learning to solve the underlying task. This problem is referred to as “representation bias”. The question of how to reduce the representation biases of a dataset is investigated and a new dataset REPresentAtion bIas Removal (REPAIR) procedure is proposed. This formulates bias minimization as an optimization problem, seeking a weight distribution that penalizes examples easy for a classifier built on a given feature representation. Bias reduction is then equated to maximizing the ratio between the classification loss on the reweighted dataset and the uncertainty of the ground-truth class labels. This is a minimax problem that REPAIR solves by alternatingly updating classifier parameters and dataset resampling weights, using stochastic gradient descent. An experimental set-up is also introduced to measure the bias of any dataset for a given representation, and the impact of this bias on the performance of recognition models. Experiments with synthetic and action recognition data show that dataset REPAIR can significantly reduce representation bias, and lead to improved generalization of models trained on REPAIRed datasets. The tools used for characterizing representation bias, and the proposed dataset REPAIR algorithm, are available at https://github.com/JerryYLi/Dataset-REPAIR/.",10.1109/CVPR.2019.00980,Datasets and Evaluation;Action Recognition ; Deep Learning ; Representation Learning; Video Analytics,29.0,
Variational Bound of Mutual Information for Fairness in Classification,Z. Alsulaimawi,2020 IEEE 22nd International Workshop on Multimedia Signal Processing (MMSP),2020.0,"Machine learning applications have emerged in many aspects of our lives, such as for credit lending, insurance rates, and employment applications. Consequently, it is required that such systems be nondiscriminatory and fair in sensitive features user, e.g., race, sexual orientation, and religion. To address this issue, this paper develops a minimax adversarial framework, called features protector (FP) framework, to achieve the information-theoretical trade-off between minimizing distortion of target data and ensuring that sensitive features have similar distributions. We evaluate the performance of the proposed framework on two real-world datasets. Preliminary empirical evaluation shows that our framework provides both accurate and fair decisions.",10.1109/MMSP48831.2020.9287139,Fairness;privacy-preserving;adversarial learning;variational mutual information;big data security;deep learning,,
Simultaneous Human Health Monitoring and Time-Frequency Sparse Representation Using EEG and ECG Signals,W. He; G. Wang; J. Hu; C. Li; B. Guo; F. Li,IEEE Access,2019.0,"In the field of human health monitoring, intelligent diagnostic methods have drawn much attention recently to tackle the health problems and challenges faced by patients. In this paper, an efficient and flexible diagnostic method is proposed, which enables the simultaneous use of a machine learning method and sparsity-based representation technique. Specifically, the proposed method is based on a convolutional neural network (CNN) and generalized minimax-concave (GMC) method. First, measured potential signals, for instance, electroencephalogram (EEG) and electrocardiogram (ECG) signals are directly inputted into the designed network based on CNN for health conditions classification. The designed network adopts small convolution kernels to enhance the performance of feature extraction. In the training process, small batch samples are applied to improve the generalization of the model. Meanwhile, the “Dropout” strategy is applied to overcome the overfitting problem in fully connected layers. Then, for a record of the interested EEG or ECG signal, the sparse representation of useful time-frequency features can be estimated via the GMC method. Case studies of seizure detection and arrhythmia signal analysis are adopted to verify the performance of the proposed method. The experimental results demonstrate that the proposed method can effectively identify different health conditions and maximally enhance the sparsity of time-frequency features.",10.1109/ACCESS.2019.2921568,Feature extraction;convolutional neural network;deep learning;sparse representation;health monitoring,7.0,
Pruning In Time (PIT): A Lightweight Network Architecture Optimizer for Temporal Convolutional Networks,M. Risso; A. Burrello; D. J. Pagliari; F. Conti; L. Lamberti; E. Macii; L. Benini; M. Poncino,2021 58th ACM/IEEE Design Automation Conference (DAC),2021.0,"Temporal Convolutional Networks (TCNs) are promising Deep Learning models for time-series processing tasks. One key feature of TCNs is time-dilated convolution, whose optimization requires extensive experimentation. We propose an automatic dilation optimizer, which tackles the problem as a weight pruning on the time-axis, and learns dilation factors together with weights, in a single training. Our method reduces the model size and inference latency on a real SoC hardware target by up to 7.4× and 3×, respectively with no accuracy drop compared to a network without dilation. It also yields a rich set of Pareto-optimal TCNs starting from a single model, outperforming hand-designed solutions in both size and accuracy.",10.1109/DAC18074.2021.9586187,Neural Architecture Search;Temporal Convolutional Networks;Edge Computing;Deep Learning,1.0,
AI Assisted Optimization of Unimorph Tapered Cantilever for Piezoelectric Energy Harvesting,O. Pertin; K. Guha; O. Jakšić; Z. Jakšić,2021 IEEE 32nd International Conference on Microelectronics (MIEL),2021.0,"This paper presents the results of the deploying machine learning models in the design and optimization of a unimorph tapered cantilever with proof mass, aimed for piezoelectric energy harvesting. Multiobjective optimization as described in the paper was performed in order to find the optimal dimensions of the structure, its length, its width at the anchor and the ratio between widths at the anchor and at the tip, with respect to the salient parameters for the energy harvesting applications, namely low frequency and high power generated by the structure. The method is applicable for the optimization of the design of more complex MEMS structures aimed for energy harvesting applications.",10.1109/MIEL52794.2021.9569184,,,
Transfer Learning for Design-Space Exploration with High-Level Synthesis,J. Kwon; L. P. Carloni,2020 ACM/IEEE 2nd Workshop on Machine Learning for CAD (MLCAD),2020.0,"High-level synthesis (HLS) raises the level of design abstraction, expedites the process of hardware design, and enriches the set of final designs by automatically translating a behavioral specification into a hardware implementation. To obtain different implementations, HLS users can apply a variety of knobs, such as loop unrolling or function inlining, to particular code regions of the specification. The applied knob configuration significantly affects the synthesized design's performance and cost, e.g., application latency and area utilization. Hence, HLS users face the design-space exploration (DSE) problem, i.e. determine which knob configurations result in Pareto-optimal implementations in this multi-objective space. Whereas it can be costly in time and resources to run HLS flows with an enormous number of knob configurations, machine learning approaches can be employed to predict the performance and cost. Still, they require a sufficient number of sample HLS runs. To enhance the training performance and reduce the sample complexity, we propose a transfer learning approach that reuses the knowledge obtained from previously explored design spaces in exploring a new target design space. We develop a novel neural network model for mixed-sharing multi-domain transfer learning. Experimental results demonstrate that the proposed model outperforms both single-domain and hard-sharing models in predicting the performance and cost at early stages of HLS-driven DSE.",10.1145/3380446.3430636,high-level synthesis;design space exploration;neural networks;machine learning;transfer learning;multi-task learning,4.0,
A Multi-objective Rule Optimizer with an Application to Risk Management,P. Pulkkinen; N. Tiwari; A. Kumar; C. Jones,2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA),2018.0,"Managing risk is important to any E-commerce merchant. Various machine learning (ML) models combined with a rule set as the decision layer is a common practice to manage the risks. Unlike the ML models that can be automatically refreshed periodically based on new risk patterns, rules are generally static and rely on manual updates. To tackle that, this paper presents a data-driven and automated rule optimization method that generates multiple Pareto-optimal rule sets representing different trade-offs between business objectives. This enables business owners to make informed decisions when choosing between optimized rule sets for changing business needs and risks. Furthermore, manual work in rule management is greatly reduced. For scalability this method leverages Apache Spark and runs either on a single host or in a distributed environment in the cloud. This allows us to perform the optimization in a distributed fashion using millions of transactions, hundreds of variables and hundreds of rules during the training. The proposed method is general but we used it for optimizing real-world E-commerce (Amazon) risk rule sets. It could also be used in other fields such as finance and medicine.",10.1109/ICMLA.2018.00018,"Big data, spark, cloud computing, risk management, multi-objective optimization, machine learning",2.0,
Discovering Knowledge Rules with Multi-Objective Evolutionary Computing,R. Giusti; G. E. A. P. A. Batista,2010 Ninth International Conference on Machine Learning and Applications,2010.0,"Most Machine Learning systems target into inducing classifiers with optimal coverage and precision measures. Although this constitutes a good approach for prediction, it might not provide good results when the user is more interested in description. In this case, the induced models should present other properties such as novelty, interestingness and so forth. In this paper we present a research work based in Multi-Objective Evolutionary Computing to construct individual knowledge rules targeting arbitrary user-defined criteria via objective quality measures such as precision, support, novelty etc. This paper also presents a comparison among multi-objective and ranking composition techniques. It is shown that multi-objective-based methods attain better results than ranking-based methods, both in terms of solution dominance and diversity of solutions in the Pareto front.",10.1109/ICMLA.2010.25,Multi-Objective Machine Learning;Knowledge Discovery in Databases;Evolutionary Computing,,
Capacity Planning as a Service for Enterprise Standard Software,H. Müller; S. Bosse; M. Pohl; K. Turowski,2017 IEEE 19th Conference on Business Informatics (CBI),2017.0,"Too often, capacity planning activities that are crucial to software performance are being pushed to late development phases where trivial measurement-based assessment techniques can be employed on enterprise applications that are nearing completion. This procedure is highly inefficient, time consuming, and may result in disproportionately high correction costs to meet existing service level agreements. However, enterprise applications nowadays excessively make use of standard software that is shipped by large software vendors to a wide range of customers. Therefore, an application similar to the one whose capacity is being planned may already be in production state and constantly produce log data as part of application performance monitoring facilities. In this paper, we demonstrate how potential capacity planning service providers can leverage the dissemination effects of standard software by applying machine learning techniques on measurement data from various running enterprise applications. Utilizing prediction models that were trained on a large scale of monitoring data enables cost-efficient measurement-based prediction techniques to be used in early design phases. Therefore, we integrate knowledge discovery activities into wellknown capacity planning steps, which we adapt to the special characteristics of enterprise applications. We evaluate the feasibility of the modeled process using measurement data from more than 1,800 productively running enterprise applications in order to predict the response time of a widely used standard business transaction. Based on the trained model, we demonstrate how to simulate and analyze future workload scenarios. Using a Pareto approach, we were able to identify cost-effective design alternatives for a planned enterprise application.",10.1109/CBI.2017.25,prediction model;machine learning;big data;service design;business transaction;response time,,
Predicting Cloud Performance for HPC Applications: A User-Oriented Approach,G. Mariani; A. Anghel; R. Jongerius; G. Dittmann,"2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)",2017.0,"Cloud computing enables end users to execute high-performance computing applications by renting the required computing power. This pay-for-use approach enables small enterprises and startups to run HPC-related businesses with a significant saving in capital investment and a short time to market. When deploying an application in the cloud, the users may a) fail to understand the interactions of the application with the software layers implementing the cloud system, b) be unaware of some hardware details of the cloud system, and c) fail to understand how sharing part of the cloud system with other users might degrade application performance. These misunderstandings may lead the users to select suboptimal cloud configurations in terms of cost or performance. To aid the users in selecting the optimal cloud configuration for their applications, we suggest that the cloud provider generate a prediction model for the provided system. We propose applying machine-learning techniques to generate this prediction model. First, the cloud provider profiles a set of training applications by means of a hardware-independent profiler and then executes these applications on a set of training cloud configurations to collect actual performance values. The prediction model is trained to learn the dependencies of actual performance data on the application profile and cloud configuration parameters. The advantage of using a hardware-independent profiler is that the cloud users and the cloud provider can analyze applications on different machines and interface with the same prediction model. We validate the proposed methodology for a cloud system implemented with OpenStack. We apply the prediction model to the NAS parallel benchmarks. The resulting relative error is below 15% and the Pareto optimal cloud configurations finally found when maximizing application speed and minimizing execution cost on the prediction model are also at most 15% away from the actual optimal solutions.",10.1109/CCGRID.2017.11,Cloud;machine learning;design of experiments;high performance computing;random forest;performance prediction,10.0,
DESCNet: Developing Efficient Scratchpad Memories for Capsule Network Hardware,A. Marchisio; V. Mrazek; M. A. Hanif; M. Shafique,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,2021.0,"Deep neural networks (DNNs) have been established as the state-of-the-art method for advanced machine learning applications. Recently proposed by the Google Brain's team, the capsule networks (CapsNets) have improved the generalization ability, as compared to DNNs, due to their multidimensional capsules and preserving the spatial relationship between different objects. However, they pose significantly high computational and memory requirements, making their energy-efficient inference a challenging task. This article provides, for the first time, an in-depth analysis to highlight the design and runtime challenges for the (on-chip scratchpad) memories deployed in hardware accelerators executing fast CapsNets inference. To enable an efficient design, we propose an application-specific memory architecture, called DESCNet, which minimizes the off-chip memory accesses, while efficiently feeding the data to the hardware accelerator executing CapsNets inference. We analyze the corresponding on-chip memory requirement and leverage it to propose a methodology for exploring different scratchpad memory (SPM) designs and their energy/area tradeoffs. Afterward, an application-specific power-gating technique for the on-chip SPM is employed to further reduce its energy consumption, depending upon the mapped dataflow of the CapsNet and the utilization across different operations of its processing. We integrated our DESCNet memory design, as well as another state-of-the-art memory design Marchisio et al. [2018] for comparison studies, with an opensource DNN accelerator executing Google's CapsNet model Sabour et al. [2017] for the MNIST dataset. We also enhanced the design to execute the recent deep CapsNet model Rajasegaran et al. [2019] for the CIFAR10 dataset. Note: we use the same benchmarks and test conditions for which these CapsNets have been proposed and evaluated by their respective teams. The complete hardware is synthesized for a 32-nm CMOS technology using the ASIC-design flow with Synopsys tools and CACTI-P, and detailed area, performance, and power/energy estimation is performed using different configurations. Our results for a selected Pareto-optimal solution demonstrate no performance loss and an energy reduction of 79% for the complete accelerator, including computational units and memories, when compared to the state-of-the-art design.",10.1109/TCAD.2020.3030610,Capsule networks (CapsNets);design space exploration (DSE);energy efficiency;machine learning (ML);memory design;memory management;performance;power gating;scratchpad memory (SPM);special-purpose hardware,2.0,
Low Power Design of Runtime Reconfigurable FPGAs through Contexts Approximations,S. Xu; B. Carrion Schafer,2019 IEEE 37th International Conference on Computer Design (ICCD),2019.0,"This paper presents a method to improve the performance and reduce the energy of applications mapped onto coarse-grain runtime reconfigurable arrays (CGRRAs) by substituting and merging different contexts by approximate predictive models. In CGRRAs applications are split into contexts. The CGRRA is then reconfigured every clock cycle by loading a new context onto the reconfigurable fabric. In this work, we propose to substitute contexts by approximate expressions using machine learning models of different complexities like linear regression (LR) and multi-layer perceptrons (MLPs) such that the CGRRA area and energy can be reduced albeit introducing different levels of errors at the output. Moreover, we propose a technique to merged these approximated contexts with other contexts leading to a set of Pareto-optimal configurations. Experimental results show that our proposed method works well and it can trade-off area, performance and energy with output error of several computationally intensive applications mapped onto a commercial CGRRA.",10.1109/ICCD46524.2019.00078,Approximate Computing;High level Synthesis;Machine Learning;Hardware Accelerator;Coarse grain Runtime Reconfigurable Architecture,,
Autonomous Virulence Adaptation Improves Coevolutionary Optimization,J. Cartlidge; D. Ait-Boudaoud,IEEE Transactions on Evolutionary Computation,2011.0,"A novel approach for the autonomous virulence adaptation (AVA) of competing populations in a coevolutionary optimization framework is presented. Previous work has demonstrated that setting an appropriate virulence, v, of populations accelerates coevolutionary optimization by avoiding detrimental periods of disengagement. However, since the likelihood of disengagement varies both between systems and over time, choosing the ideal value of v is problematic. The AVA technique presented here uses a machine learning approach to continuously tune v as system engagement varies. In a simple, abstract domain, AVA is shown to successfully adapt to the most productive values of v. Further experiments, in more complex domains of sorting networks and maze navigation, demonstrate AVA's efficiency over reduced virulence and the layered Pareto coevolutionary archive.",10.1109/TEVC.2010.2073471,Autonomous virulence adaptation;coevolution;disengagement;genetic algorithms;machine learning;maze navigation;optimization methods;reduced virulence;sorting networks,6.0,
Using Polynomial Regression and Artificial Neural Networks for Reusable Analog IC Sizing,N. Lourenço; E. Afacan; R. Martins; F. Passos; A. Canelas; R. Póvoa; N. Horta; G. Dundar,"2019 16th International Conference on Synthesis, Modeling, Analysis and Simulation Methods and Applications to Circuit Design (SMACD)",2019.0,"In this paper, the use of machine learning techniques to repurpose already available Pareto optimal fronts of analog integrated circuit blocks for new contexts (loads, supply voltage, etc.) is explored. Data from previously sized circuits is used to train models that predict both circuit performance under the new context and the corresponding device sizes. A two-model chain is proposed, where, in the first layer, a multivariate polynomial regression estimates the performance tradeoffs. The output of this performance model is then used as input of an artificial neural network that predicts the device sizing that corresponds to that performance. Moreover, the models are trained with optimized sizing solutions, leading almost instantly to predicted solutions that are near optimal for the new context. The proposed methodology was integrated into a new framework and tested against a real circuit topology, with promising results. The model was able to predict wider and, in some cases, better, performance tradeoff, when compared to independent optimization runs for the same context, despite requiring 400 times fewer circuit simulations.",10.1109/SMACD.2019.8795282,Analog IC Sizing;Retargeting;Data Mining;Artificial Neural Networks;Deep Learning,4.0,
A learning bridge from architectural synthesis to physical design for exploring power efficient high-performance adders,S. Roy; Y. Ma; J. Miao; B. Yu,2017 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED),2017.0,"In spite of maturity to the modern electronic design automation (EDA) tools, optimized designs at architectural stage may become sub-optimal after going through physical design flow. Adder design has been such a long studied fundamental problem in VLSI industry yet designers cannot achieve optimal solutions by running EDA tools on the set of available prefix adder architectures. In this paper, we enhance a state-of-the-art prefix adder synthesis algorithm to obtain a much wider solution space in architectural domain. On top of that, a machine learning based design space exploration methodology is applied to predict the Pareto frontier of the adders in physical domain, which is infeasible by exhaustively running EDA tools for innumerable architectural solutions. Experimental results demonstrate that our framework can achieve near-optimal delay vs. power/area Pareto frontier over a wide design space, bridging the gap between architeon the set of available prefix adder architectures. In this paper, we enhance a state-of-the-art prefix adder synthesis algorithm to obtain a much wider solution space in architectural domain. On top of that, a machine learning based design space exploration methodology is applied to predict the Pareto frontier of the adders in physical domain, which is infeasible by exhaustively running EDA tools for innumerable architectural solutions. Experimental results demonstrate that our framework can achieve near-optimal delay vs. power/area Pareto frontier over a wide design space, bridging the gap between architectural andctural and physical designs.",10.1109/ISLPED.2017.8009168,,4.0,
Heuristically-Accelerated Multiagent Reinforcement Learning,R. A. C. Bianchi; M. F. Martins; C. H. C. Ribeiro; A. H. R. Costa,IEEE Transactions on Cybernetics,2014.0,"This paper presents a novel class of algorithms, called Heuristically-Accelerated Multiagent Reinforcement Learning (HAMRL), which allows the use of heuristics to speed up well-known multiagent reinforcement learning (RL) algorithms such as the Minimax-Q. Such HAMRL algorithms are characterized by a heuristic function, which suggests the selection of particular actions over others. This function represents an initial action selection policy, which can be handcrafted, extracted from previous experience in distinct domains, or learnt from observation. To validate the proposal, a thorough theoretical analysis proving the convergence of four algorithms from the HAMRL class (HAMMQ, HAMQ(λ), HAMQS, and HAMS) is presented. In addition, a comprehensive systematical evaluation was conducted in two distinct adversarial domains. The results show that even the most straightforward heuristics can produce virtually optimal action selection policies in much fewer episodes, significantly improving the performance of the HAMRL over vanilla RL algorithms.",10.1109/TCYB.2013.2253094,Artificial intelligence;heuristic algorithms;machine learning;multiagent systems,53.0,
Model Change Detection With the MDL Principle,K. Yamanishi; S. Fukushima,IEEE Transactions on Information Theory,2018.0,"We are concerned with the issue of detecting model changes in probability distributions. We specifically consider the strategies based on the minimum description length (MDL) principle. We theoretically analyze their basic performance from the two aspects: data compression and hypothesis testing. From the view of data compression, we derive a new bound on the minimax regret for model changes. Here, the mini-max regret is defined as the minimum of the worst-case code-length relative to the least normalized maximum likelihood code-length over all model changes. From the view of hypothesis testing, we reduce the model change detection into a simple hypothesis testing problem. We thereby derive upper bounds on error probabilities for the MDL-based model change test. The error probabilities are valid for finite sample size and are related to the information-theoretic complexity as well as the discrepancy measure of the hypotheses to be tested.",10.1109/TIT.2018.2852747,MDL principle;change detection;hypothesis testing;machine learning,8.0,
Multiagent Multi-Armed Bandit Schemes for Gateway Selection in UAV Networks,S. Hashima; K. Hatano; E. M. Mohamed,2020 IEEE Globecom Workshops (GC Wkshps,2020.0,"Lately, unmanned aerial vehicles (UAVs) communications acquired great attention because of its weighty new applications, particularly in rescue services. In such a case, access and gateway UAVS are spread to cover and fully support communications over disaster areas where the ground network is malfunctioned or wholly damaged. Each access UAV collects essential information from its assigned area, then flies and transfers it to the nearby gateway UAVs that deliver this collected information to the closest operating ground network. Meanwhile, collisions may occur as two or more access UAVs might target the same gateway UAV. This paper leverages and modifies two multi-armed bandit (MAB) based algorithms, namely, Kullback Leibler upper confidence bound (KLUCB) and minimax optimal stochastic strategy (MOSS) to formulate the gateway UAV selection issue. The issue is modeled as a budget-constrained multiagent MAB (MA-MAB) that maximizes data rates while considering access UAVs' flight battery consumption. Hence, MA battery aware KLUCB (MABA-KLUCB) and battery aware MOSS (MA-BA-MOSS) algorithms are proposed for efficient gateway UAV selection. The proposed MAB algorithms maximize the UAV network's total sum rate over the conventional selection techniques with assuring good convergence performance.",10.1109/GCWkshps50303.2020.9367568,Multiagent MAB;Machine learning;UAV;Gateway UAV selection;KLUCB;MOSS,1.0,
Decision Support System with K-Means Clustering Algorithm for Detecting the Optimal Store Location Based on Social Network Events,M. A. Hamada; L. Naizabayeva,2020 IEEE European Technology and Engineering Management Summit (E-TEMS),2020.0,"Nowadays, the business market is more complicated and comprises many challenges; it became more competitive and surrounded by high-risk patterns. Seeking for new technologies and adopting innovation is becoming an important and crucial issue to eliminate the complexity of the decision-making process and failure probability. Decision support system (DSS) is a computerized system that encompasses mathematical and analytical models, knowledge base and a user interface to help managers for making better decisions. This research aims to develop a decision support system based on K-means clustering algorithm to detect the optimal store location through social network events. Also, this research explains how to extract data from one social network channel ""Instagram"" using the ""Octoparse API"" as a web data extraction tool. K-means algorithm identifies k- number of centroids, and allocates every data point to the nearest cluster. As a result, we analyzed 12754 posts started on the 1st of January 2019. Cleaned data are transformed using Minimax and K-means algorithms. As an output, we have got json format data file with centres which are placed on the map to provide a better understanding. The Result of this research is a visualized map pointed with places to define the optimal location of a specific store at the selected region. The practical value of this DSS tool is to help users to make a more valuable and accurate decision which lead to a decrease in the probability of ineffective business decision and minimize the business losses.",10.1109/E-TEMS46250.2020.9111758,classification model;decision support system;machine learning;optimal store location;social network events;K-means clustering algorithm,2.0,
On Stabilizing Generative Adversarial Training With Noise,S. Jenni; P. Favaro,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2019.0,"We present a novel method and analysis to train generative adversarial networks (GAN) in a stable manner. As shown in recent analysis, training is often undermined by the probability distribution of the data being zero on neighborhoods of the data space. We notice that the distributions of real and generated data should match even when they undergo the same filtering. Therefore, to address the limited support problem we propose to train GANs by using different filtered versions of the real and generated data distributions. In this way, filtering does not prevent the exact matching of the data distribution, while helping training by extending the support of both distributions. As filtering we consider adding samples from an arbitrary distribution to the data, which corresponds to a convolution of the data distribution with the arbitrary one. We also propose to learn the generation of these samples so as to challenge the discriminator in the adversarial training. We show that our approach results in a stable and well-behaved training of even the original minimax GAN formulation. Moreover, our technique can be incorporated in most modern GAN formulations and leads to a consistent improvement on several common datasets.",10.1109/CVPR.2019.01242,Image and Video Synthesis;Deep Learning,6.0,
Model-Driven GAN-Based Channel Modeling for IRS-Aided Wireless Communication,Y. Wei; M. -M. Zhao; M. -J. Zhao,2021 IEEE Global Communications Conference (GLOBECOM),2021.0,"Intelligent reflecting surface (IRS) is a promising new technology that is able to create a favorable wireless signal propagation environment by collaboratively reconfiguring the passive reflecting elements, yet with low hardware and energy cost. In IRS-aided wireless communication systems, channel modeling is a fundamental task for communication algorithm design and performance optimization, which however is also very challenging since in-depth domain knowledge and technical expertise in radio signal propagations are required, especially for modeling the high-dimensional cascaded base station (BS)-IRS and IRS-user channels (also referred to as the reflected channels). In this paper, we propose a model-driven generative adversarial network (GAN)-based channel modeling framework to autonomously learn the reflected channel distribution, without complex theoretical analysis or data processing. The designed GAN (also named as IRS-GAN) is trained to reach the Nash equilibrium of a minimax game between a generative model and a discriminative model, where the special structure of the reflected channels is incorporated to improve the modeling accuracy. Simulation results are presented to validate the effectiveness of the proposed IRS-GAN framework for IRS-related channel modeling.",10.1109/GLOBECOM46510.2021.9685602,Intelligent reflecting surface (IRS);generative adversarial network (GAN);deep learning;channel modeling;multiple-output single-input (MISO),,
Renewable Scenario Generation Based on Improved Generative Adversarial Networks,K. Wang; M. Ren; T. Qian; X. Li; L. Pei; X. Zhang,2021 IEEE 5th Conference on Energy Internet and Energy System Integration (EI2),2021.0,"With the increasing penetration rate of renewable energy sources in power systems, it is vital to characterize renewables' intrinsic variability and uncertainty. Scenario generation is a key approach which could provide a series of possible power scenarios in the future for the system planner and operator to make decisions. In this paper, a data-driven renewable scenario generation method is proposed, which utilizes improved generative adversarial networks. The minimax normalization method is used to improve the training stability of the model. And after training, the generalization performance of the proposed model is verified. Moreover, several metrics are used to assess the quality of the scenarios. The results show that the proposed model can accurately describe the uncertainty of wind and photovoltaic power.",10.1109/EI252483.2021.9713244,scenario generation;renewable energy;neural network;deep learning;generative adversarial networks,,
Partially Adversarial Learning and Adaptation,J. -T. Chien; Y. -Y. Lyu,2019 27th European Signal Processing Conference (EUSIPCO),2019.0,"An image classification system for a specific target domain is usually trained with initialization from a source domain given with a large number of classes, particularly in an application of image recognition. The classes in target domain are usually seen as a subset in source domain. Partial domain adaptation aims to tackle this generalization issue where no labeled data are provided in target domain. This paper presents an adversarial learning for partial domain adaptation where a symmetric metric based on the Wasserstein distance is adopted in an adversarial learning objective. We build a Wasserstein partial transfer network where the Wasserstein adversarial objective is jointly optimized to partially transfer the relevance knowledge from source to target domains. The geometric property for optimal transport is assured to mitigate the gradient vanishing problem in adversarial training. The neural network components for feature extraction, relevance transfer, domain matching and task classification are jointly trained by solving a minimax optimization over multiple objectives. Experiments on image classification show the merits of the proposed partially adversarial domain adaptation over different tasks.",10.23919/EUSIPCO.2019.8903147,image classification;domain adaptation;deep learning;adversarial learning;partial transfer,5.0,
Transferable Representation Learning with Deep Adaptation Networks,M. Long; Y. Cao; Z. Cao; J. Wang; M. I. Jordan,IEEE Transactions on Pattern Analysis and Machine Intelligence,2019.0,"Domain adaptation studies learning algorithms that generalize across source domains and target domains that exhibit different distributions. Recent studies reveal that deep neural networks can learn transferable features that generalize well to similar novel tasks. However, as deep features eventually transition from general to specific along the network, feature transferability drops significantly in higher task-specific layers with increasing domain discrepancy. To formally reduce the effects of this discrepancy and enhance feature transferability in task-specific layers, we develop a novel framework for deep adaptation networks that extends deep convolutional neural networks to domain adaptation problems. The framework embeds the deep features of all task-specific layers into reproducing kernel Hilbert spaces (RKHSs) and optimally matches different domain distributions. The deep features are made more transferable by exploiting low-density separation of target-unlabeled data in very deep architectures, while the domain discrepancy is further reduced via the use of multiple kernel learning that enhances the statistical power of kernel embedding matching. The overall framework is cast in a minimax game setting. Extensive empirical evidence shows that the proposed networks yield state-of-the-art results on standard visual domain-adaptation benchmarks.",10.1109/TPAMI.2018.2868685,Domain adaptation;deep learning;convolutional neural network;two-sample test;multiple kernel learning,120.0,
Unsupervised Visual Domain Adaptation: A Deep Max-Margin Gaussian Process Approach,M. Kim; P. Sahu; B. Gholami; V. Pavlovic,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2019.0,"For unsupervised domain adaptation, the target domain error can be provably reduced by having a shared input representation that makes the source and target domains indistinguishable from each other. Very recently it has been shown that it is not only critical to match the marginal input distributions, but also align the output class distributions. The latter can be achieved by minimizing the maximum discrepancy of predictors. In this paper, we take this principle further by proposing a more systematic and effective way to achieve hypothesis consistency using Gaussian processes (GP). The GP allows us to induce a hypothesis space of classifiers from the posterior distribution of the latent random functions, turning the learning into a large-margin posterior separation problem, significantly easier to solve than previous approaches based on adversarial minimax optimization. We formulate a learning objective that effectively influences the posterior to minimize the maximum discrepancy. This is shown to be equivalent to maximizing margins and minimizing uncertainty of the class predictions in the target domain. Empirical results demonstrate that our approach leads to state-to-the-art performance superior to existing methods on several challenging benchmarks for domain adaptation.",10.1109/CVPR.2019.00451,Deep Learning;Recognition: Detection;Categorization;Retrieval; Statistical Learning,16.0,
Leveraging the Invariant Side of Generative Zero-Shot Learning,J. Li; M. Jing; K. Lu; Z. Ding; L. Zhu; Z. Huang,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2019.0,"Conventional zero-shot learning (ZSL) methods generally learn an embedding, e.g., visual-semantic mapping, to handle the unseen visual samples via an indirect manner. In this paper, we take the advantage of generative adversarial networks (GANs) and propose a novel method, named leveraging invariant side GAN (LisGAN), which can directly generate the unseen features from random noises which are conditioned by the semantic descriptions. Specifically, we train a conditional Wasserstein GANs in which the generator synthesizes fake unseen features from noises and the discriminator distinguishes the fake from real via a minimax game. Considering that one semantic description can correspond to various synthesized visual samples, and the semantic description, figuratively, is the soul of the generated features, we introduce soul samples as the invariant side of generative zero-shot learning in this paper. A soul sample is the meta-representation of one class. It visualizes the most semantically-meaningful aspects of each sample in the same category. We regularize that each generated sample (the varying side of generative ZSL) should be close to at least one soul sample (the invariant side) which has the same class label with it. At the zero-shot recognition stage, we propose to use two classifiers, which are deployed in a cascade way, to achieve a coarse-to-fine result. Experiments on five popular benchmarks verify that our proposed approach can outperform state-of-the-art methods with significant improvements.",10.1109/CVPR.2019.00758,Recognition: Detection;Categorization;Retrieval;Deep Learning ; Image and Video Synthesis,62.0,
ACQUA: Adaptive and cooperative quality-aware control for automotive cyber-physical systems,K. Vatanpavar; M. A. Al Faruque,2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),2017.0,"Controllers in cyber-physical systems integrate a design-time behavioral model of the system under design to improve their own quality. In the state-of-the-art control designs, behavioral models of other interacting neighbor systems are also integrated to form a centralized behavioral model and to enable a system-level optimization and control. Although this ideal embedded control design may result in pareto-optimal solutions, it is not scalable to larger number of systems. Moreover, the behavior of the multi-domain physical systems may be too complex for a control designer to model and may dynamically change at run time. In this paper, we propose a novel Adaptive and Cooperative Quality-Aware (ACQUA) control design which addresses these challenges. In this control design, an ACQUA-based controller for the system under design will monitor the quality of the neighbor systems to dynamically learn their behavior. Therefore, it can quickly adapt its control to cooperate with other neighbor controllers for improving the quality of not only itself, but also other neighbor systems. We apply ACQUA to design a cooperative controller for automotive navigation system, motor control unit, and battery management system in an electric vehicle. We use this automotive example to analyze the performance of the design. We show that by using our ACQUA control, we can reach up to 86% improvements achievable by an ideal embedded control design such that energy consumption reduces by 18% and battery capacity loss decreases by 12% compared to the state-of-the-art on average.",10.1109/ICCAD.2017.8203778,CPS;Automotive;Electric Vehicle;Regression Modeling;Machine Learning;Model Predictive Control;Power Optimization,1.0,
Dual-Objective Mixed Integer Linear Program and Memetic Algorithm for an Industrial Group Scheduling Problem,Z. Zhao; S. Liu; M. Zhou; A. Abusorrah,IEEE/CAA Journal of Automatica Sinica,2021.0,"Group scheduling problems have attracted much attention owing to their many practical applications. This work proposes a new bi-objective serial-batch group scheduling problem considering the constraints of sequence-dependent setup time, release time, and due time. It is originated from an important industrial process, i.e., wire rod and bar rolling process in steel production systems. Two objective functions, i.e., the number of late jobs and total setup time, are minimized. A mixed integer linear program is established to describe the problem. To obtain its Pareto solutions, we present a memetic algorithm that integrates a population-based nondominated sorting genetic algorithm II and two single-solution-based improvement methods, i.e., an insertion-based local search and an iterated greedy algorithm. The computational results on extensive industrial data with the scale of a one-week schedule show that the proposed algorithm has great performance in solving the concerned problem and outperforms its peers. Its high accuracy and efficiency imply its great potential to be applied to solve industrial-size group scheduling problems.",10.1109/JAS.2020.1003539,Insertion-based local search;iterated greedy algorithm;machine learning;memetic algorithm;nondominated sorting genetic algorithm II (NSGA-II);production scheduling,14.0,
Automatic Decision Making for Parameters in Kernel Method,Y. Pei,2019 IEEE Symposium Series on Computational Intelligence (SSCI),2019.0,"We propose to use the relationship between the parameter of kernel function and its decisional angle or distance metrics for selecting the optimal setting of the parameter of kernel functions in kernel method-based algorithms. Kernel method is established in the reproducing kernel Hilbert space, the angle and distance are two metrics in such space. We analyse and investigate the relationship between the parameter of kernel function and the metrics (distance or angle) in the reproducing kernel Hilbert space. We design a target function of optimization to model the relationship between these two variables, and found that (1) the landscape shapes of parameter and the metrics are the same in Gaussian kernel function because the norm of all the vectors are equal to one in reproducing kernel Hilbert space; (2) the landscape monotonicity of that are opposite in polynomial kernel function from that of Gaussian kernel. The monotonicity of designed target functions of optimization using Gaussian kernel and polynomial kernel is different as well. The distance metric and angle metric have different distribution characteristics for the decision of parameter setting in kernel function. It needs to balance these two metrics when selecting a proper parameter of the kernel function in kernel-based algorithms. We use evolutionary multi-objective optimization algorithms to obtain the Pareto solutions for optimal selection of the parameter in kernel functions. We found that evolutionary multi-objective optimization algorithms are useful tools to balance the distance metric and angle metric in the decision of parameter setting in kernel method-based algorithms.",10.1109/SSCI44817.2019.9002691,kernel method;kernel function;reproducing kernel Hilbert space;machine learning;decision making;evolutionary multi-objective optimization,2.0,
Fast and Accurate PPA Modeling with Transfer Learning,W. R. Davis; P. Franzon; L. Francisco; B. Huggins; R. Jain,2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD),2021.0,"The power, performance and area (PPA) of digital blocks can vary 10:1 based on their synthesis, place, and route tool recipes. With rapid increase in number of PVT corners and complexity of logic functions approaching 10M gates, industry has an acute need to minimize the human resources, compute servers, and EDA licenses needed to achieve a Pareto optimal recipe. We first present models for fast accurate PPA prediction that can reduce the manual optimization iterations with EDA tools. Secondly we investigate techniques to automate the PPA optimization using evolutionary algorithms. For PPA prediction, a baseline model is trained on a known design using Latin hypercube sample runs of the EDA tool, and transfer learning is then used to train the model for an unseen design. For a known design the baseline needed 150 training runs to achieve a 95% accuracy. With transfer learning the same accuracy was achieved on a different (unseen) design in only 15 runs indicating the viability of transfer learning to generalize PPA models. The PPA optimization technique, based on evolutionary algorithms, effectively combines the PPA modeling and optimization. Our approach reached the same PPA solution as human designers in the same or fewer runs for a CORTEX-M0 system design. This shows potential for automating the recipe optimization without needing more runs than a human designer would need.",10.1109/ICCAD51958.2021.9643533,PPA;Machine Learning;Power;Performance;Area;Gradient Boost;Neural Network;Transfer Learning;Surrogate Modeling,,
PrefixRL: Optimization of Parallel Prefix Circuits using Deep Reinforcement Learning,R. Roy; J. Raiman; N. Kant; I. Elkin; R. Kirby; M. Siu; S. Oberman; S. Godil; B. Catanzaro,2021 58th ACM/IEEE Design Automation Conference (DAC),2021.0,"In this work, we present a reinforcement learning (RL) based approach to designing parallel prefix circuits such as adders or priority encoders that are fundamental to high-performance digital design. Unlike prior methods, our approach designs solutions tabula rasa purely through learning with synthesis in the loop. We design a grid-based state-action representation and an RL environment for constructing legal prefix circuits. Deep Convolutional RL agents trained on this environment produce prefix adder circuits that Pareto-dominate existing baselines with up to 16.0% and 30.2% lower area for the same delay in the 32b and 64b settings respectively. We observe that agents trained with open-source synthesis tools and cell library can design adder circuits that achieve lower area and delay than commercial tool adders in an industrial cell library.",10.1109/DAC18074.2021.9586094,machine learning;reinforcement learning;datapath optimization,1.0,
Multi-Objective Learning of Multi-Dimensional Bayesian Classifiers,J. D. Rodríguez; J. A. Lozano,2008 Eighth International Conference on Hybrid Intelligent Systems,2008.0,"Multi-dimensional classification is a generalization of supervised classification that considers more than one class variable to classify. In this paper we review the existing multi-dimensional Bayesian classifiers and introduce a new one: the KDB multi-dimensional classifier. Then we define different classification rules for multi-dimensional scope. Finally, we introduce a structural learning approach of a multi-dimensional Bayesian classifier based on the multi-objective evolutionary algorithm NSGA-II. The solution of the learning approach is a Pareto front representing different multi-dimensional classifiers and their accuracy values for the different classes, so a decision maker can easily choose the classifier which is more interesting for the particular problem and domain.",10.1109/HIS.2008.143,Machine Learning;multi-dimensional classification;Bayesian classifiers;multi-objective;NSGA-II,16.0,
Distilling Optimal Neural Networks: Rapid Search in Diverse Spaces,B. Moons; P. Noorzad; A. Skliar; G. Mariani; D. Mehta; C. Lott; T. Blankevoort,2021 IEEE/CVF International Conference on Computer Vision (ICCV),2021.0,"Current state-of-the-art Neural Architecture Search (NAS) methods neither efficiently scale to multiple hardware platforms, nor handle diverse architectural search-spaces. To remedy this, we present DONNA (Distilling Optimal Neural Network Architectures), a novel pipeline for rapid, scalable and diverse NAS, that scales to many user scenarios. DONNA consists of three phases. First, an accuracy predictor is built using blockwise knowledge distillation from a reference model. This predictor enables searching across diverse networks with varying macro-architectural parameters such as layer types and attention mechanisms, as well as across micro-architectural parameters such as block repeats and expansion rates. Second, a rapid evolutionary search finds a set of pareto-optimal architectures for any scenario using the accuracy predictor and on-device measurements. Third, optimal models are quickly fine-tuned to training-from-scratch accuracy. DONNA is up to 100× faster than MNasNet in finding state-of-the-art architectures on-device. Classifying ImageNet, DONNA architectures are 20% faster than EfficientNet-B0 and Mo-bileNetV2 on a Nvidia V100 GPU and 10% faster with 0.5% higher accuracy than MobileNetV2-1.4x on a Samsung S20 smartphone. In addition to NAS, DONNA is used for search-space extension and exploration, as well as hardware-aware model compression.",10.1109/ICCV48922.2021.01201,Machine learning architectures and formulations; Efficient training and inference methods; Optimization and learning methods; Recognition and classification; Segmentation;grouping and shape,,
Probabilistic Sequential Multi-Objective Optimization of Convolutional Neural Networks,Z. Yin; W. Gross; B. H. Meyer,"2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)",2020.0,"With the advent of deeper, larger and more complex convolutional neural networks (CNN), manual design has become a daunting task, especially when hardware performance must be optimized. Sequential model-based optimization (SMBO) is an efficient method for hyperparameter optimization on highly parameterized machine learning (ML) algorithms, able to find good configurations with a limited number of evaluations by predicting the performance of candidates before evaluation. A case study on MNIST shows that SMBO regression model prediction error significantly impedes search performance in multi-objective optimization. To address this issue, we propose probabilistic SMBO, which selects candidates based on probabilistic estimation of their Pareto efficiency. With a formulation that incorporates error in accuracy prediction and uncertainty in latency measurement, probabilistic Pareto efficiency quantifies a candidate's quality in two ways: its likelihood of being Pareto optimal, and the expected number of current Pareto optimal solutions that it will dominate. We evaluate our proposed method on four image classification problems. Compared to a deterministic approach, probabilistic SMBO consistently generates Pareto optimal solutions that perform better, and that are competitive with state-of-the-art efficient CNN models, offering tremendous speedup in inference latency while maintaining comparable accuracy.",10.23919/DATE48585.2020.9116535,,2.0,
Learning with Privileged Tasks,Y. Song; Z. Lou; S. You; E. Yang; F. Wang; C. Qian; C. Zhang; X. Wang,2021 IEEE/CVF International Conference on Computer Vision (ICCV),2021.0,"Multi-objective multi-task learning aims to boost the performance of all tasks by leveraging their correlation and conflict appropriately. Nevertheless, in real practice, users may have preference for certain tasks, and other tasks simply serve as privileged or auxiliary tasks to assist the training of target tasks. The privileged tasks thus possess less or even no priority in the final task assessment by users. Motivated by this, we propose a privileged multiple descent algorithm to arbitrate the learning of target tasks and privileged tasks. Concretely, we introduce a privileged parameter so that the optimization direction does not necessarily follow the gradient from the privileged tasks, but concentrates more on the target tasks. Besides, we also encourage a priority parameter for the target tasks to control the potential distraction of optimization direction from the privileged tasks. In this way, the optimization direction can be more aggressively determined by weighting the gradients among target and privileged tasks, and thus highlight more the performance of target tasks under the unified multi-task learning context. Extensive experiments on synthetic and real-world datasets indicate that our method can achieve versatile Pareto solutions under varying preference for the target tasks.",10.1109/ICCV48922.2021.01051,Machine learning architectures and formulations; Recognition and classification,,
Substituting Convolutions for Neural Network Compression,E. J. Crowley; G. Gray; J. Turner; A. Storkey,IEEE Access,2021.0,"Many practitioners would like to deploy deep, convolutional neural networks in memory-limited scenarios, e.g., on an embedded device. However, with an abundance of compression techniques available it is not obvious how to proceed; many bring with them additional hyperparameter tuning, and are specific to particular network types. In this paper, we propose a simple compression technique that is general, easy to apply, and requires minimal tuning. Given a large, trained network, we propose (i) substituting its expensive convolutions with cheap alternatives, leaving the overall architecture unchanged; (ii) treating this new network as a student and training it with the original as a teacher through distillation. We demonstrate this approach separately for (i) networks predominantly consisting of full 3 ×3 convolutions and (ii) 1 ×1 or pointwise convolutions which together make up the vast majority of contemporary networks. We are able to leverage a number of methods that have been developed as efficient alternatives to fully-connected layers for pointwise substitution, allowing us provide Pareto-optimal benefits in efficiency/accuracy.",10.1109/ACCESS.2021.3086321,Machine learning;deep neural networks;computer vision;DNN compression,,
Towards Organization Management Using Exploratory Screening and Big Data Tests: A Case Study of the Spanish Red Cross,M. Rodríguez-Ibáñez; S. Muñoz-Romero; C. Soguero-Ruiz; F. Gimeno-Blanes; J. L. Rojo-Álvarez,IEEE Access,2019.0,"With the emergence of information and communication technologies, a large amount of data has turned available for the organizations, which creates expectations on their value and content for management purposes. However, the exploratory analysis of available organizational data based on emerging Big Data technologies are still developing in terms of operative tools for solid and interpretable data description. In this work, we addressed the exploratory analysis of organization databases at early stages where little quantitative information is available about their efficiency. Categorical and metric single-variable tests are proposed and formalized in order to provide a mass criterion to identify regions in forms with clusters of significant variables. Bootstrap resampling techniques are used to provide nonparametric criteria in order to establish easy-to-use statistical tests, so that single-variable tests are represented each on a visual and quantitative statistical plot, whereas all the variables in a given form are jointly visualized in the so-called chromosome plots. More detailed profile plots offer deep comparison knowledge for categorical variables across the organization physical and functional structures, while histogram plots for numerical variables incorporate the statistical significance of the variables under study for preselected Pareto groups. Performance grouping is addressed by identifying two or three groups according to some representative empirical distribution of some convenient grouping feature. The method is applied to perform a Big-Data exploratory analysis on the follow-up forms of Spanish Red Cross, based on the number of interventions and on a by-record basis. Results showed that a simple one-variable blind-knowledge exploratory Big-Data analysis, as the one developed in this paper, offers unbiased comparative graphical and numerical information that characterize organizational dynamics in terms of applied resources, available capacities, and productivity. In particular, the graphical and numerical outputs of the present analysis proved to be a valid tool to isolate the underlying overloaded or under-performing resources in complex organizations. As a consequence, the proposed method allows a systematic and principled way for efficiency analysis in complex organizations, which combined with organizational internal knowledge could leverage and validate efficient decision-making.",10.1109/ACCESS.2019.2923533,Big Data;machine learning;organization management;organization efficiency;prediction model,6.0,
A Many-Objective Evolutionary Algorithm With Two Interacting Processes: Cascade Clustering and Reference Point Incremental Learning,H. Ge; M. Zhao; L. Sun; Z. Wang; G. Tan; Q. Zhang; C. L. P. Chen,IEEE Transactions on Evolutionary Computation,2019.0,"Researches have shown difficulties in obtaining proximity while maintaining diversity for many-objective optimization problems. Complexities of the true Pareto front pose challenges for the reference vector-based algorithms for their insufficient adaptability to the diverse characteristics with no priori. This paper proposes a many-objective optimization algorithm with two interacting processes: cascade clustering and reference point incremental learning (CLIA). In the population selection process based on cascade clustering (CC), using the reference vectors provided by the process based on incremental learning, the nondominated and the dominated individuals are clustered and sorted with different manners in a cascade style and are selected by round-robin for better proximity and diversity. In the reference vector adaptation process based on reference point incremental learning, using the feedbacks from the process based on CC, proper distribution of reference points is gradually obtained by incremental learning. Experimental studies on several benchmark problems show that CLIA is competitive compared with the state-of-the-art algorithms and has impressive efficiency and versatility using only the interactions between the two processes without incurring extra evaluations.",10.1109/TEVC.2018.2874465,Clustering;incremental machine learning;interacting processes;many-objective optimization;reference vector,18.0,
Algorithmic Performance-Accuracy Trade-off in 3D Vision Applications Using HyperMapper,L. Nardi; B. Bodin; S. Saeedi; E. Vespa; A. J. Davison; P. H. J. Kelly,2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),2017.0,"In this paper we investigate an emerging application, 3D scene understanding, likely to be significant in the mobile space in the near future. The goal of this exploration is to reduce execution time while meeting our quality of result objectives. In previous work, we showed for the first time that it is possible to map this application to power constrained embedded systems, highlighting that decision choices made at the algorithmic design-level have the most significant impact. As the algorithmic design space is too large to be exhaustively evaluated, we use a previously introduced multi-objective random forest active learning prediction framework dubbed HyperMapper, to find good algorithmic designs. We show that HyperMapper generalizes on a recent cutting edge 3D scene understanding algorithm and on a modern GPU-based computer architecture. HyperMapper is able to beat an expert human hand-tuning the algorithmic parameters of the class of computer vision applications taken under consideration in this paper automatically. In addition, we use crowd-sourcing using a 3D scene understanding Android app to show that the Pareto front obtained on an embedded system can be used to accelerate the same application on all the 83 smart-phones and tablets with speedups ranging from 2x to over 12x.",10.1109/IPDPSW.2017.107,design space exploration;machine learning;computer vision;SLAM;embedded systems;GPU;crowd- sourcing,9.0,
FasTrCaps: An Integrated Framework for Fast yet Accurate Training of Capsule Networks,A. Marchisio; B. Bussolino; A. Colucci; M. A. Hanif; M. Martina; G. Masera; M. Shafique,2020 International Joint Conference on Neural Networks (IJCNN),2020.0,"Recently, Capsule Networks (CapsNets) have shown improved performance compared to the traditional Convolutional Neural Networks (CNNs), by encoding and preserving spatial relationships between the detected features in a better way. This is achieved through the so-called Capsules (i.e., groups of neurons) that encode both the instantiation probability and the spatial information. However, one of the major hurdles in the wide adoption of CapsNets is their gigantic training time, which is primarily due to the relatively higher complexity of their new constituting elements that are different from CNNs.In this paper, we implement different optimizations in the training loop of the CapsNets, and investigate how these optimizations affect their training speed and the accuracy. Towards this, we propose a novel framework FasTrCaps that integrates multiple lightweight optimizations and a novel learning rate policy called WarmAdaBatch (that jointly performs warm restarts and adaptive batch size), and steers them in an appropriate way to provide high training-loop speedup at minimal accuracy loss. We also propose weight sharing for capsule layers. The goal is to reduce the hardware requirements of CapsNets by removing unused/redundant connections and capsules, while keeping high accuracy through tests of different learning rate policies and batch sizes. We demonstrate that one of the solutions generated by the FasTrCaps framework can achieve 58.6% reduction in the training time, while preserving the accuracy (even 0.12% accuracy improvement for the MNIST dataset), compared to the CapsNet by Google Brain [25]. Moreover, the Pareto-optimal solutions generated by FasTrCaps can be leveraged to realize trade-offs between training time and achieved accuracy. We have open-sourced our framework on GitHub<sup>1</sup>.",10.1109/IJCNN48605.2020.9207533,Machine Learning;Capsule Networks;Training;Accuracy;Efficiency;Performance;Weight Sharing;Decoder;Batch Sizing;Adaptivity,4.0,
Statistical Relational Learning for Game Theory,M. Lippi,IEEE Transactions on Computational Intelligence and AI in Games,2016.0,"In this paper, we motivate the use of models and algorithms from the area of Statistical Relational Learning (SRL) as a framework for the description and the analysis of games. SRL combines the powerful formalism of first-order logic with the capability of probabilistic graphical models in handling uncertainty in data and representing dependencies between random variables: for this reason, SRL models can be effectively used to represent several categories of games, including games with partial information, graphical games and stochastic games. Inference algorithms can be used to approach the opponent modeling problem, as well as to find Nash equilibria or Pareto optimal solutions. Structure learning algorithms can be applied, in order to automatically extract probabilistic logic clauses describing the strategies of an opponent with a high-level, human-interpretable formalism. Experiments conducted using Markov logic networks, one of the most used SRL frameworks, show the potential of the approach.",10.1109/TCIAIG.2015.2490279,Machine learning;probabilistic logic,2.0,
Practitioner-Centric Approach for Early Incident Detection Using Crowdsourced Data for Emergency Services,Y. Senarath; A. Mukhopadhyay; S. M. Vazirizade; H. Purohit; S. Nannapaneni; A. Dubey,2021 IEEE International Conference on Data Mining (ICDM),2021.0,"Emergency response is highly dependent on the time of incident reporting. Unfortunately, the traditional approach to receiving incident reports (e.g., calling 911 in the USA) has time delays. Crowdsourcing platforms such as Waze provide an opportunity for early identification of incidents. However, detecting incidents from crowdsourced data streams is difficult due to the challenges of noise and uncertainty associated with such data. Further, simply optimizing over detection accuracy can compromise spatial-temporal localization of the inference, thereby making such approaches infeasible for real-world deployment. This paper presents a novel problem formulation and solution approach for practitioner-centered incident detection using crowdsourced data by using emergency response management as a case-study. The proposed approach CROME (Crowdsourced Multi-objective Event Detection) quantifies the relationship between the performance metrics of incident classification (e.g., F1 score) and the requirements of model practitioners (e.g., 1 km. radius for incident detection). First, we show how crowdsourced reports, ground-truth historical data, and other relevant determinants such as traffic and weather can be used together in a Convolutional Neural Network (CNN) architecture for early detection of emergency incidents. Then, we use a Pareto optimization-based approach to optimize the output of the CNN in tandem with practitioner-centric parameters to balance detection accuracy and spatial-temporal localization. Finally, we demonstrate the applicability of this approach using crowdsourced data from Waze and traffic accident reports from Nashville, TN, USA. Our experiments demonstrate that the proposed approach outperforms existing approaches in incident detection while simultaneously optimizing the needs for real-world deployment and usability.",10.1109/ICDM51629.2021.00164,Crowdsourcing;Emergency Response;Deep Learning;Waze;Multi-Objective Optimization,,
HAO: Hardware-aware Neural Architecture Optimization for Efficient Inference,Z. Dong; Y. Gao; Q. Huang; J. Wawrzynek; H. K. H. So; K. Keutzer,2021 IEEE 29th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM),2021.0,"Automatic algorithm-hardware co-design for DNN has shown great success in improving the performance of DNNs on FPGAs. However, this process remains challenging due to the intractable search space of neural network architectures and hardware accelerator implementation. Differing from existing hardware-aware neural architecture search (NAS) algorithms that rely solely on the expensive learning-based approaches, our work incorporates integer programming into the search algorithm to prune the design space. Given a set of hardware resource constraints, our integer programming formulation directly outputs the optimal accelerator configuration for mapping a DNN subgraph that minimizes latency. We use an accuracy predictor for different DNN subgraphs with different quantization schemes and generate accuracy-latency pareto frontiers. With low computational cost, our algorithm can generate quantized networks that achieve state-of-the-art accuracy and hardware performance on Xilinx Zynq (ZU3EG) FPGA for image classification on ImageNet dataset. The solution searched by our algorithm achieves 72.5% top-1 accuracy on ImageNet at framerate 50, which is 60% faster than MnasNet [37] and 135% faster than FBNet [43] with comparable accuracy.",10.1109/FCCM51124.2021.00014,HW SW Codesign;Quantization;Neural Architecture Optimization;Image Classification;Neural Architecture Search;Efficient Deep Learning,3.0,
Wiretap Code Design by Neural Network Autoencoders,K. Besser; P. Lin; C. R. Janda; E. A. Jorswieck,IEEE Transactions on Information Forensics and Security,2020.0,"In industrial machine type communications, an increasing number of wireless devices communicate under reliability, latency, and confidentiality constraints, simultaneously. From information theory, it is known that wiretap codes can asymptotically achieve reliability (vanishing block error rate (BLER) at the legitimate receiver Bob) while also achieving secrecy (vanishing information leakage (IL) to an eavesdropper Eve). However, under finite block length, there exists a tradeoff between the BLER at Bob and the IL at Eve. In this work, we propose a flexible wiretap code design for degraded Gaussian wiretap channels under finite block length, which can change the operating point on the Pareto boundary of the tradeoff between BLER and IL given specific code parameters. To attain this goal, we formulate a multi-objective programming problem, which takes the BLER at Bob and the IL at Eve into account. During training, we approximate the BLER by the mean square error and the IL by schemes based on Jensen's inequality and the Taylor expansion and then solve the optimization problem by neural network autoencoders. Simulation results show that the proposed scheme can find codes outperforming polar wiretap codes (PWC) with respect to both BLER and IL simultaneously. We show that the codes found by the autoencoders could be implemented with real modulation schemes with only small losses in performance.",10.1109/TIFS.2019.2945619,Physical layer security;wiretap codes;deep learning;autoencoders,14.0,
Automated Test Input Generation for Convolutional Neural Networks by Implementing Multi-objective Evolutionary Algorithms,L. ZHANG; H. SATO,2020 Eighth International Symposium on Computing and Networking Workshops (CANDARW),2020.0,"Deep Neural Networks (DNNs) have been widely applied in safety- and security-critical aspects, where the robustness of the system is of great significance, especially for corner case inputs. Traditionally, a DNN is tested with manually labeled data, which is not only labor-consuming, but also unable to contain statistically rare case inputs.In our work, we design, implement and evaluate the test input generation framework guided by multi-objective functions. The multi-objective functions are formed from neuron coverage, behavioral divergence and perturbation degree. We leverage evolutionary algorithms (EAs) to resolve such optimization problem by generating approximation to Pareto-optimal solutions. By implementing our framework, we successfully generated more than 6,000 test inputs for a convolutional neural network. And the generated test inputs help to improve the system’s accuracy by up to 4.4%.",10.1109/CANDARW51189.2020.00040,Deep learning testing;Automated test input generation;Evolutionary algorithms,,
TEA-DNN: the Quest for Time-Energy-Accuracy Co-optimized Deep Neural Networks,L. Cai; A. Barneche; A. Herbout; C. S. Foo; J. Lin; V. R. Chandrasekhar; M. M. Sabry Aly,2019 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED),2019.0,"Embedded deep learning platforms have witnessed two simultaneous improvements. First, the accuracy of convolutional neural networks (CNNs) has been significantly improved through the use of automated neural-architecture search (NAS) algorithms to determine CNN structure. Second, there has been increasing interest in developing hardware accelerators for CNNs that provide improved inference performance and energy consumption compared to GPUs. Such embedded deep learning platforms differ in the amount of compute resources and memory-access bandwidth, which would affect performance and energy consumption of CNNs. It is therefore critical to consider the available hardware resources in the network architecture search. To this end, we introduce TEA-DNN, a NAS algorithm targeting multi-objective optimization of execution time, energy consumption, and classification accuracy of CNN workloads on embedded architectures. TEA-DNN leverages energy and execution time measurements on embedded hardware when exploring the Pareto-optimal curves across accuracy, execution time, and energy consumption and does not require additional effort to model the underlying hardware. We apply TEA-DNN for image classification on actual embedded platforms (NVIDIA Jetson TX2 and Intel Movidius Neural Compute Stick). We highlight the Pareto-optimal operating points that emphasize the necessity to explicitly consider hardware characteristics in the search process. To the best of our knowledge, this is the most comprehensive study of Pareto-optimal models across a range of hardware platforms using actual measurements on hardware to obtain objective values.",10.1109/ISLPED.2019.8824934,Neural architecture search;hardware constraints;multi-objective optimization,8.0,
Optimal v-SVM parameter estimation using multi objective evolutionary algorithms,J. Ethridge; G. Ditzler; R. Polikar,IEEE Congress on Evolutionary Computation,2010.0,"Using a machine learning algorithm for a given application often requires tuning design parameters of the classifier to obtain optimal classification performance without overfitting. In this contribution, we present an evolutionary algorithm based approach for multi-objective optimization of the sensitivity and specificity of a v-SVM. The v-SVM is often preferred over the standard C-SVM due to smaller dynamic range of the v parameter compared to the unlimited dynamic range of the C parameter. Instead of looking for a single optimization result, we look for a set of optimal solutions that lie along the Pareto optimality front. The traditional advantage of using the Pareto optimality is of course the flexibility to choose any of the solutions that lies on the Pareto optimality front. However, we show that simply maximizing sensitivity and specificity over the Pareto front leads to parameters that appear to be mathematically optimal yet still cause overfitting. We propose a multiple objective optimization approach with three objective functions to find additional parameter values that do not cause overfitting.",10.1109/CEC.2010.5586029,multi-objective optimization;v-SVM;evolutionary algorithms,3.0,
Enhancing Cloud Energy Models for Optimizing Datacenters Efficiency,E. Outin; J. Dartois; O. Barais; J. Pazat,2015 International Conference on Cloud and Autonomic Computing,2015.0,"Due to high electricity consumption in the Cloud datacenters, providers aim at maximizing energy efficiency through VM consolidation, accurate resource allocation or adjusting VM usage. More generally, the provider attempts to optimize resource utilization. However, while minimizing expenses, the Cloud operator still needs to conform to SLA constraints negotiated with customers (such as latency, downtime, affinity, placement, response time or duplication). Consequently, optimizing a Cloud configuration is a multi-objective problem. As a nontrivial multi-objective optimization problem, there does not exist a single solution that simultaneously optimizes each objective. There exists a (possibly infinite) number of Pareto optimal solutions. Evolutionary algorithms are popular approaches for generating Pareto optimal solutions to a multi-objective optimization problem. Most of these solutions use a fitness function to assess the quality of the candidates. However, regarding the energy consumption estimation, the fitness function can be approximative and lead to some imprecisions compared to the real observed data. This paper presents a system that uses a genetic algorithm to optimize Cloud energy consumption and machine learning techniques to improve the fitness function regarding a real distributed cluster of server. We have carried out experiments on the OpenStack platform to validate our solution. This experimentation shows that the machine learning produces an accurate energy model, predicting precise values for the simulation.",10.1109/ICCAC.2015.10,Cloud computing;energy efficiency;model,5.0,
BEOL stack-aware routability prediction from placement using data mining techniques,W. -T. J. Chan; Y. Du; A. B. Kahng; S. Nath; K. Samadi,2016 IEEE 34th International Conference on Computer Design (ICCD),2016.0,"In advanced technology nodes, physical design engineers must estimate whether a standard-cell placement is routable (before invoking the router) in order to maintain acceptable design turnaround time. Modern SoC designs consume multiple compute servers, memory, tool licenses and other resources for several days to complete routing. When the design is unroutable, resources are wasted, which increases the design cost. In this work, we develop machine learning-based models that predict whether a placement solution is routable without conducting trial or early global routing. We also use our models to accurately predict iso-performance Pareto frontiers of utilization, aspect ratio and number of layers in the back-end-of-line (BEOL) stack. Furthermore, using data mining and machine learning techniques, we develop new methodologies to generate training examples given very few placements. We conduct validation experiments in three foundry technologies (28nm FDSOI, 28nm LP and 45nm GS), and demonstrate accuracy ≥ 85.9% in predicting routability of a placement. Our predictions of Pareto frontiers in the three technologies are pessimistic by at most 2% with respect to the maximum achievable utilization for a given design in a given BEOL stack.",10.1109/ICCD.2016.7753259,,20.0,
cSmartML: A Meta Learning-Based Framework for Automated Selection and Hyperparameter Tuning for Clustering,R. ElShawi; H. Lekunze; S. Sakr,2021 IEEE International Conference on Big Data (Big Data),2021.0,"Novel technologies in automated machine learning ease the complexity of algorithm selection and hyper-parameter optimization. However, these are usually restricted to supervised learning tasks such as classification and regression, while unsupervised learning remains a largely unexplored problem. In this paper, we offer a solution for automating machine learning specifically for the case of unsupervised learning with clustering, in a domain-agnostic manner. This is achieved through a combination of state-of-the-art processes based on meta-learning for algorithm and evaluation criteria selection, and evolutionary algorithm for hyper-parameter tuning. We introduce a robust and scalable interactive tool, named cSmartML, built on scikit-learn with 8 clustering algorithms. In order to capture more than a single measure of goodness of the output clustering solution, cSmartML optimizes multiple objective functions. A pareto-approach evaluates each objective simultaneously for each clustering solution. On each of the 27 real and synthetic benchmark datasets, we show that the performance of cSmartML is often much better than using standard selection and hyper-parameter optimization methods. In addition, experimentation reveals that cSmartML takes advantage of the defined objective functions on multi-objective functions framework.",10.1109/BigData52589.2021.9671542,clustering;meta-learning;hyper-parameter optimization,,
Clockwork: Scheduling Cloud Requests in Mobile Applications,Y. Chen; Z. Yu; B. Li,"2017 14th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)",2017.0,"It is essential for mobile application developers to manage backend resources to serve dynamic user requests from the frontend. For a typical mobile application, the rate at which the user requests arrive at the backend fluctuates dramatically. However, it is difficult or expensive to frequently adjust the capacity of the backend to meet the request demand. In this paper, we present Clockwork, a third-party cloud service, which smooths the demand profile by redistributing delay-tolerant requests and prioritizing delay-sensitive requests, so that sufficient capacity can be provided with reduced cost and wastage. To begin with, Clockwork plans the optimal backend capacity on a relatively long timescale based on future demand estimated by machine learning algorithms. We discuss pros and cons of various simple machine learning algorithms and advanced deep learning algorithms, in terms of their prediction accuracy and training time. Then, Clockwork schedules user requests on a shorter timescale through a fair and Pareto- optimal rate allocation. We implemented a fully-functional prototype of Clockwork on cloud servers and user mobile devices. The experimental results show that Clockwork can effectively help developers cut cost, as well as improve the backend utilization.",10.1109/SAHCN.2017.7964910,,,
Robust Model Predictive Control of Nonlinear Systems With Unmodeled Dynamics and Bounded Uncertainties Based on Neural Networks,Z. Yan; J. Wang,IEEE Transactions on Neural Networks and Learning Systems,2014.0,"This paper presents a neural network approach to robust model predictive control (MPC) for constrained discrete-time nonlinear systems with unmodeled dynamics affected by bounded uncertainties. The exact nonlinear model of underlying process is not precisely known, but a partially known nominal model is available. This partially known nonlinear model is first decomposed to an affine term plus an unknown high-order term via Jacobian linearization. The linearization residue combined with unmodeled dynamics is then modeled using an extreme learning machine via supervised learning. The minimax methodology is exploited to deal with bounded uncertainties. The minimax optimization problem is reformulated as a convex minimization problem and is iteratively solved by a two-layer recurrent neural network. The proposed neurodynamic approach to nonlinear MPC improves the computational efficiency and sheds a light for real-time implementability of MPC technology. Simulation results are provided to substantiate the effectiveness and characteristics of the proposed approach.",10.1109/TNNLS.2013.2275948,Extreme learning machine (ELM);real-time optimization;recurrent neural networks (RNNs);robust model predictive control (MPC);unmodeled dynamics,103.0,
Pattern Classification by Evolutionary RBF Networks Ensemble Based on Multi-objective Optimization,N. Kondo; T. Hatanaka; K. Uosaki,The 2006 IEEE International Joint Conference on Neural Network Proceedings,2006.0,"In this paper, evolutionary multi-objective selection method of RBF networks structure and its application to the ensemble learning is considered. The candidates of RBF network structure are encoded into the chromosomes in GAs. Then, they evolve toward Pareto-optimal front defined by several objective functions concerning with model accuracy, model complexity and model smoothness. RBF network ensemble is constructed of the obtained Pareto-optimal models since such models are diverse. This method is applied to the pattern classification problem. Experiments on the benchmark problem demonstrate that the proposed method has comparable generalization ability to conventional ensemble methods.",10.1109/IJCNN.2006.247224,,3.0,
Deconstructing Generative Adversarial Networks,B. Zhu; J. Jiao; D. Tse,IEEE Transactions on Information Theory,2020.0,"Generative Adversarial Networks (GANs) are a thriving unsupervised machine learning technique that has led to significant advances in various fields such as computer vision, natural language processing, among others. However, GANs are known to be difficult to train and usually suffer from mode collapse and the discriminator winning problem. To interpret the empirical observations of GANs and design better ones, we deconstruct the study of GANs into three components and make the following contributions. Formulation: we propose a perturbation view of the population target of GANs. Building on this interpretation, we show that GANs can be connected to the robust statistics framework, and propose a novel GAN architecture, termed as Cascade GANs, to provably recover meaningful low-dimensional generator approximations when the real distribution is high-dimensional and corrupted by outliers. Generalization: given a population target of GANs, we design a systematic principle, projection under admissible distance, to design GANs to meet the population requirement using only finite samples. We implement our principle in three cases to achieve polynomial and sometimes near-optimal sample complexities: (1) learning an arbitrary generator under an arbitrary pseudonorm; (2) learning a Gaussian location family under total variation distance, where we utilize our principle to provide a new proof for the near-optimality of the Tukey median viewed as GANs; (3) learning a low-dimensional Gaussian approximation of a high-dimensional arbitrary distribution under Wasserstein distance. We demonstrate a fundamental trade-off in the approximation error and statistical error in GANs, and demonstrate how to apply our principle in practice with only empirical samples to predict how many samples would be sufficient for GANs in order not to suffer from the discriminator winning problem. Optimization: we demonstrate alternating gradient descent is provably not locally asymptotically stable in optimizing the GAN formulation of PCA. We found that the minimax duality gap being non-zero might be one of the causes, and propose a new GAN architecture whose duality gap is zero, where the value of the game is equal to the previous minimax value (not the maximin value). We prove the new GAN architecture is globally asymptotically stable in solving PCA under alternating gradient descent.",10.1109/TIT.2020.2983698,Generative Adversarial Networks (GANs);wasserstein distance;optimal transport;generalization error;information-theoretic limit;robust statistics,3.0,
Large margin HMMs for speech recognition,Xinwei Li; Hui Jiang; Chaojun Liu,"Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.",2005.0,"Motivated by large margin classifiers in machine learning, we propose a novel method to estimate a continuous density hidden Markov model (CDHMM) in speech recognition according to the principle of maximizing the minimum multi-class separation margin. The approach is named large margin HMM. First, we show that this type of large margin HMM estimation problem can be formulated as a standard constrained minimax optimization problem. Second, we propose an iterative localized optimization approach to perform the minimax optimization for one model at a time to guarantee that the optimal value of the objective function always exists in the course of model parameter optimization. Then, we show that during each step the optimization can be solved by the GPD (generalized probabilistic descent) algorithm if we approximate the objective function by a differentiable function, such as summation of exponential functions. The large margin HMM-based classifiers are evaluated in a speaker-independent E-set speech recognition task using the OGI ISOLET database. Experimental results show that the large margin HMMs can achieve significant word error rate (WER) reduction over conventional HMM training methods, such as maximum likelihood estimation (MLE) and minimum classification error (MCE) training.",10.1109/ICASSP.2005.1416353,,10.0,
Sharp Characterization of Optimal Minibatch Size for Stochastic Finite Sum Convex Optimization,A. Nitanda; T. Murata; T. Suzuki,2019 IEEE International Conference on Data Mining (ICDM),2019.0,"The minibatching technique has been extensively adopted to facilitate stochastic first-order methods because of their computational efficiency in parallel computing for large-scale machine learning and data mining. However, the optimal minibatch size determination for accelerated stochastic gradient methods is not completely understood. Actually, there appears trade-off between the iteration complexity and the total computational complexity; that is, the number of iterations (minibatch queries) can be decreased by increasing the minibatch size, but too large minibatch size would result in an unnecessarily large total computational cost. In this study, we give a sharp characterization of the minimax optimal minibatch size to achieve the optimal iteration complexity by providing a reachable lower bound for minimizing finite sum of convex functions and, surprisingly, show that the optimal method with the minimax optimal minibatch size can achieve both of the optimal iteration complexity and the optimal total computational complexity simultaneously. Finally, this feature is verified experimentally.",10.1109/ICDM.2019.00059,"finite-sum problem, optimal minibatch method",,
Surrogate-based Multi-Objective Particle Swarm Optimization,L. V. Santana-Quintero; C. A. Coello Coello; A. G. Hernandez-Diaz; J. M. O. Velazquez,2008 IEEE Swarm Intelligence Symposium,2008.0,"This paper presents a new algorithm that approximates real function evaluations using supervised learning with a surrogate method called support vector machine (SVM). We perform a comparative study among different leader selection schemes in a Multi-Objective Particle Swarm Optimizer (MOPSO), in order to determine the most appropriate approach to be adopted for solving the sort of problems of our interest. The resulting hybrid presents a poor spread of solutions, which motivates the introduction of a second phase to our algorithm, in which an approach called rough sets is adopted in order to improve the spread of solutions along the Pareto front. Rough sets are used as a local search engine, which is able to generate solutions in the neighborhood of the nondominated solutions previously generated by the surrogate-based algorithm. The resulting approach is able to generate reasonably good approximations of the Pareto front of problems of up to 30 decision variables with only 2,000 fitness function evaluations. Our results are compared with respect to the NSGA-II, which is a multi-objective evolutionary algorithm representative of the state-of-the-art in the area.",10.1109/SIS.2008.4668300,Multi-objective optimization;surrogates;support vector machines;PSO;rough sets;hybrid algorithms,5.0,
Applying one-class learning algorithms to predict phage-bacteria interactions,J. F. López; J. A. L. Sotelo; D. Leite; C. Peña-Reyes,2019 IEEE Latin American Conference on Computational Intelligence (LA-CCI),2019.0,"The need to predict phage-bacteria interactions is a nowadays concern to overcome bacterial resistance issue; public genome databases contain highly imbalanced datasets which have hindered this task. Throughout this paper we will investigate, implement and evaluate One-Class Learning algorithms in order to predict phage-bacteria interactions using only positive samples. We will use the programming language Python aided by Scikit-Learn, Tensorflow and keras to develop the machine learning models and test them with real phage-bacteria interactions datasets. We trained the models using cross validation technique generating a gridsearch with all the datasets to find several combinations of hyperparameters available. Furthermore, we optimized those hyperparameters by using Pareto fronts based on seven different performance metrics, improving the efficiency of each algorithm for a given dataset. To refine each algorithm's performance separately we used the ensemble learning technique with an odd number of algorithms by simple voting. Finally, we managed to achieve an overall performance of 80% in predicting phage-bacteria interactions trained only with positive classes, this percentage in practice means that when a patient has an infection resistant to antibiotics, we have 80% of saving the life rather than maybe a 0% while finding the correct phage for the pathogenic host.",10.1109/LA-CCI47412.2019.9037032,phage-bacteria interaction;One-Class Learning;optimization;imbalanced datasets,1.0,
Supervised tensor learning,Dacheng Tao; Xuelong Li; Weiming Hu; S. Maybank; Xindong Wu,Fifth IEEE International Conference on Data Mining (ICDM'05),2005.0,"This paper aims to take general tensors as inputs for supervised learning. A supervised tensor learning (STL) framework is established for convex optimization based learning techniques such as support vector machines (SVM) and minimax probability machines (MPM). Within the STL framework, many conventional learning machines can be generalized to take n/sup th/-order tensors as inputs. We also study the applications of tensors to learning machine design and feature extraction by linear discriminant analysis (LDA). Our method for tensor based feature extraction is named the tenor rank-one discriminant analysis (TR1DA). These generalized algorithms have several advantages: 1) reduce the curse of dimension problem in machine learning and data mining; 2) avoid the failure to converge; and 3) achieve better separation between the different categories of samples. As an example, we generalize MPM to its STL version, which is named the tensor MPM (TMPM). TMPM learns a series of tensor projections iteratively. It is then evaluated against the original MPM. Our experiments on a binary classification problem show that TMPM significantly outperforms the original MPM.",10.1109/ICDM.2005.139,,4.0,
Adaptive critic design in learning to play game of Go,R. Zaman; D. Prokhorov; D. C. Wunsch,Proceedings of International Conference on Neural Networks (ICNN'97),1997.0,"This paper examines the performance of an HDP-type adaptive critic design (ACD) of the game Go. The game Go is an ideal problem domain for exploring machine learning; it has simple rules but requires complex strategies to play well. All current commercial Go programs are knowledge based implementations; they utilize input feature and pattern matching along with minimax type search techniques. But the extremely high branching factor puts a limit on their capabilities, and they are very weak compared to the relative strengths of other game programs like chess. In this paper, the Go-playing ACD consists of a critic network and an action network. The HDP type critic network learns to predict the cumulative utility function of the current board position from training games, and, the action network chooses a next move which maximizes critics next step cost-to-go. After about 6000 different training games against a public domain program, WALLY, the network (playing WHITE) began to win in some of the games and showed slow but steady improvements on test games.",10.1109/ICNN.1997.611623,,12.0,
Multi-objective cost-sensitive attribute reduction,B. Xu; H. Chen; W. Zhu; X. Zhu,2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS),2013.0,"Cost-sensitive learning is both hot and difficult in data mining and machine learning applications. Some research considers only one type of cost. Others convert two or more types of cost into the same unit, and then deal with a single-objective optimization problem. However, in many cases different types of cost cannot be converted. In this paper, we define and tackle multi-objective attribute reduct problem with multiple types of test cost. First, we compute all reducts of a decision system. Then, we separately calculate the money cost and time cost of these reducts and compare them according to the two kinds of test cost. Finally, the worse ones are removed. The remaining reducts form a Pareto optimal solution set. We tested our algorithm with three representative cost distributions on four UCI datasets. Experimental results indicate that a Pareto optimal solution set is usually very small compared with the size of all reducts. Hence our approach is effective in filtering out worse solutions and helping users in scheme selection.",10.1109/IFSA-NAFIPS.2013.6608602,Cost-sensitive learning;rough sets;attribute reduction;money cost;time cost,2.0,
A Design Flow for Mapping Spiking Neural Networks to Many-Core Neuromorphic Hardware,S. Song; M. L. Varshika; A. Das; N. Kandasamy,2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD),2021.0,"The design of many-core neuromorphic hardware is becoming increasingly complex as these systems are now expected to execute large machine-learning models. A predictable design flow is needed to guarantee real-time performance such as latency and throughput without significantly increasing the buffer requirement of computing cores. Synchronous Data Flow Graphs (SDFGs) have been previously used for predictable mapping of streaming applications to multiprocessor systems. We propose an SDFG-based design flow to map spiking neural networks (SNNs) to many-core neuromorphic hardware with the objective of exploring the tradeoff between throughput and buffer-size requirements. The proposed design flow integrates an iterative partitioning approach based on Kernighan-Lin graph partitioning heuristic to create SNN clusters such that each cluster can be mapped to a core of the hardware. The partitioning approach minimizes inter-cluster spike communication, which improves latency on the shared interconnect of the hardware. Next, the design flow maps clusters to cores using Particle Swarm Optimization (PSO), an evolutionary algorithm, while exploring the design space of throughput and buffer size. Pareto-optimal mappings are retained from the design flow, allowing system designers to select a Pareto mapping that satisfies throughput and buffer-size requirements of the design. We evaluated the developed design flow using five large-scale convolutional neural network (CNN) models. Results demonstrate 63% higher maximum throughput and 10% lower buffer-size requirement compared to state-of-the-art dataflow-based mapping solutions.",10.1109/ICCAD51958.2021.9643500,neuromorphic computing;spiking neural network (SNN);design-space exploration (DSE);oxide-based resistive random access memory (OxRRAM);dataflow,2.0,
Self-Configuring Ensemble of Neural Network Classifiers for Emotion Recognition in the Intelligent Human-Machine Interaction,E. Sopov; I. Ivanov,2015 IEEE Symposium Series on Computational Intelligence,2015.0,"Reducing the dimensionality of datasets and configuring learning algorithms for solving particular practical tasks are the main problems in machine learning. In this work we propose multi-objective optimization approach to feature selection and base learners hyper-parameter optimization. The effectiveness of the proposed multi-objective approach is compared to the single-objective approach. We have chosen emotion recognition problem by audio-visual data as a benchmark for comparing the two mentioned approaches. We have chosen neural network as a base learning algorithm for testing the proposed approach to parameter optimization. As a result of multi-objective optimization applied to parameter configuration we get the Pareto set of neural networks with optimal parameter values. In order to get the single output, the Pareto optimal neural networks were combined into an ensemble. We have examined several ensemble model fusion techniques including voting, average class probabilities and meta-classification. According to results, multi-objective optimization approach to feature selection provides an average 2.8% better emotion classification rate on the given datasets than single-objective approach. Multi-objective approach is 5.4% more effective compared to principal components analysis, and 13.9% more effective compared to not using any dimensionality reduction at all. Multi-objective approach applied to neural networks parameter optimization provided on average 7.1% better classification rate than single-objective approach. The results suggest that the proposed multi-objective optimization approach is more effective at solving considered emotion recognition problem.",10.1109/SSCI.2015.252,,,
Multi-Objective Model Selection via Racing,T. Zhang; M. Georgiopoulos; G. C. Anagnostopoulos,IEEE Transactions on Cybernetics,2016.0,"Model selection is a core aspect in machine learning and is, occasionally, multi-objective in nature. For instance, hyper-parameter selection in a multi-task learning context is of multi-objective nature, since all the tasks' objectives must be optimized simultaneously. In this paper, a novel multi-objective racing algorithm (RA), namely S-Race, is put forward. Given an ensemble of models, our task is to reliably identify Pareto optimal models evaluated against multiple objectives, while minimizing the total computational cost. As a RA, S-Race attempts to eliminate non-promising models with confidence as early as possible, so as to concentrate computational resources on promising ones. Simultaneously, it addresses the problem of multi-objective model selection (MOMS) in the sense of Pareto optimality. In S-Race, the nonparametric sign test is utilized for pair-wise dominance relationship identification. Moreover, a discrete Holm's step-down procedure is adopted to control the family-wise error rate of the set of hypotheses made simultaneously. The significance level assigned to each family is adjusted adaptively during the race. In order to illustrate its merits, S-Race is applied on three MOMS problems: (1) selecting support vector machines for classification; (2) tuning the parameters of artificial bee colony algorithms for numerical optimization; and (3) constructing optimal hybrid recommendation systems for movie recommendation. The experimental results confirm that S-Race is an efficient and effective MOMS algorithm compared to a brute-force approach.",10.1109/TCYB.2015.2456187,Discrete Holm’s step-down procedure;model selection;multi-objective optimization;racing algorithm (RA);sign test (ST),6.0,
Looking for Software Defects? First Find the Nonconformists,S. Moshtari; J. C. S. Santos; M. Mirakhorli; A. Okutan,2020 IEEE 20th International Working Conference on Source Code Analysis and Manipulation (SCAM),2020.0,"Software defect prediction models play a key role to increase the quality and reliability of software systems. Because, they are used to identify defect prone source code components and assist testing activities during the development life cycle. Prior research used supervised and unsupervised Machine Learning models for software defect prediction. Supervised defect prediction models require labeled data, however it might be time consuming and expensive to obtain labeled data that has the desired quality and volume. The unsupervised defect prediction models usually use clustering techniques to relax the labeled data requirement, however labeling detected clusters as defective is a challenging task. The Pareto principle states that a small number of modules contain most of the defects. Getting inspired from the Pareto principle, this work proposes a novel, unsupervised learning approach that is based on outlier detection. We hypothesize that defect prone software components have different characteristics when compared to others and can be considered as outliers, therefore outlier detection techniques can be used to identify them. The experiment results on 16 software projects from two publicly available datasets (PROMISE and GitHub) indicate that the k-Nearest Neighbor (KNN) outlier detection method can be used to identify the majority of software defects. It could detect 94% of expected defects at best case and more than 63% of the defects in 75% of the projects. We compare our approach with the state-of-the-art supervised and unsupervised defect prediction approaches. The results of rigorous empirical evaluations indicate that the proposed approach outperforms existing unsupervised models and achieves comparable results with the leading supervised techniques that rely on complex training and tuning algorithms.",10.1109/SCAM51674.2020.00014,Defect prediction;unsupervised learning;outlier detection;software metrics;software quality,,
A Spectral Convolutional Net for Co-Optimization of Integrated Voltage Regulators and Embedded Inductors,H. M. Torun; H. Yu; N. Dasari; V. C. K. Chekuri; A. Singh; J. Kim; S. K. Lim; S. Mukhopadhyay; M. Swaminathan,2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),2019.0,"Integrated voltage regulators (IVR) with embedded inductors is an emerging technology that provides point-of-load voltage regulation to high-performance systems. Conventional two-step approaches to the design of IVRs can suffer from suboptimal design as the optimal inductor depends on the characteristics of the buck converter (BC). Furthermore, inductor-level trade-offs such as AC and DC resistance, inductance and area can not be determined independently from the BC. This co-dependency of the BC and the inductor creates a highly non-linear response surface, which raises the necessity of co-optimization, involving multiple time-consuming electromagnetics (EM) simulations. In this paper, we propose a machine learning based optimization methodology that eliminates EM simulations from the optimization loop to significantly reduce the optimization complexity. A novel technique named as Spectral Transposed Convolutional Neural Network (S-TCNN) is presented to derive an accurate predictive model of the inductor frequency response using a small amount of training data. The derived S-TCNN is then used along with a time-domain model of the BC to perform multi-objective optimization that approximates the Pareto front for 5 objectives, namely inductor area, BC settling time, voltage conversion efficiency, droop and ripple. The resulting methodology provides multiple Pareto optimal inductors in an efficient and fully automated fashion, thereby allows to rapidly determine the optimal trade-offs for possibly contradicting design objectives. We demonstrate the proposed framework on co-optimization of solenoidal inductor with magnetic core and BC that are integrated on silicon interposer.",10.1109/ICCAD45719.2019.8942109,convolutional networks;integrated voltage regulators;embedded inductors;system-level optimization,7.0,
Online knowledge-based evolutionary multi-objective optimization,B. Zhang; K. Shafi; H. Abbass,2014 IEEE Congress on Evolutionary Computation (CEC),2014.0,"Knowledge extraction from a multi-objective optimization process has important implications including a better understanding of the optimization process and the relationship between decision variables. The extant approaches, in this respect, rely on processing the post-optimization Pareto sets for automatic rule discovery using statistical or machine learning methods. However such approaches fall short of providing any information during the progress of the optimization process, which can be critical for decision analysis especially if the problem is dynamic. In this paper, we present a multi-objective optimization framework that uses a knowledge-based representation to search for patterns of Pareto optimal design variables instead of conventional point form solution search. The framework facilitates the online discovery of knowledge during the optimization process in the form of interpretable rules. The core contributing idea of our research is that we apply multi-objective evolutionary process on a population of bounding hypervolumes, or rules, instead of evolving individual point-based solutions. The framework is generic in a sense that any existing multi-objective optimization algorithm can be adapted to evaluate the rule quality based on the sampled solutions from the bounded space. An instantiation of the framework using hyperrectangular representation and non-dominated sorting based rule evaluation is presented in this paper. Experimental results on a specifically designed test function as well as some standard test functions are presented to demonstrate the working and convergence properties of our algorithm.",10.1109/CEC.2014.6900610,,1.0,
Simultaneous feature selection and clustering for categorical features using multi objective genetic algorithm,D. Dutta; P. Dutta; J. Sil,2012 12th International Conference on Hybrid Intelligent Systems (HIS),2012.0,"Clustering is unsupervised learning where ideally class levels and number of clusters (K) are not known. K-clustering can be categorized as semi-supervised learning where K is known. Here we have considered K-Clustering with simultaneous feature selection. Feature subset selection helps to identify relevant features for clustering, increase understandability, better scalability and improve accuracy. Here we have used two measures, intra-cluster distance (Homogeneity, H) and inter-cluster distances (Separation, S) for clustering. Measures are using mod distance per feature suitable for categorical features (attributes). Rather than combining H and S to frame the problem as single objective optimization problem, we use multi objective genetic algorithm (MOGA) to find out diverse solutions near to Pareto optimal front in the two-dimensional objective space. Each evolved solution represents a set of cluster modes (CMs) build by selected feature subset. Here, K-modes is hybridized with MOGA. We have used hybridized GA to combine global searching powers of GA with local searching powers of K-modes. Considering context sensitivity, we have used a special crossover operator called “pairwise crossover” and “substitution”. The main contribution of this paper is simultaneous dimensionality reduction and optimization of objectives using MOGA. Results on 3 benchmark data sets from UCI Machine Learning Repository containing categorical features shows the superiority of the algorithm.",10.1109/HIS.2012.6421332,,6.0,
Improving generalization of radial basis function network with adaptive multi-objective particle swarm optimization,S. N. Qasem; S. M. H. Shamsuddin,"2009 IEEE International Conference on Systems, Man and Cybernetics",2009.0,"In this paper, an adaptive evolutionary multi-objective selection method of RBF Networks structure is discussed. The candidates of RBF Network structures are encoded into particles in Particle Swarm Optimization (PSO). These particles evolve toward Pareto-optimal front defined by several objective functions with model accuracy and complexity. The problem of unsupervised and supervised learning is discussed with Adaptive Multi-Objective PSO (AMOPSO). This study suggests an approach of RBF Network training through simultaneous optimization of architectures and weights with Adaptive PSO-based multi-objective algorithm. Our goal is to determine whether Adaptive Multi-objective PSO can train RBF Networks, and the performance is validated on accuracy and complexity. The experiments are conducted on two benchmark datasets obtained from the machine learning repository. The results show that our proposed method provides an effective means for training RBF Networks that is competitive with PSO-based multi-objective algorithm.",10.1109/ICSMC.2009.5346876,Radial basis function network;Adaptive Multi-objective particle swarm optimization;Multi-Objective particle swarm optimization,3.0,
Radial basis function Network based on multi-objective particle swarm optimization,S. N. Qasem; S. M. H. Shamsuddin,2009 6th International Symposium on Mechatronics and its Applications,2009.0,"The problem of unsupervised and supervised learning is discussed within the context of multi-objective optimization. In this paper, an evolutionary multi-objective selection method of RBF networks structure is discussed. The candidates of RBF network structure are encoded into the particles in PSO. Then, they evolve toward Pareto-optimal front defined by several objective functions concerning with model accuracy and model complexity. This study suggests an approach of RBF network training through simultaneous optimization of architectures and weights with PSO-based multi-objective algorithm. Our goal is to determine whether multi-objective PSO can train RBF networks, and the performance is validated on accuracy and complexity. The experiments are conducted on benchmark datasets obtained from the UCI machine learning repository. The results show that our proposed method provides an effective means for training RBF networks that is competitive with other evolutionary computational-based methods.",10.1109/ISMA.2009.5164833,,11.0,
Unsupervised Adversarial Instance-Level Image Retrieval,C. Bai; H. Li; J. Zhang; L. Huang; L. Zhang,IEEE Transactions on Multimedia,2021.0,"With the wide use of visual sensors in the Internet of Things (IoT) in the past decades, huge amounts of images are captured in people's daily lives, which poses challenges to traditional deep-learning-based image retrieval frameworks. Most such frameworks need a large amount of annotated training data, which are expensive. Moreover, machines still lack human intelligence, as illustrated by the fact that they pay less attention to the interesting regions that humans generally focus on when searching for images. Hence, this paper proposes a novel unsupervised framework that focuses on the instance object in the image and integrates human intelligence into the deep-learning-based image retrieval. This framework is called adversarial instance-level image retrieval (AILIR). We incorporate adversarial training and an attention mechanism into this framework that considers human intelligence with artificial intelligence. The generator and discriminator are redesigned to guarantee that the generator retrieves similar images while the discriminator selects unmatched images and creates an adversarial reward for the generator. A minimax game is conducted by the adversarial reward retrieval mechanism until the discriminator is unable to judge whether the image sequence retrieved matches the query. Comparison and ablation experiments on four benchmark datasets prove that the proposed adversarial training framework indeed improves instance retrieval and outperforms the state-of-the-art methods focused on instance retrieval.",10.1109/TMM.2021.3065578,Generative adversarial training;human intelligence simulation;instance level image retrieval;unsupervised training,4.0,
A hyper-heuristic multi-criteria decision support system for eco-efficient product life cycle,J. R. Woodward; N. Gindy,5th International Conference on Responsive Manufacturing - Green Manufacturing (ICRM 2010),2010.0,"Decision support is required when complex situations arise during product development which takes into account the whole product life cycle. This is especially true when impacted by the ill-defined consequences on the environment in an ever increasingly eco-conscious world. Analytical Hierarchy process (AHP) is one method of providing decision support, and is an instance of a decision support heuristic. Machine learning methods have proved themselves on many well defined problems with clearly defined objectives. In particular, we focus on the recently emerging field of hyper-heuristics which is a blend of human designed heuristics, with the extension of machine designed heuristics. In essence humans can operate at the higher concept or abstract level, while machine heuristics can operate at a lower level. There are a number of issues within the proposed framework, including visualizing a multi-dimensional surface of designs along the Pareto front, as well as dealing with different types of data during the decision making process. It is proposed that Hyper-heuristics, supplemented with other methodologies to deal with vague or missing data, offer a framework in which to begin to address several of the complex compromises that arise during product development.",10.1049/cp.2010.0436,Hyper-heuristics;genetic programming;analytical hierarchal process;decision support system,,
Resource-Constrained Neural Architecture Search on Edge Devices,B. Lyu; H. Yuan; L. Lu; Y. Zhang,IEEE Transactions on Network Science and Engineering,2022.0,"The performance requirement of deep learning inevitably brings up with the expense of high computational complexity and memory requirements, to make it problematic for the deployment on resource-constrained devices. Edge computing, which distributedly organizes the computing node close to the data source and end-device, provides a feasible way to tackle the high-efficiency demand and substantial computational load. Whereas given edge device is resource-constrained and energy-sensitive, designing effective neural network architecture for specific edge device is urgent in the sense that deploys the deep learning application by the edge computing solution. Undoubtedly manually design the high-performing neural architectures is burdensome, let alone taking account of the resource-constraint for the specific platform. Fortunately, the success of Neural Architecture Search techniques come up with hope recently. This paper dedicates to directly employ multi-objective NAS on the resource-constrained edge devices. We first propose the framework of multi-objective NAS on edge device, which comprehensively considers the performance and real-world efficiency. Our improved MobileNet-V2 search space also strikes the scalability and practicality, so that a series of Pareto-optimal architectures are received. Benefits from the directness and specialization during search procedure, our experiment on JETSON NANO shows the comparable result with the state-of-the-art models on ImageNet.",10.1109/TNSE.2021.3054583,Edge computing;multi-objective;neural architecture search;reinforcement learning.,1.0,
Characterising Across-Stack Optimisations for Deep Convolutional Neural Networks,J. Turner; J. Cano; V. Radu; E. J. Crowley; M. O’Boyle; A. Storkey,2018 IEEE International Symposium on Workload Characterization (IISWC),2018.0,"Convolutional Neural Networks (CNNs) are extremely computationally demanding, presenting a large barrier to their deployment on resource-constrained devices. Since such systems are where some of their most useful applications lie (e.g. obstacle detection for mobile robots, vision-based medical assistive technology), significant bodies of work from both machine learning and systems communities have attempted to provide optimisations that will make CNNs available to edge devices. In this paper we unify the two viewpoints in a Deep Learning Inference Stack and take an across-stack approach by implementing and evaluating the most common neural network compression techniques (weight pruning, channel pruning, and quantisation) and optimising their parallel execution with a range of programming approaches (OpenMP, OpenCL) and hardware architectures (CPU, GPU). We provide comprehensive Pareto curves to instruct trade-offs under constraints of accuracy, execution time, and memory space.",10.1109/IISWC.2018.8573503,,13.0,
A Weak Supervision Technique with a Generative Model for Improved Gene Clustering,P. Dutta; S. Saha,2019 IEEE Congress on Evolutionary Computation (CEC),2019.0,"In the field of computational bioinformatics, for tasks such as medical diagnosis or disease gene classification, acquiring class labels is very much essential and costly. Now a days, labelling biomedical data became one of the crucial bottlenecks for developing supervised machine learning systems, as well as deep learning systems. On the contrary, weak supervision sources that generate diverse, semi-accurate or programmatically-generated labels are comparatively cheaper and less time consuming. In this paper, we have proposed a framework for integrating weak supervision sources with a generative model to estimate probabilistic class labels of the gene expression data. Here as the weak supervision source, a multi-objective optimization (MOO) based clustering technique is utilized. The non-dominated solutions of Pareto optimal front are viewed as the weak supervised labels. These weak supervised labels are fed to a generative model, which finally assigns probabilistic class labels to different genes. In this work, we have shown that our proposed generative model based clustering technique attains better Silhouette score (some quality measure of clustering) for three real-life NCBI gene expression data sets than other stateof-the-art methods, without using any labeled data. Finally, the superiority of the proposed method is validated by using a statistical and a biological significance test.",10.1109/CEC.2019.8790052,,1.0,
Automated High-Level Generation of Low-Power Approximate Computing Circuits,K. Nepal; S. Hashemi; H. Tann; R. I. Bahar; S. Reda,IEEE Transactions on Emerging Topics in Computing,2019.0,"Numerous application domains (e.g., signal and image processing, computer graphics, computer vision, and machine learning) are inherently error tolerant, which can be exploited to produce approximate ASIC implementations with low power consumption at the expense of negligible or small reductions in application quality. A major challenge is the need for approximate and high-level design generation tools that can automatically work on arbitrary designs. In this article, we provide an expanded and improved treatment of our ABACUS methodology, which aims to automatically generate approximate designs directly from their behavioral register-transfer level (RTL) descriptions, enabling a wider range of possible approximations. ABACUS starts by creating an abstract syntax tree (AST) from the input behavioral RTL description of a circuit, and then applies variant operators to the AST to create acceptable approximate designs. The devised variant operators include data type simplifications, arithmetic operation approximations, arithmetic expressions transformations, variable-to-constant substitutions, and loop transformations. A design space exploration technique is devised to explore the space of possible variant approximate designs and to identify the designs along the Pareto frontier that represents the trade-off between accuracy and power consumption. In addition, ABACUS prioritizes generating approximate designs that, when synthesized, lead to circuits with simplified critical paths, which are exploited to realize complementary power savings through standard voltage scaling. We integrate ABACUS with a standard ASIC design flow, and evaluate it on four realistic benchmarks from three different domains-machine learning, signal processing, and computer vision. Our tool automatically generates many approximate design variants with large power savings, while maintaining good accuracy. We demonstrate the scalability of ABACUS by parallelizing the flow and use of recent standard synthesis tools. Compared to our previous efforts, the new ABACUS tool provides up to 20.5× speed-up in runtime, while able to generate approximate circuits that lead to additional power savings reaching up to 40 percent.",10.1109/TETC.2016.2598283,Approximate computing;design space exploration;low power circuits;low area circuits;voltage scaling;critical path optimization,29.0,
ABACUS: A technique for automated behavioral synthesis of approximate computing circuits,K. Nepal; Y. Li; R. I. Bahar; S. Reda,"2014 Design, Automation & Test in Europe Conference & Exhibition (DATE)",2014.0,"Many classes of applications, especially in the domains of signal and image processing, computer graphics, computer vision, and machine learning, are inherently tolerant to inaccuracies in their underlying computations. This tolerance can be exploited to design approximate circuits that perform within acceptable accuracies but have much lower power consumption and smaller area footprints (and often better run times) than their exact counterparts. In this paper, we propose a new class of automated synthesis methods for generating approximate circuits directly from behavioral-level descriptions. In contrast to previous methods that operate at the Boolean level or use custom modifications, our automated behavioral synthesis method enables a wider range of possible approximations and can operate on arbitrary designs. Our method first creates an abstract synthesis tree (AST) from the input behavioral description, and then applies variant operators to the AST using an iterative stochastic greedy approach to identify the optimal inexact designs in an efficient way. Our method is able to identify the optimal designs that represent the Pareto frontier trade-off between accuracy and power consumption. Our methodology is developed into a tool we call ABACUS, which we integrate with a standard ASIC experimental flow based on industrial tools. We validate our methods on three realistic Verilog-based benchmarks from three different domains - signal processing, computer vision and machine learning. Our tool automatically discovers optimal designs, providing area and power savings of up to 50% while maintaining good accuracy.",10.7873/DATE.2014.374,,,
Towards Intelligent Architecting of Aerospace System-of-Systems,C. Guariniello; L. Mockus; A. K. Raz; D. A. DeLaurentis,2019 IEEE Aerospace Conference,2019.0,"System-of-Systems (SoS) are composed of large scale independent and complex heterogeneous systems which collaborate to create capabilities not achievable by a single system, for example air transportation system, satellite constellations, and space exploration architectures. Much of the research effort in the field of SoS has focused on the analysis of these complex entities, while there are still major gaps in developing tools for automated synthesis and engineering of SoS that consider all the various aspects in this problem domain. The gap we address in this paper is a mapping of clusters of SoS architecture alternatives, segmented by performance along multiple metrics, to architectural features. Building upon our previous research where we used a SoS Analytic Work Bench in combination with Model-Based Systems Engineering artifacts to perform analysis of aerospace systems, we propose to build a process for intelligent architecting of aerospace SoS. This process discovers and employs pertinent features in a complex design space to effectively meet the user needs, elevating SoS engineering from retrospective architectural analysis to automated synthesis of new architectures. As a first step towards intelligent architecture of aerospace SoS, we propose to utilize Machine Learning techniques to automate the synthesis phase of SoS. Our hypothesis is that a set of holistic metrics of aerospace architectures (cost, performance, robustness, operational risk, average delay, etc.) can be used to characterize a measure of goodness of architectures, with good architectures on a Pareto front of the multi-dimensional space of holistic metrics of interest. Each architecture or cluster may be then mapped to a set of architectural features, with the goal of identifying which features belong to good architectures. Specifically, we propose to utilize non-parametric regression on a set of training architectures (for example, Neural Networks can deal with mixed real and integer variables) to associate each one with a pattern of features. This mapping will allow the automated process to predict what metrics will be expected from SoS architectures with specific features, and therefore to automatically synthesize architectures that exhibit desired characteristics of goodness. For example, for constellations of satellites, a group of good architecture might have medium cost, high resilience, medium robustness, and low risk, and the architectural features to be mapped to each group can include number of satellites, number of components, type of orbit, type of power system, etc. Since the environment constantly evolves, architectures must adapt, and stochastic optimization can be used to switch between architectures with minimal effort. In this work we illustrate the new version of our aerospace SoS analysis and synthesis framework, which includes Machine Learning techniques to support synthesis of SoS architectures. We demonstrate the application of this process on satellite constellations and discuss challenges of this approach and future steps.",10.1109/AERO.2019.8742173,,3.0,
Learning variable importance to guide recombination,M. Sagawa; H. Aguirre; F. Daolio; A. Liefooghe; B. Derbel; S. Verel; K. Tanaka,2016 IEEE Symposium Series on Computational Intelligence (SSCI),2016.0,"In evolutionary multi-objective optimization, variation operators are crucially important to produce improving solutions, hence leading the search towards the most promising regions of the solution space. In this paper, we propose to use a machine learning modeling technique, namely random forest, in order to estimate, at each iteration in the course of the search process, the importance of decision variables with respect to convergence to the Pareto front. Accordingly, we are able to propose an adaptive mechanism guiding the recombination step with the aim of stressing the convergence of the so-obtained offspring. By conducting an experimental analysis using some of the WFG and DTLZ benchmark test problems, we are able to elicit the behavior of the proposed approach, and to demonstrate the benefits of incorporating machine learning techniques in order to design new efficient adaptive variation mechanisms.",10.1109/SSCI.2016.7850229,,3.0,
Neural networks designing neural networks: Multi-objective hyper-parameter optimization,S. C. Smithson; Guang Yang; W. J. Gross; B. H. Meyer,2016 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),2016.0,"Artificial neural networks have gone through a recent rise in popularity, achieving state-of-the-art results in various fields, including image classification, speech recognition, and automated control. Both the performance and computational complexity of such models are heavily dependant on the design of characteristic hyper-parameters (e.g., number of hidden layers, nodes per layer, or choice of activation functions), which have traditionally been optimized manually. With machine learning penetrating low-power mobile and embedded areas, the need to optimize not only for performance (accuracy), but also for implementation complexity, becomes paramount. In this work, we present a multi-objective design space exploration method that reduces the number of solution networks trained and evaluated through response surface modelling. Given spaces which can easily exceed 10<sup>20</sup> solutions, manually designing a near-optimal architecture is unlikely as opportunities to reduce network complexity, while maintaining performance, may be overlooked. This problem is exacerbated by the fact that hyper-parameters which perform well on specific datasets may yield sub-par results on others, and must therefore be designed on a per-application basis. In our work, machine learning is leveraged by training an artificial neural network to predict the performance of future candidate networks. The method is evaluated on the MNIST and CIFAR-10 image datasets, optimizing for both recognition accuracy and computational complexity. Experimental results demonstrate that the proposed method can closely approximate the Pareto-optimal front, while only exploring a small fraction of the design space.",10.1145/2966986.2967058,,12.0,
Incremental information gain analysis of input attribute impact on RBF-kernel SVM spam detection,H. He; A. Tiwari; J. Mehnen; T. Watson; C. Maple; Y. Jin; B. Gabrys,2016 IEEE Congress on Evolutionary Computation (CEC),2016.0,"The massive increase of spam is posing a very serious threat to email and SMS, which have become an important means of communication. Not only do spams annoy users, but they also become a security threat. Machine learning techniques have been widely used for spam detection. Email spams can be detected through detecting senders' behaviour, the contents of an email, subject and source address, etc, while SMS spam detection usually is based on the tokens or features of messages due to short content. However, a comprehensive analysis of email/SMS content may provide cures for users to aware of email/SMS spams. We cannot completely depend on automatic tools to identify all spams. In this paper, we propose an analysis approach based on information entropy and incremental learning to see how various features affect the performance of an RBF-based SVM spam detector, so that to increase our awareness of a spam by sensing the features of a spam. The experiments were carried out on the spambase and SMSSpemCollection databases in UCI machine learning repository. The results show that some features have significant impacts on spam detection, of which users should be aware, and there exists a feature space that achieves Pareto efficiency in True Positive Rate and True Negative Rate.",10.1109/CEC.2016.7743901,,8.0,
Efficient Bitrate Ladder Construction for Content-Optimized Adaptive Video Streaming,A. V. Katsenou; J. Sole; D. R. Bull,IEEE Open Journal of Signal Processing,2021.0,"One of the challenges faced by many video providers is the heterogeneity of network specifications, user requirements, and content compression performance. The universal solution of a fixed bitrate ladder is inadequate in ensuring a high quality of user experience without re-buffering or introducing annoying compression artifacts. However, a content-tailored solution, based on extensively encoding across all resolutions and over a wide quality range is highly expensive in terms of computational, financial, and energy costs. Inspired by this, we propose an approach that exploits machine learning to predict a content-optimized bitrate ladder for on-demand video services. The method extracts spatio-temporal features from the uncompressed content, trains machine-learning models to predict the Pareto front parameters and, based on that, builds the ladder within a defined bitrate range. The method has the benefit of significantly reducing the number of encodes required per sequence. The presented results, based on 100 HEVC-encoded sequences, demonstrate a reduction in the number of encodes required when compared to an exhaustive search and an interpolation-based method, by 89.06% and 61.46%, respectively, at the cost of an average Bjøntegaard Delta Rate difference of 1.78% compared to the exhaustive approach. Finally, a hybrid method is introduced that selects either the proposed or the interpolation-based method depending on the sequence features. This results in an overall 83.83% reduction of required encodings at the cost of an average Bjøntegaard Delta Rate difference of 1.26%.",10.1109/OJSP.2021.3086691,Bitrate ladder;adaptive video streaming;rate-quality curves;video compression;HEVC,,
Gram-Schmidt orthogonalization neural nets for OCR,Szu; Scheff,International 1989 Joint Conference on Neural Networks,1989.0,"A description is given of a three-layer neural network for pattern classification/character recognition. The first layer is a heteroassociative feedforward network with bipolar output (+or-1) and zero threshold neurons. The second layer is an autoassociative memory whose input-output characteristics are the same as those in the first layer. The third layer is used to recognize the pattern and control whether the new orthogonal feature vector should be installed by the outer product formula to increase the memory capacity to M'=M+1. With this network, conventional pattern recognition of the minimax type is used to determine the initial interconnection matrix. The samples are classified by means of supervised learning. Only a single physical layer need be built, since the same layer can repeatedly be used three times in series. The performance of the network is studied.<<ETX>></ETX>",10.1109/IJCNN.1989.118632,,5.0,
Universal Learning of Individual Data,Y. Fogel; M. Feder,2019 IEEE International Symposium on Information Theory (ISIT),2019.0,"Universal supervised learning of individual data is considered from an information theoretic point of view in the standard supervised “batch” learning where prediction is done on a test sample once the entire training data is observed. In this individual setting the features and labels, both in the training and the test, are specific individual, deterministic quantities. Prediction loss is naturally measured by the log-loss. The presented results provide a minimax universal learning scheme, termed the Predictive Normalized Maximum Likelihood (pNML) that competes with a “genie” (or reference) that knows the true test label. In addition, a pointwise learnability measure associated with the pNML, for the specific training and test, is provided. This measure may also indicate the performance of the commonly used Empirical Risk Minimizer (ERM) learner.",10.1109/ISIT.2019.8849222,,1.0,
Semi-Supervised Multiple Instance Learning and its application in visual tracking,Y. Zhou; A. Ming,2016 8th International Conference on Wireless Communications & Signal Processing (WCSP),2016.0,"In this paper, a novel Semi-Supervised Multiple Instance Learning (Semi-MIL) approach is presented. Compared with conventional approaches, we utilize a kind of “bag of instances” representation in the semi-supervised learning process, which provides an effective way to use the unlabeled data in multiple instance learning problem. We formulate the problem with a graph model based on the Minimax kernel. In addition, the Semi-MIL algorithm is readily applied for visual tracking, which can resolve the ambiguities during the tracking process. The presented approach is validated on several benchmark videos for visual tracking and MUSKs dataset for classification, the competitive experimental results demonstrate the effectiveness of our approach.",10.1109/WCSP.2016.7752532,,,
A Multi-objective Learning Algorithm for RBF Neural Network,I. Kokshenev; A. P. Braga,2008 10th Brazilian Symposium on Neural Networks,2008.0,"In this paper, the problem of multi-objective supervised learning is discussed within the non-evolutionary optimization framework. The proposed MOBJ learning algorithm performs the search of Pareto-optimal models determining weights,width, prototype vectors, and the quantity of basis functions of the RBF network. In combination with the Akaike information criterion, the algorithm provides high quality solutions.",10.1109/SBRN.2008.39,multi-objective learning;radial basis functions;generalization;regularization;LASSO,3.0,
Multi-Source Spatial Entity Linkage,S. Isaj; T. B. Pedersen; E. Zimányi,IEEE Transactions on Knowledge and Data Engineering,2022.0,"Besides the traditional cartographic data sources, spatial information can also be derived from location-based sources. However, even though different location-based sources refer to the same physical world, each one has only partial coverage of the spatial entities, describe them with different attributes, and sometimes provide contradicting information. Hence, we introduce the spatial entity linkage problem, which finds which pairs of spatial entities belong to the same physical spatial entity. Our proposed solution (<italic>QuadSky</italic>) starts with a time-efficient spatial blocking technique (<italic>QuadFlex</italic>), compares pairwise the spatial entities in the same block, ranks the pairs using Pareto optimality with the <italic>SkyRank</italic> algorithm, and finally, classifies the pairs with our novel <italic>SkyEx-*</italic> family of algorithms that yield 0.85 <italic>precision</italic> and 0.85 <italic>recall</italic> for a manually labeled dataset of 1,500 pairs and 0.87 <italic>precision</italic> and 0.6 <italic>recall</italic> for a semi-manually labeled dataset of 777,452 pairs. Moreover, we provide a theoretical guarantee and formalize the <italic>SkyEx-FES</italic> algorithm that explores only 27 percent of the skylines without any loss in <italic>F-measure</italic>. Furthermore, our fully unsupervised algorithm <italic>SkyEx-D</italic> approximates the optimal result with an <italic>F-measure</italic> loss of just 0.01. Finally, <italic>QuadSky</italic> provides the best trade-off between <italic>precision</italic> and <italic>recall</italic>, and the best <italic>F-measure</italic> compared to the existing baselines and clustering techniques, and approximates the results of supervised learning solutions.",10.1109/TKDE.2020.2990491,spatial data;entity resolution;spatial blocking;skyline-based,,
ADA: Adaptive Deep Log Anomaly Detector,Y. Yuan; S. Srikant Adhatarao; M. Lin; Y. Yuan; Z. Liu; X. Fu,IEEE INFOCOM 2020 - IEEE Conference on Computer Communications,2020.0,"Large private and government networks are often subjected to attacks like data extrusion and service disruption. Existing anomaly detection systems use offline supervised learning and employ experts for labeling. Hence they cannot detect anomalies in real-time. Even though unsupervised algorithms are increasingly used nowadays, they cannot readily adapt to newer threats. Moreover, many such systems also suffer from high cost of storage and require extensive computational resources. In this paper, we propose ADA: Adaptive Deep Log Anomaly Detector, an unsupervised online deep neural network framework that leverages LSTM networks and regularly adapts to newer log patterns to ensure accurate anomaly detection. In ADA, an adaptive model selection strategy is designed to choose pareto-optimal configurations and thereby utilize resources efficiently. Further, a dynamic threshold algorithm is proposed to dictate the optimal threshold based on recently detected events to improve the detection accuracy. We also use the predictions to guide storage of abnormal data and effectively reduce the overall storage cost. We compare ADA with state-of-the-art approaches through leveraging the Los Alamos National Laboratory cyber security dataset and show that ADA accurately detects anomalies with high F1-score ~95% and it is 97 times faster than existing approaches and incurs very low storage cost.",10.1109/INFOCOM41043.2020.9155487,Anomaly detection;deep neural networks;logs;online training;unsupervised;log-normal;threshold,7.0,
Adversarial Feature Augmentation for Unsupervised Domain Adaptation,R. Volpi; P. Morerio; S. Savarese; V. Murino,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,2018.0,"Recent works showed that Generative Adversarial Networks (GANs) can be successfully applied in unsupervised domain adaptation, where, given a labeled source dataset and an unlabeled target dataset, the goal is to train powerful classifiers for the target samples. In particular, it was shown that a GAN objective function can be used to learn target features indistinguishable from the source ones. In this work, we extend this framework by (i) forcing the learned feature extractor to be domain-invariant, and (ii) training it through data augmentation in the feature space, namely performing feature augmentation. While data augmentation in the image space is a well established technique in deep learning, feature augmentation has not yet received the same level of attention. We accomplish it by means of a feature generator trained by playing the GAN minimax game against source features. Results show that both enforcing domain-invariance and performing feature augmentation lead to superior or comparable performance to state-of-the-art results in several unsupervised domain adaptation benchmarks.",10.1109/CVPR.2018.00576,,81.0,
Transferring Structured Knowledge in Unsupervised Domain Adaptation of a Sleep Staging Network,C. Yoo; H. W. Lee; J. -W. Kang,IEEE Journal of Biomedical and Health Informatics,2022.0,"Automatic sleep staging based on deep learning (DL) has been attracting attention for analyzing sleep quality and determining treatment effects. It is challenging to acquire long-term sleep data from numerous subjects and manually labeling them even though most DL-based models are trained using large-scale sleep data to provide state-of-the-art performance. One way to overcome this data shortage is to create a pre-trained network with an existing large-scale dataset (source domain) that is applicable to small cohorts of datasets (target domain); however, discrepancies in data distribution between the domains prevent successful refinement of this approach. In this paper, we propose an unsupervised domain adaptation method for sleep staging networks to reduce discrepancies by re-aligning the domains in the same space and producing domain-invariant features. Specifically, in addition to a classical domain discriminator, we introduce local discriminators - <italic>subject and stage</italic> - to maintain the intrinsic structure of sleep data to decrease local misalignments while using adversarial learning to play a minimax game between the feature extractor and discriminators. Moreover, we present several optimization schemes during training because the conventional adversarial learning is not effective to our training scheme. We evaluate the performance of the proposed method by examining the staging performances of a baseline network compared with direct transfer (DT) learning in various conditions. The experimental results demonstrate that the proposed domain adaptation significantly improves the performance though it needs no labeled sleep data in target domain.",10.1109/JBHI.2021.3103614,Sleep staging;unsupervised domain adaptation;local alignment;knowledge transfer,1.0,
Feature-Level Frankenstein: Eliminating Variations for Discriminative Recognition,X. Liu; S. Li; L. Kong; W. Xie; P. Jia; J. You; B. V. K. Kumar,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2019.0,"Recent successes of deep learning-based recognition rely on maintaining the content related to the main-task label. However, how to explicitly dispel the noisy signals for better generalization remains an open issue. We systematically summarize the detrimental factors as task-relevant/irrelevant semantic variations and unspecified latent variation. In this paper, we cast these problems as an adversarial minimax game in the latent space. Specifically, we propose equipping an end-to-end conditional adversarial network with the ability to decompose an input sample into three complementary parts. The discriminative representation inherits the desired invariance property guided by prior knowledge of the task, which is marginally independent to the task-relevant/irrelevant semantic and latent variations. Our proposed framework achieves top performance on a serial of tasks, including digits recognition, lighting, makeup, disguise-tolerant face recognition, and facial attributes recognition.",10.1109/CVPR.2019.00073,Recognition: Detection;Categorization;Retrieval;Biometrics,20.0,
NAS-OoD: Neural Architecture Search for Out-of-Distribution Generalization,H. Bai; F. Zhou; L. Hong; N. Ye; S. . -H. G. Chan; Z. Li,2021 IEEE/CVF International Conference on Computer Vision (ICCV),2021.0,"Recent advances on Out-of-Distribution (OoD) generalization reveal the robustness of deep learning models against distribution shifts. However, existing works focus on OoD algorithms, such as invariant risk minimization, domain generalization, or stable learning, without considering the influence of deep model architectures on OoD generalization, which may lead to sub-optimal performance. Neural Architecture Search (NAS) methods search for architecture based on its performance on the training data, which may result in poor generalization for OoD tasks. In this work, we propose robust Neural Architecture Search for OoD generalization (NAS-OoD), which optimizes the architecture with respect to its performance on generated OoD data by gradient descent. Specifically, a data generator is learned to synthesize OoD data by maximizing losses computed by different neural architectures, while the goal for architecture search is to find the optimal architecture parameters that minimize the synthetic OoD data losses. The data generator and the neural architecture are jointly optimized in an end-to-end manner, and the minimax training process effectively discovers robust architectures that generalize well for different distribution shifts. Extensive experimental results show that NAS-OoD achieves superior performance on various OoD generalization benchmarks with deep models having a much fewer number of parameters. In addition, on a real industry dataset, the proposed NAS-OoD method reduces the error rate by more than 70% compared with the state-of-the-art method, demonstrating the proposed method’s practicality for real applications.",10.1109/ICCV48922.2021.00821,Transfer/Low-shot/Semi/Unsupervised Learning; Visual reasoning and logical representation,,
A Unified Classification Model Based on Robust Optimization,A. Takeda; H. Mitsugi; T. Kanamori,Neural Computation,2013.0,"A wide variety of machine learning algorithms such as the support vector machine (SVM), minimax probability machine (MPM), and Fisher discriminant analysis (FDA) exist for binary classification. The purpose of this letter is to provide a unified classification model that includes these models through a robust optimization approach. This unified model has several benefits. One is that the extensions and improvements intended for SVMs become applicable to MPM and FDA, and vice versa. For example, we can obtain nonconvex variants of MPM and FDA by mimicking Perez-Cruz, Weston, Hermann, and Schölkopf's (<xref ref-type=""bibr"" rid=""B19"">2003</xref>) extension from convex ν-SVM to nonconvex Eν-SVM. Another benefit is to provide theoretical results concerning these learning methods at once by dealing with the unified model. We give a statistical interpretation of the unified classification model and prove that the model is a good approximation for the worst-case minimization of an expected loss with respect to the uncertain probability distribution. We also propose a nonconvex optimization algorithm that can be applied to nonconvex variants of existing learning methods and show promising numerical results.",10.1162/NECO_a_00412,,,
A Military Chess Game Tree Algorithm Based on Refresh Probability Table,S. Pan; J. Wu; Y. Sun; Y. Qu,2020 Chinese Control And Decision Conference (CCDC),2020.0,"Computer game is divided into complete information game and incomplete information game. As a kind of incomplete information game, military chess game has many shortcomings in game-tree algorithm. In view of the fact that it is difficult to develop the game-tree because the specific information of the opponent's chess pieces can not be obtained accurately in military chess game, this paper proposes an algorithm to expand the game tree by refreshing the probability table formed by the rules of military chess, the result of the opponent's chess game and the result of the collision of the two opponents' chess pieces. The algorithm of the refresh probability table is used to provide the score support of the expanded game tree for the military chess game, thereby ensuring the expansion of the game tree. The initial values and weights in the refresh probability table algorithm are optimized by machine learning to improve the speed and accuracy of the refresh probability table, and the game tree is optimized by the minimax algorithm and the Alpha-beta pruning algorithm. Experiments show that the refresh probability table algorithm based on the game tree improves the winning rate in the game and has achieved good results.",10.1109/CCDC49329.2020.9164878,Computer Game;Military chess;Probability table and Game tree,1.0,
Divergence-based characterization of fundamental limitations of adaptive dynamical systems,M. Raginsky,"2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton)",2010.0,"Adaptive dynamical systems arise in a multitude of contexts, e.g., optimization, control, communications, signal processing, and machine learning. A precise characterization of their fundamental limitations is therefore of paramount importance. In this paper, we consider the general problem of adaptively controlling and/or identifying a stochastic dynamical system, where our a priori knowledge allows us to place the system in a subset of a metric space (the uncertainty set). We present an information-theoretic meta-theorem that captures the trade-off between the metric complexity (or richness) of the uncertainty set, the amount of information acquired online in the process of controlling and observing the system, and the residual uncertainty remaining after the observations have been collected. Following the approach of Zames, we quantify a priori information by the Kolmogorov (metric) entropy of the uncertainty set, while the information acquired online is expressed as a sum of information divergences. The general theory is used to derive new minimax lower bounds on the metric identification error, as well as to give a simple derivation of the minimum time needed to stabilize an uncertain stochastic linear system.",10.1109/ALLERTON.2010.5706895,,5.0,
One-Shot Face Recognition via Generative Learning,Z. Ding; Y. Guo; L. Zhang; Y. Fu,2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018),2018.0,"One-shot face recognition measures the ability to recognize persons with only seeing them once, which is a hallmark of human visual intelligence. It is challenging for existing machine learning approaches to mimic this way, since limited data cannot well represent the data variance. To this end, we propose to build a large-scale face recognizer, which is capable to fight off the data imbalance difficulty. To seek a more effective general classifier, we develop a novel generative model attempting to synthesize meaningful data for one-shot classes by adapting the data variances from other normal classes. Specifically, we formulate conditional generative adversarial networks and the general Softmax classifier into a unified framework. Such a two-player minimax optimization can guide the generation of more effective data, which benefit the classifier learning for one-shot classes. The experimental results on a large-scale face benchmark with 21K persons verify the effectiveness of our proposed algorithm in one-shot classification, as our generative model significantly improves the recognition coverage rate from 25:65% to 94:84% at the precision of 99% for the one-shot classes, while still keeps an overall Top-1 accuracy at 99:80% for the normal classes.",10.1109/FG.2018.00011,one shot learning;face recognition;generative model,18.0,
Development of a Software Library for Game Artificial Intelligence,B. Maksim; W. Pavel; V. Irina; S. Mikhail; C. Margarita,"2020 International Conference Quality Management, Transport and Information Security, Information Technologies (IT&QM&IS)",2020.0,"Today, technologies of artificial neural networks are very popular and are used in various fields and tasks, such as pattern recognition, classification. To create artificial neural networks, constructors or software libraries for machine learning can be used. However, they do not always contain the necessary functions. In some cases, you need to write your own software library to solve non-standard tasks. The article discusses the development of a software library for creating artificial neural networks. Using the developed library is demonstrated in the task of implementing the game artificial intelligence of the opponent in a simple computer game “Tic-tac-toe”. The effectiveness of the opponent's game is compared with the implementations of opponents based on the minimax algorithm and the random behavior algorithm.",10.1109/ITQMIS51053.2020.9322928,artificial neural networks;gaming artificial intelligence;game “Tic-tac-toe”;library,,
A Goal-Prioritized Algorithm for Additional Route Deployment on Existing Mass Transportation System,F. Lin; H. -P. Hsieh,2020 IEEE International Conference on Data Mining (ICDM),2020.0,"Multi-criteria path planning is an important combinatorial optimization problem with broad real-world applications. Finding the Pareto-optimal set of paths ideal for all requiring features is time-consuming and unclear to obtain the subset of optimal paths efficiently for multiple origin states in the planning space. Meanwhile, due to the rise of deep learning, hybrid systems of computational intelligence thrive in recent years. When facing non-monotonic data or heuristics derived from pre-trained neural networks, most of the existing methods for the one-to-all path problem fail to find an ideal solution. We employ Gaussian mixture model to propose a target-prioritized searching algorithm called Multi-Source Bidirectional Gaussian-Prioritized Spanning Tree (BiasSpan) in solving this non-monotonic multi-criteria route planning problem given constraints including range, must-visit vertices, and the number of recommended vertices. Experimental results on mass transportation system in Tainan and Chicago cities show that BiasSpan outperforms comparative methods from 7% to 24<sup>%</sup> and runs in a reasonable time compared to state-of-art route-planning algorithms.",10.1109/ICDM50108.2020.00137,"Constrained route planning, Bidirectional spanning tree;Gaussian mixture model (GMM);Non-monotonicity;Deep Neural Network (DNN)",,
NetCut: Real-Time DNN Inference Using Layer Removal,M. Zandigohar; D. Erdoğmuş; G. Schirner,"2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)",2021.0,"Deep Learning plays a significant role in assisting humans in many aspects of their lives. As these networks tend to get deeper over time, they extract more features to increase accuracy at the cost of additional inference latency. This accuracy-performance trade-off makes it more challenging for Embedded Systems, as resource-constrained processors with strict deadlines, to deploy them efficiently. This can lead to selection of networks that can prematurely meet a specified deadline with excess slack time that could have potentially contributed to increased accuracy. In this work, we propose: (i) the concept of layer removal as a means of constructing TRimmed Networks (TRNs) that are based on removing problem-specific features of a pretrained network used in transfer learning, and (ii) NetCut, a methodology based on an empirical or an analytical latency estimator, which only proposes and retrains TRNs that can meet the application's deadline, hence reducing the exploration time significantly. We demonstrate that TRNs can expand the Pareto frontier that trades off latency and accuracy to provide networks that can meet arbitrary deadlines with potential accuracy improvement over off-the-shelf networks. Our experimental results show that such utilization of TRNs, while transferring to a simpler dataset, in combination with NetCut, can lead to the proposal of networks that can achieve relative accuracy improvement of up to 10.43% among existing off-the-shelf neural architectures while meeting a specific deadline, and 27x speedup in exploration time.",10.23919/DATE51398.2021.9474052,,1.0,
On Resource-Efficient Bayesian Network Classifiers and Deep Neural Networks,W. Roth; F. Pernkopf; G. Schindler; H. Fröning,2020 25th International Conference on Pattern Recognition (ICPR),2021.0,"We present two methods to reduce the complexity of Bayesian network (BN) classifiers. First, we introduce quantization-aware training using the straight-through gradient estimator to quantize the parameters of BNs to few bits. Second, we extend a recently proposed differentiable tree-augmented naive Bayes (TAN) structure learning approach by also considering the model size. Both methods are motivated by recent developments in the deep learning community, and they provide effective means to trade off between model size and prediction accuracy, which is demonstrated in extensive experiments. Furthermore, we contrast quantized BN classifiers with quantized deep neural networks (DNNs) for small-scale scenarios which have hardly been investigated in the literature. We show Pareto optimal models with respect to model size, number of operations, and test error and find that both model classes are viable options.",10.1109/ICPR48806.2021.9413156,,,
Image Transfer Applied in Electric Machine Optimization,S. Yang; Y. Meng; X. Meng,2020 IEEE 61th International Scientific Conference on Power and Electrical Engineering of Riga Technical University (RTUCON),2020.0,"Researches have been conducted on the surrogate-modeling for better trade-off between solution accuracy and solving effort in design space exploration. In this paper, a robust method combining the deep-learning technique, image-transfer, with finite-element-modeling (FEM) in the electric machine optimization to accelerate the convergence is proposed. Specifically, a conditional generative-adversarial network is built to learn from the FEM simulated data about the relationship between the geometric drawing input and magnetic field plot output. The learned model can obtain the result 24x faster than finite-element modeling while maintaining the accuracy. This approximation model is then applied as the sample filter prior to the FEM in the genetic-algorithm powered optimization framework. The test done on a V-shape magnet motor optimization shows that closely matched Pareto-frontier can be found by this approach while the computing time is reduced by >50% at beginning stage for acceleration.",10.1109/RTUCON51174.2020.9316579,Statistical learning;Design optimization;Permanent magnet machines,,
Reliable lymph node metastasis prediction in head & neck cancer through automated multi-objective model,Z. Zhou; M. Dohopolski; L. Chen; X. Chen; S. Jiang; D. Sher; J. Wang,2019 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI),2019.0,"Lymph node metastasis (LNM) plays an important role for accurately diagnosing and treating the patients with head & neck cancer. Positron emission tomography (PET) and computed tomography (CT) are two primary imaging modalities used for identifying LNM status. However, the uncertainty of LNM may exist especially for reactive or small nodes. Furthermore, identifying the LNM on PET or CT is greatly dependent on the physician's experience. Therefore, developing a reliable and automatic model is essential for accurately identifying LNM. Multi-objective models have shown promising predictive results by considering different objectives such as sensitivity and specificity. However, most multi-objective models need to choose an optimal model manually. In this work, we proposed an automated multi-objective learning model (AutoMO) for predicting LNM reliably. Instead of picking one optimal model, all the Pareto-optimal models with the calculated relative weights are used in AutoMO. Then the evidential reasoning (ER) approach is used for fusing the output probability for obtaining more reliable results than traditional fusion method. We built three models for PET, CT and PET&CT and the results showed that PET&CT outperformed two single modality based models. The comparative study demonstrated that AutoMO obtained better performance than current available multi-objective and deep learning methods, and more reliable results can be acquired when using ER fusion.",10.1109/BHI.2019.8834658,Lymph node metastasis;Head & neck cancer;Automated multi-objective learning (AutoMO);Multi-objective optimization;Evidential reasoning,2.0,
A novel algorithm to keep the formation for multiple vehicles based on NSGA and DCNN,Z. Huai; H. Wang; M. Gong,2020 3rd International Conference on Unmanned Systems (ICUS),2020.0,"To address the problem of keeping the formation for multiple vehicles, this paper proposes a novel algorithm based on non-dominated sorting genetic algorithm II (NSGA- II) and deep convolutional neural network(DCNN). Firstly, the problem of keeping the formation for multiple vehicles is translated into three objective functions. Secondly, NSGA- II is used to optimize and a strategy is designed to choose the guidance command from Pareto solution set. Thirdly, in order to improve the calculation speed, a 19-layer deep learning model is built based on DCNN, residual network and batch normalization layer, then a correction part is designed to avoid the phenomenon that the accumulation of generalization error will brings obvious error on keeping the formation. Finally, three sets of simulation demonstrate the effectiveness of this algorithm.",10.1109/ICUS50048.2020.9275022,keeping the formation;multiple vehicles;nondominated sorting genetic;deep convolutional neural network,,
Towards Intelligent Architecting of Aerospace System-of-Systems: Part II,C. Guariniello; L. Mockus; A. K. Raz; D. A. DeLaurentis,2020 IEEE Aerospace Conference,2020.0,"System-of-Systems (SoS) are composed of large scale independent and complex heterogeneous systems which collaborate to create capabilities not achievable by a single system, for example air transportation system, satellite constellations, and space exploration architectures. To support architecting of aerospace SoS, in this work we present a methodology to accurately predict different aspects of performance for design/operation and SoS architecting, expanding previous work on intelligent architecting of aerospace SoS, by adding rigorous Uncertainty Quantification via Bayesian Neural Networks. A Bayesian Neural Network is a neural network with a-priori distribution on its weights. In addition to solving the overfit problem, which is common to traditional deep neural networks, Bayesian Neural Networks provide automated model pruning (or reduction of feature design space), that addresses a well-known dimensionality curse in the SoS domain. We enable SoS design/operation by using modeling and simulation, quantifying the uncertainty inherently present in SoS, and utilizing Artificial Intelligence and optimization techniques to design and operate the system so that its expected performance or behavior when the unexpected occurs (for example, a failure) still satisfies user requirements. Much of the research effort in the field of SoS has focused on the analysis of these complex entities, while there are still gaps in developing tools for automated synthesis and engineering of SoS that consider all the various aspects in this problem domain. In this expansion of the use of Artificial Intelligence towards automated design, these techniques are used not only to discover and employ features of interest in a complex design space, but also to assess how uncertainty can affect performance. This capability supports the automated design of robust architectures, that can effectively meet the user needs even in presence of uncertainty. The SoS design and evaluation methodology presented in this paper and demonstrated on a synthetic modular satellites problem starts from modeling and simulation, and design of experiments to explore the design space. The following step is deep learning, to develop a model which relates SoS architectural features with performance metrics. Uncertainty Quantification techniques are then applied to assess the performance metrics for different architectures. Once the most critical features that affect the SoS performance are identified, stochastic optimization of the SoS on a reduced design space can be performed to determine Pareto optimal features. The final step is determining if any additional design/operation measures need to be explored to further maximize the SoS performance.",10.1109/AERO47225.2020.9172585,,,
BioNetExplorer: Architecture-Space Exploration of Biosignal Processing Deep Neural Networks for Wearables,B. S. Prabakaran; A. Akhtar; S. Rehman; O. Hasan; M. Shafique,IEEE Internet of Things Journal,2021.0,"Deep learning (DL) has been shown to be highly effective in solving various problems across numerous applications and domains, such as autonomous driving and image recognition. Due to the advent of DL, plenty of research works have explored the applicability of DL, more specifically deep neural networks (DNNs), to solve pattern recognition and computer vision challenges. More recently, researchers have focused on the topic of automated generation and exploration of DNN architectures, which tend to mostly focus on image recognition or visual data sets, primarily, due to the computer vision-related DL advancements. In this work, we propose the BioNetExplorer framework to systematically generate and explore multiple DNN architectures for biosignal processing in wearable devices. Our framework varies key neural architecture parameters to search for an embedded DNN architecture with a low hardware overhead, which can be deployed in wearable edge devices to analyze the biosignal data and to extract the relevant information, such as arrhythmia and seizure. Furthermore, BioNetExplorer reduces the exploration time by deploying genetic algorithms, such as NSGA-II, SPEA-2, etc. Our framework also enables the hardware-aware DNN architecture search by imposing user requirements and hardware constraints (storage, FLOPs, etc.) during the exploration stage, thereby limiting the number of networks explored. Moreover, BioNetExplorer can also be used to search for DNNs based on the user-required output classes; for instance, a user might require a specific output class, attributed toward ventricular fibrillation, due to genetic predisposition or a preexisting heart condition. The use of genetic algorithms reduces the exploration time, on average, by 9×, compared to exhaustive exploration. We are successful in identifying Pareto-optimal designs, which can reduce the storage overhead of DNN by ~ 30 MB for a quality loss of less than 0.5%. To enable low-cost embedded DNNs, BioNetExplorer also employs different model compression techniques to further reduce the storage overhead of the network by up to 53× for a quality loss of $ <; 0.2\%$ .",10.1109/JIOT.2021.3065815,Bio-signals;convolution;deep neural networks (DNNs);efficiency;embedded systems;exploration;healthcare;long short-term memory (LSTM);performance;wearables,2.0,
A Bi-objective Hyper-Heuristic Support Vector Machines for Big Data Cyber-Security,N. R. Sabar; X. Yi; A. Song,IEEE Access,2018.0,"Cyber security in the context of big data is known to be a critical problem and presents a great challenge to the research community. Machine learning algorithms have been suggested as candidates for handling big data security problems. Among these algorithms, support vector machines (SVMs) have achieved remarkable success on various classification problems. However, to establish an effective SVM, the user needs to define the proper SVM configuration in advance, which is a challenging task that requires expert knowledge and a large amount of manual effort for trial and error. In this paper, we formulate the SVM configuration process as a bi-objective optimization problem in which accuracy and model complexity are considered as two conflicting objectives. We propose a novel hyper-heuristic framework for bi-objective optimization that is independent of the problem domain. This is the first time that a hyper-heuristic has been developed for this problem. The proposed hyper-heuristic framework consists of a high-level strategy and low-level heuristics. The high-level strategy uses the search performance to control the selection of which low-level heuristic should be used to generate a new SVM configuration. The low-level heuristics each use different rules to effectively explore the SVM configuration search space. To address bi-objective optimization, the proposed framework adaptively integrates the strengths of decompositionand Paretobased approaches to approximate the Pareto set of SVM configurations. The effectiveness of the proposed framework has been evaluated on two cyber security problems: Microsoft malware big data classification and anomaly intrusion detection. The obtained results demonstrate that the proposed framework is very effective, if not superior, compared with its counterparts and other algorithms.",10.1109/ACCESS.2018.2801792,Hyper-heuristics;big data;cyber security;optimisation,21.0,
Non-symmetric Preferences in the IPA Market with Reinforcement Learning,E. R. Gomes; R. Kowalczyk,2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,2008.0,"Machine Learning has been proposed to support and optimize market-based resource allocation. In particular, reinforcement learning (RL) has been used to improve the allocation in terms of the utility received by resource requesting agents in the iterative price adjustment (IPA) mechanism. In such an approach, utility functions describe the agents' preferences for resource attributes and are the basis for RL to learn demand functions that are optimized for the market. It has been shown that the reward functions based on the individual utility of the agents and the social welfare of the allocation can deliver similar social results when the market consists only of learning agents with symmetric preferences. In this paper we investigate the IPA market-based resource allocation with RL for the case of agents with non-symmetric preferences. We show through experimental investigation that the results observed above are also held in this case. In particular, we show that the individual-based reward function is able to approximate the solution to the fairest Pareto-optimal allocation in situations where the social-based reward function fails.",10.1109/WIIAT.2008.77,Market-based Resource Allocation;Iterative Price Adjustment;Reinforcement Learning;Individual;Social Rewards,,
Multi-Task Learning for Multi-Objective Evolutionary Neural Architecture Search,R. Cai; J. Luo,2021 IEEE Congress on Evolutionary Computation (CEC),2021.0,"Neural architecture search (NAS) is an exciting new field in automating machine learning. It can automatically search for the architecture of neural networks. But the current NAS has extremely high requirements for hardware equipment and time costs. In this work, we propose a predictor based on Radial basis function neural network (RBFNN) as a surrogate model of Bayesian optimization to predict the performance of neural architecture. The existing work does not consider the difficulty of directly searching for neural architectures that meet the performance requirements of NAS in real-world applications. Meanwhile, NAS needs to execute multiple times independently when facing multiple similar tasks. Therefore, we further propose a multi-task learning surrogate model with multiple RBFNNs. The model not only functions as a predictor, but also learns knowledge of similar tasks jointly. The performance of NAS is improved by processing multiple tasks simultaneously. Also, the current NAS is committed to searching for very high-performance networks and does not take into account that neural architectures are limited by device memory during actual deployment. The scale of architecture also needs to be considered. We use a multi-objective optimization algorithm to simultaneously balance the performance and the scale, and build a multi-objective evolutionary search framework to find the Pareto optimal front. Once the NAS is completed, decision-makers can choose the appropriate architecture for deployment according to different performance requirements and hardware conditions. Compared with existing NAS work, our proposed MT-ENAS algorithm is able to find a neural architecture with competitive performance and smaller scale in a shorter time.",10.1109/CEC45853.2021.9504721,neural architecture search;multi-task learning;surrogate model;multi-objective optimization,,
Boosting the Efficiency of the Harmonics Elimination VLSI Architecture by Arithmetic Approximations,P. da Costa; P. T. L. Pereira; G. Paim; E. da Costa; S. Bampi,"2021 28th IEEE International Conference on Electronics, Circuits, and Systems (ICECS)",2021.0,"Approximate computing emerged as a key alternative for trading off accuracy against energy efficiency and area reduction. Error-tolerant applications, such as multimedia processing, machine learning, and signal processing, can process the information with lower-than-standard accuracy at the circuit level while still fulfilling a good and acceptable service quality at the application level. Adaptive filtering-based systems have been demonstrating high resiliency against hardware errors due to their intrinsic self-healing characteristic. This paper investigates the design space exploration of arithmetic approximations in a Very Large-Scale Integration (VLSI) harmonic elimination (HE) hardware architecture based on Least Mean Square (LMS) adaptive filters. We evaluate the Pareto front of the area- and power versus quality curves by relaxing the arithmetic precision and by adopting both approximate multipliers (AxMs) in combination with approximate adders (AxAs). This paper explores the benefits and impacts of the Dynamic Range Unbiased (DRUM), Rounding-based Approximate (RoBA), and Leading one Bit-based Approximate (LoBA) multipliers in the power dissipation, circuit area, and quality of the VLSI HE architectures. Our results highlight the LoBA 0 as the most efficient AxM applied in the HE architecture. We combine the LoBA 0 with Copy and LOA AxAs with variations in the approximation level (L). Notably, LoBA 0 and LOA with <tex>$L=6$</tex> resulted in savings of 43.7% in circuit area and 45.2% in power dissipation, compared to the exact HE, which uses multiplier and adder automatically selected by the logic synthesis tool. Finally, we demonstrate that the best hardware architecture found in our investigation successfully eliminates the contaminating spurious noise (i.e., 60 Hz and its harmonics) from the signal.",10.1109/ICECS53924.2021.9665538,Approximate Computing;Approximate multipliers;VLSI Design;LMS;Harmonics Suppression,,
Hybrid knowledge-based evolutionary many-objective optimization,B. Zhang; K. Shafi; H. A. Abbass,2016 IEEE Congress on Evolutionary Computation (CEC),2016.0,"Knowledge-based optimization is a recent direction in evolutionary optimization research which aims at understanding the optimization process, discovering relationships between decision variables and performance parameters, and using discovered knowledge to improve the optimization process, using machine learning techniques. A novel evolutionary optimization framework that incorporates a knowledge-based representation to search for Pareto optimal patterns in decision space was proposed earlier. This paper extends this framework to problems with four and more objectives, commonly referred to as many-objective optimization problems, using a hybridization approach with NSGA3. Experimental results on standard test functions are presented to demonstrate the advantages of the proposed hybrid algorithm in both objective and decision spaces.",10.1109/CEC.2016.7743899,,1.0,
Alleviating Catastrophic Forgetting via Multi-Objective Learning,Yaochu Jin; B. Sendhoff,The 2006 IEEE International Joint Conference on Neural Network Proceedings,2006.0,"Handling catastrophic forgetting is an interesting and challenging topic in modeling the memory mechanisms of the human brain using machine learning models. From a more general point of view, catastrophic forgetting reflects the stability-plasticity dilemma, which is one of the several dilemmas to be addressed in learning systems: to retain the stored memory while learning new information. Different to the existing approaches, we introduce a Pareto-optimality based multi-objective learning framework for alleviating catastrophic learning. Compared to the single-objective learning methods, multi-objective evolutionary learning with the help of pseudo-rehearsal is shown to be more promising in dealing with the stability-plasticity dilemma.",10.1109/IJCNN.2006.247332,,2.0,
Personalized Neural Architecture Search,C. Kulbach; S. Thoma,2021 International Conference on Data Mining Workshops (ICDMW),2021.0,"Existing approaches for Neural Architecture Search (NAS) aim at efficiently maximizing individual or sets of objectives (e.g. high accuracy or a low number of parameters) by exploiting Reinforcement Learning (RL), evolutionary algorithms, or Bayesian optimization. Most multi-objective NAS algorithms assume that all objectives are fully known and require them to be broadly explored to successfully approximate the Pareto front, which results in computational expensive search algorithms. To address this problem, we propose an interactive machine learning approach based on preference elicitation which enables end-users to explore and find a custom loss function and can be directly used for State-of-the-Art single-objective black-box optimization. We integrate our approach into State-of-the-Art single objective NAS algorithms and evaluate it against multi-objective approaches on the NATS-Bench benchmark dataset. Furthermore, we show that diverse end-user preferences can be successfully approximated in terms of loss functions, leading to suitable neural architectures.",10.1109/ICDMW53433.2021.00077,Neural Architecture Search;Personalization;User Centric AI;Ranking,,
PASAPTO: Policy-aware Security and Performance Trade-off Analysis--Computation on Encrypted Data with Restricted Leakage,A. Fischer; J. Janneck; J. Kussmaul; N. Krätzschmar; F. Kerschbaum; E. Bodden,2020 IEEE 33rd Computer Security Foundations Symposium (CSF),2020.0,"This work considers the trade-off between security and performance when revealing partial information about encrypted data computed on. The focus of our work is on information revealed through control flow side-channels when executing programs on encrypted data. We use quantitative information flow to measure security, running time to measure performance and program transformation techniques to alter the trade-off between the two. Combined with information flow policies, we perform a policy-aware security and performance trade-off (PASAPTO) analysis. We formalize the problem of PASAPTO analysis as an optimization problem, prove the NP-hardness of the corresponding decision problem and present two algorithms solving it heuristically. We implemented our algorithms and combined them with the Dataflow Authentication (DFAuth) approach for outsourcing sensitive computations. Our DFAuth Trade-off Analyzer (DFATA) takes Java Bytecode operating on plaintext data and an associated information flow policy as input. It outputs semantically equivalent program variants operating on encrypted data which are policy-compliant and approximately Pareto-optimal with respect to leakage and performance. We evaluated DFATA in a commercial cloud environment using Java programs, e.g., a decision tree program performing machine learning on medical data. The decision tree variant with the worst performance is 357% slower than the fastest variant. Leakage varies between 0% and 17% of the input.",10.1109/CSF49147.2020.00024,,,
Throughput-Oriented Spatio-Temporal Optimization in Approximate High-Level Synthesis,M. T. Leipnitz; G. L. Nazar,2020 IEEE 38th International Conference on Computer Design (ICCD),2020.0,"Current and emerging systems for high-throughput applications, such as machine learning, cloud computing, and real-time video encoding demand real-time processing computations, heavily constrained by latency and power requirements. To deal with the increasing computational complexity, designers may resort to approximate accelerators for error-resilient compute-intensive kernels to meet such requirements with acceptable deviation from the exact implementation. However, since time-to-market is crucial when dealing with evolving applications, technologies, and standards, hand-crafting approximate accelerators may impose prohibitive development time and cost overheads. In this scenario, approximate High-Level Synthesis (HLS) methodologies have been proposed to deal with the complexity of exploring approximation techniques. Nevertheless, current tools are not suitable for exploring throughput optimizations, being instead constrained to perform specific improvements on area, power, and performance. In this work, we propose the use of HLS to generate Pareto-optimal accelerators for throughput-constrained applications. Particularly, we present a throughput-oriented approximate HLS methodology that explores both delay and area optimizations to increase the reuse over time and parallelism of such accelerators. Results show that our method is able to improve throughput by up to 80 % with no additional area costs or to sustain the same throughput of the exact design with about 45 % less area while introducing manageable error for most applications. Moreover, our method can attain throughput improvements of up to 18% when compared with recent works focusing only on performance or area optimizations, with no additional costs.",10.1109/ICCD50377.2020.00060,High-Level Synthesis;Approximate Computing;Design Space Exploration,1.0,
Automatic Fuzzy Clustering Using Non-Dominated Sorting Particle Swarm Optimization Algorithm for Categorical Data,T. P. Q. Nguyen; R. J. Kuo,IEEE Access,2019.0,"Categorical data clustering has been attracted a lot of attention recently due to its necessary in the real-world applications. Many clustering methods have been proposed for categorical data. However, most of the existing algorithms require the predefined number of clusters which is usually unavailable in real-world problems. Only a few works focused on automatic clustering, but mainly handled for numerical data. This study develops a novel automatic fuzzy clustering using non-dominated sorting particle swarm optimization (AFC-NSPSO) algorithm for categorical data. The proposed AFC-NSPSO algorithm can automatically identify the optimal number of clusters and exploit the clustering result with the corresponding selected number of clusters. In addition, a new technique is investigated to identify the maximum number of clusters in a dataset based on the local density. To select a final solution in the first Pareto front, some internal validation indices are used. The performance of the proposed AFC-NSPSO on the real-world datasets collected from the UCI machine learning repository exhibits effectiveness compared with some other existing automatic categorical clustering algorithms. Besides, this study also applies the proposed algorithm to analyze a real-world case study with an unknown number of clusters.",10.1109/ACCESS.2019.2927593,Automatic clustering;categorical data;local density;NSPSO,6.0,
A Chance Constrained Programming Based Multi-Criteria Decision Making Under Uncertainty,P. D. Pantula; S. S. Miriyala; K. Mitra,2019 Fifth Indian Control Conference (ICC),2019.0,"Multi-criteria decision making under uncertainty is a common practice followed in industries and academia. Among several types of uncertainty handling techniques, Chance Constrained Programming (CCP) is considered as an efficient and tractable approach provided one has accessibility to distribution of the data for uncertain parameters. However, the assumption that the uncertain parameters must follow some well-behaved probability distribution is a myth for most of the practical applications. This paper proposes a methodology to amalgamate machine learning algorithms with CCP and thereby make it data-driven. A novel fuzzy clustering mechanism is implemented to transcript the uncertain space such that the exact regions of uncertainty are identified. Subsequently, density based boundary point detection and Delaunay triangulation based boundary construction enable intelligent Sobol based sampling in these regions for use in CCP. The Fuzzy clustering mechanism used in the proposed method transforms the existing fuzzy C-means technique such that the decision variables are significantly reduced. This enables evolutionary optimizers to obtain better approximations of the uncertain space by identifying the true clusters. A highly nonlinear real life model for continuous casting from steelmaking industries is considered as a case study for testing the efficiency of data based CCP along with a comprehensive comparison between conventional CCP approach using box uncertainty set and proposed methodology. As the resulting CCP problem is multi-objective in nature, the Pareto solutions are obtained by NSGA II.",10.1109/INDIANCC.2019.8715586,Data Driven Chance Constrained Programming;Fuzzy Clustering;Multi-objective Optimization,1.0,
A Novel Feature Selection with Many-Objective Optimization and Learning Mechanism,L. Shu; F. He; X. Hu; H. Li,2021 IEEE 24th International Conference on Computer Supported Cooperative Work in Design (CSCWD),2021.0,"Feature selection is extremely important in machine learning and data mining. Typical two-objective feature selection methods aim to minimize the number of features and maximize classification performance. However, they overlook the fact that there may be multiple subsets with similar information content for a given cardinality. The paper presents a many-objective feature selection approach to address this problem. Firstly, we establish a five-objective optimization model, which consists of classification accuracy, the number of features, feature relevance, feature redundancy, and feature complementarity. Therefore, the proposed model can enlarge the search space with more Pareto solutions. Secondly, we propose a wrapper structure for many-objective feature selection, which integrates a learning algorithm. Thirdly, in order to reduce the computional overhead, we propose a filter structure, which separates the learning algorithm. For implementation, we adopt NSGA-III multi-objective evolutionary algorithm and extreme learning machine. The experiments on mainstream datasets confirm the superiority of the proposed method.",10.1109/CSCWD49262.2021.9437707,optimization driven design;intelligent cloud manufacturing;collaborative processing of big data;feature selection;classification;many-objective optimization;extreme learning machine,,
Ready-to-Fabricate RF Circuit Synthesis Using a Layout- and Variability-Aware Optimization-Based Methodology,F. Passos; E. Roca; R. Martins; N. Lourenço; S. Ahyoune; J. Sieiro; R. Castro-López; N. Horta; F. V. Fernández,IEEE Access,2020.0,"In this paper, physical implementations and measurement results are presented for several Voltage Controlled Oscillators that were designed using a fully-automated, layout- and variability-aware optimization-based methodology. The methodology uses a highly accurate model, based on machine-learning techniques, to characterize inductors, and a multi-objective optimization algorithm to achieve a Pareto-optimal front containing optimal circuit designs offering different performance trade-offs. The final outcome of the proposed methodology is a set of design solutions (with their GDSII description available and ready-to-fabricate) that need no further designer intervention. Two key elements of the proposed methodology are the use of an optimization algorithm linked to an off-the-shelf simulator and an inductor model that yield EM-like accuracy but with much shorter evaluation times. Furthermore, the methodology guarantees the same high level of robustness against layout parasitics and variability that an expert designer would achieve with the verification tools at his/her disposal. The methodology is technology-independent and can be used for the design of radio frequency circuits. The results are validated with experimental measurements on a physical prototype.",10.1109/ACCESS.2020.2980211,Integrated circuit synthesis;electronic design automation and methodology;inductors;metamodeling;radio frequency;voltage-controlled oscillator,3.0,
ParDen: Surrogate Assisted Hyper-Parameter Optimisation for Portfolio Selection,T. L. van Zyl; M. Woolway; A. Paskaramoorthy,2021 8th International Conference on Soft Computing & Machine Intelligence (ISCMI),2021.0,"Portfolio optimisation is a multi-objective optimisation problem (MOP), where an investor aims to optimise the conflicting criteria of maximising a portfolio’s expected return whilst minimising its risk and other costs. However, selecting a portfolio is a computationally expensive problem because of the cost associated with performing multiple evaluations on test data (""backtesting"") rather than solving the convex optimisation problem itself. In this research, we present ParDen, an algorithm for the inclusion of any discriminative or generative machine learning model as a surrogate to mitigate the computationally expensive backtest procedure. In addition, we compare the performance of alternative metaheuristic algorithms: NSGA-II, R-NSGA-II, NSGA-III, R-NSGA-III, U-NSGA-III, MO-CMA-ES, and COMO-CMA-ES. We measure performance using multi-objective performance indicators, including Generational Distance Plus, Inverted Generational Distance Plus and Hypervol-ume. We also consider meta-indicators, Success Rate and Average Executions to Success Rate, of the Hypervolume to provide more insight into the quality of solutions. Our results show that ParDen can reduce the number of evaluations required by almost a third while obtaining an improved Pareto front over the state-of-the-art for the problem of portfolio selection.",10.1109/ISCMI53840.2021.9654934,metaheuristics;genetic algorithms;surrogate modelling;multi-objective optimisation;portfolio selection;hyper-parameter optimisation,,
Multi-Objective Optimization for Personalized Prediction of Venous Thromboembolism in Ovarian Cancer Patients,M. E. Frésard; R. Erices; M. L. Bravo; M. Cuello; G. I. Owen; C. Ibáñez; M. Rodriguez-Fernandez,IEEE Journal of Biomedical and Health Informatics,2020.0,"Thrombotic events are one of the leading causes of mortality and morbidity related to cancer, with ovarian cancer having one of the highest incidence rates. The need to prevent these events through the prescription of adequate schemes of antithrombotic prophylaxis has motivated the development of models that aid the identification of patients at higher risk of thrombotic events with lethal consequences. However, antithrombotic prophylaxis increases the risk of bleeding and this risk depends on the class and intensity of the chosen antithrombotic prophylactic scheme, the clinical and personal condition of the patient and the disease characteristics. Moreover, the datasets used to obtain current models are imbalanced, i.e., they incorporate more patients who did not suffer thrombotic events than patients who experienced them what can lead to wrong predictions, especially for the clinically relevant patient group at high risk of thrombosis. Herein, predictive models based on machine learning were developed utilizing 121 high-grade serous ovarian carcinoma patients, considering the clinical variables of the patients and those typical of the disease. To properly manage the data imbalance, cost-sensitive classification together with multi-objective optimization was performed considering different combinations of metrics. In this way, five Pareto fronts and a series of optimal models with different false positive and false negative rates were obtained. With this novel approach to the development of clinical predictive models, personalized models can be developed, helping the clinician to achieve a better balance between the risk of bleeding and the risk of thrombosis.",10.1109/JBHI.2019.2943499,Clinical biomarkers;prediction models;ovarian cancer;venous thromboembolism;multi-objective optimization,6.0,
HL-Pow: A Learning-Based Power Modeling Framework for High-Level Synthesis,Z. Lin; J. Zhao; S. Sinha; W. Zhang,2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC),2020.0,"High-level synthesis (HLS) enables designers to customize hardware designs efficiently. However, it is still challenging to foresee the correlation between power consumption and HLS-based applications at an early design stage. To overcome this problem, we introduce HL-Pow, a power modeling framework for FPGA HLS based on state-of-the-art machine learning techniques. HL-Pow incorporates an automated feature construction flow to efficiently identify and extract features that exert a major influence on power consumption, simply based upon HLS results, and a modeling flow that can build an accurate and generic power model applicable to a variety of designs with HLS. By using HL-Pow, the power evaluation process for FPGA designs can be significantly expedited because the power inference of HL-Pow is established on HLS instead of the time-consuming register-transfer level (RTL) implementation flow. Experimental results demonstrate that HL-Pow can achieve accurate power modeling that is only 4.67% (24.02 mW) away from onboard power measurement. To further facilitate power-oriented optimizations, we describe a novel design space exploration (DSE) algorithm built on top of HL-Pow to trade off between latency and power consumption. This algorithm can reach a close approximation of the real Pareto frontier while only requiring running HLS flow for 20% of design points in the entire design space.",10.1109/ASP-DAC47756.2020.9045442,,4.0,
VeriGOOD-ML: An Open-Source Flow for Automated ML Hardware Synthesis,H. Esmaeilzadeh; S. Ghodrati; J. Gu; S. Guo; A. B. Kahng; J. K. Kim; S. Kinzer; R. Mahapatra; S. D. Manasi; E. Mascarenhas; S. S. Sapatnekar; R. Varadarajan; Z. Wang; H. Xu; B. R. Yatham; Z. Zeng,2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD),2021.0,"This paper introduces VeriGOOD-ML, an automated methodology for generating Verilog with no human in the loop, starting from a high-level description of a machine learning (ML) algorithm in a standard format such as ONNX. The Verilog RTL is then translated through a back-end design flow to GDSII, driven by a design planning approach that is well tailored to the macro-intensive nature of ML platforms. VeriGOOD-ML uses three approaches to build ML hardware: the TABLA platform uses a dataflow architecture that is well suited to non-DNN ML algorithms; the GeneSys platform, with a systolic array and a SIMD array, is optimized for implementing DNNs; and the Axiline approach synthesizes small ML algorithms by hardcoding the structure of the algorithm into hardware, thus trading off flexibility for performance and power. The overall approach explores the design space of platform configurations and Pareto-optimal-PPA back-end implementations to yield designs that represent different tradeoffs at the algorithmic level between area, power, performance, and execution time. The overall methodology, from architecture to back-end design to hardware implementation, is described in this paper, and the results of VeriGOOD-ML are demonstrated on a set of ML benchmarks.",10.1109/ICCAD51958.2021.9643449,,,
Shallow Versus Deep Neural Networks in Gear Fault Diagnosis,G. Cirrincione; R. R. Kumar; A. Mohammadi; S. H. Kia; P. Barbiero; J. Ferretti,IEEE Transactions on Energy Conversion,2020.0,"Accurate gear defect detection in induction machine-based systems is a fundamental issue in several industrial applications. At this aim, shallow neural networks, i.e., architectures with only one hidden layer, have been used after a feature extraction step from vibration, torque, acoustic pressure and electrical signals. Their additional complexity is justified by their ability in extracting its own features and in the very high-test classification rates. These signals are here analyzed, both geometrically and topologically, in order to estimate the class manifolds and their reciprocal positioning. At this aim, the different states of the gears are studied by using linear (Pareto charts, biplots, principal angles) and nonlinear (curvilinear component analysis) techniques, while the class clusters are visualized by using the parallel coordinates. It is deduced that the class manifolds are compact and well separated. This result justifies the use of a shallow neural network, instead of a deep one, as already remarked in the literature, but with no theoretical justification. The experimental section confirms this assertion, and also compares the shallow neural network results with the other machine learning techniques used in the literature.",10.1109/TEC.2020.2978155,Classification algorithm;fault detection;fault diagnosis;gears;induction motors;multilayer perceptron;neural networks;principal component analysis;vibrations,6.0,
Predicting best design trade-offs: A case study in processor customization,M. Zuluaga; E. Bonilla; N. Topham,"2012 Design, Automation & Test in Europe Conference & Exhibition (DATE)",2012.0,"Given the high level description of a task, many different hardware modules may be generated while meeting its behavioral requirements. The characteristics of the generated hardware can be tailored to favor energy efficiency, performance, accuracy or die area. The inherent trade-offs between such metrics need to be explored in order to choose a solution that meets design and cost expectations. We address the generic problem of automatically deriving a hardware implementation from a high-level task description. In this paper we present a novel technique that exploits previously explored implementation design spaces in order to find optimal trade-offs for new high-level descriptions. This technique is generalizable to a range of high-level synthesis problems in which trade-offs can be exposed by changing the parameters of the hardware generation tool. Our strategy, based upon machine learning techniques, models the impact of the parameterization of the tool on the target objectives, given the characteristics of the input. Thus, a predictor is able to suggest a subset of parameters that are likely to lead to optimal hardware implementations. The proposed method is evaluated on a resource sharing problem which is typical in high level synthesis, where the trade-offs between area and performance need to be explored. In this case study, we show that the technique can reduce by two orders of magnitude the number of design points that need to be explored in order to find the Pareto optimal solutions.",10.1109/DATE.2012.6176647,,6.0,
An Agent-Based Hierarchical Bargaining Framework for Power Management of Multiple Cooperative Microgrids,K. Dehghanpour; H. Nehrir,IEEE Transactions on Smart Grid,2019.0,"In this paper, we propose an agent-based hierarchical power management model in a power distribution system composed of several microgrids (MGs). At the lower level of the model, multiple MGs bargain with each other to cooperatively obtain a fair, and Pareto-optimal solution to their power management problem, employing the concept of Nash bargaining solution and using a distributed optimization framework. At the highest level of the model, a distribution system power supplier, e.g., a utility company, interacts with both the cluster of the MGs and the wholesale market. The goal of the utility company is to facilitate power exchange between the regional distribution network consisting of multiple MGs and the wholesale market to achieve its own private goals. The power exchange is controlled through dynamic energy pricing at the distribution level, at the day-ahead and real-time stages. To implement energy pricing at the utility company level, an iterative machine learning mechanism is employed, where the utility company develops a price-sensitivity model of the aggregate response of the MGs to the retail price signal through a learning process. This learned model is then used to perform optimal energy pricing. To verify its applicability, the proposed decision model is tested on a system with multiple MGs, with each MG having different load/generation data.",10.1109/TSG.2017.2746014,Microgrids;bargaining games;distributed optimization;agent-based modeling;power management,47.0,
Yield Learning for Complex FinFET Defect Mechanisms Based on Volume Scan Diagnosis Results,H. Tang; M. Sharma; W. -T. Cheng; G. Veda; D. Gehringer; M. Knowles; J. D’Souza; K. Sekar; N. Bawaskar; Y. Pan,2019 30th Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC),2019.0,"Device complexity is reaching all-time highs with the adoption of high aspect ratio FinFETs created using multi- patterning process technologies. Simultaneously, new product segments such as AI and automotive are being fabricated on such advanced processes. In this dynamic environment, new complex defect modes have challenged manufacturers to ramp and sustain quality and yield at advanced nodes. Process variability of the standard cell introduces new transistor-level defect modes. Meanwhile the cost of traditional failure analysis has continued to skyrocket. How will the industry reduce the defect-rate and ramp yield to meet these aggressive market demands? This article will detail a new breakthrough in the field of scan diagnosis using machine learning. For the first time, cell-internal defects are detected, diagnosed and now resolved with RCD (Root Cause Deconvolution). Experimental FA results will show how RCD is used to build an accurate defect pareto and pick targeted die for FA for faster and cheaper root cause identification.",10.1109/ASMC.2019.8791755,scan diagnosis;volume diagnosis;yield analysis;root cause identification;cell aware diagnosis,2.0,
Solving Combinatorial Multi-Objective Bi-Level Optimization Problems Using Multiple Populations and Migration Schemes,R. Said; S. Bechikh; A. Louati; A. Aldaej; L. B. Said,IEEE Access,2020.0,"Many decision making situations are characterized by a hierarchical structure where a lower-level (follower) optimization problem appears as a constraint of the upper-level (leader) one. Such kind of situations is usually modeled as a BLOP (Bi-Level Optimization Problem). The resolution of the latter usually has a heavy computational cost because the evaluation of a single upper-level solution requires finding its corresponding (near) optimal lower-level one. When several objectives are optimized in each level, the BLOP becomes a multi-objective task and more computationally costly as the optimum corresponds to a whole non-dominated solution set, called the PF (Pareto Front). Despite the considerable number of recent works in multi-objective evolutionary bi-level optimization, the number of methods that could be applied to the combinatorial (discrete) case is much reduced. Motivated by this observation, we propose in this paper an Indicator-Based version of our recently proposed Co-Evolutionary Migration-Based Algorithm (CEMBA), that we name IB-CEMBA, to solve combinatorial multi-objective BLOPs. The indicator-based search choice is justified by two arguments. On the one hand, it allows selecting the solution having the maximal marginal contribution in terms of the performance indicator from the lower-level PF. On the other hand, it encourages both convergence and diversity at the upper-level. The comparative experimental study reveals the outperformance of IB-CEMBA on a multi-objective bi-level production-distribution problem. From the effectiveness viewpoint, the upper-level hyper-volume values and inverted generational distance ones vary in the intervals [0.8500, 0.9710] and [0.0072, 0.2420], respectively. From the efficiency viewpoint, IB-CEMBA has a good reduction rate of the Number of Function Evaluations (NFEs), lying in the interval [30.13%, 54.09%]. To further show the versatility of our algorithm, we have developed a case study in machine learning, and more specifically we have addressed the bi-level multi-objective feature construction problem.",10.1109/ACCESS.2020.3013568,Combinatorial bi-level multi-objective optimization;computational cost;indicator-based evolutionary algorithms;population decomposition;migration schemes,4.0,
Determination of Event Patterns for Complex Event Processing Using Fuzzy Unordered Rule Induction Algorithm with Multi-objective Evolutionary Feature Subset Selection,N. Mehdiyev; J. Krumeich; D. Werth; P. Loos,2016 49th Hawaii International Conference on System Sciences (HICSS),2016.0,"Complex Event Processing (CEP) is an emerging technology to process streaming data and to generate response actions in real time. CEP systems treat all sensor data as primitive events and attempt to detect semantically high level events and related actions by matching them using event patterns. These event patterns are the rules which combine primitive events according to temporal, logical, or spatial correlations among them. Although event patterns (decision rules) can be provided by experts in simplistic scenarios, the huge amount of sensor data makes this unfeasible. The main purpose of the underlying paper is replacing manual identification of event patterns. Considering the uncertainty related to the sensor data, Fuzzy Unordered Rule Induction Algorithm (FURIA) was implemented to identify event patterns after selecting the relevant feature subset using Elitist Pareto-based Multi-Objective Evolutionary Algorithm for Diversity Reinforcement (ENORA). The results were compared to the alternative machine learning approaches.",10.1109/HICSS.2016.216,Complex Event Processing;Fuzzy Unordered Rule Induction;Multi-Objective Evolutionary Feature Subset Selection,10.0,
Corrections to “Pareto-Based Multiobjective Machine Learning: An Overview and Case Studies” [May 08 397-415],Y. Jin; B. Sendhoff,"IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",2009.0,"In the above titled paper (ibid., vol. 38, no. 3, pp. 397-415, May 08), there are three sites where an inequality is put wrongly.  The corrections are presented here.",10.1109/TSMCC.2009.2018893,,8.0,
Guest Editorial Evolutionary Computation Meets Deep Learning,W. Ding; W. Pedrycz; G. G. Yen; B. Xue,IEEE Transactions on Evolutionary Computation,2021.0,"Deep learning is a timely research direction in machine learning, where breakthrough progress has been made in both academe and industries, bringing promising results in speech recognition, computer vision, industrial control and automation, etc. The motivation of deep learning is primarily to establish a model to simulate the neural connection structure of the human brain. While dealing with complex tasks, deep learning adopts a number of transformation stages to deliver the in-depth description and interpretation of the data. Deep learning achieves exceptional power and flexibility by learning to represent the task through a nested hierarchy of layers, with more abstract representations formed successively in terms of less abstract ones. One of the key issues of existing deep learning approaches is that the meaningful representations can be learned only when their hyperparameter settings are properly specified beforehand, and general parameters are learned during the training process. Until now, not much research has been dedicated to automatically set the hyperparameters, and accurately find the globally optimal general parameters. However, this problem can be formulated as optimization problems, including discrete optimization, constrained optimization, large-scale global optimization, and multiobjective optimization, by engaging mechanisms of evolutionary computation.",10.1109/TEVC.2021.3096336,,,
Keynote talks: Evolutionary feature selection and dimensionality reduction,M. Zhang,2017 21st Asia Pacific Symposium on Intelligent and Evolutionary Systems (IES),2017.0,"In data mining and machine learning, many real-world problems such as bio-data classification and biomarker detection, image analysis, text mining often involve a large number of features/attributes. However, not all the features are essential since many of them are redundant or even irrelevant, and the useful features are typically not equally important. Using all the features for classification or other data mining tasks typically does not produce good results due to the big dimensionality and the large search space. This problem can be solved by feature selection to select a small subset of original (relevant) features or feature construction to create a smaller set of high-level features using the original low-level features. Feature selection and construction are very challenging tasks due to the large search space and feature interaction problems. Exhaustive search for the best feature subset of a given dataset is practically impossible in most situations. A variety of heuristic search techniques have been applied to feature selection and construction, but most of the existing methods still suffer from stagnation in local optima and/or high computational cost. Due to the global search potential and heuristic guidelines, evolutionary computation techniques such as genetic algorithms, genetic programming, particle swarm optimisation, ant colony optimisation, differential evolution and evolutionary multiobjective optimisation have been recently used for feature selection and construction for dimensionality reduction, and achieved great success. Many of these methods only select/construct a small number of important features, produce higher accuracy, and generated small models that are efficient on unseen data. Evolutionary computation techniques have now become an important means for handle big dimensionality and feature selection and construction. The talk will introduce the general framework within which evolutionary feature selection and construction can be studied and applied, sketching a schematic taxonomy of the field and providing examples of successful real-world applications. The application areas to be covered will include bio-data classification and biomarker detection, image analysis and object recognition and pattern classification, symbolic regression, network security and intrusion detection, and text mining. EC techniques to be covered will include genetic algorithms, genetic programming, particle swarm optimisation, differential evolution, ant colony optimisation, artificial bee colony optimisation, and evolutionary multi-objective optimisation. We will show how such evolutionary computation techniques can be effectively applied to feature selection/construction and dimensionality reduction and provide promising results.",10.1109/IESYS.2017.8233551,,,
[Front cover],,2010 Ninth International Conference on Machine Learning and Applications,2010.0,"The following topics are dealt with: machine learning; data acquisition, cleansing and categorization; planning and reinforcement learning; supervised learning; multiparty, multimodal and multiobjective learning; feature selection; probabilistic and model based learning; similarity learning; pattern recognition; kernel learning; unsupervised learning; bioinformatics and computational biology; parallel computing; ensemble learning; Bayesian learning; incremental learning; biomedical literature analysis and text retrieval; dynamic learning; multimedia data; and cancer and radiation therapy.",10.1109/ICMLA.2010.172,,,
Table of contents,,2014 IEEE Congress on Evolutionary Computation (CEC),2014.0,The following topics are dealt with: computational intelligence; computational games; memetic computing; evolutionary computer vision; bio-inspired computation; evolutionary multiobjective optimization; decision making; differential evolution; combinatorial optimization; artificial bee colony algorithms; swarm intelligence; real-world engineering optimization; complex networks; machine learning; statistical learning; nature-inspired constrained optimization; bioinformatics; data mining; evolutionary games; multiagent systems; hybrid evolutionary computational methods; large scale global optimization; cloud computing; learning classifier systems; opposition-based learning; genetic programming; hyperheuristics; predictive maintenance; cultural algorithms; knowledge extraction; single objective numerical optimization; process mining; estimation of distribution algorithms; biomedical applications; robotics applications; engineering applications; constraint handling; preference handling; and firework algorithms.,10.1109/CEC.2014.6900664,,,
[Title page i],,"2015 IEEE International Conference on Systems, Man, and Cybernetics",2015.0,"The following topics are dealt with: Systems Science and Engineering; Decision Support Systems Based on Multicriteria Models; Distributed Adaptive Systems; Intelligent Learning in Control Systems; Robotics, Human Machine Interface, and Haptics; Intelligent Manufacturing Processes and Innovative Applications; Discrete Event and Hybrid Systems; Meta-synthesis and Complex Systems; Conflict Resolution; Grey Systems: Theory and Applications; Global Energy Internet; Medical Mechatronics; SMC: Systems Science; Human-Machine Systems; Automation Design and Intelligence Control Applications; Risk Management of Information and Control Systems; Proactive Health Care Systems: Methodologies and Applications; Human Centered Transportation Systems; Collaborative Wireless Sensor Networks and Internet of Things; Collaborative Technologies and Applications; Interaction-Centered Analytics and Design for Human-Centric Systems; Assistive and Rehabilitative Technology and Applications; Cybernetics; Intelligent Internet Systems; Fuzzy Methods for Uncertain Data Mining; Quantum Cybernetics and Learning Systems; Soft Computing; Machine Learning and Information Retrieval in Big Data Environment; Matrix and Tensor Analysis for Big Vision; Innovations in Fuzzy Systems and Applications; Knowledge Engineering in Medical Informatics; Medical and Health Care Engineering; Cognitive Agents and Robotics for Human-Centric Systems: New Models and Challenges; Intelligent Healthcare; Intelligent Vehicle Systems and Control; Granular Computing; Tensor Product Applications; Computing Aspects of Big Data; Emerging Technologies and Applications in Computer Intelligence; Computational Intelligence Methods for Big Data Analytics; BMI Workshop; Shared Control; Multimodal Brain Computer Interface and Physiological Computing; Real World Applications of Brain Computer Interface Systems; User-Training in EEG-Based Brain-Computer Interfaces; Computational Intelligence and Machine Learning for BCI.",10.1109/SMC.2015.1,,,
Table of contents,,"2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",2014.0,"The following topics are dealt with: collaborative wireless sensor networks; agent-based modeling; human performance modeling; biometric systems and bioinformatics; heuristic algorithms; advance on discrete event systems: theory and applications; image processing/pattern recognition; innovative technologies and applications in computer intelligence; machine vision; optimization; swarm intelligence; decision support systems based on multicriteria models; sytems engineering; ecological interface design: 25 years and counting; computational intelligence; human-machine systems; medical informatics; cybernetics for informatics; workshop on system engineering human-centered intelligent vehicles; computational awareness; computational intelligence methods for big data processing; modelling, analysis and control of human-machine motor coordination; intelligent robots and systems; human-machine systems; distributed intelligent systems & micro and/or nano systems; neural networks and applications; frontiers in model-based systems engineering; brain machine interfaces for practical applications; non-BMI assistive technologies; collaborative technologies and applications; emerging technologies in medical mechatronics; fuzzy systems and applications; discrete event systems and Petri nets; distributed adaptive systems; shared control; human centered transportation systems; conflict resolution; large-scale system of systems; machine learning; grey systems: theory and applications; proactive health care systems: methodologies and applications; intelligent learning in control systems; intelligent Internet systems; brain computer interface for communication, control and rehabilitation; assistive technology & human machine interface; human factors; expert and knowledge-based systems; friendly design for disability in intelligent transportation systems; human machine interface, haptics and robotics; assistive and rehabilitative technology; BMI methods; uncertainly in big data; data science for big data; collaborative processing of big data; automated intelligence and innovative applications; evolutionary computation; model-based healthcare; knowledge engineering in medicine and health informatics; manufacturing systems and automation; medical informatics; fault monitoring and diagnosis; granular computing; intelligent power grid; eye-tracking; image quality assessment, security, and systems; control of uncertain systems; enterprise architecture and engineering; soft computing; intelligent media and new-generarion software; and mechatronics.",10.1109/SMC.2014.6973874,,,
The 2005 IEEE Congress on Evolutionary Computation,,2005 IEEE Congress on Evolutionary Computation,2005.0,"The following topics are dealt with: evolutionary multiobjective optimization; bioinformatics and bioscience applications; particle swarm optimisation; complex adaptive systems; real-parameter optimization; evolutionary design; artificial life; classifier systems; machine learning; data mining; genome informatics; combinatorial and numerical optimization; games; hybridization; adaptation; evolutionary clustering; evolvable hardware; genetic programming; probabilistic models; ant systems, collective behaviour and co-evolution; dynamic and uncertain environments; and planning and scheduling",10.1109/CEC.2005.1554789,,,
The 2005 IEEE Congress on Evolutionary Computation (IEEE Cat. No. 05TH8834),,2005 IEEE Congress on Evolutionary Computation,2005.0,"The following topics are dealt with: evolutionary multiobjective optimization; bioinformatics and bioscience applications; particle swarm optimisation; complex adaptive systems; real-parameter optimization; evolutionary design; artificial life; classifier systems; machine learning; data mining; genome informatics; combinatorial and numerical optimization; games; hybridization; adaptation; evolutionary clustering; evolvable hardware; genetic programming; probabilistic models; ant systems, collective behaviour and co-evolution; dynamic and uncertain environments; and planning and scheduling.",10.1109/CEC.2005.1554654,,,
Table of contents,,2013 Brazilian Conference on Intelligent Systems,2013.0,The following topics are dealt with: machine learning and data mining; agent-based and multi-agent systems; evolutionary computation and swarm intelligence; pattern recognition and cluster analysis; logic-based knowledge representation and reasoning; constraints and search planning and scheduling; and multiobjective optimization.,10.1109/BRACIS.2013.4,,,
Table of contents,,"2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",2012.0,The following topics are dealt with: grey system; intelligent systems; intelligent control; biological systems; biological evolution; innovative soft computing methodologies; pattern recognition; medical imaging; computer vision; body signal analysis; brain signal analysis; multiagent systems; decision support systems; multicriteria models; business applications; discrete event; machine vision; collaborative manufacturing; supply chains; human-computer interaction; collaborative computing; Petri net; biometric systems; machine learning; health care technology; medical care technology; collaborative technologies; cognitive engineering; human centered transportation systems; new-generation software; intelligent media; complex systems; artificial intelligence; human perception; human-oriented convergence research; intelligent learning; control systems; intelligent multimedia-mobile communications; information assurance; intelligent Internet systems; intelligent vehicular systems; neural computation; knowledge representation; data mining; nature-inspired algorithms; computational collective intelligence; computational awareness; human machine interface; system modeling; ambient-adaptive intelligent lighting systems; knowledge acquisition; knowledge mining; video processing; image processing; communication systems; human centered tagging; shared control; and 3D visualization.,10.1109/ICSMC.2012.6377666,,,
Table of contents,,2011 IEEE Symposium on Computational Intelligence in Multicriteria Decision-Making (MDCM),2011.0,The following topics are dealt with: computational intelligence; decision making; evolutionary multiobjective optimization; and machine learning.,10.1109/SMDCM.2011.5949294,,,
[Title Page i],,2012 Brazilian Symposium on Neural Networks,2012.0,The following topics are dealt with: unsupervised machine learning; bioinformatics; semisupervised learning; neural networks; robotics; evolutionary systems; swarm intelligence; hybrid intelligent systems; multiobjective optimization and pattern recognition.,10.1109/SBRN.2012.1,,,
[Front cover],,2009 Ninth International Conference on Intelligent Systems Design and Applications,2009.0,The following topics are dealt with: intelligent systems design; evolutionary algorithms; soft computing in intelligent agents and Web technologies; genetic fuzzy systems; intelligent image processing and artificial vision; hybrid learning for artificial neural networks; swarm intelligence; tags and recommendations in Web 2.0; representation and approximation of fuzzy numbers; from business intelligence to business artificial intelligence; evolutionary multiobjective optimization design and applications; designing comprehensible intelligent systems; computational intelligence in business management and risk analysis; hybrid metaheuristics; intelligent systems for data reduction; intelligent signal and image analysis in remote sensing; human monitoring and machine learning strategies; innovative networking and communication techniques; intelligent control and automation; intelligent Internet modeling; consensus and decision making; education and learning models; provisioning of smart services in ontology-based systems; intelligent e-learning systems; intelligent systems for industrial processes; intelligent data mining; educational data mining; intelligent knowledge management; bioinformatics; neural networks and neuro-fuzzy systems.,10.1109/ISDA.2009.262,,,
Table of contents,,2014 Brazilian Conference on Intelligent Systems,2014.0,The following topics are dealt with: machine learning; data mining; natural language processing; social media analysis; recommender systems; neural networks; agents; planning; scheduling; regression; classification; knowledge representation; reasoning; information retrieval; pattern recognition; fuzzy systems; multiobjective optimization; concept drift; adaptive method; dynamic model and complex network.,10.1109/BRACIS.2014.4,,,
Seventh International Conference on Intelligent Systems Design and Applications - Title page,,Seventh International Conference on Intelligent Systems Design and Applications (ISDA 2007),2007.0,The following topics are dealt with: intelligent business systems; intelligent agents; artificial neural networks; data clustering; bioinformatics; intelligent control; intelligent educational systems; fuzzy systems; genetic algorithms; genetic programming; intelligent image processing; intelligent Internet modeling; intelligent knowledge management; machine learning; intelligent data mining; natural language processing; neural systems; mobile robots; computer security; information security; intelligent signal processing; swarm intelligent systems; intelligent text categorization; intelligent text clustering; parallel evolutionary computation; biometrics; evolutionary multiobjective optimisations; nature imitation methods; dynamic intelligent systems.,10.1109/ISDA.2007.167,,,
Keynote3: Contention and disruption,J. van den Herik,2016 Conference on Technologies and Applications of Artificial Intelligence (TAAI),2016.0,"The development of science is clear. From 1950 to 1990 we lived in a world of Contention, with as main question: Will Contention between Paradigms lead to a Paradigm Shift? This development is nicely described by Popper (Logic of Scientific Discovery), Kuhn (The Structure of Scientific Revolutions), Lakatos (The Methodology of Scientific Research Programmes), and Feyerabend (Against Method). In the world of Games, this development is seen in the transition from Minimax to Monte Carlo Tree Search (MCTS). Apparently, the successor of Contention is called Disruption. Currently, we live in a world full of disruptions (1990-2030). In the lecture, I will show the current development by Daniel Dennett (Consciousness Explained, 1990), Richard Susskind (The Future of Law, 1998), Nick Bostron (Superintelligence, 2014), and my own thoughts on Intuition is Programmable (Van den Herik, 2016). The latter is extremely well identified by the power of Deep Learning in the Game of Go (congratulations to Aja Huang). Around 2030 we may expect to see a quantum computer in operation. It will not only produce prime numbers, but also give us the solution of the game of chess (draw or a win for White), and thereafter even for Go (i.e., at a later date). Next to game results, we will observe a continuous development: from decisions made by humans to decisions made by computers. Here, moral constraints are important. Examples will be given.",10.1109/TAAI.2016.7880106,,,
Processor Design Space Exploration via Statistical Sampling and Semi-Supervised Ensemble Learning,"Li, DD; Yao, SZ; Wang, Y",IEEE ACCESS,2018.0,"The design space exploration (DSE) has become a major challenge in microprocessors design due to the increasing complexity of microprocessor architecture and the extremely time-consuming software simulation technology. To more effectively and efficiently perform DSE, recently machine learning techniques are widely explored to build predictive models with a small set of simulations. However, most previous models are supervised models and the training samples are randomly selected. Thus they still suffered from high simulation cost or low prediction accuracy. In order to minimize the simulation overhead for DSE, this paper proposes an efficient DSE method which combines Latin hypercube sampling and semi-supervised ensemble learning technique. Latin hypercube sampling is first employed to select a small set of representative design points for simulation. Then a semi-supervised learning based AdaBoost model (SemiBoost) is proposed to predict the responses of the configurations that have not been simulated. We conduct extensive evaluations on the benchmarks of SPEC CPU2006 suite, and the experimental results demonstrate that the proposed SemiBoost model is superior to existing state-of-the-art models in terms of both efficiency and effectiveness.",10.1109/ACCESS.2018.2831079,Design space exploration; Latin hypercube sampling; adaboost; microprocessor design,,
Label propagation through minimax paths for scalable semi-supervised learning,"Kim, KH; Choi, S",PATTERN RECOGNITION LETTERS,2014.0,"Semi-supervised learning (SSL) is attractive for labeling a large amount of data. Motivated from cluster assumption, we present a path-based SSL framework for efficient large-scale SSL, propagating labels through only a few important paths between labeled nodes and unlabeled nodes. From the framework, minimax paths emerge as a minimal set of important paths in a graph, leading us to a novel algorithm, minimax label propagation. With an appropriate stopping criterion, learning time is (1) linear with respect to the number of nodes in a graph and (2) independent of the number of classes. Experimental results show the superiority of our method over existing SSL methods, especially on large-scale data with many classes. (C) 2014 Elsevier B.V. All rights reserved.",10.1016/j.patrec.2014.02.020,Label propagation; Minimax path; Semi-supervised learning,,
A Large Size image Classification Method Based on Semi-supervised Learning,"Luo, D; Wang, XL",RECENT ADVANCES IN ELECTRICAL & ELECTRONIC ENGINEERING,2020.0,"Background: Semi-supervised learning in the machine learning community has received widespread attention. Semi-supervised learning can use a small number of tagged samples and a large number of untagged samples for efficient learning. Methods: In 2014, Kim proposed a new semi-supervised learning method: the minimax label propagation (MMLP) method. This method reduces time complexity to O (n), with a smaller computation cost and stronger classification ability than traditional methods. However, classification results are not accurate in large-scale image classifications. Thus, in this paper, we propose a semisupervised image classification method, which is an MMLP-based algorithm. The main idea is threefold: (1) Improving connectivity of image pixels by pixel sampling to reduce the image size, at the same time, reduce the diversity of image characteristics; (2) Using a recall feature to improve the MMLP algorithm; (3) through classification mapping, gaining the classification of the original data from the classification of the data reduction. Results: In the end, our algorithm also gains a minimax path from untagged samples to tagged samples. The experimental results proved that this algorithm is applicable to semi-supervised learning on small-size and that it can also gain better classification results for large-size image at the same time. Conclusion: In our paper, considering the connectivity of the neighboring matrix and the diversity of the characteristics, we used meanshift clustering algorithm, next we will use fuzzy energy clustering on our algorithm. We will study the function of these paths.",10.2174/1874476105666190830110150,Graph-based semi-supervised learning; MMLP algorithm; data reduction; recall feature; classification mapping; neural network models,,
On the minimax optimality and superiority of deep neural network learning over sparse parameter spaces,"Hayakawa, S; Suzuki, T",NEURAL NETWORKS,2020.0,"Deep learning has been applied to various tasks in the field of machine learning and has shown superiority to other common procedures such as kernel methods. To provide a better theoretical understanding of the reasons for its success, we discuss the performance of deep learning and other methods on a nonparametric regression problem with a Gaussian noise. Whereas existing theoretical studies of deep learning have been based mainly on mathematical theories of well-known function classes such as Holder and Besov classes, we focus on function classes with discontinuity and sparsity, which are those naturally assumed in practice. To highlight the effectiveness of deep learning, we compare deep learning with a class of linear estimators representative of a class of shallow estimators. It is shown that the minimax risk of a linear estimator on the convex hull of a target function class does not differ from that of the original target function class. This results in the suboptimality of linear methods over a simple but non-convex function class, on which deep learning can attain nearly the minimax-optimal rate. In addition to this extreme case, we consider function classes with sparse wavelet coefficients. On these function classes, deep learning also attains the minimax rate up to log factors of the sample size, and linear methods are still suboptimal if the assumed sparsity is strong. We also point out that the parameter sharing of deep neural networks can remarkably reduce the complexity of the model in our setting. (C) 2019 The Author(s). Published by Elsevier Ltd.",10.1016/j.neunet.2019.12.014,Neural network; Deep learning; Linear estimator; Nonparametric regression; Minimax optimality,,
Combining Pareto-optimal clusters using supervised learning for identifying co-expressed genes,"Maulik, U; Mukhopadhyay, A; Bandyopadhyay, S",BMC BIOINFORMATICS,2009.0,"Background: The landscape of biological and biomedical research is being changed rapidly with the invention of microarrays which enables simultaneous view on the transcription levels of a huge number of genes across different experimental conditions or time points. Using microarray data sets, clustering algorithms have been actively utilized in order to identify groups of co-expressed genes. This article poses the problem of fuzzy clustering in microarray data as a multiobjective optimization problem which simultaneously optimizes two internal fuzzy cluster validity indices to yield a set of Pareto-optimal clustering solutions. Each of these clustering solutions possesses some amount of information regarding the clustering structure of the input data. Motivated by this fact, a novel fuzzy majority voting approach is proposed to combine the clustering information from all the solutions in the resultant Pareto-optimal set. This approach first identifies the genes which are assigned to some particular cluster with high membership degree by most of the Pareto-optimal solutions. Using this set of genes as the training set, the remaining genes are classified by a supervised learning algorithm. In this work, we have used a Support Vector Machine (SVM) classifier for this purpose. Results: The performance of the proposed clustering technique has been demonstrated on five publicly available benchmark microarray data sets, viz., Yeast Sporulation, Yeast Cell Cycle, Arabidopsis Thaliana, Human Fibroblasts Serum and Rat Central Nervous System. Comparative studies of the use of different SVM kernels and several widely used microarray clustering techniques are reported. Moreover, statistical significance tests have been carried out to establish the statistical superiority of the proposed clustering approach. Finally, biological significance tests have been carried out using a web based gene annotation tool to show that the proposed method is able to produce biologically relevant clusters of co-expressed genes. Conclusion: The proposed clustering method has been shown to perform better than other well-known clustering algorithms in finding clusters of co-expressed genes efficiently. The clusters of genes produced by the proposed technique are also found to be biologically significant, i.e., consist of genes which belong to the same functional groups. This indicates that the proposed clustering method can be used efficiently to identify co-expressed genes in microarray gene expression data. Supplementary Website The pre-processed and normalized data sets, the matlab code and other related materials are available at http://anirbanmukhopadhyay.50webs.com/mogasvm.html.",10.1186/1471-2105-10-27,,,
Using deep learning to predict beam-tunable Pareto optimal dose distribution for intensity-modulated radiation therapy,"Bohara, G; Barkousaraie, AS; Jiang, S; Nguyen, D",MEDICAL PHYSICS,2020.0,"Purpose Many researchers have developed deep learning models for predicting clinical dose distributions and Pareto optimal dose distributions. Models for predicting Pareto optimal dose distributions have generated optimal plans in real time using anatomical structures and static beam orientations. However, Pareto optimal dose prediction for intensity-modulated radiation therapy (IMRT) prostate planning with variable beam numbers and orientations has not yet been investigated. We propose to develop a deep learning model that can predict Pareto optimal dose distributions by using any given set of beam angles, along with patient anatomy, as input to train the deep neural networks. We implement and compare two deep learning networks that predict with two different beam configuration modalities. Methods We generated Pareto optimal plans for 70 patients with prostate cancer. We used fluence map optimization to generate 500 IMRT plans that sampled the Pareto surface for each patient, for a total of 35 000 plans. We studied and compared two different models, Models I and II. Although they both used the same anatomical structures - including the planning target volume (PTV), organs at risk (OARs), and body - these models were designed with two different methods for representing beam angles. Model I directly uses beam angles as a second input to the network as a binary vector. Model II converts the beam angles into beam doses that are conformal to the PTV. We divided the 70 patients into 54 training, 6 validation, and 10 testing patients, thus yielding 27 000 training, 3000 validation, and 5000 testing plans. Mean square loss (MSE) was taken as the loss function. We used the Adam optimizer with a default learning rate of 0.01 to optimize the network's performance. We evaluated the models' performance by comparing their predicted dose distributions with the ground truth (Pareto optimal) dose distribution, in terms of dose volume histogram (DVH) plots and evaluation metrics such as PTV D-98, D-95, D-50, D-2, D-max, D-mean, Paddick Conformation Number, R50, and Homogeneity index. Results Our deep learning models predicted voxel-level dose distributions that precisely matched the ground truth dose distributions. The DVHs generated also precisely matched the ground truth. Evaluation metrics such as PTV statistics, dose conformity, dose spillage (R50), and homogeneity index also confirmed the accuracy of PTV curves on the DVH. Quantitatively, Model I's prediction error of 0.043 (confirmation), 0.043 (homogeneity), 0.327 (R50), 2.80% (D95), 3.90% (D98), 0.6% (D50), and 1.10% (D2) was lower than that of Model II, which obtained 0.076 (confirmation), 0.058 (homogeneity), 0.626 (R50), 7.10% (D95), 6.50% (D98), 8.40% (D50), and 6.30% (D2). Model I also outperformed Model II in terms of the mean dose error and the max dose error on the PTV, bladder, rectum, left femoral head, and right femoral head. Conclusions Treatment planners who use our models will be able to use deep learning to control the trade-offs between the PTV and OAR weights, as well as the beam number and configurations in real time. Our dose prediction methods provide a stepping stone to building automatic IMRT treatment planning.",10.1002/mp.14374,beam tunable; deep learning; intensity-modulated radiation therapy; neural networks; pareto optimality; prostate cancer,,
Human Gender Classification Using Transfer Learning via Pareto Frontier CNN Networks,"Islam, MM; Tasnim, N; Baek, JH",INVENTIONS,2020.0,"Human gender is deemed as a prime demographic trait due to its various usage in the practical domain. Human gender classification in an unconstrained environment is a sophisticated task due to large variations in the image scenarios. Due to the multifariousness of internet images, the classification accuracy suffers from traditional machine learning methods. The aim of this research is to streamline the gender classification process using the transfer learning concept. This research proposes a framework that performs automatic gender classification in unconstrained internet images deploying Pareto frontier deep learning networks; GoogleNet, SqueezeNet, and ResNet50. We analyze the experiment with three different Pareto frontier Convolutional Neural Network (CNN) models pre-trained on ImageNet. The massive experiments demonstrate that the performance of the Pareto frontier CNN networks is remarkable in the unconstrained internet image dataset as well as in the frontal images that pave the way to developing an automatic gender classification system.",10.3390/inventions5020016,automatic gender classification; pre-trained CNN; Pareto frontier networks; transfer learning; GoogLeNet; SqueezeNet; ResNet50,,
An alternative approach to dimension reduction for pareto distributed data: a case study,"Roccetti, M; Delnevo, G; Casini, L; Mirri, S",JOURNAL OF BIG DATA,2021.0,"Deep learning models are tools for data analysis suitable for approximating (non-linear) relationships among variables for the best prediction of an outcome. While these models can be used to answer many important questions, their utility is still harshly criticized, being extremely challenging to identify which data descriptors are the most adequate to represent a given specific phenomenon of interest. With a recent experience in the development of a deep learning model designed to detect failures in mechanical water meter devices, we have learnt that a sensible deterioration of the prediction accuracy can occur if one tries to train a deep learning model by adding specific device descriptors, based on categorical data. This can happen because of an excessive increase in the dimensions of the data, with a correspondent loss of statistical significance. After several unsuccessful experiments conducted with alternative methodologies that either permit to reduce the data space dimensionality or employ more traditional machine learning algorithms, we changed the training strategy, reconsidering that categorical data, in the light of a Pareto analysis. In essence, we used those categorical descriptors, not as an input on which to train our deep learning model, but as a tool to give a new shape to the dataset, based on the Pareto rule. With this data adjustment, we trained a more performative deep learning model able to detect defective water meter devices with a prediction accuracy in the range 87-90%, even in the presence of categorical descriptors.",10.1186/s40537-021-00428-8,Deep learning models; Categorical data; Learning space dimensions; Pareto analysis; Imbalanced datasets; Dataset coherence analysis; Principal component analysis; Binning; Machine learning,,
Machine learning meets continuous flow chemistry: Automated optimization towards the Pareto front of multiple objectives,"Schweidtmann, AM; Clayton, AD; Holmes, N; Bradford, E; Bourne, RA; Lapkin, AA",CHEMICAL ENGINEERING JOURNAL,2018.0,"Automated development of chemical processes requires access to sophisticated algorithms for multi-objective optimization, since single-objective optimization fails to identify the trade-offs between conflicting performance criteria. Herein we report the implementation of a new multi-objective machine learning optimization algorithm for self-optimization, and demonstrate it in two exemplar chemical reactions performed in continuous flow. The algorithm successfully identified a set of optimal conditions corresponding to the trade-off curve (Pareto front) between environmental and economic objectives in both cases. Thus, it reveals the complete underlying trade-off and is not limited to one compromise as is the case in many other studies. The machine learning algorithm proved to be extremely data efficient, identifying the optimal conditions for the objectives in a lower number of experiments compared to single-objective optimizations. The complete underlying trade-off between multiple objectives is identified without arbitrary weighting factors, but via true multi-objective optimization.",10.1016/j.cej.2018.07.031,Automated flow reactor; Environmental chemistry; Machine learning; Reaction engineering; Sustainable chemistry,,
Response prediction of laced steel-concrete composite beams using machine learning algorithms,"Thirumalaiselvi, A; Verma, M; Anandavalli, N; Rajasankar, J",STRUCTURAL ENGINEERING AND MECHANICS,2018.0,"This paper demonstrates the potential application of machine learning algorithms for approximate prediction of the load and deflection capacities of the novel type of Laced Steel Concrete-Composite(1) (LSCC) beams proposed by Anandavalli et al (Engineering Structures 2012). Initially, global and local responses measured on LSCC beam specimen min an experiment are used to validate nonlinear FE model of the LSCC beams The data for the machine learning algorithms is then generated using validated FE model for a range of values of the identified sensitive parameters. The performance of four well-known machine learning algorithms, viz, Support Vector Regression (SVR), Minimax Probability Machine Regression (MPMR), Relevance Vector Machine (RVM) and Multigene Genetic Programing (MGGP) for the approximate estimation of the load and deflection capacities are compared in terms of well-defined error indices. Through relative comparison of the estimated values, it is demonstrated that the algorithms explored in the present study provide a good alternative to expensive experimental testing and sophisticated numerical simulation of the response of LSCC beams. The load carrying and displacement capacity of the LSCC was predicted well by MGGP and MPMR, respectively.",10.12989/sem.2018.66.3.399,composite structures; machine learning algorithms; ultimate strength; displacement,,
Unsupervised and Supervised Learning Approaches Together for Microarray Analysis,"Saha, I; Maulik, U; Bandyopadhyay, S; Plewczynski, D",FUNDAMENTA INFORMATICAE,2011.0,"In this article, a novel concept is introduced by using both unsupervised and supervised learning. For unsupervised learning, the problem of fuzzy clustering in microarray data as a multiobjective optimization is used, which simultaneously optimizes two internal fuzzy cluster validity indices to yield a set of Pareto-optimal clustering solutions. In this regards, a new multiobjective differential evolution based fuzzy clustering technique has been proposed. Subsequently, for supervised learning, a fuzzy majority voting scheme along with support vector machine is used to integrate the clustering information from all the solutions in the resultant Pareto-optimal set. The performances of the proposed clustering techniques have been demonstrated on five publicly available benchmark microarray data sets. A detail comparison has been carried out with multiobjective genetic algorithm based fuzzy clustering, multiobjective differential evolution based fuzzy clustering, single objective versions of differential evolution and genetic algorithm based fuzzy clustering as well as well known fuzzy c-means algorithm. While using support vector machine, comparative studies of the use of four different kernel functions are also reported. Statistical significance test has been done to establish the statistical superiority of the proposed multiobjective clustering approach. Finally, biological significance test has been carried out using a web based gene annotation tool to show that the proposed integrated technique is able to produce biologically relevant clusters of co-expressed genes.",10.3233/FI-2011-376,Fuzzy clustering; improved differential evolution; multiobjective optimization; Pareto-optimal; support vector machine,,
Pareto Inspired Multi-objective Rule Fitness for Adaptive Rule-based Machine Learning,"Urbanowicz, RJ; Olson, RS; Moore, JH",PROCEEDINGS OF THE 2016 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE (GECCO'16 COMPANION),2016.0,"Learning classifier systems (LCSs) are rule-based evolutionary algorithms uniquely suited to classification and data mining in complex, multi-factorial, and heterogeneous problems. LCS rule fitness is commonly based on accuracy, but this metric alone is not ideal for assessing global rule 'value' in noisy problem domains, and thus impedes effective knowledge extraction. Multi-objective fitness functions are promising but rely on knowledge of how to weigh objective importance. Prior knowledge would be unavailable in most real-world problems. The Pareto-front concept offers a strategy for multi-objective machine learning that is agnostic to objective importance. We propose a Pareto-inspired multi objective rule fitness (PIMORF) for LCS, and combine it with a complimentary rule-compaction approach (SRC). We implemented these strategies in ExSTraCS, a successful supervised LCS and evaluated performance over an array of complex simulated noisy and clean problems (i.e. genetic and multiplexer) that each concurrently model pure interaction effects and heterogeneity. While evaluation over multiple performance metrics yielded mixed results, this work represents an important first step towards efficiently learning complex problem spaces without the advantage of prior problem knowledge. Overall the results suggest that PIMORF paired with SRC improved rule set interpretability, particularly with regard to heterogeneous patterns.",10.1145/2908961.2931736,data mining; classifier systems; fitness evaluation; multi-objective optimization; machine learning,,
Ensemble learning by means of a multi-objective optimization design approach for dealing with imbalanced data sets,"Ribeiro, VHA; Reynoso-Meza, G",EXPERT SYSTEMS WITH APPLICATIONS,2020.0,"Ensemble learning methods have already shown to be powerful techniques for creating classifiers. However, when dealing with real-world engineering problems, class imbalance is usually found. In such scenario, canonical machine learning algorithms may not present desirable solutions, and techniques for overcoming this problem must be used. In addition to using learning algorithms that alleviate the imbalance between classes, multi-objective optimization design (MOOD) approaches can be used to improve the prediction performance of ensembles of classifiers. This paper proposes a study of different MOOD approaches for ensemble learning. First, a taxonomy on multi-objective ensemble learning (MOEL) is proposed. In it, four types of existing approaches are defined: multi-objective ensemble member generation, multi-objective ensemble member selection, multi-objective ensemble member combination, and multiobjective ensemble member selection and combination. Additionally, new approaches can be derived by combining the previous ones, such as multi-objective ensemble member generation and selection, multiobjective ensemble member generation and combination and multi-objective ensemble member generation, selection and combination. With the given taxonomy, two experiments are conducted for comparing (1) the performance of the MOEL techniques for generating and aggregating base models on several imbalanced benchmark problems and (2) the performance of MOEL techniques against other machine learning techniques in a real-world imbalanced drinking-water quality anomaly detection problem. Finally, results indicate that MOOD is able to improve the predictive performance of existing ensemble learning techniques. (C) 2020 Elsevier Ltd. All rights reserved.",10.1016/j.eswa.2020.113232,Ensemble learning; Multi-objective optimization; Imbalanced data sets,,
Machine learning based decision support for many-objective optimization problems,"Duro, JA; Saxena, DK; Deb, K; Zhang, QF",NEUROCOMPUTING,2014.0,"Multiple Criteria Decision-Making (MCDM) based Multi-objective Evolutionary Algorithms (MOEAs) are increasingly becoming popular for dealing with optimization problems with more than three objectives, commonly termed as many-objective optimization problems (MaOPs). These algorithms elicit preferences from a single or multiple Decision Makers (DMs), a priori or interactively, to guide the search towards the solutions most preferred by the DM(s), as against the whole Pareto-optimal Front (POF). Despite its promise for dealing with MaOPs, the utility of this approach is impaired by the lack of-objectivity; repeatability; consistency; and coherence in DM's preferences. This paper proposes a machine learning based framework to counter the above limitations. Towards it, the preference-structure of the different objectives embedded in the problem model is learnt in terms of: a smallest set of conflicting objectives which can generate the same POF as the original problem; the smallest objective sets corresponding to pre-specified errors; and the objective sets of pre-specified sizes that correspond to minimum error. While the focus is on demonstrating how the proposed framework could serve as a decision support for the DM, its performance is also studied vis-a-vis an alternative approach (based on dominance relation preservation), for a wide range of test problems and a real-world problem. The results mark a new direction for MCDM based MOEAs for MaOPs. (C) 2014 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2014.06.076,Evolutionary many-objective optimization; Principal component analysis; Maximum variance unfolding; Kernels and multiple criteria decision-making,,
Laplacian minimax probability machine,"Yoshiyama, K; Sakurai, A",PATTERN RECOGNITION LETTERS,2014.0,"In this paper, we propose a Laplacian minimax probability machine, which is a semi-supervised version of minimax probability machine based on the manifold regularization framework. We also show that the proposed method can be kernelized on the basis of a theorem similar to the representer theorem for non-linear cases. Experiments confirm that the proposed methods achieve competitive results, as compared to existing graph-based learning methods such as the Laplacian support vector machine and the Laplacian regularized least square, for publicly available datasets from the UCI machine learning repository. (C) 2013 Elsevier B.V. All rights reserved.",10.1016/j.patrec.2013.01.004,Semi-supervised learning; Manifold regularization; Minimax probability machine; Laplacian SVM; Laplacian RLS,,
The fairness-accuracy Pareto front,"Wei, SS; Niethammer, M",STATISTICAL ANALYSIS AND DATA MINING,,"Algorithmic fairness seeks to identify and correct sources of bias in machine learning algorithms. Confoundingly, ensuring fairness often comes at the cost of accuracy. We provide formal tools in this work for reconciling this fundamental tension in algorithm fairness. Specifically, we put to use the concept of Pareto optimality from multiobjective optimization and seek the fairness-accuracy Pareto front of a neural network classifier. We demonstrate that many existing algorithmic fairness methods are performing the so-called linear scalarization scheme, which has severe limitations in recovering Pareto optimal solutions. We instead apply the Chebyshev scalarization scheme which is provably superior theoretically and no more computationally burdensome at recovering Pareto optimal solutions compared to the linear scheme.",10.1002/sam.11560,fairness; multiobjective optimization; neural network; Pareto front; Pareto optimality,,
Machine learning methods to assist energy system optimization,"Perera, ATD; Wickramasinghe, PU; Nik, VM; Scartezzini, JL",APPLIED ENERGY,2019.0,"This study evaluates the potential of supervised and transfer learning techniques to assist energy system optimization. A surrogate model is developed with the support of a supervised learning technique (by using artificial neural network) in order to bypass computationally intensive Actual Engineering Model (AEM). Eight different neural network architectures are considered in the process of developing the surrogate model. Subsequently, a hybrid optimization algorithm (HOA) is developed combining Surrogate and AEM in order to speed up the optimization process while maintaining the accuracy. Pareto optimization is conducted considering Net Present Value and Grid Integration level as the objective functions. Transfer learning is used to adapt the surrogate model (trained using supervised learning technique) for different scenarios where solar energy potential, wind speed and energy demand are notably different. Results reveal that the surrogate model can reach to Pareto solutions with a higher accuracy when grid interactions are above 10% (with reasonable differences in the decision space variables). HOA can reach to Pareto solutions (similar to the solutions obtained using AEM) around 17 times faster than AEM. The Surrogate Models developed using Transfer Learning (SMTL) shows a similar capability. SMTL combined with the optimization algorithm can predict Pareto fronts efficiently even when there are significant changes in the initial conditions. Therefore, STML can be used along with the HOA, which reduces the computational time required for energy system optimization by 84%. Such a significant reduction in computational time enables the approach to be used for energy system optimization at regional or national scale.",10.1016/j.apenergy.2019.03.202,Distributed energy systems; Supervised learning; Transfer-learning; Multi-objective optimization,,
Short-term load forecasting using multimodal evolutionary algorithm and random vector functional link network based ensemble learning,"Hu, Y; Qu, BY; Wang, J; Liang, J; Wang, YL; Yu, KJ; Li, YX; Qiao, KJ",APPLIED ENERGY,2021.0,"Increasing the accuracy and intelligence of short-term load forecasting system can improve modern power systems management and economic power generation. In recent decades, the optimized machine learning methods have been widely used in load forecasting problems because of their predictability with higher accuracy and robustness. However, most related researches only use evolutionary algorithms for parameters fine-tuning and ignore the evolutionary algorithm based decision-making support and the matching relation between the used evolutionary algorithm and machine learning method, which greatly limit the improvement of forecasting system. To dissolve the above issues, a data-driven evolutionary ensemble learning forecasting model is proposed in this paper. Firstly, a novel multimodal evolutionary algorithm based on comprehensive weighted vector angle and shift-based density estimation is proposed. Secondly, based on the proposed multimodal evolutionary algorithm, an intelligent decision-making support scheme including predictive performance evaluation, model properties analysis, structure and fusion strategy optimization, and optimal model preference selection is designed to improve the random vector functional link network based ensemble learning model and boost the forecasting accuracy. Thirdly, experimental studies on 15 test problems with up to 6000 decision variables are conducted to validate the excellent optimization ability of the proposed evolutionary algorithm. Finally, the proposed evolutionary ensemble learning method is compared with 6 other representative forecast methods on real-world short-term load forecasting datasets from Australia, Great Britain, and Norway. The experiment results verify the superiority and applicability of the proposed method.",10.1016/j.apenergy.2020.116415,Short-term load forecasting; Machine learning; Evolutionary algorithm; Ensemble learning; Multimodal multi-objective optimization; Random vector functional link network,,
Predicting drug properties with parameter-free machine learning: pareto-optimal embedded modeling (POEM),"Brereton, AE; MacKinnon, S; Safikhani, Z; Reeves, S; Alwash, S; Shahani, V; Windemuth, A",MACHINE LEARNING-SCIENCE AND TECHNOLOGY,2020.0,"The prediction of absorption, distribution, metabolism, excretion, and toxicity (ADMET) of small molecules from their molecular structure is a central problem in medicinal chemistry with great practical importance in drug discovery. Creating predictive models conventionally requires substantial trial-and-error for the selection of molecular representations, machine learning (ML) algorithms, and hyperparameter tuning. A generally applicable method that performs well on all datasets without tuning would be of great value but is currently lacking. Here, we describe pareto-optimal embedded modeling (POEM), a similarity-based method for predicting molecular properties. POEM is a non-parametric, supervised ML algorithm developed to generate reliable predictive models without need for optimization. POEM's predictive strength is obtained by combining multiple different representations of molecular structures in a context-specific manner, while maintaining low dimensionality. We benchmark POEM relative to industry-standard ML algorithms and published results across 17 classifications tasks. POEM performs well in all cases and reduces the risk of overfitting.",10.1088/2632-2153/ab891b,POEM; small molecule; ADMET; prediction; molecular graph convolution; SVM; random forest,,
A selective ensemble learning based two-sided cross-domain collaborative filtering algorithm,"Yu, X; Peng, QL; Xu, LW; Jiang, F; Du, JW; Gong, DW",INFORMATION PROCESSING & MANAGEMENT,2021.0,"ABS T R A C T Recently, various Cross-Domain Collaborative Filtering (CDCF) algorithms are presented to address the sparsity problem, leveraging ratings of auxiliary domains to improve target domain's recommendation performance. Therein, two-sided CDCF algorithms have shown better performance, given the fact that they can extract both user and item information. However, as the auxiliary domains are not all related to the target domain, utilizing information from all the auxiliary domains may not be optimal and would lead to low efficiency. A Two-Sided CDCF model based on Selective Ensemble learning considering both Accuracy and Efficiency (TSSEAE) is proposed to balance recommendation accuracy and efficiency. In TSSEAE, user-sided and item-sided auxiliary domains are firstly combined to improve performance of target domain. Then, CDCF problems are converted to ensemble learning problems, with each combination corresponding to a classifier. In this way, the problem of selecting combinations can be converted to that of selecting classifiers, which is a selective ensemble learning problem. Finally, a bi-objective optimization problem is solved to obtain Pareto optimal solutions for the selective ensemble learning problem. The experimental result on Amazon dataset shows the effectiveness of TSSEAE.",10.1016/j.ipm.2021.102691,Cross-domain collaborative filtering; Selective ensemble; Ensemble learning; Pareto optimal solutions; Bi-objective optimization&nbsp; problem,,
An asymptotically minimax kernel machine,"Ghosh, D",STATISTICS & PROBABILITY LETTERS,2014.0,"Recently, a class of machine learning-inspired procedures, termed kernel machine methods, has been extensively developed in the statistical literature. In this note, we construct a so-called 'adaptively minimax' kernel machine. Such a construction highlights the limits on the interpretability of such kernel machines. (C) 2014 Elsevier B.V. All rights reserved.",10.1016/j.spl.2014.08.005,Data mining; Decision theory; Hard-thresholding; Nonparametric regression; Support vector machines,,
A Neural Network Extension for Solving the Pareto/Negative Binomial Distribution Model,"Xie, SM",INTERNATIONAL JOURNAL OF MARKET RESEARCH,,"This research proposes a new estimation scheme to solve the estimation burden of the Pareto/NBD model and to release its power in out-of-sample prediction and then builds a neural network-based model (NNA-based model) for parameter estimation of the Pareto/NBD model, by designing a loss function to include the likelihood of the Pareto/NBD model and the mean absolute error. Following empirical analysis with a real dataset and simulation datasets, the main results are as follows. (1) The NNA-based model encounters a relatively skewed prediction in inactive prediction and focuses more on inactive prediction, while the heuristic method has a balanced prediction. (2) The Pareto/NBD model has better repeat purchase prediction compared to that under the mean absolute error, but the NNA-based model has a better fitting in the repeat purchase distribution. (3) The distribution of estimated parameters between these two models is quite different, but is able to realize the same prediction purpose. The NNA-based model also has greater application opportunities in the commercial environment because the learned estimator is applicable to different transaction timing patterns. These findings clarify that the NNA-based model solves the estimation burden and releases the predictive power of the Pareto/NBD model.",10.1177/14707853211072817,neural network algorithm; pareto; negative binomial distribution model; heuristic method; neural network-based model; out-of-sample prediction,,
Unsupervised representation learning with Minimax distance measures,"Chehreghani, MH",MACHINE LEARNING,2020.0,"We investigate the use of Minimax distances to extract in a nonparametric way the features that capture the unknown underlying patterns and structures in the data. We develop a general-purpose and computationally efficient framework to employ Minimax distances with many machine learning methods that perform on numerical data. We study both computing the pairwise Minimax distances for all pairs of objects and as well as computing the Minimax distances of all the objects to/from a fixed (test) object. We first efficiently compute the pairwise Minimax distances between the objects, using the equivalence of Minimax distances over a graph and over a minimum spanning tree constructed on that. Then, we perform an embedding of the pairwise Minimax distances into a new vector space, such that their squared Euclidean distances in the new space equal to the pairwise Minimax distances in the original space. We also study the case of having multiple pairwise Minimax matrices, instead of a single one. Thereby, we propose an embedding via first summing up the centered matrices and then performing an eigenvalue decomposition to obtain the relevant features. In the following, we study computing Minimax distances from a fixed (test) object which can be used for instance inK-nearest neighbor search. Similar to the case of all-pair pairwise Minimax distances, we develop an efficient and general-purpose algorithm that is applicable with any arbitrary base distance measure. Moreover, we investigate in detail the edges selected by the Minimax distances and thereby explore the ability of Minimax distances in detecting outlier objects. Finally, for each setting, we perform several experiments to demonstrate the effectiveness of our framework.",10.1007/s10994-020-05886-4,Representation learning; Distance measure; Computational efficiency; Minimax distances,,
An imprecise extension of SVM-based machine learning models,"Utkin, LV",NEUROCOMPUTING,2019.0,"A general approach for incorporating imprecise prior knowledge and for robustifying the machine learning SVM-based models is proposed in the paper. The main idea underlying the approach is to use a double duality representation in the framework of the minimax strategy of decision making. This idea allows us to get simple extensions of SVMs including additional constraints for optimization variables (the Lagrange multipliers) formalizing the incorporated imprecise information. The approach is applied to regression, binary classification and one-class classification SVM-based problems. Moreover, it is adopted to set-valued or interval-valued training data. For every problem, numerical examples are provided which illustrate how imprecise information may improve the machine learning algorithm performance. (C) 2018 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2018.11.053,Machine learning; Support vector machine; Duality; Classification; Regression; Interval-valued data; Imprecise model,,
Machine Learning Based Power Estimation for CMOS VLSI Circuits,"Govindaraj, V; Arunadevi, B",APPLIED ARTIFICIAL INTELLIGENCE,2021.0,"Nowadays, machine learning (ML) algorithms are receiving massive attention in most of the engineering application since it has capability in complex systems modeling using historical data. Estimation of power for CMOS VLSI circuit using various circuit attributes is proposed using passive machine learning-based technique. The proposed method uses supervised learning method, which provides a fast and accurate estimation of power without affecting the accuracy of the system. Power estimation using random forest algorithm is relatively new. Accurate estimation of power of CMOS VLSI circuits is estimated by using random forest model which is optimized and tuned by using multiobjective NSGA-II algorithm. It is inferred from the experimental results testing error varies from 1.4% to 6.8% and in terms of and Mean Square Error is 1.46e-06 in random forest method when compared to BPNN. Statistical estimation like coefficient of determination (R) and Root Mean Square Error (RMSE) are done and it is proven that random Forest is best choice for power estimation of CMOS VLSI circuits with high coefficient of determination of 0.99938, and low RMSE of 0.000116.",10.1080/08839514.2021.1966885,,,
Constructing accuracy and diversity ensemble using Pareto-based multi-objective learning for evolving data streams,"Sun, YG; Dai, HH",NEURAL COMPUTING & APPLICATIONS,2021.0,"Ensemble learning is one of the most frequently used techniques for handling concept drift, which is the greatest challenge for learning high-performance models from big evolving data streams. In this paper, a Pareto-based multi-objective optimization technique is introduced to learn high-performance base classifiers. Based on this technique, a multi-objective evolutionary ensemble learning scheme, named Pareto-optimal ensemble for a better accuracy and diversity (PAD), is proposed. The approach aims to enhance the generalization ability of ensemble in evolving data stream environment by balancing the accuracy and diversity of ensemble members. In addition, an adaptive window change detection mechanism is designed for tracking different kinds of drifts constantly. Extensive experiments show that PAD is capable of adapting to dynamic change environments effectively and efficiently in achieving better performance.",10.1007/s00521-020-05386-5,Data streams; Concept drift; Ensemble learning; Diversity; Classifier selection; Multi-objective optimization,,
A Multiobjective Evolutionary Nonlinear Ensemble Learning With Evolutionary Feature Selection for Silicon Prediction in Blast Furnace,"Wang, XP; Hu, TH; Tang, LX",IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS,,"In the blast furnace ironmaking process, accurate prediction of silicon content in molten iron is of great significance for maintaining stable furnace conditions, improving hot metal quality, and reducing energy consumption. However, most of the current research works employ linear correlation coefficient methods to select input features in modeling, which may not fully take the nonlinear and coupling relationships between features into account. Therefore, this article considers the input feature selection issue of silicon content prediction model from a new perspective and proposes a multiobjective evolutionary nonlinear ensemble learning model with evolutionary feature selection mechanism (MOENE-EFS), in which extreme learning machine is adopted as the base learner. MOENE-EFS takes the input feature scheme of each base learner as well as their network structure and parameters as decision variables and proposes a modified nondominated sorting differential evolution algorithm to optimize two conflicting objectives, i.e., accuracy and diversity of base learners, simultaneously. Through the optimization, a set of Pareto optimal base learners with high accuracy and strong diversity can be obtained. Moreover, different from the linear ensemble methods commonly used in classical evolutionary ensemble learning, this article proposes a nonlinear ensemble method to combine the obtained base learners based on differential evolution. Experimental results indicate that the two proposed strategies, i.e., evolutionary feature selection and nonlinear ensemble, are very effective in improving the accuracy and stability of the prediction model. MOENE-EFS also outperforms the other prediction models in both benchmark data and practical industrial data. Furthermore, analysis on the input features of all Pareto optimal base learners shows that the evolutionary feature selection is capable of selecting essential features and is consistent with human experience, which indicates it is a promising method to deal with the input feature selection issue in silicon content prediction.",10.1109/TNNLS.2021.3059784,Silicon; Predictive models; Feature extraction; Metals; Optimization; Modeling; Iron; Extreme learning machine (ELM); multiobjective evolutionary ensemble learning; nonlinear ensemble; silicon content,,
Performance analysis of advanced decision tree-based ensemble learning algorithms for landslide susceptibility mapping,"Sahin, EK; Colkesen, I",GEOCARTO INTERNATIONAL,2021.0,"Landslide susceptibility mapping (LSM) is a major area of interest within the field of disaster risk management that involves planning and decision-making activities. Therefore, preparation of dataset, construction of predictive model and analysis of results are considered to be important stages for effective and efficient disaster management in LSM. In recent years, a large number of studies has mainly focused on the effects of using machine learning (ML) algorithms as a predictive model in LSM. Decision tree-based ensemble learning algorithms known as decision forest is one of the popular ML techniques based on a combination of several decision tree algorithms to construct an optimal prediction model. In this study, prediction performances of recently proposed decision tree-based ensemble-based algorithms namely canonical correlation forest (CCF) and rotation forest (RotFor) are tested on LSM. In order to compare their performances, popular ensemble learning algorithms including random forest (RF), AdaBoost and bagging algorithms are also considered. For this purpose, first, twelve conditioning factors are determined in the study area, Karabuk province of Turkey. Second, individual importance of the factors on LSM process is evaluated using Fischer score analysis and selected factors are used as an input dataset for the construction of landslide susceptibility prediction models of CCF, RotFor, RF, AdaBoost and bagging algorithms. For the assessment of the performances, overall accuracy (OA), success rate curves and the area under the curve (AUC) analysis are utilized. Furthermore, chi-squared-based McNemar's test and well-known accuracy measures known as receiver operating characteristic (ROC) curves are employed to evaluate the pairwise comparison of the ensemble learning methods. Results show that CCF method outperforms the RotFor method by about 4%, and there is no statistically significant difference between CFF and other methods.",10.1080/10106049.2019.1641560,Landslide susceptibility; feature selection; advanced decision tree; canonical correlation forest; bagging methods; AdaBoost,,
LOCAL NEAREST NEIGHBOUR CLASSIFICATION WITH APPLICATIONS TO SEMI-SUPERVISED LEARNING,"Cannings, TI; Berrett, TB; Samworth, RJ",ANNALS OF STATISTICS,2020.0,"We derive a new asymptotic expansion for the global excess risk of a local-k-nearest neighbour classifier, where the choice of k may depend upon the test point. This expansion elucidates conditions under which the dominant contribution to the excess risk comes from the decision boundary of the optimal Bayes classifier, but we also show that if these conditions are not satisfied, then the dominant contribution may arise from the tails of the marginal distribution of the features. Moreover, we prove that, provided the d-dimensional marginal distribution of the features has a finite rho th moment for some rho > 4 (as well as other regularity conditions), a local choice of k , can yield a rate of convergence of the excess risk of O(n(-4/(d+4))), where n is the sample size, whereas for the standard k-nearest neighbour classifier, our theory would require d >= 5 and rho > 4d/(d - 4) finite moments to achieve this rate. These results motivate a new k-nearest neighbour classifier for semi-supervised learning problems, where the unlabelled data are used to obtain an estimate of the marginal feature density, and fewer neighbours are used for classification when this density estimate is small. Our worst-case rates are complemented by a minimax lower bound, which reveals that the local, semi-supervised k-nearest neighbour classifier attains the minimax optimal rate over our classes for the excess risk, up to a subpolynomial factor in n. These theoretical improvements over the standard k-nearest neighbour classifier are also illustrated through a simulation study.",10.1214/19-AOS1868,Classification problems; nearest neighbours; nonparametric classification; semi- supervised learning,,
A multiobjective single bus corridor scheduling using machine learning-based predictive models,"Chen, B; Bai, RB; Li, JW; Liu, YN; Xue, N; Ren, JF",INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH,,"Many real-life optimisation problems, including those in production and logistics, have uncertainties that pose considerable challenges for practitioners. In spite of considerable efforts, the current methods are still not satisfactory. This is primarily caused by a lack of effective methods to deal with various uncertainties. Existing literature comes from two isolated research communities, namely the operations research community and the machine learning community. In the operations research community, uncertainties are often modelled and solved through techniques like stochastic programming or robust optimisation, which are often criticised for their over conservativeness. In the machine learning community, the problem is formulated as a dynamic control problem and solved through techniques like supervised learning and/or reinforcement learning, which could suffer from being myopic and unstable. In this paper, we aim to fill this research gap and develop a novel framework that takes advantages of both short-term accuracy from mathematical models and high-quality future forecasts from machine learning modules. We demonstrate the practicality and feasibility of our approach for a real-life bus scheduling problem and two controlled bus scheduling instances that are generated artificially. To our knowledge, the proposed framework represents the first multi-objective bus-headway-optimisation method for non-timetabled bus schedule with major practical constraints being considered. The advantages of our proposed methods are also discussed, along with factors that need to be carefully considered for practical applications.",10.1080/00207543.2020.1766716,bus scheduling; multi-objective optimisation; combinatorial optimisation; machine learning,,
Multidirectional harmony search algorithm for solving integer programming and minimax problems,"Tawhid, MA; Ali, AF",INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION,2019.0,"Integer programming and minimax problems are essential tools in solving various problems that arise in data mining and machine learning such as multi-class data classification and feature selection problems. In this paper, we propose a new hybrid harmony search algorithm by combining the harmony search algorithm with the multidirectional search method in order to solve the integer programming and minimax problems. The proposed algorithm is called multidirectional harmony search algorithm (MDHSA). MDHSA starts the search by applying the standard harmony search for numbers of iteration then the best-obtained solution is passing to the multidirectional search method as an intensification process in order to accelerate the search and overcome the slow convergence of the standard harmony search algorithm. The proposed algorithm is balancing between the global exploration of the harmony search algorithm and the deep exploitation of the multidirectional search method. MDHSA algorithm is tested on seven integer programming problems and 15 minimax problems and compared against 12 algorithms for solving integer programming problems and 11 algorithms for solving minimax problems. The experiments results show the efficiency of the proposed algorithm and its ability to solve integer programming and minimax problems in reasonable time.",10.1504/IJBIC.2019.10020492,evolutionary computation; global optimisation; harmony search algorithm; direct search algorithm; multidirectional search; integer programming problems; minimax problems,,
Inverse Design of Materials by Machine Learning,"Wang, J; Wang, YX; Chen, YA",MATERIALS,2022.0,"It is safe to say that every invention that has changed the world has depended on materials. At present, the demand for the development of materials and the invention or design of new materials is becoming more and more urgent since peoples' current production and lifestyle needs must be changed to help mitigate the climate. Structure-property relationships are a vital paradigm in materials science. However, these relationships are often nonlinear, and the pattern is likely to change with length scales and time scales, posing a huge challenge. With the development of physics, statistics, computer science, etc., machine learning offers the opportunity to systematically find new materials. Especially by inverse design based on machine learning, one can make use of the existing knowledge without attempting mathematical inversion of the relevant integrated differential equation of the electronic structure but by using backpropagation to overcome local minimax traps and perform a fast calculation of the gradient information for a target function concerning the design variable to find the optimizations. The methodologies have been applied to various materials including polymers, photonics, inorganic materials, porous materials, 2-D materials, etc. Different types of design problems require different approaches, for which many algorithms and optimization approaches have been demonstrated in different scenarios. In this mini-review, we will not specifically sum up machine learning methodologies, but will provide a more material perspective and summarize some cut-edging studies.",10.3390/ma15051811,inverse design; materials design; machine learning; polymer; photonic; inorganic materials; porous materials,,
Fairer Machine Learning Through Multi-objective Evolutionary Learning,"Zhang, QQ; Liu, JL; Zhang, ZQ; Wen, JY; Mao, BF; Yao, X","ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2021, PT IV",2021.0,"Dilemma between model accuracy and fairness in machine learning models has been shown theoretically and empirically. So far, dozens of fairness measures have been proposed, among which incompatibility and complementarity exist. However, no fairness measure has been universally accepted as the single fairest measure. No one has considered multiple fairness measures simultaneously. In this paper, we propose a multi-objective evolutionary learning framework for mitigating unfairness caused by considering a single measure only, in which a multi-objective evolutionary algorithm is used during training to balance accuracy and multiple fairness measures simultaneously. In our case study, besides the model accuracy, two fairness measures that are conflicting to each other are selected. Empirical results show that our proposed multi-objective evolutionary learning framework is able to find Pareto-front models efficiently and provide fairer machine learning models that consider multiple fairness measures.",10.1007/978-3-030-86380-7_10,Fairness in machine learning; Discrimination in machine learning; AI ethics; Fairness measures; Multi-objective learning,,
Bridging symbolic and subsymbolic reasoning with minimax entropy models,"Marra, G",INTELLIGENZA ARTIFICIALE,2021.0,"In this paper, we investigate MiniMax Entropy models, a class of neural symbolic models where symbolic and subsymbolic features are seamlessly integrated. We show how these models recover classical algorithms from both the deep learning and statistical relational learning scenarios. Novel hybrid settings are defined and experimentally explored, showing state-of-the-art performance in collective classification, knowledge base completion and graph (molecular) data generation.",10.3233/IA-210088,Neural symbolic artificial intelligence; statistical relational artificial intelligence; deep learning; logic,,
A comparative study of minimax probability machine-based approaches for face recognition,"Ng, JKC; Zhong, YZ; Yang, SQ",PATTERN RECOGNITION LETTERS,2007.0,"Automatic face recognition is a challenging problem in the biometric recognition area. Minimax Probability Machine (MPM) and its extension, Minimum Error Minimax Probability Machine, have shown advantages in the machine learning literature. In this paper, we incorporate the MPM-based approaches into our face recognition system for further study. To test the performance of our new system, we compare the MPM-based approaches with SVM, a PCA-based and a LDA-based algorithms on the FERET database for both verification and identification. The experimental results demonstrate that MPM-based approaches are promising for automatic face recognition. (C) 2007 Elsevier B.V. All rights reserved.",10.1016/j.patrec.2007.05.021,face recognition; support vector machine; minimax probability machine; minimum error minimax probability machine,,
Deep learning algorithms to develop Flood susceptibility map in Data-Scarce and Ungauged River Basin in India,"Saha, S; Gayen, A; Bayen, B",STOCHASTIC ENVIRONMENTAL RESEARCH AND RISK ASSESSMENT,,"Flood is considered the most extensive natural disaster around the globe. Kunur River, a riverine landscape of Rarh Bengal, was selected as the study area because this basin has undergone several floods. This research work applied deep learning and benchmark machine learning methods for preparing the flood susceptibility maps (FSMs) at a basin scale. For this work, sixteen flood controlling factors were applied. These predisposing factors were chosen based on field knowledge, previous researchs, and data availability. The FSMs were produced for the better palling and management of natural resources of Kunur River Basin, applying one deep learning model (DLM) includes convolution neural network (CNN) model and three benchmark machine learning methods (BMLMs) including multilayer perceptron (MLP), Bagging, and random forest (RF). The differences in prediction capacity between the models were assessed by applying the Friedman rank test and Wilcoxon test. Performance of the FSMs, evaluated through the precision, accuracy, AUC (area under the curve), and statistical measures revealed that CNN has the highest AUC values (0.934) followed by MLP (0.927), Bagging (0.897), and RF (0.900) respectively. The CNN model's prediction capacity is slightly better than Bagging, RF, and MLP models. Finally, we can conclude that the deep learning model is more robust than the benchmark MLMs (RF, MLP and Bagging) and CNN is excellent alternative for FSMs considering the used variables.",10.1007/s00477-022-02195-1,Convolution neural network; Benchmark machine learning models; Deep learning; flood susceptibility; Friedman and wilcoxon rank test,,
A new ensemble learning methodology based on hybridization of classifier ensemble selection approaches,"Mousavi, R; Eftekhari, M",APPLIED SOFT COMPUTING,2015.0,"Ensemble learning is a system that improves the performance and robustness of the classification problems. How to combine the outputs of base classifiers is one of the fundamental challenges in ensemble learning systems. In this paper, an optimized Static Ensemble Selection (SES) approach is first proposed on the basis of NSGA-II multi-objective genetic algorithm (called SES-NSGAII), which selects the best classifiers along with their combiner, by simultaneous optimization of error and diversity objectives. In the second phase, the Dynamic Ensemble Selection-Performance (DES-P) is improved by utilizing the first proposed method. The second proposed method is a hybrid methodology that exploits the abilities of both SES and DES approaches and is named Improved DES-P (IDES-P). Accordingly, combining static and dynamic ensemble strategies as well as utilizing NSGA-II are the main contributions of this research. Findings of the present study confirm that the proposed methods outperform the other ensemble approaches over 14 datasets in terms of classification accuracy. Furthermore, the experimental results are described from the view point of Pareto front with the aim of illustrating the relationship between diversity and the over-fitting problem. (C) 2015 Elsevier B.V. All rights reserved.",10.1016/j.asoc.2015.09.009,Ensemble learning system; Static ensemble selection; Dynamic ensemble selection; Classifier combination; Classifier diversity; Multi-objective optimization,,
Portfolio management via two-stage deep learning with a joint cost,"Yun, H; Lee, M; Kang, YS; Seok, J",EXPERT SYSTEMS WITH APPLICATIONS,2020.0,"Portfolio management is a series of processes that maximize returns and minimize risk by allocating assets efficiently. Along with the developments in machine learning technology, it has been studied to apply machine learning methods to prediction-based portfolio management. However, such methods have a few limitations. First, they do not consider the relations between assets for the prediction. In addition, the studies commonly focus on the prediction accuracy, neglecting the construction of portfolios. Furthermore, the methods have usually been evaluated with index data, which hardly represent actual prices to buy or sell an asset. To overcome these problems, Exchange Traded Funds (ETFs) are employed for base assets for the evaluation, and we propose a two-stage deep learning framework, called Grouped-ETFs Model (GEM), with a joint cost function. The GEM is designed to learn the features of inter-asset and groups in each stage. Also, the proposed joint cost can consider relative returns for the training while the relative returns are a crucial factor to construct a portfolio. The results of a rigorous evaluation with global ETF data indicate that the proposed GEM with the joint cost outperforms the equally weighted portfolio and the ordinary deep learning model by 33.7% and 30.1%, respectively. An additional experiment using sector ETFs verifies the generality of the proposed model where the results accord with those of the previous experiment. (C) 2019 Elsevier Ltd. All rights reserved.",10.1016/j.eswa.2019.113041,Deep learning; Long short-term memory; Portfolio management; Joint cost function,,
Multiobjective sparse ensemble learning by means of evolutionary algorithms,"Zhao, JQ; Jiao, LC; Xia, SX; Fernandes, VB; Yevseyeva, I; Zhou, Y; Emmerich, MTM",DECISION SUPPORT SYSTEMS,2018.0,"Ensemble learning can improve the performance of individual classifiers by combining their decisions. The sparseness of ensemble learning has attracted much attention in recent years. In this paper, a novel multi objective sparse ensemble learning (MOSEL) model is proposed. Firstly, to describe the ensemble classifiers more precisely the detection error trade-off (DET) curve is taken into consideration. The sparsity ratio (sr) is treated as the third objective to be minimized, in addition to false positive rate (fpr) and false negative rate (fnr) minimization. The MOSEL turns out to be augmented DET (ADET) convex hull maximization problem. Secondly, several evolutionary multiobjective algorithms are exploited to find sparse ensemble classifiers with strong performance. The relationship between the sparsity and the performance of ensemble classifiers on the ADET space is explained. Thirdly, an adaptive MOSEL classifiers selection method is designed to select the most suitable ensemble classifiers for a given dataset. The proposed MOSEL method is applied to well-known MNIST datasets and a real-world remote sensing image change detection problem, and several datasets are used to test the performance of the method on this problem. Experimental results based on both MNIST datasets and remote sensing image change detection show that MOSEL performs significantly better than conventional ensemble learning methods.",10.1016/j.dss.2018.05.003,Ensemble learning; Sparse representation; Classification; Multiobjective optimization; Change detection,,
Catalyst Design by Machine Learning and Multiobjective Optimization,"Kurogi, T; Etou, M; Hamada, R; Sakai, S",JOURNAL OF THE JAPAN PETROLEUM INSTITUTE,2020.0,"The computer technologies of machine learning and multiobjective optimization were introduced to develop the catalyst for fluid catalytic cracking (FCC). Response surface methodology was applied for a training set consist-ing of 1000 data points with varied catalyst compositions which consist of a variety of catalysts compositions, feedstock properties, pseudo-equilibrium conditions, cracking performance test conditions as input parameters and the cracking test results as outputs. At first, response surface model (RSM) was obtained with four approxima-tion methods, among which the radial basis function (RBF) method was found to give the highest score accurate RSM with the smallest average error and the highest coefficient of determination among them. Then the virtual experiments were carried out with the RSM applied with multiobjective genetic algorithm (MOGA) to optimize the catalyst design considering the multiobjective; to yield less bottoms, less coke, more gasoline and less gas. After 5000 virtual experiments with RSM were carried out, we found that the pareto front was obtained. Finally, the optimum catalyst design was selected from the designs on the pareto front. As a result, the selected catalyst design showed 2.7 % higher gasoline yield and was confirmed to show the excellent performance over conven-tional FCC catalyst.",10.1627/jpi.64.256,Machine learning; &nbsp; Multiobjective optimization; &nbsp; Catalyst design; Petroleum refining; &nbsp; Fluid catalytic cracking,,
Using financial risk measures for analyzing generalization performance of machine learning models,"Takeda, A; Kanamori, T",NEURAL NETWORKS,2014.0,"We propose a unified machine learning model (UMLM) for two-class classification, regression and outlier (or novelty) detection via a robust optimization approach. The model embraces various machine learning models such as support vector machine-based and minimax probability machine-based classification and regression models. The unified framework makes it possible to compare and contrast existing learning models and to explain their differences and similarities. In this paper, after relating existing learning models to UMLM, we show some theoretical properties for UMLM. Concretely, we show an interpretation of UMLM as minimizing a well-known financial risk measure (worst-case value-at risk (VaR) or conditional VaR), derive generalization bounds for UMLM using such a risk measure, and prove that solving problems of UMLM leads to estimators with the minimized generalization bounds. Those theoretical properties are applicable to related existing learning models. (C) 2014 Elsevier Ltd. All rights reserved.",10.1016/j.neunet.2014.05.006,Support vector machine; Minimax probability machine; Financial risk measure; Generalization performance,,
DrugEx v2: de novo design of drug molecules by Pareto-based multi-objective reinforcement learning in polypharmacology,"Liu, XH; Ye, K; van Vlijmen, HWT; Emmerich, MTM; IJzerman, AP; van Westen, GJP",JOURNAL OF CHEMINFORMATICS,2021.0,"In polypharmacology drugs are required to bind to multiple specific targets, for example to enhance efficacy or to reduce resistance formation. Although deep learning has achieved a breakthrough in de novo design in drug discovery, most of its applications only focus on a single drug target to generate drug-like active molecules. However, in reality drug molecules often interact with more than one target which can have desired (polypharmacology) or undesired (toxicity) effects. In a previous study we proposed a new method named DrugEx that integrates an exploration strategy into RNN-based reinforcement learning to improve the diversity of the generated molecules. Here, we extended our DrugEx algorithm with multi-objective optimization to generate drug-like molecules towards multiple targets or one specific target while avoiding off-targets (the two adenosine receptors, A(1)AR and A(2A)AR, and the potassium ion channel hERG in this study). In our model, we applied an RNN as the agent and machine learning predictors as the environment. Both the agent and the environment were pre-trained in advance and then interplayed under a reinforcement learning framework. The concept of evolutionary algorithms was merged into our method such that crossover and mutation operations were implemented by the same deep learning model as the agent. During the training loop, the agent generates a batch of SMILES-based molecules. Subsequently scores for all objectives provided by the environment are used to construct Pareto ranks of the generated molecules. For this ranking a non-dominated sorting algorithm and a Tanimoto-based crowding distance algorithm using chemical fingerprints are applied. Here, we adopted GPU acceleration to speed up the process of Pareto optimization. The final reward of each molecule is calculated based on the Pareto ranking with the ranking selection algorithm. The agent is trained under the guidance of the reward to make sure it can generate desired molecules after convergence of the training process. All in all we demonstrate generation of compounds with a diverse predicted selectivity profile towards multiple targets, offering the potential of high efficacy and low toxicity.",10.1186/s13321-021-00561-9,Deep learning; Adenosine receptors; Cheminformatics; Reinforcement learning; Multi-objective optimization; Exploration strategy,,
EXACT LOWER BOUNDS FOR THE AGNOSTIC PROBABLY-APPROXIMATELY-CORRECT (PAC) MACHINE LEARNING MODEL,"Kontorovich, A; Pinelis, I",ANNALS OF STATISTICS,2019.0,"We provide an exact nonasymptotic lower bound on the minimax expected excess risk (EER) in the agnostic probably-approximately-correct (PAC) machine learning classification model and identify minimax learning algorithms as certain maximally symmetric and minimally randomized voting procedures. Based on this result, an exact asymptotic lower bound on the minimax EER is provided. This bound is of the simple form c(infinity)/root nu as v -> infinity, where c(infinity) = 0.16997... is a universal constant, nu = mid, m is the size of the training sample and d is the Vapnik-Chervonenkis dimension of the hypothesis class. It is shown that the differences between these asymptotic and nonasymptotic bounds, as well as the differences between these two bounds and the maximum EER of any learning algorithms that minimize the empirical risk, are asymptotically negligible, and all these differences are due to ties in the mentioned voting procedures. A few easy to compute nonasymptotic lower bounds on the minimax EER are also obtained, which are shown to be close to the exact asymptotic lower bound c(infinity)/root nu even for rather small values of the ratio nu = m/d. As an application of these results, we substantially improve existing lower bounds on the tail probability of the excess risk. Among the tools used are Bayes estimation and apparently new identities and inequalities for binomial distributions.",10.1214/18-AOS1766,PAC learning theory; classification; generalization error; minimax decision rules; Bayes decision rules; empirical estimators; binomial distribution,,
Distributed Deep Learning on Data Systems: A Comparative Analysis of Approaches,"Zhang, YH; McQuillan, F; Jayaram, N; Kak, N; Khanna, E; Kislal, O; Valdano, D; Kumar, A",PROCEEDINGS OF THE VLDB ENDOWMENT,2021.0,"Deep learning (DL) is growing in popularity for many data analytics applications, including among enterprises. Large business-critical datasets in such settings typically reside in RDBMSs or other data systems. The DB community has long aimed to bring machine learning (ML) to DBMS-resident data. Given past lessons from in-DBMS ML and recent advances in scalable DL systems, DBMS and cloud vendors are increasingly interested in adding more DL support for DB-resident data. Recently, a new parallel DL model selection execution approach called Model Hopper Parallelism (MOP) was proposed. In this paper, we characterize the particular suitability of MOP for DL on data systems, but to bring MOP-based DL to DB-resident data, we show that there is no single best approach, and an interesting tradeoff space of approaches exists. We explain four canonical approaches and build prototypes upon Greenplum Database, compare them analytically on multiple criteria (e.g., runtime efficiency and ease of governance) and compare them empirically with large-scale DL workloads. Our experiments and analyses show that it is non-trivial to meet all practical desiderata well and there is a Pareto frontier; for instance, some approaches are 3x-6x faster but fare worse on governance and portability. Our results and insights can help DBMS and cloud vendors design better DL support for DB users. All of our source code, data, and other artifacts are available at https://github.com/makemebitter/cerebro- ds.",10.14778/3467861.3467867,,,
How fair can we go in machine learning? Assessing the boundaries of accuracy and fairness,"Valdivia, A; Sanchez-Monedero, J; Casillas, J",INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS,2021.0,"Fair machine learning has been focusing on the development of equitable algorithms that address discrimination. Yet, many of these fairness-aware approaches aim to obtain a unique solution to the problem, which leads to a poor understanding of the statistical limits of bias mitigation interventions. In this study, a novel methodology is presented to explore the tradeoff in terms of a Pareto front between accuracy and fairness. To this end, we propose a multiobjective framework that seeks to optimize both measures. The experimental framework is focused on logistiregression and decision tree classifiers since they are well-known by the machine learning community. We conclude experimentally that our method can optimize classifiers by being fairer with a small cost on the classification accuracy. We believe that our contribution will help stakeholders of sociotechnical systems to assess how far they can go being fair and accurate, thus serving in the support of enhanced decision making where machine learning is used.",10.1002/int.22354,algorithmic fairness; group fairness; multiobjective optimization,,
Application of spatial multicriteria decision analysis in healthcare: Identifying drivers and triggers of infectious disease outbreaks using ensemble learning,"Devarakonda, P; Sadasivuni, R; Nobrega, RAA; Wu, JH",JOURNAL OF MULTI-CRITERIA DECISION ANALYSIS,,"Modelling infectious diseases is a complex and multi-disciplinary problem that necessitates the combined use of multicriteria decision analysis (MCDA) and machine learning (ML) in a spatial framework. This research attempts to demonstrate the extensive applications of MCDA in the field of public health and to illustrate its utility with the combined use of spatial models and machine learning. The study investigates the risk factors for communicable diseases with a focus on vector-borne infectious diseases, such as West Nile Virus (WNV), malaria, dengue, etc. It aims to quantify vector-borne disease risk by examining the geographic contextual effects of socio-economic, climatic, and environmental factors using the objective-weighting technique adopted from MCDA and machine learning in a geographic information systems (GIS) framework. The authors attempted to minimize subjective bias from the decision space by utilizing an objective-weighted technique to quantify the risk. The study adopted Shannon's entropy to derive weights for each factor and its classes. The derived weighted layers are fed to an artificial neural network to obtain a final map of risk susceptibility. This final risk map allows policymakers to examine vulnerable areas and identify the factors pivotal to the contribution of risk. Findings show the traffic volume as the most influential variable, and terrain slope as the least one in the disease spread for the study area. The risk appears to be concentrated and distributed along vegetation, wetlands, and around water bodies. The results produced by ensemble learning show great promise with more than 94% accuracy. The accuracy of the results was determined by the confusion matrix and the kappa index of agreement (KIA). The vector control programmes need to adapt to better manage the dynamic changes in patterns involving vector-borne infectious diseases.",10.1002/mcda.1732,artificial neural networks; entropy; geographic information systems (GIS); infectious disease modelling; machine learning; multi-criteria decision analysis (MCDA),,
Dynamic algorithm selection for pareto optimal set approximation,"Steponavice, I; Hyndman, RJ; Smith-Miles, K; Villanova, L",JOURNAL OF GLOBAL OPTIMIZATION,2017.0,This paper presents a meta-algorithm for approximating the Pareto optimal set of costly black-box multiobjective optimization problems given a limited number of objective function evaluations. The key idea is to switch among different algorithms during the optimization search based on the predicted performance of each algorithm at the time. Algorithm performance is modeled using a machine learning technique based on the available information. The predicted best algorithm is then selected to run for a limited number of evaluations. The proposed approach is tested on several benchmark problems and the results are compared against those obtained using any one of the candidate algorithms alone.,10.1007/s10898-016-0420-x,Multiobjective optimization; Expensive black-box function; Machine learning; Classification; Algorithm selection; Hypervolume metric; Features,,
Predicting concrete compressive strength using hybrid ensembling of surrogate machine learning models,"Asteris, PG; Skentou, AD; Bardhan, A; Samui, P; Pilakoutas, K",CEMENT AND CONCRETE RESEARCH,2021.0,"This study aims to implement a hybrid ensemble surrogate machine learning technique in predicting the compressive strength (CS) of concrete, an important parameter used for durability design and service life prediction of concrete structures in civil engineering projects. For this purpose, an experimental database consisting of 1030 records has been compiled from the machine learning repository of the University of California, Irvine. The database was used to train and validate four conventional machine learning (CML) models, namely Artificial Neural Network (ANN), Linear and Non-Linear Multivariate Adaptive Regression Splines (MARS-L and MARS-C), Gaussian Process Regression (GPR), and Minimax Probability Machine Regression (MPMR). Subsequently, the predicted outputs of CML models were combined and trained using ANN to construct the Hybrid Ensemble Model (HENSM). It is observed that the proposed HENSM produces higher predictive accuracy compared to the CML models used in the present study. The predictive performance of all models for CS prediction was compared using the testing dataset and it is found that the HENSM model attained the highest predictive accuracy in both phases. Based on the experimental results, the newly constructed HENSM model is very potential to be a new alternative in handling the overfitting issues of CML models and hence, can be used to predict the concrete CS, including the design of less polluting and more sustainable concrete constructions.",10.1016/j.cemconres.2021.106449,Artificial intelligence; Soft computing; Compressive strength; Hybrid modelling; Score analysis,,
Horseshoe Regularisation for Machine Learning in Complex and Deep Models,"Bhadra, A; Datta, J; Li, YF; Polson, NG",INTERNATIONAL STATISTICAL REVIEW,2020.0,"Since the advent of the horseshoe priors for regularisation, global-local shrinkage methods have proved to be a fertile ground for the development of Bayesian methodology in machine learning, specifically for high-dimensional regression and classification problems. They have achieved remarkable success in computation and enjoy strong theoretical support. Most of the existing literature has focused on the linear Gaussian case; for which systematic surveys are available. The purpose of the current article is to demonstrate that the horseshoe regularisation is useful far more broadly, by reviewing both methodological and computational developments in complex models that are more relevant to machine learning applications. Specifically, we focus on methodological challenges in horseshoe regularisation in non-linear and non-Gaussian models, multivariate models and deep neural networks. We also outline the recent computational developments in horseshoe shrinkage for complex models along with a list of available software implementations that allows one to venture out beyond the comfort zone of the canonical linear regression problems.",10.1111/insr.12360,complex data; deep learning; large scale machine learning; non-linear; non-Gaussian; shrinkage,,
StressGAN: A Generative Deep Learning Model for Two-Dimensional Stress Distribution Prediction,"Jiang, HL; Nie, ZG; Yeo, R; Farimani, AB; Kara, LB",JOURNAL OF APPLIED MECHANICS-TRANSACTIONS OF THE ASME,2021.0,"Using deep learning to analyze mechanical stress distributions is gaining interest with the demand for fast stress analysis. Deep learning approaches have achieved excellent outcomes when utilized to speed up stress computation and learn the physical nature without prior knowledge of underlying equations. However, most studies restrict the variation of geometry or boundary conditions, making it difficult to generalize the methods to unseen configurations. We propose a conditional generative adversarial network (cGAN) model called StressGAN for predicting 2D von Mises stress distributions in solid structures. The StressGAN model learns to generate stress distributions conditioned by geometries, loads, and boundary conditions through a two-player minimax game between two neural networks with no prior knowledge. By evaluating the generative network on two stress distribution datasets under multiple metrics, we demonstrate that our model can predict more accurate stress distributions than a baseline convolutional neural-network model, given various and complex cases of geometries, loads, and boundary conditions.",10.1115/1.4049805,StressGAN; stress; conditional generative adversarial network; deep learning,,
Comparative study between deep learning and QSAR classifications for TNBC inhibitors and novel GPCR agonist discovery,"Tsou, LK; Yeh, SH; Ueng, SH; Chang, CP; Song, JS; Wu, MH; Chang, HF; Chen, SR; Shih, C; Chen, CT; Ke, YY",SCIENTIFIC REPORTS,2020.0,"Machine learning is a well-known approach for virtual screening. Recently, deep learning, a machine learning algorithm in artificial neural networks, has been applied to the advancement of precision medicine and drug discovery. In this study, we performed comparative studies between deep neural networks (DNN) and other ligand-based virtual screening (LBVS) methods to demonstrate that DNN and random forest (RF) were superior in hit prediction efficiency. By using DNN, several triple-negative breast cancer (TNBC) inhibitors were identified as potent hits from a screening of an in-house database of 165,000 compounds. In broadening the application of this method, we harnessed the predictive properties of trained model in the discovery of G protein-coupled receptor (GPCR) agonist, by which computational structure-based design of molecules could be greatly hindered by lack of structural information. Notably, a potent (similar to 500 nM) mu-opioid receptor (MOR) agonist was identified as a hit from a small-size training set of 63 compounds. Our results show that DNN could be an efficient module in hit prediction and provide experimental evidence that machine learning could identify potent hits in silico from a limited training set.",10.1038/s41598-020-73681-1,,,
An efficient algorithm for nonconvex-linear minimax optimization problem and its application in solving weighted maximin dispersion problem,"Pan, WW; Shen, JJ; Xu, Z",COMPUTATIONAL OPTIMIZATION AND APPLICATIONS,2021.0,"In this paper, we study the minimax optimization problem that is nonconvex in one variable and linear in the other variable, which is a special case of nonconvex-concave minimax problem, which has attracted significant attention lately due to their applications in modern machine learning tasks, signal processing and many other fields. We propose a new alternating gradient projection algorithm and prove that it can find an epsilon-first-order stationary solution within O(epsilon(-3)) projected gradient step evaluations. Moreover, we apply it to solve the weighted maximin dispersion problem and the numerical results show that the proposed algorithm outperforms the state-of-the-art algorithms.",10.1007/s10589-020-00237-4,Nonconvex-linear minimax problem; Complexity analysis; Weighted maximin dispersion problem,,
A comparative study of heterogeneous ensemble-learning techniques for landslide susceptibility mapping,"Fang, ZC; Wang, Y; Peng, L; Hong, HY",INTERNATIONAL JOURNAL OF GEOGRAPHICAL INFORMATION SCIENCE,2021.0,"This study introduces four heterogeneous ensemble-learning techniques, that is, stacking, blending, simple averaging, and weighted averaging, to predict landslide susceptibility in Yanshan County, China. These techniques combine several state-of-the-art classifiers of convolutional neural network, recurrent neural network, support vector machine, and logistic regression in specific ways to produce reliable results and avoid problems with the model selection. The study consists of three main steps. The first step establishes a spatial database consisting of 16 landslide conditioning factors and 380 historical landslide locations. The second step randomly selects training (70% of the total) and test (30%) datasets out of grid cells corresponding to landslide and non-slide locations in the study area. The final step constructs the proposed heterogeneous ensemble-learning methods for landslide susceptibility mapping. The proposed ensemble-learning methods show higher prediction accuracy than the individual classifiers mentioned above based on statistical measures. The blending ensemble-learning method achieves the highest overall accuracy of 80.70% compared to the other ensemble-learning methods.",10.1080/13658816.2020.1808897,Landslide susceptibility mapping; heterogeneous ensemble; stacking; blending; deep neural networks,,
An Augmented Lagrangian Deep Learning Method for Variational Problems with Essential Boundary Conditions,"Huang, JG; Wang, HQ; Zhou, T",COMMUNICATIONS IN COMPUTATIONAL PHYSICS,2022.0,"This paper is concerned with a novel deep learning method for variational problems with essential boundary conditions. To this end, we first reformulate the original problem into a minimax problem corresponding to a feasible augmented Lagrangian, which can be solved by the augmented Lagrangian method in an infinite dimensional setting. Based on this, by expressing the primal and dual variables with two individual deep neural network functions, we present an augmented Lagrangian deep learning method for which the parameters are trained by the stochastic optimization method together with a projection technique. Compared to the traditional penalty method, the new method admits two main advantages: i) the choice of the penalty parameter is flexible and robust, and ii) the numerical solution is more accurate in the same magnitude of computational cost. As typical applications, we apply the new approach to solve elliptic problems and (nonlinear) eigenvalue problems with essential boundary conditions, and numerical experiments are presented to show the effectiveness of the new method.",10.4208/cicp.OA-2021-0176,The augmented Lagrangian method; deep learning; variational problems; saddle point problems; essential boundary conditions,,
Pareto analysis of evolutionary and learning systems,"Jin, YC; Gruna, R; Sendhoff, B",FRONTIERS OF COMPUTER SCIENCE IN CHINA,2009.0,"This paper attempts to argue that most adaptive systems, such as evolutionary or learning systems, have inherently multiple objectives to deal with. Very often, there is no single solution that can optimize all the objectives. In this case, the concept of Pareto optimality is key to analyzing these systems. To support this argument, we first present an example that considers the robustness and evolvability trade-off in a redundant genetic representation for simulated evolution. It is well known that robustness is critical for biological evolution, since without a sufficient degree of mutational robustness, it is impossible for evolution to create new functionalities. On the other hand, the genetic representation should also provide the chance to find new phenotypes, i.e., the ability to innovate. This example shows quantitatively that a trade-off between robustness and innovation does exist in the studied redundant representation. Interesting results will also be given to show that new insights into learning problems can be gained when the concept of Pareto optimality is applied to machine learning. In the first example, a Pareto-based multi-objective approach is employed to alleviate catastrophic forgetting in neural network learning. We show that learning new information and memorizing learned knowledge are two conflicting objectives, and a major part of both information can be memorized when the multi-objective learning approach is adopted. In the second example, we demonstrate that a Pareto-based approach can address neural network regularization more elegantly. By analyzing the Pareto-optimal solutions, it is possible to identifying interesting solutions on the Pareto front.",10.1007/s11704-009-0004-8,Pareto analysis; multi-objective optimization; evolution; evolvability; robustness; learning; accuracy; complexity,,
A novel twin minimax probability machine for classification and regression,"Ma, J; Shen, JM",KNOWLEDGE-BASED SYSTEMS,2020.0,"As an excellent machine learning tool, the minimax probability machine (MPM) has been widely used in many fields. However, MPM does not include a regularization term for the construction of the separating hyperplane, and it needs to solve a large-scale second-order cone programming problem (SOCP) in the solution process, which greatly limits it development and application. In this paper, to improve the performance of MPM, we propose a novel binary classification method called twin minimax probability machine classification (TMPMC). The TMPMC constructs two non-parallel hyperplanes for final classification by solving two smaller SOCPs to improve the performance of the MPM. For each hyperplane, TMPMC attempts to minimize the worst case (maximum) probability that a class of samples is misclassified while being as far away as possible from the other class. Additionally, we extend TMPMC to the regression problem and propose a new regularized twin minimax probability machine regression (TMPMR). Intuitively, the idea of TMPMR is to convert the regression problem into a classification problem to solve. Both TMPMC and TMPMR avoid the assumption of distribution of conditional density. Finally, we extend the linear models of TMPMC and TMPMR to nonlinear case. Experimental results on several datasets show that TMPMC and TMPMR are competitive in terms of generalization performance compared to other algorithms. (C) 2020 Elsevier B.V. All rights reserved.",10.1016/j.knosys.2020.105703,Minimax probability machine; Classification; Regression; Non-parallel hyperplane; Second-order cone programming,,
Performance evaluation of evolutionary multiobjective optimization algorithms for multiobjective fuzzy genetics-based machine learning,"Ishibuchi, H; Nakashima, Y; Nojima, Y",SOFT COMPUTING,2011.0,"Recently, evolutionary multiobjective optimization (EMO) algorithms have been utilized for the design of accurate and interpretable fuzzy rule-based systems. This research area is often referred to as multiobjective genetic fuzzy systems (MoGFS), where EMO algorithms are used to search for non-dominated fuzzy rule-based systems with respect to their accuracy and interpretability. In this paper, we examine the ability of EMO algorithms to efficiently search for Pareto optimal or near Pareto optimal fuzzy rule-based systems for classification problems. We use NSGA-II (elitist non-dominated sorting genetic algorithm), its variants, and MOEA/D (multiobjective evolutionary algorithm based on decomposition) in our multiobjective fuzzy genetics-based machine learning (MoFGBML) algorithm. Classification performance of obtained fuzzy rule-based systems by each EMO algorithm is evaluated for training data and test data under various settings of the available computation load and the granularity of fuzzy partitions. Experimental results in this paper suggest that reported classification performance of MoGFS in the literature can be further improved using more computation load, more efficient EMO algorithms, and/or more antecedent fuzzy sets from finer fuzzy partitions.",10.1007/s00500-010-0669-9,Fuzzy rule-based classification; Genetic algorithms; Genetics-based machine learning; Multiobjective machine learning; Evolutionary multiobjective optimization,,
A machine learning-based usability evaluation method for eLearning systems,"Oztekin, A; Delen, D; Turkyilmaz, A; Zaim, S",DECISION SUPPORT SYSTEMS,2013.0,"The research presented in this paper proposes a new machine learning-based evaluation method for assessing the usability of eLearning systems. Three machine learning methods (support vector machines, neural networks and decision trees) along with multiple linear regression are used to develop prediction models in order to discover the underlying relationship between the overall eLearning system usability and its predictor factors. A subsequent sensitivity analysis is conducted to determine the rank-order importance of the predictors. Using both sensitivity values along with the usability scores, a metric (called severity index) is devised. By applying a Pareto-like analysis, the severity index values are ranked and the most important usability characteristics are identified. The case study results show that the proposed methodology enhances the determination of eLearning system problems by identifying the most pertinent usability factors. The proposed method could provide an invaluable guidance to the usability experts as to what measures should be improved in order to maximize the system usability for a targeted group of end-users of an eLearning system. (C) 2013 Elsevier B.V. All rights reserved.",10.1016/j.dss.2013.05.003,eLearning (web-based learning/distance learning); Usability engineering; Severity index; Information fusion; Sensitivity analysis; Machine learning,,
"A MINIMAX THEOREM WITH APPLICATIONS TO MACHINE LEARNING, SIGNAL PROCESSING, AND FINANCE","Kim, SJ; Boyd, S",SIAM JOURNAL ON OPTIMIZATION,2008.0,"This paper concerns a fractional function of the form x(T)a/root x(T)Bx, where B is positive definite. We consider the game of choosing x from a convex set, to maximize the function, and choosing (a, B) from a convex set, to minimize it. We prove the existence of a saddle point and describe an efficient method, based on convex optimization, for computing it. We describe applications in machine learning (robust Fisher linear discriminant analysis), signal processing (robust beamforming and robust matched filtering), and finance (robust portfolio selection). In these applications, x corresponds to some design variables to be chosen, and the pair (a, B) corresponds to the statistical model, which is uncertain.",10.1137/060677586,convex optimization; minimax theorem; robust optimization,,
An efficient multi-objective learning algorithm for RBF neural network,"Kokshenev, I; Braga, AP",NEUROCOMPUTING,2010.0,"Most of modern multi-objective machine learning methods are based on evolutionary optimization algorithms. They are known to be global convergent, however, usually deliver nondeterministic results. In this work we propose the deterministic global solution to a multi-objective problem of supervised learning with the methodology of nonlinear programming. As the result, the proposed multi-objective algorithm performs a global search of Pareto-optimal hypotheses in the space of RBF networks, determining their weights and basis functions. In combination with the Akaike and Bayesian information criteria, the algorithm demonstrates a high generalization efficiency on several synthetic and real-world benchmark problems. (C) 2010 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2010.06.022,Multi-objective learning; Radial-basis functions; Pareto-optimality; Model selection; Regularization,,
The stochastic multi-gradient algorithm for multi-objective optimization and its application to supervised machine learning,"Liu, S; Vicente, LN",ANNALS OF OPERATIONS RESEARCH,,"Optimization of conflicting functions is of paramount importance in decision making, and real world applications frequently involve data that is uncertain or unknown, resulting in multi-objective optimization (MOO) problems of stochastic type. We study the stochastic multi-gradient (SMG) method, seen as an extension of the classical stochastic gradient method for single-objective optimization. At each iteration of the SMG method, a stochastic multi-gradient direction is calculated by solving a quadratic subproblem, and it is shown that this direction is biased even when all individual gradient estimators are unbiased. We establish rates to compute a point in the Pareto front, of order similar to what is known for stochastic gradient in both convex and strongly convex cases. The analysis handles the bias in the multi-gradient and the unknown a priori weights of the limiting Pareto point. The SMG method is framed into a Pareto-front type algorithm for calculating an approximation of the entire Pareto front. The Pareto-front SMG algorithm is capable of robustly determining Pareto fronts for a number of synthetic test problems. One can apply it to any stochastic MOO problem arising from supervised machine learning, and we report results for logistic binary classification where multiple objectives correspond to distinct-sources data groups.",10.1007/s10479-021-04033-z,Multi-objective optimization; Pareto front; Stochastic gradient descent; Supervised machine learning,,
A Hybrid Machine Learning and Population Knowledge Mining Method to Minimize Makespan and Total Tardiness of Multi-Variety Products,"Qiu, YT; Ji, WX; Zhang, CY",APPLIED SCIENCES-BASEL,2019.0,"Nowadays, the production model of many enterprises is multi-variety customized production, and the makespan and total tardiness are the main metrics for enterprises to make production plans. This requires us to develop a more effective production plan promptly with limited resources. Previous research focuses on dispatching rules and algorithms, but the application of the knowledge mining method for multi-variety products is limited. In this paper, a hybrid machine learning and population knowledge mining method to minimize makespan and total tardiness for multi-variety products is proposed. First, through offline machine learning and data mining, attributes of operations are selected to mine the initial population knowledge. Second, an addition-deletion sorting method (ADSM) is proposed to reprioritize operations and then form the rule-based initial population. Finally, the nondominated sorting genetic algorithm II (NSGA-II) hybrid with simulated annealing is used to obtain the Pareto solutions. To evaluate the effectiveness of the proposed method, three other types of initial populations were considered under different iterations and population sizes. The experimental results demonstrate that the new approach has a good performance in solving the multi-variety production planning problems, whether it is the function value or the performance metric of the acquired Pareto solutions.",10.3390/app9245286,initial population; data mining; multi-variety; machine learning; production planning,,
Evolved GANs for generating Pareto set approximations,"Garciarena, U; Santana, R; Mendiburu, A",GECCO'18: PROCEEDINGS OF THE 2018 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE,2018.0,"In machine learning, generative models are used to create data samples that mimic the characteristics of the training data. Generative adversarial networks (GANs) are neural-network based generator models that have shown their capacity to produce realistic samples in different domains. In this paper we propose a neuro-evolutionary approach for evolving deep GAN architectures together with the loss function and generator-discriminator synchronization parameters. We also propose the problem of Pareto set (PS) approximation as a suitable benchmark to evaluate the quality of neural-network based generators in terms of the accuracy of the solutions they generate. The covering of the Pareto front (PF) by the generated solutions is used as an indicator of the mode-collapsing behavior of GANs. We show that it is possible to evolve GANs that generate good PS approximations. Our method scales to up to 784 variables and that it is capable to create architecture transferable across dimensions and functions.",10.1145/3205455.3205550,machine learning; generative adversarial network; neuroevolution,,
Orthogonalized Kernel Debiased Machine Learning for Multimodal Data Analysis,"Dai, XW; Li, LX",JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,,"Multimodal imaging has transformed neuroscience research. While it presents unprecedented opportunities, it also imposes serious challenges. Particularly, it is difficult to combine the merits of the interpretability attributed to a simple association model with the flexibility achieved by a highly adaptive nonlinear model. In this article, we propose an orthogonalized kernel debiased machine learning approach, which is built upon the Neyman orthogonality and a form of decomposition orthogonality, for multimodal data analysis. We target the setting that naturally arises in almost all multimodal studies, where there is a primary modality of interest, plus additional auxiliary modalities. We establish the root-N-consistency and asymptotic normality of the estimated primary parameter, the semi-parametric estimation efficiency, and the asymptotic validity of the confidence band of the predicted primary modality effect. Our proposal enjoys, to a good extent, both model interpretability and model flexibility. It is also considerably different from the existing statistical methods for multimodal data integration, as well as the orthogonality-based methods for high-dimensional inferences. We demonstrate the efficacy of our method through both simulations and an application to a multimodal neuroimaging study of Alzheimer's disease. Supplementary materials for this article are available online.",10.1080/01621459.2021.2013851,Basis expansion; High-dimensional inference; Multimodal data integration; Neuroimaging analysis; Neyman orthogonality; Reproducing kernel Hilbert space,,
Design of Cyclone Separator Critical Diameter Model Based on Machine Learning and CFD,"Park, D; Go, JS",PROCESSES,2020.0,"In this paper, the characteristics of the cyclone separator was analyzed from the Lagrangian perspective for designing the important dependent variables. The neural network network model was developed for predicting the separation performance parameter. Further, the predictive performances were compared between the traditional surrogate model and the developed neural network model. In order to design the important parameters of the cyclone separator based on the particle separation theory, the force acting until the particles are separated was calculated using the Lagrangian-based computational fluid dynamics (CFD) methodology. As a result, it was proved that the centrifugal force and drag acting on the critical diameter having a separation efficiency of 50% were similar, and the particle separation phenomenon in the cyclone occurred from the critical diameter, and it was set as an important dependent variable. For developing a critical diameter prediction model based on machine learning and multiple regression methods, unsteady-Reynolds averaged Navier-Stokes analyzes according to shape dimensions were performed. The input design variables for predicting the critical diameter were selected as four geometry parameters that affect the turbulent flow inside the cyclone. As a result of comparing the model prediction performances, the machine learning (ML) model, which takes into account the critical diameter and the nonlinear relationship of cyclone design variables, showed a 32.5% improvement in R-square compared to multi linear regression (MLR). The proposed techniques have proven to be fast and practical tools for cyclone design.",10.3390/pr8111521,cyclone separator; computational fluid dynamics (CFD); machine learning; unsteady RANS; critical diameter,,
Prediction of multi-criteria optimization (MCO) parameter efficiency in volumetric modulated arc therapy (VMAT) treatment planning using machine learning (ML),"Harrer, C; Ullrich, W; Wilkens, JJ",PHYSICA MEDICA-EUROPEAN JOURNAL OF MEDICAL PHYSICS,2021.0,"Purpose: To predict the impact of optimization parameter changes on dosimetric plan quality criteria in multicriteria optimized volumetric-modulated-arc therapy (VMAT) planning prior to optimization using machine learning (ML). Methods: A data base comprising a total of 21,266 VMAT treatment plans for 44 cranial and 18 spinal patient geometries was generated. The underlying optimization algorithm is governed by three highly composite parameters which model a combination of important aspects of the solution. Patient geometries were parametrized via volume- and shape properties of the voxel objects and overlap-volume histograms (OVH) of the planningtarget-volume (PTV) and a relevant organ-at-risk (OAR). The impact of changes in one of the three optimization parameters on the maximally achievable value range of five dosimetric properties of the resulting dose distributions was studied. To predict the extent of this impact based on patient geometry, treatment site, and current parameter settings prior to optimization, three different ML-models were trained and tested. Precision-recall curves, as well as the area-under-curve (AUC) of the resulting receiver-operator-characteristic (ROC) curves were analyzed for model assessment. Results: Successful identification of parameter regions resulting in a high variability of dosimetric plan properties depended on the choice of geometry features, the treatment indication and the plan property under investigation. AUC values between 0.82 and 0.99 could be achieved. The best average-precision (AP) values obtained from the corresponding precision/recall curves ranged from 0.71 to 0.99. Conclusions: Machine learning models trained on a database of pre-optimized treatment plans can help finding relevant optimization parameter ranges prior to optimization.",10.1016/j.ejmp.2020.12.004,Machine learning; Radiotherapy treatment planning; External beam radiotherapy; Volumetric modulated arc therapy; Multicriteria optimization; VMAT,,
Automated ReaxFF parametrization using machine learning,"Daksha, CM; Yeon, J; Chowdhury, SC; Gillespie, JW",COMPUTATIONAL MATERIALS SCIENCE,2021.0,"Molecular dynamics (MD) simulation requires an accurate potential energy function to describe atomic interactions of interest. Optimization of the function's numerous parameters is often time-consuming and laborintensive. In this study, a machine learning inspired evolutionary parametrization technique using the genetic algorithm is developed to decrease the time required to optimize the parameters of the ReaxFF interatomic potential. An artificial neural network is used as a surrogate for the ReaxFF potential to reduce computational time. Changes to the genetic algorithm are incrementally benchmarked for accuracy and time cost with respect to a moderately complex zinc-oxide model to find superior operators for ReaxFF parametrization. It is found that utilizing an artificial neural network significantly boosted performance, as measured by the final total error and the rate of decrease of total error with respect to time. The double-Pareto probability density based crossover operator and a multiple standard deviation based Gaussian mutation scheme outperform their counterparts. The computational time cost to achieve the same level of accuracy relative to manual training is decreased from months to days.",10.1016/j.commatsci.2020.110107,Genetic algorithm; Machine learning; Neural network; Reactive potential; Molecular dynamics simulation,,
Co-optimizing water-alternating-carbon dioxide injection projects using a machine learning assisted computational framework,"You, JY; Ampomah, W; Sun, Q",APPLIED ENERGY,2020.0,"In this article, a robust machine-learning-based computational framework that couples multi-layer neural network (MLNN) proxies and a multi-objective particle swarm optimizer (MOPSO) to design water-alternating carbon dioxide injection (CO2-WAG) projects is presented. The proposed optimization protocol considers various objectives, including oil recovery and CO2 storage volume. Expert MLNN systems are trained and employed as surrogate models of the high-fidelity compositional simulator in the optimization workflow. When multiple objective functions are considered, two approaches are employed to treat the objectives: the weighted sum method and the Pareto-front-based scheme. A field-scale implementation focusing on tertiary recovery in the Morrow B formation at Farnsworth Unit (FWU) is presented. The developed Pareto-optimal solutions indicate the maximal available oil production can be 1.64 x 10(7) barrels and maximal carbon storage can achieve 2.35 x 10(7) tons. Trade-offs factor is defined to divide the constructed Pareto front into 4 sections with the trade-off factors' value ranges from 0.35 to 49.9. This work also compares the optimum solution found by the aggregative objective function and the solution repository covered by the Pareto front that considers the physical and operational constraints and reduces uncertainties involved by the multi-objective optimization process. Our comparison indicates multiple solutions exist to satisfy the objective criteria of the WAG design, and these results cannot be found using the traditional weighted sum method. The Pareto front solution can provide more options for project designers, but decisions regarding necessary trade-offs must be made using the solution repository to balance the project economics and CO2 storage amount.",10.1016/j.apenergy.2020.115695,Carbon dioxide sequestration; CO2-EOR; Multi-objective optimization; Artificial neural network,,
A biased minimax probability machine-based scheme for relevance feedback in image retrieval,"Peng, X; King, I",NEUROCOMPUTING,2009.0,"In recent years, minimax probability machines (MPMs) have demonstrated excellent performance in a variety of pattern recognition problems. At the same time various machine learning methods have been applied on relevance feedback tasks in content-based image retrieval (CBIR). One of the problems in typical techniques for relevance feedback is that they treat the relevant feedback and irrelevant feedback equally. Since the negative instances largely outnumber the positive instances, the assumption that they are balanced is incorrect as the data are biased. In this paper we study how biased minimax probability machine (BMPM), a variation of MPM, can be applied for relevance feedback in image retrieval tasks. Different from previous methods, this model directly controls the accuracy of classification of the future data to construct biased classifiers. Hence, it provides a rigorous treatment on imbalanced dataset. Mathematical formulation and explanations are provided to demonstrate the advantages. Experiments are conducted to evaluate the performance of our proposed framework, in which encouraging and promising experimental results are obtained. Crown Copyright (c) 2008 Published by Elsevier B.V. All rights reserved.",10.1016/j.neucom.2008.11.020,Biased minimax probability machine; Relevance feedback; Content-based image retrieval,,
Probabilistic Pareto plan generation for semiautomated multicriteria radiation therapy treatment planning,"Zhang, TF; Bokrantz, R; Olsson, J",PHYSICS IN MEDICINE AND BIOLOGY,2022.0,"Objective. We propose a semiautomatic pipeline for radiation therapy treatment planning, combining ideas from machine learning-automated planning and multicriteria optimization (MCO). Approach. Using knowledge extracted from historically delivered plans, prediction models for spatial dose and dose statistics are trained and furthermore systematically modified to simulate changes in tradeoff priorities, creating a set of differently biased predictions. Based on the predictions, an MCO problem is subsequently constructed using previously developed dose mimicking functions, designed in such a way that its Pareto surface spans the range of clinically acceptable yet realistically achievable plans as exactly as possible. The result is an algorithm outputting a set of Pareto optimal plans, either fluence-based or machine parameter-based, which the user can navigate between in real time to make adjustments before a final deliverable plan is created. Main results. Numerical experiments performed on a dataset of prostate cancer patients show that one may often navigate to a better plan than one produced by a single-plan-output algorithm. Significance. We demonstrate the potential of merging MCO and a data-driven workflow to automate labor-intensive parts of the treatment planning process while maintaining a certain extent of manual control for the user.",10.1088/1361-6560/ac4da5,knowledge-based planning; multicriteria optimization; dose prediction; dose-volume histogram prediction; uncertainty modeling; dose mimicking,,
Confident Predictability: Identifying reliable gene expression patterns for individualized tumor classification using a local minimax kernel algorithm,"Jones, LK; Zou, F; Kheifets, A; Rybnikov, K; Berry, D; Tan, AC",BMC MEDICAL GENOMICS,2011.0,"Background: Molecular classification of tumors can be achieved by global gene expression profiling. Most machine learning classification algorithms furnish global error rates for the entire population. A few algorithms provide an estimate of probability of malignancy for each queried patient but the degree of accuracy of these estimates is unknown. On the other hand local minimax learning provides such probability estimates with best finite sample bounds on expected mean squared error on an individual basis for each queried patient. This allows a significant percentage of the patients to be identified as confidently predictable, a condition that ensures that the machine learning algorithm possesses an error rate below the tolerable level when applied to the confidently predictable patients. Results: We devise a new learning method that implements: (i) feature selection using the k-TSP algorithm and (ii) classifier construction by local minimax kernel learning. We test our method on three publicly available gene expression datasets and achieve significantly lower error rate for a substantial identifiable subset of patients. Our final classifiers are simple to interpret and they can make prediction on an individual basis with an individualized confidence level. Conclusions: Patients that were predicted confidently by the classifiers as cancer can receive immediate and appropriate treatment whilst patients that were predicted confidently as healthy will be spared from unnecessary treatment. We believe that our method can be a useful tool to translate the gene expression signatures into clinical practice for personalized medicine.",10.1186/1755-8794-4-10,,,
MODE: multiobjective differential evolution for feature selection and classifier ensemble,"Sikdar, UK; Ekbal, A; Saha, S",SOFT COMPUTING,2015.0,"In this paper, we propose a multiobjective differential evolution (MODE)-based feature selection and ensemble learning approaches for entity extraction in biomedical texts. The first step of the algorithm concerns with the problem of automatic feature selection in a machine learning framework, namely conditional random field. The final Pareto optimal front which is obtained as an output of the feature selection module contains a set of solutions, each of which represents a particular feature representation. In the second step of our algorithm, we combine a subset of these classifiers using a MODE-based ensemble technique. Our experiments on three benchmark datasets namely GENIA, GENETAG and AIMed show the F-measure values of 76.75, 94.15 and 91.91 %, respectively. Comparisons with the existing systems show that our proposed algorithm achieves the performance levels which are at par with the state of the art. These results also exhibit that our method is general in nature and because of this it performs well across the several domain of datasets. The key contribution of this work is the development of MODE-based generalized feature selection and ensemble learning techniques with the aim of extracting entities from the biomedical texts of several domains.",10.1007/s00500-014-1565-5,,,
Prediction of Weight Percentage Alumina and Pore Volume Fraction in Bio-Ceramics Using Gaussian Process Regression and Minimax Probability Machine Regression,"Gopinath, KGS; Pal, S; Tambe, P",MATERIALS TODAY-PROCEEDINGS,2018.0,"In Bio-ceramics, the alumina weight percentage and pore volume fraction play a vital role for its biocompatibility in human body. There are many experimental methods which are employed for achieving the required quality in it. In this work, for preparation of Al2O3/SiC ceramic cake, the amount of Silicon Carbide (SiC) is taken as input parameter. The weight percentage Alumina and pore volume fraction are taken as output parameters. Two machine learning models such as Gaussian Process Regression (GPR) and Minimax Probability Machine Regression (MPMR) are applied for predicting the above two output parameters. The performance of the above two models are compared. The Gaussian Process Regression outperforms the Minimax Probability Machine Regression marginally and the result of the Gaussian is encouraging for predicting the above two outputs.",10.1016/j.matpr.2018.02.200,Gaussian Process Regression; Minimax Probability Machine Regression; Weight Percentage Alumina; Pore Volume,,
A novel feature selection approach with Pareto optimality for multi-label data,"Li, GH; Li, Y; Zheng, YF; Li, Y; Hong, YF; Zhou, XM",APPLIED INTELLIGENCE,2021.0,"Multi-label learning has widely applied in machine learning and data mining. The purpose of feature selection is to select an approximately optimal feature subset to characterize the original feature space. Similar to single-label data, feature selection is an import preprocessing step to enhance the performance of multi-label classification model. In this paper, we propose a multi-label feature selection approach with Pareto optimality for continuous data, called MLFSPO. It maps multi-label features to high-dimensional space to evaluate the correlation between features and labels by utilizing the Hilbert-Schmidt Independence Criterion (HSIC). Then, the feature subset obtains by combining the Pareto optimization with feature ordering criteria and label weighting. Eventually, extensive experimental results on publicly available data sets show the effectiveness of the proposed algorithm in multi-label tasks.",10.1007/s10489-021-02228-2,Feature selection; Multi-label learning; Pareto optimality; Hilbert-Schmidt independence criterion,,
Pareto Inspired Multi-objective Rule Fitness for Noise-Adaptive Rule-Based Machine Learning,"Urbanowicz, RJ; Olson, RS; Moore, JH",PARALLEL PROBLEM SOLVING FROM NATURE - PPSN XIV,2016.0,"Learning classifier systems (LCSs) are rule-based evolutionary algorithms uniquely suited to classification and data mining in complex, multi-factorial, and heterogeneous problems. The fitness of individual LCS rules is commonly based on accuracy, but this metric alone is not ideal for assessing global rule 'value' in noisy problem domains and thus impedes effective knowledge extraction. Multi-objective fitness functions are promising but rely on prior knowledge of how to weigh objective importance (typically unavailable in real world problems). The Pareto-front concept offers a multi-objective strategy that is agnostic to objective importance. We propose a Pareto-inspired multi-objective rule fitness (PIMORF) for LCS, and combine it with a complimentary rule-compaction approach (SRC). We implemented these strategies in ExSTraCS, a successful supervised LCS and evaluated performance over an array of complex simulated noisy and clean problems (i.e. genetic and multiplexer) that each concurrently model pure interaction effects and heterogeneity. While evaluation over multiple performance metrics yielded mixed results, this work represents an important first step towards efficiently learning complex problem spaces without the advantage of prior problem knowledge. Overall the results suggest that PIMORF paired with SRC improved rule set interpretability, particularly with regard to heterogeneous patterns.",10.1007/978-3-319-45823-6_48,Data mining; Classifier systems; Fitness evaluation; Multi-objective optimization; Machine learning,,
A Pareto-smoothing method for causal inference using generalized Pareto distribution,"Zhu, FJ; Lu, J; Lin, A; Zhang, GQ",NEUROCOMPUTING,2020.0,"Causal inference aims to estimate the treatment effect of an intervention on the target outcome variable and has received great attention across fields ranging from economics and statistics to machine learning. Observational causal inference is challenging because the pre-treatment variables may influence both the treatment and the outcome, resulting in confounding bias. The classic inverse propensity weighting (IPW) estimator is theoretically able to eliminate the confounding bias. However, in observational studies, the propensity scores used in the IPW estimator must be estimated from finite observational data and may be subject to extreme values, leading to the problem of highly variable importance weights, which consequently makes the estimated causal effect unstable or even misleading. In this paper, by reframing the IPW estimator in the importance sampling framework, we propose a Pareto-smoothing method to tackle this problem. The generalized Pareto distribution (GPD) from extreme value theory is used to fit the upper tail of the estimated importance weights and to replace them using the order statistics of the fitted GPD. To validate the performance of the new method, we conducted extensive experiments on simulated and semi-simulated datasets. Compared with two existing methods for importance weight stabilization, i.e., weight truncation and self-normalization, the proposed method generally achieves better performance in settings with a small sample size and high-dimensional covariates. Its application on a real-world heath dataset indicates its utility in estimating causal effects for program evaluation. (C) 2019 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2019.09.095,Causality; Causal inference; Machine learning; Treatment effect; Importance sampling,,
A novel machine learning approach for software reliability growth modelling with pareto distribution function,"Sudharson, D; Prabha, D",SOFT COMPUTING,2019.0,"Software reliability is the important quantifiable attribute in gaining reliability by assessing faults at the time of testing in the software products. Time-based software reliability models used to identify the defects in the product, and it is not suitable for dynamic situations. Instead of time, test effect is used in few explorations through effort function and it is not realistic for infinite testing time. Identifying number of defects is essential in software reliability models, and this research work presents a Pareto distribution (PD) to predict the fault distribution of software under homogenous and nonhomogeneous conditions along with artificial neural network (ANN). This methodology enables the parallel evolution of a product through NN models which exhibit estimated Pareto optimality with respect to multiple error measures. The proposed PD-ANN-based SRGM describes types of failure data and also improves the accuracy of parameter estimation more than existing growth models such as homogeneous poison process and two fuzzy time series-based software reliability models. Experimental evidence is presented for general application and the proposed framework by generating solutions for different product and developer indexes.",10.1007/s00500-019-04047-7,Software reliability; Artificial neural networks; Pareto distribution; Distribution parameter estimation,,
Identifying Pareto-based solutions for regression subset selection via a feasible solution algorithm,"Lambert, JW; Hawk, GS",INTERNATIONAL JOURNAL OF DATA SCIENCE AND ANALYTICS,2020.0,"The concept of Pareto optimality has been utilized in fields such as engineering and economics to understand fluid dynamics and consumer behavior. In machine learning contexts, Pareto-optimality has been used to identify tuning parameters that best optimize a set ofmcriteria (multi-objective optimization). During the process of regression model selection, data scientists are often concerned with choosing a model which has the best single criterion (e.g., Akaike information criterion (AIC) orR-squared (R-2)) before continuing to check a number of other regression model characteristics (e.g., model size, form, diagnostics, and interpretability). This strategy is multi-objective in nature but single objective in its numeric execution. This paper will first introduce a feasible solution algorithm (FSA) and explain how it can be applied to multi-objective problems for regression subset selection. Then we introduce the general framework of Pareto optimality within the regression setting. We then apply the algorithm in a simulation setting where we seek to estimate the first four Pareto boundaries for regression models using two model fit criteria. Finally, we present an application where we use a US communities and crime dataset.",10.1007/s41060-020-00218-0,Pareto; Optimal; Feasible solution; Multiple; Objective; Subset selection; Regression,,
Customer tiered purchase forecast by mobile edge computing based on Pareto/NBD and SVR,"Li, Y; Zhang, Y; Luo, F; Zou, W; Zhang, Y; Zhou, KJ",CHINA COMMUNICATIONS,2021.0,"Mobile edge computing is trending nowadays for its computation efficiency and privacy. The rapid development of e-commerce show great interest in mobile edge computing due to numerous rise of small and middle-sized enterprises(SMEs) in the internet. This paper predicts the overall sales volume of the enterprise through the classic ARIMA model, and notes that the behavior and arrival differences between the new and old customer groups will affect the accuracy of our forecasts, so we then use Pareto/NBD to explore the repeated purchases of customers at the individual level of the old customer and the SVR model to predict the arrival of new customers, thus helping the enterprise to make layered and accurate marketing of new and old customers through machine learning. In general, machine learning relies on powerful computation and storage resources, while mobile edge computing typically provides limited computation resources locally. Therefore, it is essential to combine machine learning with mobile edge computing to further promote the proliferation of data analysis among SMEs.",10.23919/JCC.2021.11.001,Predictive models; Mathematical models; Electronic commerce; Solid modeling; Multi-access edge computing; Industries; Data models; e-commerce; customer behavior; Pareto; NBD model; SVR model; ARIMA model; mobile edge computing,,
Smart Design Space Sampling to Predict Pareto-Optimal Solutions,"Zuluaga, M; Krause, A; Milder, P; Puschel, M",ACM SIGPLAN NOTICES,2012.0,"Many high-level synthesis tools offer degrees of freedom in mapping high-level specifications to Register-Transfer Level descriptions. These choices do not affect the functional behavior but span a design space of different cost-performance tradeoffs. In this paper we present a novel machine learning-based approach that efficiently determines the Pareto-optimal designs while only sampling and synthesizing a fraction of the design space. The approach combines three key components: (1) A regression model based on Gaussian processes to predict area and throughput based on synthesis training data. (2) A smart sampling strategy, GP-PUCB, to iteratively refine the model by carefully selecting the next design to synthesize to maximize progress. (3) A stopping criterion based on assessing the accuracy of the model without access to complete synthesis data. We demonstrate the effectiveness of our approach using IP generators for discrete Fourier transforms and sorting networks. However, our algorithm is not specific to this application and can be applied to a wide range of Pareto front prediction problems.",10.1145/2345141.2248436,Algorithms; Performance; Pareto Optimality; High-Level Synthesis; Area and Performance Estimation; Machine Learning,,
An integrated decision analytic framework of machine learning with multi-criteria decision making for multi-attribute inventory classification,"Kartal, H; Oztekin, A; Gunasekaran, A; Cebi, F",COMPUTERS & INDUSTRIAL ENGINEERING,2016.0,"The purpose of this study is to develop a hybrid methodology that integrates machine learning algorithms with multi-criteria decision making (MCDM) techniques to effectively conduct multi-attribute inventory analysis. In the proposed methodology, first, ABC analyses using three different MCDM methods (i.e. simple-additive weighting, analytical hierarchy process, and VIKOR) are employed to determine the appropriate class for each of the inventory items. Following this, na ve Bayes, Bayesian network, artificial neural network (ANN), and support vector machine (SVM) algorithms are implemented to predict classes of initially determined stock items. Finally, the detailed prediction performance metrics of algorithms for each method are determined. The comprehensive case study executed at a large-scale automotive company revealed that the best classification accuracy is achieved by SVMs. The results also revealed that Bayesian networks, SVMs and ANNs are all capable of successfully dealing with the unbalanced data problems associated with Pareto distribution, and each of these algorithms performed well against all examined measures, thus validating the fact that machine learning algorithms are highly applicable to inventory classification problems. Therefore, this study presents uniqueness in that it is the first and foremost of its kind to effectively combine MCDM methods with machine learning algorithms in multi attribute inventory classification and is practically applicable in various inventory settings. Furthermore, this study also provides a comprehensive chronological overview of the existing literature of machine learning methods within inventory classification problems. (C) 2016 Elsevier Ltd. All rights reserved.",10.1016/j.cie.2016.06.004,Multi-attribute inventory classification; ABC analysis; Business analytics; Data mining,,
Adversarial Machine Learning in Recommender Systems (AML-RecSys),"Deldjoo, Y; Di Noia, T; Merra, FA",PROCEEDINGS OF THE 13TH INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM '20),2020.0,"Recommender systems (RS) are an integral part of many online services aiming to provide an enhanced user-oriented experience. Machine learning (ML) models are nowadays broadly adopted in modern state-of-the-art approaches to recommendation, which are typically trained to maximize a user-centred utility (e.g., user satisfaction) or a business-oriented one (e.g., profitability or sales increase). They work under the main assumption that users' historical feedback can serve as proper ground-truth for model training and evaluation. However, driven by the success in the ML community, recent advances show that state-of-the-art recommendation approaches such as matrix factorization (MF) models or the ones based on deep neural networks can be vulnerable to adversarial perturbations applied on the input data. These adversarial samples can impede the ability for training high-quality MF models and can put the driven success of these approaches at high risk. As a result, there is a new paradigm of secure training for RS that takes into account the presence of adversarial samples into the recommendation process. We present adversarial machine learning in Recommender Systems (AML-RecSys), which concerns the study of effective ML techniques in RS to fight against an adversarial component. AML-RecSys has been proposed in two main fashions within the RS literature: (i) adversarial regularization, which attempts to combat against adversarial perturbation added to input data or model parameters of a RS and, (ii) generative adversarial network (GAN)-based models, which adopt a generative process to train powerful ML models. We discuss a theoretical framework to unify the two above models, which is performed via a minimax game between an adversarial component and a discriminator. Furthermore, we explore various examples illustrating the successful application of AML to solve various RS tasks. Finally, we present a global taxonomy/overview of the academic literature based on several identified dimensions, namely (i) research goals and challenges, (ii) application domains and (iii) technical overview.",10.1145/3336191.3371877,,,
Deep learning for discrete-time hedging in incomplete markets,"Fecamp, S; Mikael, J; Warin, X",JOURNAL OF COMPUTATIONAL FINANCE,2021.0,"This paper presents several algorithms based on machine learning to solve hedging problems in incomplete markets. The sources of incompleteness considered here are illiquidity, nontradable risk factors, discrete hedging dates and proportional transaction costs. Hedging strategies suggested by the algorithms introduced in this paper are compared with classical stochastic-control techniques on several payoffs using a mean squared error (MSE) criterion. Some of the proposed algorithms are flexible enough to deal with innovative loss criteria, and the profit and loss (P&L) distributions of the hedging strategies obtained with these new criteria are compared to the P&L distributions obtained with the classical MSE criterion. The most efficient algorithm is tested on a case with nonzero transaction costs, and we show how to obtain a whole Pareto frontier in a single training phase by randomly combining the criteria of average cost and variance during the learning phase.",10.21314/JCF.2021.006,incomplete markets; transaction costs; deep learning; long short-term memory (LSTM); discrete-time hedging,,
Comparison of machine learning techniques to predict compressive strength of concrete,"Dutta, S; Samui, P; Kim, D",COMPUTERS AND CONCRETE,2018.0,"In the present study, soft computing i.e., machine learning techniques and regression models algorithms have earned much importance for the prediction of the various parameters in different fields of science and engineering. This paper depicts that how regression models can be implemented for the prediction of compressive strength of concrete. Three models are taken into consideration for this; they are Gaussian Process for Regression (GPR), Multi Adaptive Regression Spline (MARS) and Minimax Probability Machine Regression (MPMR). Contents of cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, fine aggregate and age in days have been taken as inputs and compressive strength as output for GPR, MARS and MPMR models. A comparatively large set of data including 1030 normalized previously published results which were obtained from experiments were utilized. Here, a comparison is made between the results obtained from all the above mentioned models and the model which provides the best fit is established. The experimental results manifest that proposed models are robust for determination of compressive strength of concrete.",10.12989/cac.2018.21.4.463,concrete; compressive strength; Gaussian Process for Regression (GPR); Multi Adaptive Regression Spline (MARS); Minimax Probability Machine Regression (MPMR),,
Machine Learning for Materials Research and Development,"Xie, JX; Su, YJ; Xue, DZ; Jiang, X; Fu, HD; Huang, HY",ACTA METALLURGICA SINICA,2021.0,"The rapid advancement of big data and artificial intelligence has resulted in new data-driven materials research and development (R&D), which has achieved substantial progress. This fourth paradigm is believed to improve materials design efficiency and industrialized application and stimulate the discovery of new materials. The focus of this work is on the emerging field of machine learning-assisted material R&D, with an emphasis on machine learning predictions and optimization design. Following a brief description of feature construction and selection, recent developments in material predictions on phases/structures, processing-structure-property relationships, microstructure, and material performance are reviewed. This paper also summarizes the research progress on optimization algorithms with machine learning models, which is expected to overcome the bottlenecks such as the small size and high noise level of material data samples and huge space for exploration. The challenges and future opportunities for machine learning applications in materials R&D are discussed and prospected.",10.11900/0412.1961.2021.00357,materials data; data mining; machine learning; material design; material genome engineering,,
A two-stage minimax concave penalty based method in pruned AdaBoost ensemble,"Jiang, H; Zheng, WH; Luo, LQ; Dong, Y",APPLIED SOFT COMPUTING,2019.0,"AdaBoost is a highly effective ensemble learning method that combines several weak learners to produce a strong committee with higher accuracy. However, similar to other ensemble methods, AdaBoost uses a large number of base learners to produce the final outcome while addressing high-dimensional data. Thus, it poses a critical challenge in the form of high memory-space consumption. Feature selection methods can significantly reduce dimensionality in regression and have been established to be applicable in ensemble pruning. By pruning the ensemble, it is possible to generate a simpler ensemble with fewer base learners but a higher accuracy. In this article, we propose the minimax concave penalty (MCP) function to prune an AdaBoost ensemble to simplify the model and improve its accuracy simultaneously. The MCP penalty function is compared with LASSO and SCAD in terms of performance in pruning the ensemble. Experiments performed on real datasets demonstrate that MCP-pruning outperforms the other two methods. It can reduce the ensemble size effectively, and generate marginally more accurate predictions than the unpruned AdaBoost model. (C) 2019 Elsevier B.V. All rights reserved.",10.1016/j.asoc.2019.105674,Ensemble pruning; Feature selection; Minimax concave penalty; AdaBoost,,
MultiETSC: automated machine learning for early time series classification,"Ottervanger, G; Baratchi, M; Hoos, HH",DATA MINING AND KNOWLEDGE DISCOVERY,2021.0,"Early time series classification (EarlyTSC) involves the prediction of a class label based on partial observation of a given time series. Most EarlyTSC algorithms consider the trade-off between accuracy and earliness as two competing objectives, using a single dedicated hyperparameter. To obtain insights into this trade-off requires finding a set of non-dominated (Pareto efficient) classifiers. So far, this has been approached through manual hyperparameter tuning. Since the trade-off hyperparameters only provide indirect control over the earliness-accuracy trade-off, manual tuning is tedious and tends to result in many sub-optimal hyperparameter settings. This complicates the search for optimal hyperparameter settings and forms a hurdle for the application of EarlyTSC to real-world problems. To address these issues, we propose an automated approach to hyperparameter tuning and algorithm selection for EarlyTSC, building on developments in the fast-moving research area known as automated machine learning (AutoML). To deal with the challenging task of optimising two conflicting objectives in early time series classification, we propose MultiETSC, a system for multi-objective algorithm selection and hyperparameter optimisation (MO-CASH) for EarlyTSC. MultiETSC can potentially leverage any existing or future EarlyTSC algorithm and produces a set of Pareto optimal algorithm configurations from which a user can choose a posteriori. As an additional benefit, our proposed framework can incorporate and leverage time-series classification algorithms not originally designed for EarlyTSC for improving performance on EarlyTSC; we demonstrate this property using a newly defined, naive fixed-time algorithm. In an extensive empirical evaluation of our new approach on a benchmark of 115 data sets, we show that MultiETSC performs substantially better than baseline methods, ranking highest (avg. rank 1.98) compared to conceptually simpler single-algorithm (2.98) and single-objective alternatives (4.36).",10.1007/s10618-021-00781-5,Early classification; Time series classification; Automated machine learning,,
MODC: A Pareto-Optimal Optimization Approach for Network Traffic Classification Based on the Divide and Conquer Strategy,"Nascimento, Z; Sadok, D",INFORMATION,2018.0,"Network traffic classification aims to identify categories of traffic or applications of network packets or flows. It is an area that continues to gain attention by researchers due to the necessity of understanding the composition of network traffics, which changes over time, to ensure the network Quality of Service (QoS). Among the different methods of network traffic classification, the payload-based one (DPI) is the most accurate, but presents some drawbacks, such as the inability of classifying encrypted data, the concerns regarding the users' privacy, the high computational costs, and ambiguity when multiple signatures might match. For that reason, machine learning methods have been proposed to overcome these issues. This work proposes a Multi-Objective Divide and Conquer (MODC) model for network traffic classification, by combining, into a hybrid model, supervised and unsupervised machine learning algorithms, based on the divide and conquer strategy. Additionally, it is a flexible model since it allows network administrators to choose between a set of parameters (pareto-optimal solutions), led by a multi-objective optimization process, by prioritizing flow or byte accuracies. Our method achieved 94.14% of average flow accuracy for the analyzed dataset, outperforming the six DPI-based tools investigated, including two commercial ones, and other machine learning-based methods.",10.3390/info9090233,network traffic classification; machine learning; hybrid model; multi-objective genetic algorithm; extreme learning machine; growing hierarchical self-organizing map,,
The explanation game: a formal framework for interpretable machine learning,"Watson, DS; Floridi, L",SYNTHESE,2021.0,"We propose a formal framework for interpretable machine learning. Combining elements from statistical learning, causal interventionism, and decision theory, we design an idealised explanation game in which players collaborate to find the best explanation(s) for a given algorithmic prediction. Through an iterative procedure of questions and answers, the players establish a three-dimensional Pareto frontier that describes the optimal trade-offs between explanatory accuracy, simplicity, and relevance. Multiple rounds are played at different levels of abstraction, allowing the players to explore overlapping causal patterns of variable granularity and scope. We characterise the conditions under which such a game is almost surely guaranteed to converge on a (conditionally) optimal explanation surface in polynomial time, and highlight obstacles that will tend to prevent the players from advancing beyond certain explanatory thresholds. The game serves a descriptive and a normative function, establishing a conceptual space in which to analyse and compare existing proposals, as well as design new and improved solutions.",10.1007/s11229-020-02629-9,Algorithmic explainability; Explanation game; Interpretable machine learning; Pareto frontier; Relevance,,
Pragmatic generative optimization of novel structural lattice metamaterials with machine learning,"Garland, AP; White, BC; Jensen, SC; Boyce, BL",MATERIALS & DESIGN,2021.0,"Metamaterials, otherwise known as architected or programmable materials, enable designers to tailor mesoscale topology and shape to achieve unique material properties that are not present in nature. Additionally, with the recent proliferation of additive manufacturing tools across industrial sectors, the ability to readily fabricate geometrically complex metamaterials is now possible. However, in many high-performance applications involving complex multi-physics interactions, design of novel lattice metamaterials is still difficult. Design is primarily guided by human intuition or gradient optimization for simple problems. In this work, we show how machine learning guides discovery of new unit cells that are Pareto optimal for multiple competing objectives; specifically, maximizing elastic stiffness during static loading and minimizing wave speed through the metamaterial during an impact event. Additionally, we show that our artificial intelligence approach works with relatively few (3500) simulation calls. (C) 2021 The Authors. Published by Elsevier Ltd.",10.1016/j.matdes.2021.109632,,,
Minimax optimal goodness-of-fit testing for densities and multinomials under a local differential privacy constraint,"Lam-Weil, J; Laurent, B; Loubes, JM",BERNOULLI,2022.0,"Finding anonymization mechanisms to protect personal data is at the heart of recent machine learning research. Here, we consider the consequences of local differential privacy constraints on goodness-of-fit testing, that is, the statistical problem assessing whether sample points are generated from a fixed density f(0), or not. The observations are kept hidden and replaced by a stochastic transformation satisfying the local differential privacy constraint. In this setting, we propose a testing procedure which is based on an estimation of the quadratic distance between the density f of the unobserved samples and f(0). We establish an upper bound on the separation distance associated with this test, and a matching lower bound on the minimax separation rates of testing under non-interactive privacy in the case that f(0) is uniform, in discrete and continuous settings. To the best of our knowledge, we provide the first minimax optimal test and associated private transformation under a local differential privacy constraint over Besov balls in the continuous setting, quantifying the price to pay for data privacy. We also present a test that is adaptive to the smoothness parameter of the unknown density and remains minimax optimal up to a logarithmic factor. Finally, we note that our results can be translated to the discrete case, where the treatment of probability vectors is shown to be equivalent to that of piecewise constant densities in our setting. That is why we work with a unified setting for both the continuous and the discrete cases.",10.3150/21-BEJ1358,Local differential privacy; non-interactive privacy; goodness-of-fit testing; minimax separation rates; continuous and discrete distributions,,
Transition Modeling for Low Pressure Turbines Using Computational Fluid Dynamics Driven Machine Learning,"Akolekar, HD; Waschkowski, F; Zhao, YM; Pacciani, R; Sandberg, RD",ENERGIES,2021.0,"Existing Reynolds Averaged Navier-Stokes-based transition models do not accurately predict separation induced transition for low pressure turbines. Therefore, in this paper, a novel framework based on computational fluids dynamics (CFD) driven machine learning coupled with multi-expression and multi-objective optimization is explored to develop models which can improve the transition prediction for the T106A low pressure turbine at an isentropic exit Reynolds number of Re-2is=100,000. Model formulations are proposed for the transfer and laminar eddy viscosity terms of the laminar kinetic energy transition model using seven non-dimensional pi groups. The multi-objective optimization approach makes use of cost functions based on the suction-side wall-shear stress and the pressure coefficient. A family of solutions is thus developed, whose performance is assessed using Pareto analysis and in terms of physical characteristics of separated-flow transition. Two models are found which bring the wall-shear stress profile in the separated region at least two times closer to the reference high-fidelity data than the baseline transition model. As these models are able to accurately predict the flow coming off the blade trailing edge, they are also able to significantly enhance the wake-mixing prediction over the baseline model. This is the first known study which makes use of 'CFD-driven' machine learning to enhance the transition prediction for a non-canonical flow.",10.3390/en14154680,machine learning; multi-objective optimization; low pressure turbine; transition; turbulence modeling,,
Design of Alumina Reinforced Aluminium Alloy Composites with Improved Tribo-Mechanical Properties: A Machine Learning Approach,"Banerjee, T; Dey, S; Sekhar, AP; Datta, S; Das, D",TRANSACTIONS OF THE INDIAN INSTITUTE OF METALS,2020.0,"Artificial intelligence approach for data-driven design is employed to design an alumina reinforced aluminium matrix composite (AMC) with improved tribo-mechanical properties. Machine learning tool, viz. Artificial neural network (ANN), is used as a tool to create a set of models describing the properties of the AMC. The database required for the ANN modelling was extracted from published literature. The objective functions to search the optimum combinations of composition, size and morphological properties were provided from those ANN models. Since the objectives are conflicting in nature, a multi-objective optimization is introduced using genetic algorithm as a tool and the achieved Pareto solutions are used for designing the composite with tailored properties.",10.1007/s12666-020-02108-2,Metal matrix composite; Aluminium; Alumina; Mechanical behavior; Wear; Artificial neural network; Genetic algorithm; Multi-objective optimization; Pareto front,,
Compressive strength prediction of fly ash concrete by using machine learning techniques,"Khursheed, S; Jagan, J; Samui, P; Kumar, S",INNOVATIVE INFRASTRUCTURE SOLUTIONS,2021.0,"In this research, the machine learning techniques such as, minimax probability machine regression (MPMR), relevance vector machine (RVM), genetic programming (GP), emotional neural network (ENN) and extreme learning machine (ELM) were utilized in the event of forecasting the 28 days compressive strength of fly ash concrete. In the present examination, exploratory database enveloping appropriate information recovered from a few past investigations has been made and used to prepare and approve the abovementioned MPMR, RVM, GP, ENN and ELM models. The database consists of cement, fly ash, coarse aggregate, fine aggregate, water, and water-binder ratio as the inputs whereas compressive strength of the concrete for 28 days is the output. The capability of the described models can be assessed by distinctive statistical parameters. The results from the mentioned models have been compared and decided that the MPMR model (R = 0.992) could be occupied as a decisive and authoritative data astute approach for forecasting the compressive strength of concrete which was fusion with fly ash as the admixture, thus preserving the tedious laboratory works. The accuracy of the adopted techniques was justified by comparing the distinct statistical parameters, distribution figures, and Taylor diagrams.",10.1007/s41062-021-00506-z,Compressive strength; Fly ash; Minimax probability machine regression; Prediction; Relevance vector machine,,
Application of Parallel Distributed Implementation to Multiobjective Fuzzy Genetics-Based Machine Learning,"Nojima, Y; Takahashi, Y; Ishibuchi, H","INTELLIGENT INFORMATION AND DATABASE SYSTEMS, PT I",2015.0,"Fuzzy genetics-based machine learning is one of data mining techniques based on evolutionary computation. It can generate accurate classifiers with a small number of fuzzy if-then rules from numerical data. Its multiobjective version can provide a number of classifiers with a different tradeoff between accuracy and complexity. One major drawback of this method is the computation time when we use it for large data sets. In our previous study, we proposed parallel distributed implementation of single-objective fuzzy genetics-based machine learning which could drastically reduce the computation time. In this paper, we apply our idea of parallel distributed implementation to multiobjective fuzzy genetics- based machine learning. Through computational experiments on large data sets, we examine the effects of parallel distributed implementation on the search performance of our multiobjective fuzzy genetics- based machine learning and its computation time.",10.1007/978-3-319-15702-3_45,Fuzzy genetics-based machine learning; Multiobjective genetic fuzzy systems; Parallel distributed implementation,,
Improving accuracy of local geoid model using machine learning approaches and residuals of GPS/levelling geoid height,"Kaloop, MR; Pijush, S; Rabah, M; Al-Ajami, H; Hu, JW; Zaki, A",SURVEY REVIEW,,"This study aims to use GPS/Levelling data and machine learning techniques (MLs) to model a high precision local geoid for Kuwait. To improve the accuracy of a local geoid the global geopotential model and local terrain effect should be incorporated. The geoid model was improved based on the modelling of geoid residuals using three MLs. Minimax Probability Machine Regression (MPMR), Gaussian Process Regression (GPR), and Multivariate Adaptive Regression Splines (MARS) MLs were developed for modelling the calculated geoid residuals. The results show that the accuracy of the three MLs was improved compared to previous studies, and the accuracy of the GPR model was better than the other models. The standard deviations of Kuwait geoid undulation determined by GPS/Levelling, gravimetric, and developed GPR models were 1.377, 1.375, 1.375 m, respectively. Thus, the developed GPR model has successfully predicted an accurate geoid height of Kuwait with maximum variation approaches +/- 0.02 m.",10.1080/00396265.2021.1970918,Geoid; Machine learning; Geoid residuals; GPS; Levelling; GGM,,
Comparison between deep learning and fully connected neural network in performance prediction of power cycles: Taking supercritical CO2 Brayton cycle as an example,"Diao, CH; Liu, TY; Yang, Z; Duan, YY",INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS,2021.0,"AI is becoming increasingly important in promoting the energy revolution of carbon-neutral to achieve sustainable development. Induced by the large implementation of renewable energy, the more complexities and uncertainties in the future carbon-neutral energy systems make their designs hard accessible to the conventional methods, so machine learning (ML) especially the neural network becomes under focus. Here, we design a deep learning architecture based on convolutional neural networks (DL-CNN) known for its powerful predicting ability, and first utilize it in a case study of performance prediction of supercritical CO2 Brayton cycle. The design paradigm of DL-CNN architecture for performance prediction of power cycle is proposed. We also summarize the commonly used fully connected neural network (FC-NN) in related studies of power cycle design. Through systematically comparing the prediction performance of DL-CNN and FC-NN, their respective advantages and application scenarios in energy system design are discussed. In addition, a multiobjective design approach based on DL-CNN combined with random search is proposed and proved to be feasible by comparing with genetic algorithm. The results show that our proposed DL-CNN model is much more competitive than FC-NN model when the training data is sufficient and the prediction condition is complex, in which the prediction accuracy can achieve 99.6%. In the future, our deep learning model may help solve the complex design problems of hybrid carbon-neutral energy systems.",10.1002/int.22603,deep learning; fully connected neural network; optimization; power cycle design; supercritical CO2 Brayton cycles,,
SceneNet: Remote sensing scene classification deep learning network using multi-objective neural evolution architecture search,"Ma, AL; Wan, YT; Zhong, YF; Wang, JJ; Zhang, LP",ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,2021.0,"The scene classification approaches using deep learning have been the subject of much attention for remote sensing imagery. However, most deep learning networks have been constructed with a fixed architecture for natural image processing, and they are difficult to apply directly to remote sensing images, due to the more complex geometric structural features. Thus, there is an urgent need for automatic search for the most suitable neural network architecture from the image data in scene classification, in which a powerful search mechanism is required, and the computational complexity and performance error of the searched network should be balanced for a practical choice. In this article, a framework for scene classification network architecture search based on multi-objective neural evolution (SceneNet) is proposed. In SceneNet, the network architecture coding and searching are achieved using an evolutionary algorithm, which can implement a more flexible hierarchical extraction of the remote sensing image scene information. Moreover, the computational complexity and the performance error of the searched network are balanced by employing the multi-objective optimization method, and the competitive neural architectures are obtained in a Pareto solution set. The effectiveness of SceneNet is demonstrated by experimental comparisons with several deep neural networks designed by human experts.",10.1016/j.isprsjprs.2020.11.025,Scene classification; deep neural network; remote sensing; multi-objective optimization; evolutionary algorithm; neural architecture search,,
An exploratory analysis of data noisy scenarios in a Pareto-front based dynamic feature selection method,"Jesus, J; Canuto, A; Araujo, D",APPLIED SOFT COMPUTING,2021.0,"Feature selection has become a mandatory step in several data exploration and Machine Learning applications since data quality can have a strong impact in the performance of machine learning models. Many feature selection strategies have been developed in the past decades, using different criteria to select the most relevant features. The use of dynamic feature selection, however, has showed that the use of multiple simultaneously criteria to determine the best attribute subset for similar instances can deliver encouraging results. In this context, this paper proposes to analyze the performance of a pareto-front based dynamic feature selection (PF-DFS) method under data noise scenarios. In order to do this, we intentionally added noise in 15 datasets and evaluated the PF-DFS performance in order to measure its stability under two different data noise scenarios. The obtained results are compared to some state-of-the-art algorithms and show that, in terms of accuracy, the PF-DFS method is more robust to the other methods for the majority of the analyzed scenarios.",10.1016/j.asoc.2020.106951,Feature selection; Clustering algorithms; Pareto-front selection; Noisy data,,
Compact MILP models for optimal and Pareto-optimal LAD patterns,"Guo, C; Ryoo, HS",DISCRETE APPLIED MATHEMATICS,2012.0,"This paper develops MILP models for various optimal and Pareto-optimal LAD patterns that involve at most 2n 0-1 decision variables, where n is the number of support features for the data under analysis, which usually is small. Noting that the previous MILP pattern generation models are defined in 2n + m 0-1 variables, where m is the number of observations in the dataset with m >> n in general, the new models are expected to generate useful LAD patterns more efficiently. With experiments on six well-studied machine learning datasets, we first demonstrate the efficiency of the new MILP models and next use them to show different utilities of strong prime patterns and strong spanned patterns in enhancing the overall classification accuracy of a LAD decision theory. (C) 2012 Elsevier B.V. All rights reserved.",10.1016/j.dam.2012.05.006,LAD; MILP; Strong prime pattern; Strong spanned pattern; Maximum prime pattern; Maximum spanned pattern,,
Pareto Multi-task Deep Learning,"Riccio, SD; Dyankov, D; Jansen, G; Di Fatta, G; Nicosia, G","ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING, ICANN 2020, PT II",2020.0,"Neuroevolution has been used to train Deep Neural Networks on reinforcement learning problems. A few attempts have been made to extend it to address either multi-task or multi-objective optimization problems. This research work presents the Multi-Task Multi-Objective Deep Neuroevolution method, a highly parallelizable algorithm that can be adopted for tackling both multi-task and multi-objective problems. In this method prior knowledge on the tasks is used to explicitly define multiple utility functions, which are optimized simultaneously. Experimental results on some Atari 2600 games, a challenging testbed for deep reinforcement learning algorithms, show that a single neural network with a single set of parameters can outperform previous state of the art techniques. In addition to the standard analysis, all results are also evaluated using the Hypervolume indicator and the Kallback-Leibler divergence to get better insights on the underlying training dynamics. The experimental results show that a neural network trained with the proposed evolution strategy can outperform networks individually trained respectively on each of the tasks.",10.1007/978-3-030-61616-8_11,Multi-task learning; Multi-objective learning; Deep Neuroevolution; Hypervolume; Kullback-Leibler divergence; Pareto front; Evolution strategy; Atari 2600 games,,
Bi-Objective Dispatch of Multi-Energy Virtual Power Plant: Deep-Learning-Based Prediction and Particle Swarm Optimization,"Zhang, JH; Xu, ZY; Xu, WS; Zhu, FY; Lyu, XY; FU, M",APPLIED SCIENCES-BASEL,2019.0,"This paper addresses the coordinative operation problem of multi-energy virtual power plant (ME-VPP) in the context of energy internet. A bi-objective dispatch model is established to optimize the performance of ME-VPP in terms of economic cost (EC) and power quality (PQ). Various realistic factors are considered, which include environmental governance, transmission ratings, output limits, etc. Long short-term memory (LSTM), a deep learning method, is applied to the promotion of the accuracy of wind prediction. An improved multi-objective particle swarm optimization (MOPSO) is utilized as the solving algorithm. A practical case study is performed on Hongfeng Eco-town in Southwestern China. Simulation results of three scenarios verify the advantages of bi-objective optimization over solely saving EC and enhancing PQ. The Pareto frontier also provides a visible and flexible way for decision-making of ME-VPP operator. Two strategies, improvisational and foresighted, are compared by testing on the Institute of Electrical and Electronic Engineers (IEEE) 118-bus benchmark system. It is revealed that foresighted strategy, which incorporates LSTM prediction and bi-objective optimization over a 5-h receding horizon, takes 10 Pareto dominances in 24 h.",10.3390/app9020292,multi-energy virtual power plant; economic cost; power quality; bi-objective dispatch; long short-term memory; multi-objective particle swarm optimization,,
Machine learning in cutting processes as enabler for smart sustainable manufacturing,"du Preez, A; Oosthuizen, GA",SUSTAINABLE MANUFACTURING FOR GLOBAL CIRCULAR ECONOMY,2019.0,"Machine learning is becoming an increasingly popular concept in the modern world since its most common goal is to optimize systems by allowing one to make smarter use of products and services. In the manufacturing industry machine learning can lead to cost savings, time savings, increased quality and waste reduction. At the same time, it enables systems to be designed for managing human behaviour. This research study used a systematic review to investigate the different machine learning algorithms within the sustainable manufacturing context. The study focuses specifically on cutting processes. (C) 2019 The Authors. Published by Elsevier B.V.",10.1016/j.promfg.2019.04.102,machine learning; manufacturing; cutting processes,,
Statistical and machine learning models for optimizing energy in parallel applications,"Endrei, M; Jin, C; Dinh, MN; Abramson, D; Poxon, H; DeRose, L; de Supinski, BR",INTERNATIONAL JOURNAL OF HIGH PERFORMANCE COMPUTING APPLICATIONS,2019.0,"Rising power costs and constraints are driving a growing focus on the energy efficiency of high performance computing systems. The unique characteristics of a particular system and workload and their effect on performance and energy efficiency are typically difficult for application users to assess and to control. Settings for optimum performance and energy efficiency can also diverge, so we need to identify trade-off options that guide a suitable balance between energy use and performance. We present statistical and machine learning models that only require a small number of runs to make accurate Pareto-optimal trade-off predictions using parameters that users can control. We study model training and validation using several parallel kernels and more complex workloads, including Algebraic Multigrid (AMG), Large-scale Atomic Molecular Massively Parallel Simulator, and Livermore Unstructured Lagrangian Explicit Shock Hydrodynamics. We demonstrate that we can train the models using as few as 12 runs, with prediction error of less than 10%. Our AMG results identify trade-off options that provide up to 45% improvement in energy efficiency for around 10% performance loss. We reduce the sample measurement time required for AMG by 90%, from 13 h to 74 min.",10.1177/1094342019842915,Energy efficiency; performance; regression modeling; machine learning; high performance computing,,
Ensemble Learning in Non-Gaussian Data Assimilation,"Seybold, H; Ravela, S; Tagade, P","DYNAMIC DATA-DRIVEN ENVIRONMENTAL SYSTEMS SCIENCE, DYDESS 2014",2015.0,"The demand for tractable non-Gaussian Bayesian estimation has increased the popularity of kernel and mixture density representations. Here, using Gaussian Mixture Models (GMM), we posit that the reduction of total variance also remains an important objective in nonlinear filtering, particularly in the presence of bias. We propose multiobjective estimation as an essential ingredient in data assimilation. Using Ensemble Learning, two relatively weak estimators, namely the EnKF and Mixture Ensemble Filter (MEnF), are combined to produce a strong one. The Boosted-MEnF (B-MEnF) stacks MEnF and EnKF to mitigate bias and uses cascade generalization to reduce variance. In the Lorenz-63 model, it lowers mixture complexity without resampling and reduces posterior variance without increasing estimation error. Our MEnF is a purely ensemble-based GMM filter with a reduced dimensionality burden and without ad-hoc ensemble-mixture member associations. It is expressed as a compact ensemble transform which enables efficient fixed-interval and fixed-lag smoothers (MEnS) as well as the B-MEnF/S.",10.1007/978-3-319-25138-7_21,Data assimilation; Gaussian mixture models; Ensemble learning; Multi-objective assimilation; Non-linear filtering and smoothing; Non-Gaussian estimation,,
FPGA Logic Block Architectures for Efficient Deep Learning Inference,"Eldafrawy, M; Boutros, A; Yazdanshenas, S; Betz, V",ACM TRANSACTIONS ON RECONFIGURABLE TECHNOLOGY AND SYSTEMS,2020.0,"Reducing the precision of deep neural network (DNN) inference accelerators can yield large efficiency gains with little or no accuracy degradation compared to half or single precision floating-point by enabling more multiplication operations per unit area. A wide range of precisions fall on the pareto-optimal curve of hardware efficiency vs. accuracy with no single precision dominating, making the variable precision capabilities of FPGAs very valuable. We propose three types of logic block architectural enhancements and fully evaluate a total of six architectures that improve the area efficiency of multiplications and additions implemented in the soft fabric. Increasing the LUT fracturability and adding two adders to the ALM (4-bit Adder Double Chain architecture) leads to a 1.5x area reduction for arithmetic heavy machine learning (ML) kernels, while increasing their speed. In addition, this architecture also reduces the logic area of general applications by 6%, while increasing the critical path delay by only 1%. However, our highest impact option, which adds a 9-bit shadow multiplier to the logic clusters, reduces the area and critical path delay of ML kernels by 2.4x and 1.2x, respectively. These large gains come at a cost of 15% logic area increase for general applications.",10.1145/3393668,Deep neural networks; FPGA; CAD tools,,
P3GA: An Algorithm for Technology Characterization,"Galvan, E; Malak, RJ",JOURNAL OF MECHANICAL DESIGN,2015.0,"It is important for engineers to understand the capabilities and limitations of the technologies they consider for use in their systems. However, communicating this information can be a challenge. Mathematical characterizations of technical capabilities are of interest as a means to reduce ambiguity in communication and to increase opportunities to utilize design automation methods. The parameterized Pareto frontier (PPF) was introduced in prior work as a mathematical basis for modeling technical capabilities. One advantage of PPFs is that, in many cases, engineers can model a system by composing frontiers of its components. This allows for rapid technology evaluation and design space exploration. However, finding the PPF can be difficult. The contribution of this article is a new algorithm for approximating the PPF, called predictive parameterized Pareto genetic algorithm (P3GA). The proposed algorithm uses concepts and methods from multi-objective genetic optimization and machine learning to generate a discrete approximation of the PPF. If needed, designers can generate a continuous approximation of the frontier by generalizing beyond these data. The algorithm is explained, its performance is analyzed on numerical test problems, and its use is demonstrated on an engineering example. The results of the investigation indicate that P3GA may be effective in practice.",10.1115/1.4028101,,,
Landslide Susceptibility Modeling: An Integrated Novel Method Based on Machine Learning Feature Transformation,"Al-Najjar, HAH; Pradhan, B; Kalantar, B; Sameen, MI; Santosh, M; Alamri, A",REMOTE SENSING,2021.0,"Landslide susceptibility modeling, an essential approach to mitigate natural disasters, has witnessed considerable improvement following advances in machine learning (ML) techniques. However, in most of the previous studies, the distribution of input data was assumed as being, and treated, as normal or Gaussian; this assumption is not always valid as ML is heavily dependent on the quality of the input data. Therefore, we examine the effectiveness of six feature transformations (minimax normalization (Std-X), logarithmic functions (Log-X), reciprocal function (Rec-X), power functions (Power-X), optimal features (Opt-X), and one-hot encoding (Ohe-X) over the 11conditioning factors (i.e., altitude, slope, aspect, curvature, distance to road, distance to lineament, distance to stream, terrain roughness index (TRI), normalized difference vegetation index (NDVI), land use, and vegetation density). We selected the frequent landslide-prone area in the Cameron Highlands in Malaysia as a case study to test this novel approach. These transformations were then assessed by three benchmark ML methods, namely extreme gradient boosting (XGB), logistic regression (LR), and artificial neural networks (ANN). The 10-fold cross-validation method was used for model evaluations. Our results suggest that using Ohe-X transformation over the ANN model considerably improved performance from 52.244 to 89.398 (37.154% improvement).",10.3390/rs13163281,landslide susceptibility; feature transformations; machine learning; remote sensing; LiDAR; GIS,,
Margin-Based Pareto Ensemble Pruning: An Ensemble Pruning Algorithm That Learns to Search Optimized Ensembles,"Hu, RH; Zhou, SB; Liu, YS; Tang, ZR",COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE,2019.0,"The ensemble pruning system is an effective machine learning framework that combines several learners as experts to classify a test set. Generally, ensemble pruning systems aim to define a region of competence based on the validation set to select the most competent ensembles from the ensemble pool with respect to the test set. However, the size of the ensemble pool is usually fixed, and the performance of an ensemble pool heavily depends on the definition of the region of competence. In this paper, a dynamic pruning framework called margin-based Pareto ensemble pruning is proposed for ensemble pruning systems. The framework explores the optimized ensemble pool size during the overproduction stage and finetunes the experts during the pruning stage. The Pareto optimization algorithm is used to explore the size of the overproduction ensemble pool that can result in better performance. Considering the information entropy of the learners in the indecision region, the marginal criterion for each learner in the ensemble pool is calculated using margin criterion pruning, which prunes the experts with respect to the test set. The effectiveness of the proposed method for classification tasks is assessed using datasets. The results show that margin-based Pareto ensemble pruning can achieve smaller ensemble sizes and better classification performance in most datasets when compared with state-of-the-art models.",10.1155/2019/7560872,,,
Stacking-based ensemble learning of decision trees for interpretable prostate cancer detection,"Wang, YY; Wang, DJ; Geng, N; Wang, YZ; Yin, YQ; Jin, YC",APPLIED SOFT COMPUTING,2019.0,"Prostate cancer is a highly incident malignant cancer among men. Early detection of prostate cancer is necessary for deciding whether a patient should receive costly and invasive biopsy with possible serious complications. However, existing cancer diagnosis methods based on data mining only focus on diagnostic accuracy, while neglecting the interpretability of the diagnosis model that is necessary for helping doctors make clinical decisions. To take both accuracy and interpretability into consideration, we propose a stacking-based ensemble learning method that simultaneously constructs the diagnostic model and extracts interpretable diagnostic rules. For this purpose, a multi-objective optimization algorithm is devised to maximize the classification accuracy and minimize the ensemble complexity for model selection. As for model combination, a random forest classifier-based stacking technique is explored for the integration of base learners, i.e., decision trees. Empirical results on real-world data from the General Hospital of PLA demonstrate that the classification performance of the proposed method outperforms that of several state-of-the-art methods in terms of the classification accuracy, sensitivity and specificity. Moreover, the results reveal that several diagnostic rules extracted from the constructed ensemble learning model are accurate and interpretable. (C) 2019 Elsevier B.V. All rights reserved.",10.1016/j.asoc.2019.01.015,Prostate cancer detection; Ensemble learning; Stacking; Rule extraction; Multi-objective optimization,,
FUZZY ARTMAP - A NEURAL NETWORK ARCHITECTURE FOR INCREMENTAL SUPERVISED LEARNING OF ANALOG MULTIDIMENSIONAL MAPS,"CARPENTER, GA; GROSSBERG, S; MARKUZON, N; REYNOLDS, JH; ROSEN, DB",IEEE TRANSACTIONS ON NEURAL NETWORKS,1992.0,"A new neural network architecture is introduced for incremental supervised learning of recognition categories and multidimensional maps in response to arbitrary sequences of analog or binary input vectors, which may represent fuzzy or crisp sets of features. The architecture, called fuzzy ARTMAP, achieves a synthesis of fuzzy logic and adaptive resonance theory (ART) neural networks by exploiting a close formal similarity between the computations of fuzzy subsethood and ART category choice, resonance, and learning. Fuzzy ARTMAP also realizes a new minimax learning rule that conjointly minimizes predictive error and maximizes code compression, or generalization. This is achieved by a match tracking process that increases the ART vigilance parameter by the minimum amount needed to correct a predictive error. As a result, the system automatically learns a minimal number of recognition categories, or hidden units, to meet accuracy criteria. Category proliferation is prevented by normalizing input vectors at a preprocessing stage. A normalization procedure called complement coding leads to a symmetric theory in which the AND Operator (OR) and the OR operator (AND) of fuzzy logic play complementary roles. Complement coding uses on cells and off cells to represent the input pattern, and preserves individual feature amplitudes while normalizing the total on cell/off cell vector. Learning is stable because all adaptive weights can only decrease in time. Decreasing weights correspond to increasing sizes of category boxes. Smaller vigilance values lead to larger category boxes. Improved prediction is achieved by training the system several times using different orderings of the input set. This voting strategy can also be used to assign confidence estimates to competing predictions given small, noisy, or incomplete training sets. Four classes of simulations illustrate fuzzy ARTMAP performance in relation to benchmark back-propagation and genetic algorithm systems. These simulations include (i) finding points inside versus outside a circle; (ii) learning to tell two spirals apart, (iii) incremental approximation of a piecewise-continuous function; and (iv) a letter recognition database. The fuzzy ARTMAP system is also compared with Salzberg's NGE system and with Simpson's FMMC system.",10.1109/72.159059,,,
Generating Pareto Optimal Dose Distributions for Radiation Therapy Treatment Planning,"Nguyen, D; Barkousaraie, AS; Shen, CY; Jia, X; Jiang, S","MEDICAL IMAGE COMPUTING AND COMPUTER ASSISTED INTERVENTION - MICCAI 2019, PT VI",2019.0,"Radiotherapy treatment planning currently requires many trial-and-error iterations between the planner and treatment planning system, as well as between the planner and physician for discussion/consultation. The physician's preferences for a particular patient cannot be easily quantified and precisely conveyed to the planner. In this study we present a real-time volumetric Pareto surface dose generation deep learning neural network that can be used after segmentation by the physician, adding a tangible and quantifiable endpoint to portray to the planner. From 70 prostate patients, we first generated 84,000 intensity modulated radiation therapy plans (1,200 plans per patient) sampling the Pareto surface, representing various tradeoffs between the planning target volume (PTV) and the organs-at-risk (OAR), including bladder, rectum, left femur, right femur, and body. We divided the data to 10 test patients and 60 training/validation patients. We then trained a hierarchically densely connected convolutional U-net (HD U-net), to take the PTV and avoidance map representing OARs masks and weights, and predict the optimized plan. The HD U-net is capable of accurately predicting the 3D Pareto optimal dose distributions, with average [mean, max] dose errors of [3.4%, 7.7%](PTV), [1.6%, 5.6%] (bladder), [3.7%, 4.2%](rectum), [3.2%, 8.0%](left femur), [2.9%, 7.7%](right femur), and [0.04%, 5.4%](body) of the prescription dose. The PTV dose coverage prediction was also very similar, with errors of 1.3% (D98) and 2.0% (D99). Homogeneity was also similar, differing by 0.06 on average. The neural network can predict the dose within 1.7 s. Clinically, the optimization and dose calculation is much slower, taking 5-10 min.",10.1007/978-3-030-32226-7_7,Radiation therapy treatment planning; Intensity modulation; Pareto surface; Dose distribution; Deep learning; U-net; Neural network,,
A Dual-Dimer method for training physics-constrained neural networks with minimax architecture,"Liu, DH; Wang, Y",NEURAL NETWORKS,2021.0,"Data sparsity is a common issue to train machine learning tools such as neural networks for engineering and scientific applications, where experiments and simulations are expensive. Recently physics-constrained neural networks (PCNNs) were developed to reduce the required amount of training data. However, the weights of different losses from data and physical constraints are adjusted empirically in PCNNs. In this paper, a new physics-constrained neural network with the minimax architecture (PCNN-MM) is proposed so that the weights of different losses can be adjusted systematically. The training of the PCNN-MM is searching the high-order saddle points of the objective function. A novel saddle point search algorithm called Dual-Dimer method is developed. It is demonstrated that the Dual-Dimer method is computationally more efficient than the gradient descent ascent method for nonconvex-nonconcave functions and provides additional eigenvalue information to verify search results. A heat transfer example also shows that the convergence of PCNN-MMs is faster than that of traditional PCNNs. (C) 2021 Elsevier Ltd. All rights reserved.",10.1016/j.neunet.2020.12.028,Machine learning; Physics-constrained neural networks; Partial differential equation; Minimax problem; Saddle point search,,
Application of Machine Learning in Prediction of Shear Capacity of Headed Steel Studs in Steel-Concrete Composite Structures,"Avci-Karatas, C",INTERNATIONAL JOURNAL OF STEEL STRUCTURES,,"Headed studs are generally utilized as shear connectors at the interface between steel and concrete in composite structures primarily to transfer longitudinal shear force. This paper presents regression methodologies to predict the shear capacity of headed steel studs by using the concepts of minimax probability machine regression (MPMR) and extreme machine learning (EML). MPMR is carried out based on a minimax probability machine classification. EML is an updated version of a single hidden layer feedforward network. From the experimental data presented in extensive literature, key input parameters influencing the shear capacity have been identified and consolidated. The identified parameters include (i) steel stud shank diameter, (ii) compressive strength of concrete, and (iii) tensile strength of headed steel stud. After careful examination of the data and their limits, about 70-75% of the mixed dataset comprising the range of the values has been used for developing MPMR and EML-based models. The input data has been normalized based on the limits of individual parameters. The remaining data has been utilized for verification of the developed models. It is observed that the predicted shear strength capacity is comparable with the experimental observations. Further, the efficacy of the models has been evaluated through several statistical parameters, namely; root mean square error, mean absolute error, the coefficient of efficiency, root mean square error to observation's standard deviation ratio, normalized mean bias error, performance index, and variance account factor. It is found that the R-2 value is 0.9913 and 0.9479, respectively, for the models developed based on the concepts of MPMR and EML, indicating that the predicted value is closer to the experimental data.",10.1007/s13296-022-00589-z,Headed stud; Steel-concrete composite structure; Shear strength; Minimax probability machine regression; Extreme machine learning; Statistical modeling technique,,
A feasibility study on deep learning-based individualized 3D dose distribution prediction,"Ma, JH; Nguyen, D; Bai, T; Folkerts, M; Jia, X; Lu, WG; Zhou, LH; Jiang, S",MEDICAL PHYSICS,2021.0,"Purpose Radiation therapy treatment planning is a trial-and-error, often time-consuming process. An approximately optimal dose distribution corresponding to a specific patient's anatomy can be predicted by using pre-trained deep learning (DL) models. However, dose distributions are often optimized based not only on patient-specific anatomy but also on physicians' preferred trade-offs between planning target volume (PTV) coverage and organ at risk (OAR) sparing or among different OARs. Therefore, it is desirable to allow physicians to fine-tune the dose distribution predicted based on patient anatomy. In this work, we developed a DL model to predict the individualized 3D dose distributions by using not only the patient's anatomy but also the desired PTV/OAR trade-offs, as represented by a dose volume histogram (DVH), as inputs. Methods In this work, we developed a modified U-Net network to predict the 3D dose distribution by using patient PTV/OAR masks and the desired DVH as inputs. The desired DVH, fine-tuned by physicians from the initially predicted DVH, is first projected onto the Pareto surface, then converted into a vector, and then concatenated with feature maps encoded from the PTV/OAR masks. The network output for training is the dose distribution corresponding to the Pareto optimal DVH. The training/validation datasets contain 77 prostate cancer patients, and the testing dataset has 20 patients. Results The trained model can predict a 3D dose distribution that is approximately Pareto optimal while having the DVH closest to the input desired DVH. We calculated the difference between the predicted dose distribution and the optimized dose distribution that has a DVH closest to the desired one for the PTV and for all OARs as a quantitative evaluation. The largest absolute error in mean dose was about 3.6% of the prescription dose, and the largest absolute error in the maximum dose was about 2.0% of the prescription dose. Conclusions In this feasibility study, we have developed a 3D U-Net model with the patient's anatomy and the desired DVH curves as inputs to predict an individualized 3D dose distribution that is approximately Pareto optimal while having the DVH closest to the desired one. The predicted dose distributions can be used as references for dosimetrists and physicians to rapidly develop a clinically acceptable treatment plan.",10.1002/mp.15025,deep learning; dose volume histogram; Pareto optimal dose distribution prediction; physicians' preferred trade-offs,,
Leakage aware resource management approach with machine learning optimization framework for partially reconfigurable architectures,"Pham, NK; Kumar, A; Singh, AK; Khin, MMA",MICROPROCESSORS AND MICROSYSTEMS,2016.0,"Shrinking size of transistors has enabled us to integrate more and more logic elements into FPGA chips leading to higher computing power. However, it also brings a serious concern to the leakage power dissipation of the FPGA devices. One of the major reasons for leakage power dissipation in FPGA is the utilization of prefetching technique to minimize the reconfiguration overhead (delay) in Partially Reconfigurable (PR) FPGAs. This technique creates delays between the reconfiguration and execution parts of a task, which may lead up to 38% leakage power of FPGA since the SRAM-cells containing reconfiguration information cannot be powered down. In this work, a resource management approach (RMA) containing scheduling, placement and post-placement stages has been proposed to address the aforementioned issue. In scheduling stage, a leakage-aware priority function is derived to cope with the leakage power. The placement stage uses a cost function that allows designers to determine the desired trade-off between performance and leakage-saving. The post-placement stage employs a heuristic approach to close the gaps between reconfiguration and execution of tasks, hence further reduce leakage waste. To further examine the trade-off between performance (schedule length) and leakage waste, we propose a framework to utilize the Genetic Algorithm (GA) for exploring the design space and obtaining Pareto optimal design points. Addressing the time-consuming limitation of GA, we apply Regression technique and Clustering algorithm to build predictive models for the Pareto fronts using a training task graph dataset. Experiments show that our approach can achieve large leakage savings for both synthetic and real-life applications with acceptable extended deadline. Furthermore, different variants of the proposed approach can reduce leakage power by 40-65% when compared to a performance-driven approach and by 15-43% when compared to state-of-the-art works. It's also proven that our Machine Learning Optimization framework can estimate the Pareto front for new coming task graphs 10x faster than well-established GA approach with only 10% degradation in quality. (C) 2016 Elsevier B.V. All rights reserved.",10.1016/j.micpro.2016.09.012,Scheduling; Mapping; Resource management; Design space exploration; Machine learning,,
Multi-objective optimization techniques to design the Pareto front of organic dielectric polymers,"Mannodi-Kanakkithodi, A; Pilania, G; Ramprasad, R; Lookman, T; Gubernatis, JE",COMPUTATIONAL MATERIALS SCIENCE,2016.0,"We present two Monte Carlo algorithms to find the Pareto front of the chemical space of a class of dielectric polymers that is most interesting with respect to optimizing both the bandgap and dielectric constant. Starting with a dataset generated from density functional theory calculations, we used machine learning to construct surrogate models for the bandgaps and dielectric constants of all physically meaningful 4-block polymers (that is, polymer systems with a 4-block repeat unit). We parameterized these machine learning models in such a way that the surrogates built for the 4-block polymers were readily extendable to polymers beyond a 4-block repeat unit. By using translational invariance, chemical intuition, and domain knowledge, we were able to enumerate all possible 4, 6, and 8 block polymers and benchmark our Monte Carlo sampling of the chemical space against the exact enumeration of the surrogate predictions. We obtained exact agreement for the fronts of 4-block polymers and at least a 90% agreement for those of 6 and 8-block polymers. We present fronts for 10-block polymer that are not possible to obtain by direct enumeration. We note that our Monte Carlo methods also return polymers close to the predicted front and a measure of the closeness. Both quantities are useful information for the design and discovery of new polymers. (C) 2016 Elsevier B.V. All rights reserved.",10.1016/j.commatsci.2016.08.018,Materials informatics; Density functional theory; Multi-objective optimization,,
Strip Hardness Prediction in Continuous Annealing Using Multiobjective Sparse Nonlinear Ensemble Learning With Evolutionary Feature Selection,"Wang, XP; Wang, Y; Tang, LX",IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING,,"In the iron and steel industry, the hardness of steel strips is one of the key performance indicators to evaluate strip quality and guide production for the continuous annealing production line (CAPL). However, the hardness cannot be measured online in the actual production process. Consequently, the precise prediction of the strip hardness based on practical data becomes one of the key tasks during production. In this article, a multiobjective sparse nonlinear ensemble learning with evolutionary feature selection (MOSNE-EFS) method is proposed, which is data-driven modeling of the soft sensor. The method mainly consists of two stages: 1) the construction of individual learners based on multiobjective feature selection learning (MOFSL) and 2) the selection and ensemble of individual learners based on sparse nonlinear ensemble learning via differential evolution (SNEL-DE). The final ensemble model obtained by SNEL-DE is used as the prediction model for strip hardness in CAPL. The proposed method is evaluated with industrial production data. Experimental results indicate that the two strategies, i.e., evolutionary feature selection and sparse nonlinear ensemble, are effective in improving the accuracy and robustness of the prediction model, and further comparison results demonstrate the superiority of the MOSNE-EFS model over the other existing methods.",10.1109/TASE.2021.3083670,Strips; Predictive models; Production; Feature extraction; Annealing; Steel; Optimization; Iron and steel industry; multiobjective evolutionary feature selection; sparse nonlinear ensemble learning; strip hardness prediction,,
Machine learning-aided cost prediction and optimization in construction operations,"Sharma, V; Zaki, M; Jha, KN; Krishnan, NMA",ENGINEERING CONSTRUCTION AND ARCHITECTURAL MANAGEMENT,,"Purpose This paper aims to use a data-driven approach towards optimizing construction operations. To this extent, it presents a machine learning (ML)-aided optimization approach, wherein the construction cost is predicted as a function of time, resources and environmental impact, which is further used as a surrogate model for cost optimization. Design/methodology/approach Taking a dataset from literature, the paper has applied various ML algorithms, namely, simple and regularized linear regression, random forest, gradient boosted trees, neural network and Gaussian process regression (GPR) to predict the construction cost as a function of time, resources and environmental impact. Further, the trained models were used to optimize the construction cost applying single-objective (with and without constraints) and multi-objective optimizations, employing Bayesian optimization, particle swarm optimization (PSO) and non-dominated sorted genetic algorithm. Findings The results presented in the paper demonstrate that the ensemble methods, such as gradient boosted trees, exhibit the best performance for construction cost prediction. Further, it shows that multi-objective optimization can be used to develop a Pareto front for two competing variables, such as cost and environmental impact, which directly allows a practitioner to make a rational decision. Research limitations/implications Note that the sequential nature of events which dictates the scheduling is not considered in the present work. This aspect could be incorporated in the future to develop a robust scheme that can optimize the scheduling dynamically. Originality/value The paper demonstrates that a ML approach coupled with optimization could enable the development of an efficient and economic strategy to plan the construction operations.",10.1108/ECAM-10-2020-0778,Optimization; Construction planning; Methodology; Simulation,,
Deep learning for EEG-based Motor Imagery classification: Accuracy-cost trade-off,"Leon, J; Escobar, JJ; Ortiz, A; Ortega, J; Gonzalez, J; Martin-Smith, P; Gan, JQ; Damas, M",PLOS ONE,2020.0,"Electroencephalography (EEG) datasets are often small and high dimensional, owing to cumbersome recording processes. In these conditions, powerful machine learning techniques are essential to deal with the large amount of information and overcome the curse of dimensionality. Artificial Neural Networks (ANNs) have achieved promising performance in EEG-based Brain-Computer Interface (BCI) applications, but they involve computationally intensive training algorithms and hyperparameter optimization methods. Thus, an awareness of the quality-cost trade-off, although usually overlooked, is highly beneficial. In this paper, we apply a hyperparameter optimization procedure based on Genetic Algorithms to Convolutional Neural Networks (CNNs), Feed-Forward Neural Networks (FFNNs), and Recurrent Neural Networks (RNNs), all of them purposely shallow. We compare their relative quality and energy-time cost, but we also analyze the variability in the structural complexity of networks of the same type with similar accuracies. The experimental results show that the optimization procedure improves accuracy in all models, and that CNN models with only one hidden convolutional layer can equal or slightly outperform a 6-layer Deep Belief Network. FFNN and RNN were not able to reach the same quality, although the cost was significantly lower. The results also highlight the fact that size within the same type of network is not necessarily correlated with accuracy, as smaller models can and do match, or even surpass, bigger ones in performance. In this regard, overfitting is likely a contributing factor since deep learning approaches struggle with limited training examples.",10.1371/journal.pone.0234178,,,
Simulation of the entire range of daily precipitation using a hybrid probability distribution,"Li, C; Singh, VP; Mishra, AK",WATER RESOURCES RESEARCH,2012.0,"Underestimation of extreme values is a widely acknowledged issue in daily precipitation simulation. Nonparametric precipitation generators have inherent limitations in representing extremes. Parametric generators can realistically model the full spectrum of precipitation amount through compound distributions. Nevertheless, fitting these distributions suffers from numerical instability, supervised learning, and computational demand. This study presents an easy-to-implement hybrid probability distribution to model the full spectrum of precipitation amount. The basic idea for the hybrid distribution lies in synthesizing low to moderate precipitation by an exponential distribution and extreme precipitation by a generalized Pareto distribution. By forcing the two distributions to be continuous at the junction point, the threshold of the generalized Pareto distribution can be implicitly learned in an unsupervised manner. Monte Carlo simulation shows that the hybrid distribution is capable of modeling heavy tailed data. Performance of the distribution is further evaluated using 49 daily precipitation records across Texas. Results show that the model is able to capture both the bulk and the tail of daily precipitation amount. The maximum goodness-of-fit and penalized maximum likelihood methods are found to be reliable complements to the maximum likelihood method, in that generally they can provide adequate goodness-of-fit. The proposed distribution can be incorporated into precipitation generators and downscaling models in order to realistically simulate the entire range of precipitation without losing extreme values.",10.1029/2011WR011446,,,
Multi-objective optimization of a microchannel membrane-based absorber with inclined grooves based on CFD and machine learning,"Sui, ZG; Sui, YR; Wu, W",ENERGY,2022.0,"A novel microchannel membrane-based absorber with inclined grooves is proposed and studied by a three-dimensional CFD model. Parametric analysis is carried out to analyze the effects of structural parameters on the absorption rate and pressure drop. Results indicate that the groove introduces a swirling effect in the solution channel, interrupting the boundary layer at the solution-membrane interface and increasing the solution residence time inside the microchannel. The absorption rate in the grooved channel is up to 1.55 times higher, while the pressure drop is 0.77e0.96 times lower. To optimize the novel absorber geometries and maximize the integrated performance, the Pareto front is obtained by performing a multi-objective optimization, in which a machine learning method based on ANN and NSGA-CYRILLIC CAPITAL LETTER BYELORUSSIAN-UKRAINIAN ICYRILLIC CAPITAL LETTER BYELORUSSIAN-UKRAINIAN I is developed. The optimal design parameters from the Pareto front are identified by two well-known decision-making methods, LINMAP and TOPSIS. Compared to the basic smooth channel, these methods generate 1.41 and 1.47 times improvement in volumetric cooling capacities, at a much lower solution pressure drop. Moreover, a high absorption rate equivalent to that of a 200 pm-thick smooth channel is achieved by LINMAP and TOPSIS, with pressure drops lower by 6.29 and 5.63 times, respectively. (c) 2021 Elsevier Ltd. All rights reserved.",10.1016/j.energy.2021.122809,Absorption refrigeration; Microchannel membrane absorber; Groove structure; ML and CFD; Multi-objective optimization,,
Super resolution reconstruction algorithm of video image based on deep self encoding learning,"Xi, S; Wu, CX; Jiang, LH",MULTIMEDIA TOOLS AND APPLICATIONS,2019.0,"Super resolution reconstruction of video image is a research hotspot in the field of image processing, and it is widely used in video surveillance, image processing, criminal analysis and other fields. Super resolution image reconstruction can reconstruct a high resolution image from low resolution images, and this technology has become a research hotspot in the field of image processing. In recent years, deep learning has been developed rapidly in the field of multimedia processing, and image super resolution restoration technology based on deep learning has gradually become the mainstream technology. In view of the existing image super-resolution algorithm problems, such as more parameters, larger amount of calculation, longer training time, blurred image texture, we use the deep self-coding learning method to improve it. We analyze the advantages and disadvantages of the existing technology from the network type, network structure, training methods and so on, and sort out the development of the technology. The experimental results show that the improved network model achieves better super-resolution results, and the subjective visual effect and objective evaluation index are improved obviously, and the image sharpness and edge sharpness are improved obviously.",10.1007/s11042-018-6062-x,Depth learning; Self-coding learning; Video image; Super-resolution reconstruction; Algorithm; Experimental analysis; Framework,,
Robust Design of Suspension System with Polynomial Chaos Expansion and Machine Learning,"Gao, H; Jezeque, L; Cabrol, E; Vitry, B",SCIENCE & TECHNIQUE,2020.0,"During the early development of a new vehicle project, the uncertainty of parameters should be taken into consideration because the design may be perturbed due to real components' complexity and manufacturing tolerances. Thus, the numerical validation of critical suspension specifications, such as durability and ride comfort should be carried out with random factors. In this article a multi-objective optimization methodology is proposed which involves the specification's robustness as one of the optimization objectives. To predict the output variation from a given set of uncertain-but-bounded parameters proposed by optimization iterations, an adaptive chaos polynomial expansion (PCE) is applied to combine a local design of experiments with global response surfaces. Furthermore, in order to reduce the additional tests required for PCE construction, a machine learning algorithm based on inter-design correlation matrix firstly classifies the current design points through data mining and clustering. Then it learns how to predict the robustness of future optimized solutions with no extra simulations. At the end of the optimization, a Pareto front between specifications and their robustness can be obtained which represents the best compromises among objectives. The optimum set on the front is classified and can serve as a reference for future design. An example of a quarter car model has been tested for which the target is to optimize the global durability based on real road excitations. The statistical distribution of the parameters such as the trajectories and speeds is also taken into account. The result shows the natural incompatibility between the durability of the chassis and the robustness of this durability. Here the term robustness does not mean strength, but means that the performance is less sensitive to perturbations. In addition, a stochastic sampling verifies the good robustness prediction of PCE method and machine learning, based on a greatly reduced number of tests. This example demonstrates the effectiveness of the approach, in particular its ability to save computational costs for full vehicle simulation.",10.21122/2227-1031-2020-19-1-43-54,chassis durability; data mining; machine learning; multi-objective optimization; polynomial chaos expansion; robust design,,
Machine Learning into Metaheuristics: A Survey and Taxonomy,"Talbi, EG",ACM COMPUTING SURVEYS,2021.0,"During the past few years, research in applying machine learning (ML) to design efficient, effective, and robust metaheuristics has become increasingly popular. Many of those machine learning-supported metaheuristics have generated high-quality results and represent state-of-the-art optimization algorithms. Although various appproaches have been proposed, there is a lack of a comprehensive survey and taxonomy on this research topic. In this article, we will investigate different opportunities for using ML into metaheuristics. We define uniformly the various ways synergies that might be achieved. A detailed taxonomy is proposed according to the concerned search component: target optimization problem and low-level and high-level components of metaheuristics. Our goal is also to motivate researchers in optimization to include ideas from ML into metaheuristics. We identify some open research issues in this topic that need further in-depth investigations.",10.1145/3459664,Metaheuristics; machine learning; optimization; ML-supported metaheuristics,,
Semi-supervised learning with summary statistics,"Qin, HH; Guo, X",ANALYSIS AND APPLICATIONS,2019.0,"Nowadays, the extensive collection and analyzing of data is stimulating widespread privacy concerns, and therefore is increasing tensions between the potential sources of data and researchers. A privacy-friendly learning framework can help to ease the tensions, and to free up more data for research. We propose a new algorithm, LESS (Learning with Empirical feature-based Summary statistics from Semi-supervised data), which uses only summary statistics instead of raw data for regression learning. The selection of empirical features serves as a trade-off between prediction precision and the protection of privacy. We show that LESS achieves the minimax optimal rate of convergence in terms of the size of the labeled sample. LESS extends naturally to the applications where data are separately held by different sources. Compared with the existing literature on distributed learning, LESS removes the restriction of minimum sample size on single data sources.",10.1142/S0219530519400037,Distributed learning; semi-supervised learning; empirical features; summary statistics; privacy protection,,
Optimization of Formulations Using Robotic Experiments Driven by Machine Learning DoE,"Cao, LW; Russo, D; Felton, K; Salley, D; Sharma, A; Keenan, G; Mauer, W; Gao, HH; Cronin, L; Lapkin, AA",CELL REPORTS PHYSICAL SCIENCE,2021.0,"Formulated products are complex mixtures of ingredients whose time to market can be difficult to speed due to the lack of general predictable physical models for the desired properties. Here, we report the coupling of a machine learning classification algorithm with the Thompson sampling efficient multiobjective optimization (TSEMO) algorithm for the simultaneous optimization of continuous and discrete outputs. The methodology is successfully applied to the design of a formulated liquid product of commercial interest for which no physical models are available. Experiments are carried out in a semiautomated fashion using robotic platforms triggered by the machine learning algorithms. The procedure allows one to find nine suitable recipes meeting the customer-defined criteria within 15 working days, outperforming human intuition in the target performance of the formulations.",10.1016/j.xcrp.2020.100295,,,
A Novel Machine Learning Model for Dose Prediction in Prostate Volumetric Modulated Arc Therapy Using Output Initialization and Optimization Priorities,"Jensen, PJ; Zhang, JH; Koontz, BF; Wu, QJ",FRONTIERS IN ARTIFICIAL INTELLIGENCE,2021.0,"Treatment planning for prostate volumetric modulated arc therapy (VMAT) can take 5-30 min per plan to optimize and calculate, limiting the number of plan options that can be explored before the final plan decision. Inspired by the speed and accuracy of modern machine learning models, such as residual networks, we hypothesized that it was possible to use a machine learning model to bypass the time-intensive dose optimization and dose calculation steps, arriving directly at an estimate of the resulting dose distribution for use in multi-criteria optimization (MCO). In this study, we present a novel machine learning model for predicting the dose distribution for a given patient with a given set of optimization priorities. Our model innovates upon the existing machine learning techniques by utilizing optimization priorities and our understanding of dose map shapes to initialize the dose distribution before dose refinement via a voxel-wise residual network. Each block of the residual network individually updates the initialized dose map before passing to the next block. Our model also utilizes contiguous and atrous patch sampling to effectively increase the receptive fields of each layer in the residual network, decreasing its number of layers, increasing model prediction and training speed, and discouraging overfitting without compromising on the accuracy. For analysis, 100 prostate VMAT cases were used to train and test the model. The model was evaluated by the training and testing errors produced by 50 iterations of 10-fold cross-validation, with 100 cases randomly shuffled into the subsets at each iteration. The error of the model is modest for this data, with average dose map root-mean-square errors (RMSEs) of 2.38 +/- 0.47% of prescription dose overall patients and all optimization priority combinations in the patient testing sets. The model was also evaluated at iteratively smaller training set sizes, suggesting that the model requires between 60 and 90 patients for optimal performance. This model may be used for quickly estimating the Pareto set of feasible dose objectives, which may directly accelerate the treatment planning process and indirectly improve final plan quality by allowing more time for plan refinement.",10.3389/frai.2021.624038,dose prediction; multi-criterial optimization; treatment planning; prostate VMAT; machine learning; artificial intelligence; residual neural networks,,
Non-parametric empirical machine learning for short-term and long-term structural health monitoring,"Entezami, A; Shariatmadar, H; De Michele, C",STRUCTURAL HEALTH MONITORING-AN INTERNATIONAL JOURNAL,,"Early damage detection is an initial step of structural health monitoring. Thanks to recent advances in sensing technology, the application of data-driven methods based on the concept of machine learning has significantly increased among civil engineers and researchers. On this basis, this article proposes a novel non-parametric anomaly detection method in an unsupervised learning manner via the theory of empirical machine learning. The main objective of this method is to define a new damage index by using some empirical measure and the concept of minimum distance value. For this reason, an empirical local density is initially computed for each feature and then multiplied by the minimum distance of that feature to derive a new damage index for decision-making. The minimum distance is obtained by calculating the distances between each feature and training samples and finding the minimum quantity. The major contributions of this research contain developing a novel non-parametric algorithm for decision-making under high-dimensional and low-dimensional features and proposing a new damage index. To detect early damage, a threshold boundary is computed by using the extreme value theory, generalized Pareto distribution, and peak-over-threshold approach. Dynamic and statistical features of two full-scale bridges are used to verify the effectiveness and reliability of the proposed non-parametric anomaly detection. In order to further demonstrate its accuracy and proper performance, it is compared with some classical and recently published anomaly detection techniques. Results show that the proposed non-parametric method can effectively discriminate a damaged state from its undamaged condition with high damage detectability and inconsiderable false positive and false negative errors. This method also outperforms the anomaly detection techniques considered in the comparative studies.",10.1177/14759217211069842,Structural health monitoring; non-parametric anomaly detection; empirical machine learning; environmental variability; bridges,,
Multi-objective Optimization of Integrated Iron Ore Sintering Process Using Machine Learning and Evolutionary Algorithms,"Singh, K; Vakkantham, P; Nistala, SH; Runkana, V",TRANSACTIONS OF THE INDIAN INSTITUTE OF METALS,2020.0,"In the iron ore sintering process, it is desirable to maximize the productivity and quality of sinter while minimizing the fuel consumption for any given raw material (iron ore, flux and solid fuel) quality. However, given the complexity of the sintering process and the large number of manipulated variables, it is not practical for operators to identify appropriate set points for the manipulated variables to achieve these conflicting objectives. While significant amount of research is devoted to optimization of the on-strand sintering process, optimization of the integrated sintering process, viz. granulation and on-strand sintering together, has not received much attention. This is, however, necessary as the granulation process dictates the moisture content, mean size and voidage of the green mix bed, which in turn have a very strong influence on the sintering process and sinter quality. In this work, we have formulated and solved a multi-objective optimization problem to maximize both sinter productivity and quality for the integrated iron ore sintering process. Predictive models for productivity and quality parameters such as tumbler index (TI) and reduction degradation index (RDI) are built using machine learning algorithms. The optimization problem is solved using an evolutionary algorithm called non-dominated sorting genetic algorithm II (NSGA-II) to obtain a set of Pareto-optimal solutions. Optimal settings for key manipulated variables such as moisture content of green mix, fuel content, bed height and strand speed are obtained for the Pareto solutions. The optimization results are useful for identifying the operational range of the sintering process and can be used by operators for running the sinter plant optimally for a given set of raw materials.",10.1007/s12666-020-01920-0,Sintering; Granulation; Multi-objective optimization; Data-based modeling,,
Deep learning-based surrogate modeling and optimization for microalgal biofuel production and photobioreactor design,"del Rio-Chanona, EA; Wagner, JL; Ali, H; Fiorelli, F; Zhang, DD; Hellgardt, K",AICHE JOURNAL,2019.0,"Identifying optimal photobioreactor configurations and process operating conditions is critical to industrialize microalgae-derived biorenewables. Traditionally, this was addressed by testing numerous design scenarios from integrated physical models coupling computational fluid dynamics and kinetic modeling. However, this approach presents computational intractability and numerical instabilities when simulating large-scale systems, causing time-intensive computing efforts and infeasibility in mathematical optimization. Therefore, we propose an innovative data-driven surrogate modeling framework, which considerably reduces computing time from months to days by exploiting state-of-the-art deep learning technology. The framework built upon a few simulated results from the physical model to learn the sophisticated hydrodynamic and biochemical kinetic mechanisms; then adopts a hybrid stochastic optimization algorithm to explore untested processes and find optimal solutions. Through verification, this framework was demonstrated to have comparable accuracy to the physical model. Moreover, multi-objective optimization was incorporated to generate a Pareto-frontier for decision-making, advancing its applications in complex biosystems modeling and optimization. (c) 2018 American Institute of Chemical Engineers AIChE J, 65: 915-923, 2019",10.1002/aic.16473,surrogate modeling; convolutional neural network; hybrid stochastic optimization; excreted biofuel; photobioreactor design,,
Machine learning in materials design and discovery: Examples from the present and suggestions for the future,"Gubernatis, JE; Lookman, T",PHYSICAL REVIEW MATERIALS,2018.0,"We provide a brief discussion of What is machine learning? and then give a number of examples of how these methods have recently aided the design and discovery of new materials, such as new shape memory alloys, with enhanced targeted properties, such as lower hysteresis. These examples illustrate how discoveries can be made from large databases, for example, those generated by high throughput DFT calculations and also how they can be made from experimentally growing smaller databases in an active learning manner. Additionally, we discuss such advanced machine learning methods as multiobjective and multifidelity optimization that permit proposing new materials with the simultaneous optimization of more than one targeted property, such as a material with low hysteresis and high Curie temperature, and permit using fewer costly experiments and calculations by combining them with less costly ones to achieve modeling comparable to using only many costly ones. We conclude with a brief discussion of future machine learning opportunities in the context of high throughput experiment and on-the-fly adjustment of synthesis. More speculatively, we end by discussing how might we mesh materials science more fittingly with machine learning.",10.1103/PhysRevMaterials.2.120301,,,
Experimentally validated machine learning frameworks for accelerated prediction of cyclic steady state and optimization of pressure swing adsorption processes,"Pai, KN; Prasad, V; Rajendran, A",SEPARATION AND PURIFICATION TECHNOLOGY,2020.0,"Machine learning-based surrogate models are presented to accelerate the optimization of pressure swing adsorption processes. Various supervised machine learning algorithms, such as decision trees, random forests, support vector machines, Gaussian process regression, and artificial neural networks, are tested for their ability to predict key performance indicators for a given set of operating conditions. Among the algorithms studied, Gaussian process regression-based surrogate models were found to be the best at predicting process outputs, with minimal training effort. The adjusted coefficient of determination for predictions using the surrogate model is greater than 0.98 using a sampled training set of 400 operating conditions. A surrogate model based on artificial neural networks is also presented to predict the bed profiles of the intensive variables at cyclic steady state. The surrogate models show very good agreement with the detailed model simulations. Experiments performed on a lab-scale two-column rig, for the concentration of CO2 from a mixture of CO2+N-2 on Zeolite-13X, confirm performance indicators such as purity, recovery and axial profiles predicted by the surrogate models. Two new optimization frameworks are presented: Surrogate optimization in which the trained surrogate model is used to provide the process performance; and cyclic steady state optimization in which the predicted cyclic steady state profiles are provided as an initial condition for detailed model in order to accelerate convergence. Both techniques are shown to accurately predict Pareto fronts of Purity-Recovery and Energy-Productivity calculated from optimization that uses a detailed process model. The Surrogate optimization and the cyclic steady state accelerated Detailed optimization show approximate to 23x and 6x reduction in computational load, respectively, when compared to the traditional optimization using detailed models.",10.1016/j.seppur.2020.116651,Machine learning; Neural networks; Surrogate modelling and optimization; Multi-objective optimization; Vacuum swing adsorption; Post combustion carbon capture,,
Machine Learning-Based Multiobjective Optimization of Pressure Swing Adsorption,"Subraveti, SG; Li, ZK; Prasad, V; Rajendran, A",INDUSTRIAL & ENGINEERING CHEMISTRY RESEARCH,2019.0,"The transient, cyclic nature and flexibility in process design make the optimization of pressure swing adsorption (PSA) computationally intensive. Two hybrid approaches incorporating machine learning methods into optimization routines are described. The first optimization approach uses artificial neural networks as surrogate models for function evaluations. The surrogates are constructed in the course of the initial optimization and utilized for function evaluations in subsequent optimization. In the second optimization approach, important design variables are identified to reduce the high-dimensional search space to a lower dimension based on partial least squares regression. The accuracy, robustness, and reliability of these approaches are demonstrated by considering a complex eight-step PSA process for precombustion CO2 capture as a case study. The machine learning-based optimization offers similar to 10x reduction in computational efforts while achieving the same performance as that of the detailed models.",10.1021/acs.iecr.9b04173,,,
Multi-objective optimization of concrete mixture proportions using machine learning and metaheuristic algorithms,"Zhang, JF; Huang, YM; Wang, YH; Ma, GW",CONSTRUCTION AND BUILDING MATERIALS,2020.0,"For the optimization of concrete mixture proportions, multiple objectives (e.g., strength, cost, slump) with many variables (e.g., concrete components) under highly nonlinear constraints need to be optimized simultaneously. The current single-objective optimization models are not applicable to multi-objective optimization (MOO). This study proposes an MOO method based on machine learning (ML) and metaheuristic algorithms to optimize concrete mixture proportions. First, the performances of different ML models in the prediction of concrete objectives are compared on data sets collected from the published literature. The winner is selected as the objective function for the optimization procedure. In the optimization step, a multi-objective particle swarm optimization algorithm is used to optimize mixture proportions to achieve optimal objectives. The results show that the backpropagation neural network has better performance on continuous data (e.g., strength), whereas the random forest algorithm has higher prediction accuracy on more discrete data (e.g., slump). The Pareto fronts of a bi-objective mixture optimization problem for high-performance concrete and a tri-objective mixture optimization problem for plastic concrete are successfully obtained by the MOO model. The MOO model can serve as a design guide to facilitate decision-making before the construction phase. (C) 2020 Elsevier Ltd. All rights reserved.",10.1016/j.conbuildmat.2020.119208,Concrete; Multi-objective optimization; Machine learning; Particle swarm optimization; Compressive strength; Slump,,
A New Automated Design Method Based on Machine Learning for CMOS Analog Circuits,"Moradi, B; Mirzaei, A",INTERNATIONAL JOURNAL OF ELECTRONICS,2016.0,"A new simulation based automated CMOS analog circuit design method which applies a multi-objective non-Darwinian-type evolutionary algorithm based on Learnable Evolution Model (LEM) is proposed in this article. The multi-objective property of this automated design of CMOS analog circuits is governed by a modified Strength Pareto Evolutionary Algorithm (SPEA) incorporated in the LEM algorithm presented here. LEM includes a machine learning method such as the decision trees that makes a distinction between high- and low-fitness areas in the design space. The learning process can detect the right directions of the evolution and lead to high steps in the evolution of the individuals. The learning phase shortens the evolution process and makes remarkable reduction in the number of individual evaluations. The expert designer's knowledge on circuit is applied in the design process in order to reduce the design space as well as the design time. The circuit evaluation is made by HSPICE simulator. In order to improve the design accuracy, bsim3v3 CMOS transistor model is adopted in this proposed design method. This proposed design method is tested on three different operational amplifier circuits. The performance of this proposed design method is verified by comparing it with the evolutionary strategy algorithm and other similar methods.",10.1080/00207217.2016.1138538,CMOS analogue circuits; automated design; Learnable Evolution Model; multi-objective; operational amplifier,,
Optimization of solidification in die casting using numerical simulations and machine learning,"Shahane, S; Aluru, N; Ferreira, P; Kapoor, SG; Vanka, SP",JOURNAL OF MANUFACTURING PROCESSES,2020.0,"In this paper, we demonstrate the combination of machine learning and three dimensional numerical simulations for multi-objective optimization of low pressure die casting. The cooling of molten metal inside the mold is achieved typically by passing water through the cooling lines in the die. Depending on the cooling line location, coolant flow rate and die geometry, nonuniform temperatures are imposed on the molten metal at the mold wall. This boundary condition along with the initial molten metal temperature affect the product quality quantified in terms of micro-structure parameters and yield strength. A finite volume based numerical solver is used to determine the temperature-time history and correlate the inputs to outputs. The objective of this research is to develop and demonstrate a procedure to obtain the initial and wall temperatures so as to optimize the product quality. The non-dominated sorting genetic algorithm (NSGA-II) is used for multi-objective optimization in this work. The number of function evaluations required for NSGA-II can be of the order of millions and hence, the finite volume solver cannot be used directly for optimization. Therefore, a multilayer perceptron feed-forward neural network is first trained using the results from the numerical solution of the fluid flow and energy equations and is subsequently used as a surrogate model. As an assessment, simplified versions of the actual problem are designed to first verify results of the genetic algorithm. An innovative local sensitivity based approach is then used to rank the final Pareto optimal solutions and select a single best design.",10.1016/j.jmapro.2020.01.016,Die casting; Deep neural networks; Multi-objective optimization,,
Reduced-Dimensional Gaussian Process Machine Learning for Groundwater Allocation Planning Using Swarm Theory,"Siade, AJ; Cui, T; Karelse, RN; Hampton, C",WATER RESOURCES RESEARCH,2020.0,"Groundwater management and allocation planning involves a rigorous assessment of the performance of operational decisions such as extraction/injection rates on community and environmental objectives. Maximizing performance through numerical optimization can be essential for high-value resources and is often computationally infeasible due to long simulation model run times combined with nonconvex objectives and constraints. In order to mitigate these drawbacks, surrogate models can be used in place of complex models during the optimization process. There exist a number of machine learning techniques that can be used to develop a data-driven surrogate model. However, the curse of dimensionality, common to groundwater management, limits the use of these techniques due to the necessity for large training data sets. Even though it is now possible to handle large data sets, the generation of these data sets themselves remains computationally prohibitive as they require numerous simulations to produce accurate surrogates. In this study, we integrate a dimensionality reduction method using truncated singular value decomposition to reduce the number of decision variables, thereby reducing the size of the training data set needed. Correspondingly, we demonstrate a simple technique for acquiring an approximate minimax Latin Hypercube design from within the subspace. We also implement a novel technique for adaptive resampling through particle swarm optimization in order to maintain accuracy of the surrogate model throughout the optimization process. The resulting accurate surrogate model for the Perth regional aquifer system of Western Australia runs in a matter of seconds. Adopting this approach can produce timely solutions, making formal optimization tractable for practitioners.",10.1029/2019WR026061,,,
Interpreting the optical properties of oxide glasses with machine learning and Shapely additive explanations,"Zaki, M; Venugopal, V; Bhattoo, R; Bishnoi, S; Singh, SK; Allu, AR; Jayadeva; Krishnan, NMA",JOURNAL OF THE AMERICAN CERAMIC SOCIETY,,"Due to their excellent optical properties, glasses are used for various applications ranging from smartphone screens to telescopes. Developing compositions with tailored Abbe number (V-d) and refractive index at 587.6 nm (n(d)), two crucial optical properties, is a major challenge. To this extent, machine learning (ML) approaches have been successfully used to develop composition-property models. However, these models are essentially black boxes in nature and suffer from the lack of interpretability. In this paper, we demonstrate the use of ML models to predict the composition-dependent variations of V-d and n(d). Further, using Shapely additive explanations (SHAP), we interpret the ML models to identify the contribution of each of the input components toward target prediction. We observe that glass formers such as SiO2, B2O3, and P2O5 and intermediates such as TiO2, PbO, and Bi2O3 play a significant role in controlling the optical properties. Interestingly, components contributing toward increasing the n(d) are found to decrease the V-d and vice versa. Finally, we develop the Abbe diagram, using the ML models, allowing accelerated discovery of new glasses for optical properties beyond the experimental pareto front. Overall, employing explainable ML, we predict and interpret the compositional control on the optical properties of oxide glasses.",10.1111/jace.18345,glass; lead-free glass; optical materials; properties; refractive index,,
Machine learning in drying,"Martynenko, A; Misra, NN",DRYING TECHNOLOGY,2020.0,"Although very important for analysis of drying processes, physics-based models are limited in terms of their prediction ability and in most cases are unsuitable for real-time process control and optimization of industrial drying. In this paper, we provide an overview of the machine learning (ML) techniques and the state-of-the-art ML applications in drying of food and biomaterials. The applications include but not limited to data-driven models, nonlinear control and multi-objective optimization. The advantages of integration of ML with machine vision for real-time observation of product quality and fine-tuning control strategies are briefly discussed. Future research should focus on the integration of ML software tools with sensors to measure process and product variables. In addition, the drying research community should contribute towards building of open-source datasets, which is extremely important to leverage the power of ML algorithms. Integration of sensors, process analysis and software engineering will enable the development of intelligent drying systems.",10.1080/07373937.2019.1690502,Intelligence; data; model; optimization; control; food quality,,
Multi-objective Decision in Machine Learning,"de Medeiros, TH; Rocha, HP; Torres, FS; Takahashi, RHC; Braga, AP",JOURNAL OF CONTROL AUTOMATION AND ELECTRICAL SYSTEMS,2017.0,"This work presents a novel approach for decisionmaking for multi-objective binary classification problems. The purpose of the decision process is to select within a set of Pareto-optimal solutions, one model that minimizes the structural risk (generalization error). This new approach utilizes a kind of prior knowledge that, if available, allows the selection of a model that better represents the problem in question. Prior knowledge about the imprecisions of the collected data enables the identification of the region of equivalent solutions within the set of Pareto-optimal solutions. Results for binary classification problems with sets of synthetic and real data indicate equal or better performance in terms of decision efficiency compared to similar approaches.",10.1007/s40313-016-0295-6,Machine learning; Multi-objective optimization; Decision-making; Classification,,
From Conventional to Machine Learning Methods for Maritime Risk Assessment,"Rawson, A; Brito, M; Sabeur, Z; Tran-Thanh, L",TRANSNAV-INTERNATIONAL JOURNAL ON MARINE NAVIGATION AND SAFETY OF SEA TRANSPORTATION,2021.0,"Within the last thirty years, the range and complexity of methodologies proposed to assess maritime risk have increased significantly. Techniques such as expert judgement, incident analysis, geometric models, domain analysis and Bayesian Networks amongst many others have become dominant within both the literature and industry. On top of this, advances in machine learning algorithms and big data have opened opportunities for new methods which might overcome some limitations of conventional approaches. Yet, determining the suitability or validity of one technique over another is challenging as it requires a systematic multicriteria approach to compare the inputs, assumptions, methodologies and results of each method. Within this paper, such an approach is proposed and tested within an isolated waterway in order to justify the proposed advantages of a machine learning approach to maritime risk assessment and should serve as inspiration for future work.",10.12716/1001.15.04.06,,,
An evolutionary generation method of deep neural network sets combined with Gaussian random field,"Zhang, C; Dai, ZF; Liang, XL; Xu, GH; Zhang, CS; Zhang, B",WIRELESS NETWORKS,,"As a research hotspot in the field of machine learning, ensemble learning improved the prediction accuracy of the final model by constructing and combining multiple basic models. In recent years, many experts and scholars are committed to combining deep networks with ensemble learning to improve the accuracy of neural network models in various scenarios and tasks. But not all neural networks are suitable for participating in the construction of ensemble models. Deep networks with ensemble learning require that the single neural network involved in the integration has high accuracy and great discrepancy with other networks. In the initial stage of deep networks with ensemble learning, the process of generating sets of candidate deep networks is first required. After studying an existing multiobjective deep belief networks ensemble (MODBNE) method, the Gaussian random field model is used as a pre-screening strategy in the process of generating the candidate deep network sets. Individuals with great potential for improvement are selected for fitness function evaluation so that a large number of neural network models with higher accuracy and the larger discrepancy between networks can be easily obtained, which effectively improves the quality of the solution and reduces the time consumed in training the neural networks.",10.1007/s11276-021-02677-0,Gaussian random field; Deep neural network; Differential evolution algorithm; Ensemble learning,,
"Mixture optimization for environmental, economical and mechanical objectives in silica fume concrete: A novel frame-work based on machine learning and a new meta-heuristic algorithm","Zhang, JF; Huang, YM; Ma, GW; Nener, B",RESOURCES CONSERVATION AND RECYCLING,2021.0,"Partial replacement of cement by silica fume in concrete provides advantages such as mitigation of the impact on the environment of carbon dioxide emitted during cement production, recycling of industrial by-products and improvement of concrete strength and durability. The optimization of the mixture of silica fume concrete (SFC) requires trade-off among multiple objectives (strength, cost and embodied CO2) and consideration of a large number of variables under highly nonlinear constraints. Obtaining the Pareto front of this multi-objective optimization (MOO) problem is computationally expensive. To address this issue, the present study develops a MOO model using machine learning (ML) techniques and a new meta-heuristic algorithm. Firstly, the relationships between components and SFC properties are modelled on a dataset using a back propagation neural network (BPNN) model. Then an individual-intelligence-based multi-objective beetle antennae search algorithm (MOBAS) is developed to search for optimal SFC mixtures that maximize UCS, and minimize cost and embodied CO2 under defined constraints. Results indicate that the proposed MOBAS is more computationally efficient with satisfactory accuracy in comparison with algorithms based on swarm intelligence. The MOO model achieves reliable predictions for UCS with a very high correlation coefficient (0.9663) on the test set. The Pareto front of optimal SFC mixture proportions of the MOO problem is successfully obtained using the proposed model. The proposed frame-work improves the efficiency in SFC mixture optimization and can facilitate appropriate decision making before construction.",10.1016/j.resconrec.2021.105395,Silica fume concrete; Mixture design; Multi-objective optimization; Uniaxial compressive strength; Cost; Carbon dioxide,,
Probabilistic dynamic security assessment of large power systems using machine learning algorithms,"Jafarzadeh, S; Genc, VMI",TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES,2018.0,"Due to extensive utilization of intermittent energy sources in recent years, deterministic approaches cannot provide an accurate security assessment for power systems under large uncertainties. Therefore, probabilistic approaches have become crucial for making decisions based on more reliable assessments. In this paper, a new method based on machine learning and proper sampling techniques is proposed to overcome the difficulties of the conventional Monte Carlo approaches used in power system security assessment. The main purpose of the proposed method is to accurately quantify the dynamic security related risk at a forecasted operating condition of a power system utilizing a large number of intermittent energy sources, e.g., wind, which greatly extends the uncertainties in its operation. This is achieved through the proposed method, which captures an accurate probability distribution of the system's dynamic performance associated with both transient and small-signal angle stability. The accuracy of the fitted distribution is attained by adopting a generalized Pareto (GP) distribution for the left-tailed region that includes severe and rare cases using a multilayered perceptron neural network with the Relief feature selection technique, which speeds up the exceedance sample generation process required for the GP distribution. The Latin hypercube sampling technique, which samples the search space evenly, is proposed to create a dataset for training the neural network. To generate the Monte Carlo instances, the Gibbs sampling approach, which considers the correlation between random variables besides its simplicity, is utilized.",10.3906/elk-1709-247,Power system stability; probabilistic security assessment; power system security; neural networks; feature selection,,
Mapping membrane activity in undiscovered peptide sequence space using machine learning,"Lee, EY; Fulan, BM; Wong, GCL; Ferguson, AL",PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA,2016.0,"There are some similar to 1,100 known antimicrobial peptides (AMPs), which permeabilize microbial membranes but have diverse sequences. Here, we develop a support vector machine (SVM)-based classifier to investigate.-helical AMPs and the interrelated nature of their functional commonality and sequence homology. SVM is used to search the undiscovered peptide sequence space and identify Pareto-optimal candidates that simultaneously maximize the distance sigma from the SVM hyperplane (thus maximize its antimicrobialness) and alpha-helicity, but minimize mutational distance to known AMPs. By calibrating SVM machine learning results with killing assays and small-angle X-ray scattering (SAXS), we find that the SVM metric sigma correlates not with a peptide's minimum inhibitory concentration (MIC), but rather its ability to generate negative Gaussian membrane curvature. This surprising result provides a topological basis for membrane activity common to AMPs. Moreover, we highlight an important distinction between the maximal recognizability of a sequence to a trained AMP classifier (its ability to generate membrane curvature) and its maximal antimicrobial efficacy. As mutational distances are increased from known AMPs, we find AMP-like sequences that are increasingly difficult for nature to discover via simple mutation. Using the sequence map as a discovery tool, we find a unexpectedly diverse taxonomy of sequences that are just as membrane-active as known AMPs, but with a broad range of primary functions distinct from AMP functions, including endogenous neuropeptides, viral fusion proteins, topogenic peptides, and amyloids. The SVM classifier is useful as a general detector of membrane activity in peptide sequences.",10.1073/pnas.1609893113,machine learning; membrane curvature; membrane permeation; antimicrobial peptides; cell-penetrating peptides,,
Machine Learning Methods Applied for Modeling the Process of Obtaining Bricks Using Silicon-Based Materials,"Anton, C; Curteanu, S; Lisa, C; Leon, F",MATERIALS,2021.0,"Most of the time, industrial brick manufacture facilities are designed and commissioned for a particular type of manufacture mix and a particular type of burning process. Productivity and product quality maintenance and improvement is a challenge for process engineers. Our paper aims at using machine learning methods to evaluate the impact of adding new auxiliary materials on the amount of exhaust emissions. Experimental determinations made in similar conditions enabled us to build a database containing information about 121 brick batches. Various models (artificial neural networks and regression algorithms) were designed to make predictions about exhaust emission changes when auxiliary materials are introduced into the manufacture mix. The best models were feed-forward neural networks with two hidden layers, having MSE < 0.01 and r(2) > 0.82 and, as regression model, kNN with error < 0.6. Also, an optimization procedure, including the best models, was developed in order to determine the optimal values for the parameters that assure the minimum quantities for the gas emission. The Pareto front obtained in the multi-objective optimization conducted with grid search method allows the user the chose the most convenient values for the dry product mass, clay, ash and organic raw materials which minimize gas emissions with energy potential.",10.3390/ma14237232,machine learning; neural networks; random forest; bricks; influence of additives,,
Evolutionary deep learning: A survey,"Zhan, ZH; Li, JY; Zhang, J",NEUROCOMPUTING,2022.0,"As an advanced artificial intelligence technique for solving learning problems, deep learning (DL) has achieved great success in many real-world applications and attracted increasing attention in recent years. However, as the performance of DL depends on many factors such as the architecture and hyperparameters, how to optimize DL has become a hot research topic in the field of DL and artificial intelligence. Evolutionary computation (EC), including evolutionary algorithm and swarm intelligence, is a kind of efficient and intelligent optimization methodology inspired by the mechanisms of biological evolution and behaviors of swarm organisms. Therefore, a large number of researches have proposed EC algorithms to optimize DL, so called evolutionary deep learning (EDL), which have obtained promising results. Given the great progress and rapid development of EDL in recent years, it is quite necessary to review these developments in order to summarize previous research experiences and knowledge, as well as provide references to benefit the development of more researches and applications. For this aim, this paper categorizes existing works in a two-level taxonomy. The higher level includes four categories based on when the EC can be adopted in optimizing the DL, which are the four procedures of the whole DL lifetime, including data processing, model search, model training, and model evaluation and utilization. In the lower level, related works in each category are further classified according to the functionality and the aim of using EC in the corresponding DL procedure, i.e., why using EC in this DL procedure. As a result, the taxonomy can clearly show how an EC algorithm can be used to optimize and improve DL. Moreover, this survey also discusses the potential research directions to provide the prospect of EDL in the future. (c) 2022 Published by Elsevier B.V.",10.1016/j.neucom.2022.01.099,Deep learning; Evolutionary computation; Evolutionary algorithm; Swarm intelligence; Evolutionary deep learning; Artificial intelligence,,
Buckling of laminated composite skew plate using FEM and machine learning methods,"Mishra, BB; Kumar, A; Samui, P; Roshni, T",ENGINEERING COMPUTATIONS,2021.0,"Purpose The purpose of this paper is to attempt the buckling analysis of a laminated composite skew plate using the C(0)finite element (FE) model based on higher-order shear deformation theory (HSDT) in conjunction with minimax probability machine regression (MPMR) and multivariate adaptive regression spline (MARS). Design/methodology/approach HSDT considers the third-order variation of in-plane displacements which eliminates the use of shear correction factor owing to realistic parabolic transverse shear stresses across the thickness coordinate. At the top and bottom of the plate, zero transverse shear stress condition is imposed. C0FE model based on HSDT is developed and coded in formula translation (FORTRAN). FE model is validated and found efficient to create new results. MPMR and MARS models are coded in MATLAB. Using skew angle (alpha), stacking sequence (Ai) and buckling strength (Y) as input parameters, a regression problem is formulated using MPMR and MARS to predict the buckling strength of laminated composite skew plates. Findings The results of the MPMR and MARS models are in good agreement with the FE model result. MPMR is a better tool than MARS to analyze the buckling problem. Research limitations/implications The present work considers the linear behavior of the laminated composite skew plate. Originality/value To the authors' best of knowledge, there is no work in the literature on the buckling analysis of a laminated composite skew plate using C0FE formulation based on third-order shear deformation theory in conjunction with MPMR and MARS. These machine-learning techniques increase efficiency, reduce the computational time and reduce the cost of analysis. Further, an equation is generated with the MARS model via which the buckling strength of the laminated composite skew plate can be predicted with ease and simplicity.",10.1108/EC-08-2019-0346,FEM; Regression; Buckling; Composite laminate; MARS; MPMR; Skew,,
The discernible and hidden effects of clonality on the genotypic and genetic states of populations: Improving our estimation of clonal rates,"Stoeckel, S; Porro, B; Arnaud-Haond, S",MOLECULAR ECOLOGY RESOURCES,2021.0,"Partial clonality is widespread across the tree of life, but most population genetic models are designed for exclusively clonal or sexual organisms. This gap hampers our understanding of the influence of clonality on evolutionary trajectories and the interpretation of population genetic data. We performed forward simulations of diploid populations at increasing rates of clonality (c), analysed their relationships with genotypic (clonal richness, R, and distribution of clonal sizes, Pareto beta) and genetic (F-IS and linkage disequilibrium) indices, and tested predictions of c from population genetic data through supervised machine learning. Two complementary behaviours emerged from the probability distributions of genotypic and genetic indices with increasing c. While the impact of c on R and Pareto beta was easily described by simple mathematical equations, its effects on genetic indices were noticeable only at the highest levels (c > 0.95). Consequently, genotypic indices allowed reliable estimates of c, while genetic descriptors led to poorer performances when c < 0.95. These results provide clear baseline expectations for genotypic and genetic diversity and dynamics under partial clonality. Worryingly, however, the use of realistic sample sizes to acquire empirical data systematically led to gross underestimates (often of one to two orders of magnitude) of c, suggesting that many interpretations hitherto proposed in the literature, mostly based on genotypic richness, should be reappraised. We propose future avenues to derive realistic confidence intervals for c and show that, although still approximate, a supervised learning method would greatly improve the estimation of c from population genetic data.",10.1111/1755-0998.13316,F-statistics; genotypic diversity; population genetics; rates of clonality; sampling,,
A machine-learning based memetic algorithm for the multi-objective permutation flowshop scheduling problem,"Wang, XP; Tang, LX",COMPUTERS & OPERATIONS RESEARCH,2017.0,"In recent years, the historical data during the search process of evolutionary algorithms has received increasing attention from many researchers, and some hybrid evolutionary algorithms with machine-learning have been proposed. However, the majority of the literature is centered on continuous problems with a single optimization objective. There are still a lot of problems to be handled for multi-objective combinatorial optimization problems. Therefore, this paper proposes a machite-learning based multi-objective memetic algorithm (ML-MOMA) for the discrete permutation flowshop scheduling problem. There are two main features in the proposed ML-MOMA. First, each solution is assigned with an individual archive to store the non-dominated solutions found by it and based on these individual archives a new population update method is presented. Second, an adaptive multi-objective local search is developed, in which the analysis of historical data accumulated during the search process is used to adaptively determine which non-dominated solutions should be selected for local search and how the local search should be applied. Computational results based on benchmark problems show that the cooperation of the above two features can help to achieve a balance between evolutionary global search and local search. In addition, many of the best known Pareto fronts for these benchmark problems in the literature can be improved by the proposed ML-MOMA.",10.1016/j.cor.2016.10.003,Multi-objective permutation flowshop scheduling; Memetic algorithm; Data analysis,,
A Comparative Evaluation of Supervised Machine Learning Classification Techniques for Engineering Design Applications,"Sharpe, C; Wiest, T; Wang, PF; Seepersad, CC",JOURNAL OF MECHANICAL DESIGN,2019.0,"Supervised machine learning techniques have proven to be effective tools for engineering design exploration and optimization applications, in which they are especially useful for mapping promising or feasible regions of the design space. The design space mappings can be used to inform early-stage design exploration, provide reliability assessments, and aid convergence in multiobjective or multilevel problems that require collaborative design teams. However, the accuracy of the mappings can vary based on problem factors such as the number of design variables, presence of discrete variables, multimodality of the underlying response function, and amount of training data available. Additionally, there are several useful machine learning algorithms available, and each has its own set of algorithmic hyperparameters that significantly affect accuracy and computational expense. This work elucidates the use of machine learning for engineering design exploration and optimization problems by investigating the performance of popular classification algorithms on a variety of example engineering optimization problems. The results are synthesized into a set of observations to provide engineers with intuition for applying these techniques to their own problems in the future, as well as recommendations based on problem type to aid engineers in algorithm selection and utilization.",10.1115/1.4044524,design automation; simulation-based design; classifiers; machine learning; design exploration,,
Efficient Resource-Aware Convolutional Neural Architecture Search for Edge Computing with Pareto-Bayesian Optimization,"Yang, Z; Zhang, SB; Li, RX; Li, CX; Wang, M; Wang, DH; Zhang, M",SENSORS,2021.0,"With the development of deep learning technologies and edge computing, the combination of them can make artificial intelligence ubiquitous. Due to the constrained computation resources of the edge device, the research in the field of on-device deep learning not only focuses on the model accuracy but also on the model efficiency, for example, inference latency. There are many attempts to optimize the existing deep learning models for the purpose of deploying them on the edge devices that meet specific application requirements while maintaining high accuracy. Such work not only requires professional knowledge but also needs a lot of experiments, which limits the customization of neural networks for varied devices and application scenarios. In order to reduce the human intervention in designing and optimizing the neural network structure, multi-objective neural architecture search methods that can automatically search for neural networks featured with high accuracy and can satisfy certain hardware performance requirements are proposed. However, the current methods commonly set accuracy and inference latency as the performance indicator during the search process, and sample numerous network structures to obtain the required neural network. Lacking regulation to the search direction with the search objectives will generate a large number of useless networks during the search process, which influences the search efficiency to a great extent. Therefore, in this paper, an efficient resource-aware search method is proposed. Firstly, the network inference consumption profiling model for any specific device is established, and it can help us directly obtain the resource consumption of each operation in the network structure and the inference latency of the entire sampled network. Next, on the basis of the Bayesian search, a resource-aware Pareto Bayesian search is proposed. Accuracy and inference latency are set as the constraints to regulate the search direction. With a clearer search direction, the overall search efficiency will be improved. Furthermore, cell-based structure and lightweight operation are applied to optimize the search space for further enhancing the search efficiency. The experimental results demonstrate that with our method, the inference latency of the searched network structure reduced 94.71% without scarifying the accuracy. At the same time, the search efficiency increased by 18.18%.",10.3390/s21020444,edge computing; neural architecture search; latency profiling model; Pareto-Bayesian optimization,,
Applying Deep Learning to the Heat Production Planning Problem in a District Heating System,"Lee, D; Yoon, SM; Lee, J; Kim, K; Song, SH",ENERGIES,2020.0,"District heating system is designed to minimize energy consumption and environmental pollution by employing centralized production facilities connected to demand regions. Traditionally, optimization based algorithms were applied to the heat production planning problem in the district heating systems. Optimization-based models provide near optimal solutions, while it takes a while to generate solutions due to the characteristics of the underlying solution mechanism. When prompt re-planning due to any parameter changes is necessary, the traditional approaches might be inefficient to generate modified solutions quickly. In this study, we developed a two-phase solution mechanism, where deep learning algorithm is applied to learn optimal production patterns from optimization module. In the first training phase, the optimization module generates optimal production plans for the input scenarios derived from operations history, which are provided to the deep learning module for training. In the second planning phase, the deep learning module with trained parameters predicts production plan for the test scenarios. The computational experiments show that after the training process is completed, it has the characteristic of quickly deriving results appropriate to the situation. By combining optimization and deep learning modules in a solution framework, it is expected that the proposed algorithm could be applied to online optimization of district heating systems.",10.3390/en13246641,district heating; optimization; deep learning; planning; heat production,,
A wind speed interval prediction system based on multi-objective optimization for machine learning method,"Li, RR; Jin, Y",APPLIED ENERGY,2018.0,"Accurate forecast of wind speed is the first prerequisite to supply high quality power energy to customer in a secure and economic manner. However, traditional point forecast may not be sufficiently reliable and accurate for decision-makers to perform operational strategies purely when the uncertainty level increases. For the sake of quantifying the uncertainty associated with point predictions, it is necessary to conduct interval prediction to provide reliable and accurate wind speed information. In this work, a hybrid model framework based on combinatorial modules was proposed and successfully adopted to construct the prediction intervals of the future wind speed. Feature selection methods are developed to determine the most suitable modes of original time series and the optimal input form of the model, while the optimization forecasting module is applied to model the wind speed series based on the machine learning method and the multi-objective optimization algorithm, then the compromise solution of Pareto front is chosen by Min-max method. Finally, the proposed combined model was investigated via the hourly wind speed data from two different periods in Penglai, China. Besides, the study's experimental results indicated that the prediction intervals generated perform well and are satisfactory in both criterion functions of high coverage and small width through discussion among single-objective models and other multi-objective models (signal pre-processing method comparison included).",10.1016/j.apenergy.2018.07.032,Wind speed forecasting; Prediction intervals; Multi-objective optimization; Least squares support vector machines; Feature selection,,
A comprehensive techno-eco-assessment of CO2 enhanced oil recovery projects using a machine-learning assisted workflow,"You, JY; Ampomah, W; Morgan, A; Sun, Q; Huang, XL",INTERNATIONAL JOURNAL OF GREENHOUSE GAS CONTROL,2021.0,"Carbon dioxide enhanced oil recovery (CO2-EOR) projects not only extract residual oil but also sequestrate CO2 in the depleted reservoirs. This study develops a machine-learning-based workflow to co-optimize the hydrocarbon recovery, CO2 sequestration volume and project net present value (NPV) simultaneously. Considering the trade-off relationships among the objective functions, support vector regression with Gaussian kernel (Gaussian-SVR) proxies are coupled with multi-objective particle swarm optimization (PSO) protocol and generate Pareto optimal solutions. Taking advantage of the high computational efficacy of the proxy model, economic uncertainties introduced by tax credits, capital costs and oil price are investigated by this study. The results indicate that the tax incentive policy (Section 45Q) plays a vital role in enhancing the economic returns of CO2-EOR projects, especially under the depression of crude oil market. The proposed workflow has been successfully implemented to optimize a water alternative CO2 (CO2-WAG) injection project in a depleted oil sand in the US. The optimization results yield an incremental oil production of 15.8 MM STB and 1.37 MM metric tons of CO2 storage in a 20-year development strategy, with the highest project NPV to be 205.6 MM US dollars.",10.1016/j.ijggc.2021.103480,CCUS; CO2-EOR; Multi-objective optimization; Economics assessment; Machine learning,,
"2L(2), a simple reinforcement learning scheme for two-player zero-sum Markov games","Frenay, B; Saerens, M",NEUROCOMPUTING,2009.0,"Markov games is a framework which can be used to formalise n-agent reinforcement learning (RL). Littman (Markov games as a framework for multi-agent reinforcement learning, in: Proceedings of the 11th International Conference on Machine Learning (ICML-94), 1994.) uses this framework to model two-agent zero-sum problems and, within this context, proposes the minimax-Q algorithm. This paper reviews RL algorithms for two-player zero-sum Markov games and introduces a new, simple, fast. algorithm, called 2L(2).2L(2) is compared to several standard algorithms (Q-learning, Minimax and minimax-Q) implemented with the)ash library written in Python. The experiments show that 222 converges empirically to optimal mixed policies, as minimax-Q, but uses a surprisingly simple and cheap updating rule. (C) 2009 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2008.12.022,Reinforcement learning; Q-Learning; Markov games; Two-player zero-sum games; Multi-agent,,
Technical note: Interpolated Pareto surface similarity metrics for multi-criteria optimization in radiation therapy,"Jensen, PJ; Zhang, JH; Wu, QJ",MEDICAL PHYSICS,2020.0,"Purpose There is a strong clinical need to evaluate different multi-criteria optimization (MCO) algorithms, including inverse optimization sampling algorithms and machine learning-based predictions. This study aims to develop and compare several interpolated Pareto surface similarity metrics. Materials and methods The first metric is the root-mean-square error (RMSE) evaluated between vertices on the interpolated surfaces, augmented by intra-simplex sampling of the barycentric coordinates of the surfaces' simplicial complexes. The second metric is the average projected distance (APD), which evaluates the displacements between the vertices and computes their projections along the mean displacement. The third metric is the average nearest-point distance (ANPD), which numerically integrates point-to-simplex distances over the sampled simplices of the interpolated surfaces. These metrics were compared by their convergence rates, the times required to achieve convergence, and their representation of the underlying surface interpolations. For analysis, several interpolated Pareto surface pairs were constructed abstractly, with one pair from a nasopharyngeal treatment planning case using MCO. Results Convergence within 1% is typically achieved at approximately 50 and 80 samples per barycentric dimension for the RMSE and the ANPD, respectively. Calculation requires approximately 1 and 10 ms to achieve convergence for the RMSE and the ANPD in two dimensions, respectively, while the APD always requires < 1 ms. These time costs are much higher in higher dimensions for just the RMSE and ANPD. The APD values more closely approximated the ANPD limits than the RMSE limits. Conclusion The ANPD's formulation and generality make it likely more meaningful than the RMSE and APD for representing the similarity between the underlying interpolated surfaces rather than the sampling points on the surfaces. However, in situations requiring high-speed evaluations, the APD may be more desirable due to its speed, independence from a subjectively chosen sampling rate, and similarity to the ANPD limits.",10.1002/mp.14541,artificial intelligence; machine learning; multi&#8208; criteria optimization; Pareto surface; radiation therapy,,
An efficient primal dual prox method for non-smooth optimization,"Yang, TB; Mahdavi, M; Jin, R; Zhu, SH",MACHINE LEARNING,2015.0,"We study the non-smooth optimization problems in machine learning, where both the loss function and the regularizer are non-smooth functions. Previous studies on efficient empirical loss minimization assume either a smooth loss function or a strongly convex regularizer, making them unsuitable for non-smooth optimization. We develop a simple yet efficient method for a family of non-smooth optimization problems where the dual form of the loss function is bilinear in primal and dual variables. We cast a non-smooth optimization problem into a minimax optimization problem, and develop a primal dual prox method that solves the minimax optimization problem at a rate of assuming that the proximal step can be efficiently solved, significantly faster than a standard subgradient descent method that has an convergence rate. Our empirical studies verify the efficiency of the proposed method for various non-smooth optimization problems that arise ubiquitously in machine learning by comparing it to the state-of-the-art first order methods.",10.1007/s10994-014-5436-1,Non-smooth optimization; Primal dual method; Convergence rate; Sparsity; Efficiency,,
"Pushing nanomaterials up to the kilogram scale - An accelerated approach for synthesizing antimicrobial ZnO with high shear reactors, machine learning and high-throughput analysis","Jose, NA; Kovalev, M; Bradford, E; Schweidtmann, AM; Zeng, HC; Lapkin, AA",CHEMICAL ENGINEERING JOURNAL,2021.0,"Novel materials are the backbone of major technological advances. However, the development and wide-scale introduction of new materials, such as nanomaterials, is limited by three main factors-the expense of experiments, inefficiency of synthesis methods and complexity of scale-up. Reaching the kilogram scale is a hurdle that takes years of effort for many nanomaterials. We introduce an improved methodology for materials development, combining state-of-the-art techniques-multi-objective machine learning optimization, high yield microreactors and high throughput analysis. We demonstrate this approach through the optimization of ZnO nanoparticle synthesis, simultaneously targeting high yield and high antibacterial activity. In fewer than 100 experiments, we developed a 1 kg day(-1) continuous synthesis for ZnO (with a space-time-yield of 62.4 kg day(-1) m(-3)), having an antibacterial activity comparable to hydrothermally synthesized nano-ZnO and cetrimonium bromide. Following this, we provide insights into the mechanistic factors underlying the performance-yield tradeoffs of synthesis and highlight the need for benchmarking machine learning models with traditional chemical engineering methods. Methods for increasing model accuracy at steep pareto fronts, in this case at yields close to 1 kg per day, should also be improved. To project the next steps for process scale-up and the potential advantages of this methodology, we conduct a scalability analysis in comparison to conventional batch production methods, in which there is a significant reduction in degrees of freedom. The proposed method has the potential to significantly reduce experimental costs, increase process efficiency and enhance material performance, which culminate to form a new pathway for materials discovery.",10.1016/j.cej.2021.131345,Machine learning; Scale-up; Nanomaterials; Antibacterial; Reactor,,
An Aggregated Cross-Validation Framework for Computational Discovery of Disease-Associative Genes,"Ogutcen, OF; Gormez, Z; Tahir, MA; Seker, H",XIV MEDITERRANEAN CONFERENCE ON MEDICAL AND BIOLOGICAL ENGINEERING AND COMPUTING 2016,2016.0,"Numerous computational techniques have been applied to identify vital features of gene expression datasets that aim to increase efficiency of biomedical applications. Classification of samples is an important task to correctly recognize diseased people by identifying small but clinically meaningful genes. Conversely, it is a challenging issue for machine learning algorithms. In this paper, we apply a two-stage feature selection approach by using ensemble filter methods and Pareto Optimality. Although filter methods provide ranked lists of all features, they do not give any information about required (optimum) subset sizes of the features, namely, genes in this study. In order to address this issue, PO is incorporated with filter methods. The main aim of this study is therefore to develop a robust framework with PO, multiple feature selection methods and cross-validated subsets of the samples, which is also applicable to not only similar datasets but also different feature selection methods. The robustness of the framework has been successfully demonstrated over three well-known microarray gene expression data sets. The framework has been shown to yield equal or higher predictive accuracy with comparatively smaller feature sizes. Furthermore, the cross-validation and data variation approaches are considered in the framework. Consequently, the framework reduces the over-fitting problem and is observed to have made the gene selection more stable on different conditions.",10.1007/978-3-319-32703-7_94,Pareto Optimality; k-nearest neighbour classifier; statistical feature selection; ensemble feature selection,,
Machine Learning-Driven Multiobjective Optimization: An Opportunity of Microfluidic Platforms Applied in Cancer Research,"Liu, Y; Li, SJ; Liu, YL",CELLS,2022.0,"Cancer metastasis is one of the primary reasons for cancer-related fatalities. Despite the achievements of cancer research with microfluidic platforms, understanding the interplay of multiple factors when it comes to cancer cells is still a great challenge. Crosstalk and causality of different factors in pathogenesis are two important areas in need of further research. With the assistance of machine learning, microfluidic platforms can reach a higher level of detection and classification of cancer metastasis. This article reviews the development history of microfluidics used for cancer research and summarizes how the utilization of machine learning benefits cancer studies, particularly in biomarker detection, wherein causality analysis is useful. To optimize microfluidic platforms, researchers are encouraged to use causality analysis when detecting biomarkers, analyzing tumor microenvironments, choosing materials, and designing structures.",10.3390/cells11050905,cancer; cell sorting; circulating tumor cells; microfluidics; machine-learning,,
A predictive equation for residual strength using a hybrid of subset selection of maximum dissimilarity method with Pareto optimal multi-gene genetic programming,"Riahi-Madvar, H; Gholami, M; Gharabaghi, B; Seyedian, SM",GEOSCIENCE FRONTIERS,2021.0,"More accurate and reliable estimation of residual strength friction angle (phi(r)) of clay is crucial in many geotechnical engineering applications, including riverbank stability analysis, design, and assessment of earthen dam slope stabilities. However, a general predictive equation for phi(r), with applicability in a wide range of effective parameters, remains an important research gap. The goal of this study is to develop a more accurate equation for phi(r) using the Pareto Optimal Multi-gene Genetic Programming (POMGGP) approach by evaluating a comprehensive dataset of 290 experiments compiled from published literature databases worldwide. A new framework for integrated equation derivation proposed that hybridizes the Subset Selection of Maximum Dissimilarity Method (SSMD) with Multi-gene Genetic Programming (MGP) and Pareto-optimality (PO) to find an accurate equation for phi(r) with wide range applicability. The final predictive equation resulted from POMGGP modeling was assessed in comparison with some previously published machine learning-based equations using statistical error analysis criteria, Taylor diagram, revised discrepancy ratio (RDR), and scatter plots. Base on the results, the POMGGP has the lowest uncertainty with U95 = 2.25, when compared with Artificial Neural Network (ANN) (U95 = 2.3), Bayesian Regularization Neural Network (BRNN) (U95 = 2.94), Levenberg-Marquardt Neural Network (LMNN) (U95 = 3.3), and Differential Evolution Neural Network (DENN) (U95 = 2.37). The more reliable results in estimation of phi(r) derived by POMGGP with reliability 59.3%, and resiliency 60% in comparison with ANN (reliability = 30.23%, resiliency = 28.33%), BRNN (reliability = 10.47%, resiliency = 10.39%), LMNN (reliability = 19.77%, resiliency = 20.29%) and DENN (reliability = 27.91%, resiliency = 24.19%). Besides the simplicity and ease of application of the new POMGGP equation to a broad range of conditions, using the uncertainty, reliability, and resilience analysis confirmed that the derived equation for phi(r) significantly outperformed other existing machine learning methods, including the ANN, BRNN, LMNN, and DENN equations. (C) 2021 China University of Geosciences (Beijing) and Peking University. Production and hosting by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).",10.1016/j.gsf.2021.101222,Earth slopes; Friction angle; Maximum dissimilarity; Multi-gene genetic programming; Pareto-optimality; Residual strength,,
Screening Tool for Dam Hazard Potential Classification Using Machine Learning and Multiobjective Parameter Tuning,"Kravits, J; Kasprzyk, J; Baker, K; Andreadis, K",JOURNAL OF WATER RESOURCES PLANNING AND MANAGEMENT,2021.0,"Within the United States' National Inventory of Dams, 15,000 dams have been classified as having a high hazard potential, meaning failure or misoperation would lead to probable loss of human life. However, state dam officials evaluate dam hazard potential on a case-by-case basis, ultimately relying on human judgement. Such a process lacks objectivity and consistency across state boundaries and can be time-consuming. Here, the authors present a parameterized geospatial and machine learning dam hazard potential classification model to overcome these limitations. The parameters of this model can be tuned for optimal performance. However, for this classification problem, the regulatory and physical implications of the types of model misclassifications are best captured through multiple objectives. Therefore, this research additionally contributes a novel multiobjective approach to machine learning parameter tuning. This research demonstrates the utility of this approach for dams in Massachusetts, United States, using a multiobjective evolutionary algorithm to explore different model parameterizations and identify analyst-relevant tradeoffs among objectives describing model performance. Such an approach allows for greater justification of model parameters as well as greater insights into the complexities of the dam hazard potential classification problem.",10.1061/(ASCE)WR.1943-5452.0001414,Dam hazard classification; Machine learning; Tuning; Multiobjective,,
Electrical load forecasting: A deep learning approach based on K-nearest neighbors,"Dong, YX; Ma, XJ; Fu, TL",APPLIED SOFT COMPUTING,2021.0,"Deep learning approaches have shown superior advantages than shallow techniques in the field of electrical load forecasting; however, their applications in existing studies encounter thorny issues despite their excellent forecasting performance: heavy computing costs due to complicated network structure and restricted to the deterministic point forecasting. This paper aims to solve above two problems by proposing a deep learning approach based on K-nearest neighbors to capture uncertainty and reflect the range of electrical load fluctuation. First, the K-nearest neighbors algorithm is applied to seek features of historical electrical load time series that are similar to the future values by calculating the distance between the training and testing datasets. Then the second generation of non-dominated sorting genetic algorithm is adopted for multi-objective optimization to find out the smallest category number of K-nearest neighbors and the highest forecasting accuracy. Based on the forecasting results of the deep belief network, modified non-parameter kernel density estimation is used to obtain the prediction intervals. Five datasets collected from Australia are employed to examine the effectiveness of the proposed model. By a series of comparisons with other state-of-the-art models, experimental results confirm that the proposed interval forecasting model cannot only improve the forecasting efficiency and accuracy, but also simplify the forecasting process of deep learning approaches, which can provide great referential value for future work. (C) 2020 Elsevier B.V. All rights reserved.",10.1016/j.asoc.2020.106900,Electrical load interval forecasting; Deep learning approach; K-nearest neighbors; Kernel density estimation,,
Machine learning-aided design optimization of a mechanical micromixer,"Granados-Ortiz, FJ; Ortega-Casanova, J",PHYSICS OF FLUIDS,2021.0,"In real-life mechanical engineering applications, it is often complex to achieve an optimal multi-objective design, because of the costs related to fabrication and test of different prototypes. For this reason, the use of computational tools is a recommended practice. In this work, the design of an efficient mixing mechanical device composed of a rectangular pillar confined in a microchannel is aided by machine learning techniques (addressed as machine learning-aided design optimization, MLADO, proposed in this work). A random forest classifier is trained to predict which geometric configuration may lead to vortex shedding. Later, a multi-objective optimization problem is investigated, which consists of minimizing the required pumping power and maximizing the mixing efficiency under some design constrains. If extra training data are desired for surrogates, the random forest classifier can be used to predict whether it is worthy or not to simulate the new configuration, avoiding to run irrelevant computational intensive cases and accelerating the data-driven process. The resulting optimal designs from using the NSGA-II genetic algorithm on the surrogates are simulated, and their performance is shown. The optimal geometric configurations, even for very unfavorable mixing conditions and a medium-low Reynolds number of 200, give a maximum mixing efficiency of around 50% at very low pumping power cost in a short channel, outperforming existing devices in the literature. The MLADO framework followed in this work can be easily extendable and automated for other similar design processes in mechanical engineering at any scale, by including shape parameterization strategies.",10.1063/5.0048771,,,
Bayesian set pair analysis and machine learning based ensemble surrogates for optimal multi-aquifer system remediation design,"Yin, J; Tsai, FTC",JOURNAL OF HYDROLOGY,2020.0,"Surrogate models are often adopted to substitute computationally intensive groundwater simulation models for aquifer management due to their high effectiveness and computing efficiency. However, solutions of using only one surrogate model are prone to large prediction uncertainty. This study compares individual surrogate models and an ensemble surrogate model in an optimal groundwater remediation design problem. Three machine learning based surrogate models (response surface regression model, artificial neural network and support vector machine) were developed to replace a high-fidelity solute transport model for predicting saltwater intrusion and assisting saltwater scavenging design. An optimal Latin hypercube design was employed to generate training and testing datasets. Set pair analysis was employed to construct a more reliable ensemble surrogate and to address prediction uncertainty arising from individual surrogate models. Bayesian set pair weights were derived by utilizing full information from both training and testing data and improved typical set pair weights. The individual and ensemble surrogate models were applied to the salinization remediation problem in the Baton Rouge area, southeast Louisiana. The optimal remediation design includes two conflicting objectives: minimizing total groundwater extraction from a horizontal scavenger well while maximizing chloride concentration difference to the MCL (maximum contamination level) at monitoring locations. NSGA-II (Non-dominated Sorting Genetic Algorithm II) was employed to solve the nonlinear optimization model and obtain Pareto-optimal pumping schedules. The optimal pumping schedules from ensemble surrogate models and individual surrogate models were verified by the solute transport model. The study found that Bayesian set pair analysis builds robust ensemble surrogates and accounts for model prediction uncertainty. The ensemble-surrogate-assisted optimization model provides stable and reliable solutions while considerably alleviating computational burden.",10.1016/j.jhydrol.2019.124280,Saltwater scavenging; Bi-objective optimization; Ensemble surrogate; Bayesian set pair analysis; Uncertainty; Machine learning,,
Enhancing energo-exergo-economic performance of Kalina cycle for low- to high-grade waste heat recovery: Design and optimization through deep learning methods,"Dehghani, MJ",APPLIED THERMAL ENGINEERING,2021.0,"The Kalina cycle has proven to be a reliable bottoming cycle for low-grade waste heat recovery. However, compared with other recovery cycles (e.g. the Rankine cycle), it is characterized by a lower efficiency rate and constraints on the heating medium inlet temperature. This study proposes a systematic method for configuring and optimizing three novel Kalina-trilateral-based systems to overcome those disadvantages. This accurate technique integrates thermodynamics with deep learning to accelerate the computation process. First, the actual thermodynamic-economic features of the alternative systems are modeled through the analyses of energy, exergy, and economy. Second, the surrogate models of the systems are developed through a long short-term memory (LSTM) network. Third, the direct and hybrid optimization algorithms are applied separately to the actual and surrogate models. The objective functions yield thermal efficiency, exergy efficiency, and distributed payback time. Moreover, the strength Pareto evolutionary algorithm (SPEA-II) is employed to solve the multiobjective optimization problem. According to the results, the LSTM network had considerable power to simulate and predict the energo-exergo-economic performance of an energy system. Furthermore, the computation time was much shorter for a hybrid algorithm with the maintained accuracy. Consequently, the heating source temperature constraint was eliminated in the final alternative cycle (KTS-36). Compared with the base system (KCS-34), the thermodynamic and economic objective functions were improved by 74.3% and 34%, respectively.",10.1016/j.applthermaleng.2021.117221,Kalina cycle; Trilateral cycle (TLC); LSTM; SPEA-II; Hybrid optimization algorithm (HOA),,
COMPARATIVE MODELS IN CUSTOMER BASE ANALYSIS: PARAMETRIC MODEL AND OBSERVATION-DRIVEN MODEL,"Xie, SM",JOURNAL OF BUSINESS ECONOMICS AND MANAGEMENT,2020.0,"This study conducts a dynamic rolling comparison between the Pareto/NBD model (parametric model) and machine learning algorithms (observation-driven models) in customer base analysis, which the literature has not comprehensively investigated before. The aim is to find the comparative edge of these two approaches under customer base analysis and to define the implementation timing of these two paradigms. This research utilizes Pareto/NBD (Abe) as representative of Buy-Till-You-Die (BTYD) models in order to compete with machine learning algorithms and presents the following results. (1) The parametric model wins in transaction frequency prediction, whereas it loses in inactivity prediction. (2) The BTYD model outperforms machine learning in inactivity prediction when the customer base is active, performs better in an inactive customer base when competing with Poisson regression, and wins in a short-term active customer base when competing with a neural network algorithm in transaction frequency prediction. (3) The parametric model benefits more from a short calibration length and a long holdout/target period, which exhibit uncertainty. (4) The covariate effect helps Pareto/NBD (Abe) gain a better predictive result. These findings assist in defining the comparative edge and implementation timing of these two approaches and are useful for modeling and business decision making.",10.3846/jbem.2020.13194,BTYD; parametric model; Pareto/NBD model; observation-driven model; machine learning; customer base analysis; non-contractual setting,,
Regional Flood Frequency Analysis Through Some Machine Learning Models in Semi-arid Regions,"Allahbakhshian-Farsani, P; Vafakhah, M; Khosravi-Farsani, H; Hertig, E",WATER RESOURCES MANAGEMENT,2020.0,"The machine learning models (MLMs), including support vector regression (SVR), multivariate adaptive regression spline (MARS), boosted regression trees (BRT), and projection pursuit regression (PPR) are compared to traditional method i.e. nonlinear regression (NLR) in regional flood frequency analysis (RFFA). In this study, the Karun and Karkheh watersheds, which is located in the southwestern of Iran, with the same climatic and physiographic conditions are considered. Fifty-four hydrometric stations with a period of 21 years (1993-2013) are selected based on the instructions of U.S. Federal Agencies Bulletin 17 B were applied for RFFA. The generalized normal (GNO) probability distribution function (PDF) is selected by the L-moment method among five PDFs, including the GNO, generalized Pareto (GP), generalized logistic (GL), generalized extreme value (GEV) and Pearson type 3 (P CYRILLIC CAPITAL LETTER BYELORUSSIAN-UKRAINIAN ICYRILLIC CAPITAL LETTER BYELORUSSIAN-UKRAINIAN ICYRILLIC CAPITAL LETTER BYELORUSSIAN-UKRAINIAN I) to estimate flood discharge for the expected return periods. Twenty-five predictor variables, such as physiographic, climatologic, geologic, soil and land use variables are extracted. Follow land, maximum 24-h rainfall, mean watershed slope, compactness coefficient, mean and maximum watershed elevation variables are recognized as the appropriate combination of input using gamma test (GT). The overall results indicate that the SVR, PPR, and MARS models in comparison to the NLR and BRT models have a better performance to estimate flood discharge with the expected return periods. Future, the SVR model based on radial basis function (RBF) kernel is chosen as the best model in terms of the mean of the Nash-Sutcliff coefficient (M-Ef) and the mean of relative root mean squared error (M-RMSEr) (i.e. 0.94 and 63.93, respectively) for different return periods.",10.1007/s11269-020-02589-2,Regionalization; Data Driven models; Land use; Maximum instantaneous discharge; L-moment; Karun and Karkheh watersheds,,
Application and performance of machine learning techniques in manufacturing sector from the past two decades: A review,"Paturi, UMR; Cheruku, S",MATERIALS TODAY-PROCEEDINGS,2021.0,"Advancement in technology has created wide opportunities for the researchers to utilize artificial intelligence in various fields. Numerous attempts have been made in the use of machine learning tools in the manufacturing and production sector. However, variation in the performance of techniques is creating a major quagmire for the researchers. In many cases, some methods have shown similar results while in some cases one outperformed another. Choosing the best and suitable technique for process modelling and optimization is still a challenging task for the researchers. Hence, to present a direction for the prospect investigators, in this study, the performance of different machine learning techniques applied in the manufacturing sector is reviewed by assessing many articles from the past two decades. Among several machine learning techniques reviewed in this study, application of artificial neural networks (ANN) in process modelling and optimization has become quite noticeable because of its ability to predict the output quickly and accurately. The effectiveness and practicality of ANN models in manufacturing applications are reviewed for demonstrating its pivotal role in process modeling. Observations are reported in the study. (C) 2020 The Authors. Published by Elsevier Ltd.",10.1016/j.matpr.2020.07.209,Machine learning; ANN; Process modelling; Manufacturing; Review,,
Systematic comparisons of customer base prediction accuracy: Pareto/NBD versus neural network,"Xie, SM; Huang, CY",ASIA PACIFIC JOURNAL OF MARKETING AND LOGISTICS,2021.0,"Purpose - Predicting the inactivity and the repeat transaction frequency of a firm's customer base is critical for customer relationship management. The literature offers two main approaches to such predictions: stochastic modeling efforts represented by Pareto/NBD and machine learning represented by neural network analysis. As these two approaches have been developed and applied in parallel, this study systematically compares the two approaches in their prediction accuracy and defines the relatively appropriate implementation scenarios of each model. Design/methodology/approach - By designing a rolling exploration scheme with moving calibration/holdout combinations of customer data, this research explores the two approaches' relative performance by first utilizing three real world datasets and then a wide range of simulated datasets. Findings - The empirical result indicates that neither approach is dominant and identifies patterns of relative applicability between the two. Such patterns are consistent across the empirical and the simulated datasets. Originality/value - This study contributes to the literature by bridging two previously parallel analytical approaches applicable to customer base predictions. No prior research has rendered a comprehensive comparison on the two approaches' relative performance in customer base predictions as this study has done. The patterns identified in the two approaches' relative prediction performance provide practitioners with a clear-cut menu upon selecting approaches for customer base predictions. The findings further urge marketing scientists to reevaluate prior modeling efforts during the past half century by assessing what can be replaced by black boxes such as NNA and what cannot.",10.1108/APJML-09-2019-0520,Pareto/NBD; Neural network analysis; Transaction frequency; Activity prediction; Rolling comparison,,
A collaborative machine learning-optimization algorithm to improve the finite element model updating of civil engineering structures,"Naranjo-Perez, J; Infantes, M; Jimenez-Alonso, JF; Saez, A",ENGINEERING STRUCTURES,2020.0,"Finite element model updating has become a key tool to improve the numerical modelling of existing civil engineering structures, by adjusting the numerical response to the observed experimental behaviour of the structure. At present, model updating is mostly conducted using the maximum likelihood method. Following this approach, the updating problem can be transformed into a multi-objective optimization problem. Due to the complex nonlinear behaviour of the resulting objective functions, metaheuristic optimization algorithms are normally employed to solve such optimization problem. However, and although this is nowadays a wellestablished technique, there are still two main drawbacks that need to be addressed for practical engineering applications, namely: (i) the high simulation time required to compute the problem; and (ii) the uncertainty associated with the selection of the best updated model among all the Pareto optimal solutions. In order to overcome these limitations, a new collaborative algorithm is proposed herein, which takes advantage of the collaborative coupling among two optimization algorithms (harmony search and active-set algorithms), a machine learning technique (artificial neural networks) and a statistical tool (principal component analysis). The implementation details of our proposal are discussed in detail throughout the paper and its performance is illustrated with a case study addressing the model updating of a real steel footbridge. Two are the main advantages of the newly proposed algorithm: (i) it leads to a clear reduction of the simulation time; and (ii) it further permits a robust selection of the best updated model.",10.1016/j.engstruct.2020.111327,Multi-objective harmony search optimization; Machine learning; Collaborative algorithm; Best Pareto solution; Finite element model updating; Maximum likelihood method,,
AN ADAPTIVE EVOLUTIONARY ALGORITHM FOR UWB MICROSTRIP ANTENNAS OPTIMIZATION USING A MACHINE LEARNING TECHNIQUE,"Silva, CRM; Martins, SR",MICROWAVE AND OPTICAL TECHNOLOGY LETTERS,2013.0,"This article presents an application of a machine learning technique to enhance a multiobjective evolutionary algorithm to estimate fitness function behaviors from a set of experiments made in laboratory to analyze a microstrip antenna used in ultra wideband wireless devices. These function behaviors are related to three objectives: bandwidth, return loss, and central frequency deviation. Each objective is used inside an aggregate adaptive weighted fitness function that estimates the behavior in the algorithm. The machine learning technique enabled a dynamic estimation of an aggregated compound fitness function and made it possible to a prototype algorithm to learn and adapt with a set of experiments stored in a web system repository. The final results were then compared with the ones obtained with a similar antenna modeled in a simulator program and with the ones of a real prototype antenna built from the optimal values obtained after the optimization. (c) 2013 Wiley Periodicals, Inc. Microwave Opt Technol Lett 55:1864-1868, 2013",10.1002/mop.27692,multiobjective evolutionary algorithms; machine learning; genetic algorithm optimization,,
Machine Learning Directed Optimization of Classical Molecular Modeling Force Fields,"Befort, BJ; DeFever, RS; Tow, GM; Dowling, AW; Maginn, EJ",JOURNAL OF CHEMICAL INFORMATION AND MODELING,2021.0,"Accurate force fields are necessary for predictive molecular simulations. However, developing force fields that accurately reproduce experimental properties is challenging. Here, we present a machine learning directed, multiobjective optimization workflow for force field parametrization that evaluates millions of prospective force field parameter sets while requiring only a small fraction of them to be tested with molecular simulations. We demonstrate the generality of the approach and identify multiple low-error parameter sets for two distinct test cases: simulations of hydrofluorocarbon (HFC) vapor-liquid equilibrium (VLE) and an ammonium perchlorate (AP) crystal phase. We discuss the challenges and implications of our force field optimization workflow.",10.1021/acs.jcim.1c00448,,,
Multi-task prediction and optimization of hydrochar properties from high-moisture municipal solid waste: Application of machine learning on waste-to-resource,"Li, J; Zhu, XZ; Li, YA; Tong, YW; Ok, YS; Wang, XN",JOURNAL OF CLEANER PRODUCTION,2021.0,"Hydrothermal carbonization (HTC) is a promising technology for valuable resources recovery from high-moisture wastes without pre-drying, while optimization of operational conditions for desired products preparation through experiments is always energy and time consuming. To accelerate the experiments in an efficient, sustainable, and economic way, machine learning (ML) tools were employed for bridging the inputs and outputs, which can realize the prediction of hydrochar properties, and development of ML-based optimization for achieving desired hydrochar. The results showed that deep neural network (DNN) model was the best one for joint prediction of both fuel properties (FP) and carbon capture and storage (CCS) stability of hydrochar with an average R-2 and root mean squared error (RMSE) of 0.91 and 3.29. The average testing prediction errors for all the targets were below 20%, furtherly within 10% for HHV, carbon content and H/C predictions. ML-based feature analysis unveiled that both elementary composition and temperature were crucial to FP and CCS. Furthermore, a ML-based software interface was provided for practitioners and researchers to freely access. The insights and Pareto solution provided from ML-based multi-objective optimization benefitted desired hydrochar preparation for the potential application of fuel substitution or carbon sequestration in soil. (C) 2020 Elsevier Ltd. All rights reserved.",10.1016/j.jclepro.2020.123928,Waste-to-energy; Biochar; Hydrothermal carbonization; Renewable energy; Carbon sequestration; Multi-objective optimization,,
Clustering Method using Pareto Corner Search Evolutionary Algorithm for Objective Reduction in Many-Objective Optimization Problems,"Nguyen, XH; Bui, LT; Tran, CT",SOICT 2019: PROCEEDINGS OF THE TENTH INTERNATIONAL SYMPOSIUM ON INFORMATION AND COMMUNICATION TECHNOLOGY,2019.0,"Many-objective optimization problems (MaOPs) have been gained considerable attention for researcher, recently. MaOPs make a number of difficulties for multi-objective optimization evolutionary algorithms (MOEAs) when solving them. Although, there exist a number of many-objective optimization evolutionary algorithms (MaOEAs) for solving MaOPs, they still face difficulties when the number of objectives of MaOPs increases. One common method to reduce or alleviate these difficulties is to use objective dimensionality reduction (or objective reduction for briefly). Moreover, instead of searching the whole of objective space like existing MOEAs or MaOEAs, Pareto Corner Search Evolutionary (PCSEA) concentrates only on some places of objective space, so it decreases time consuming and then speeds up objective reduction. However, PCSEA-based objective reduction needs to specify a threshold to select or remove objectives, which is not straightforward to do. Based on the idea that more conflict two objectives are, more distant two objectives are; in this paper, we introduce a new objective reduction by integrating PCSEA and k-means, DBSCAN clustering algorithms for solving MaOPs which are assumed containing redundant objectives. The experimental results show that the introduced method can reducing redundant objectives better than PCSEA-based objective reduction. The results further strengthen the links between evolutionary computation and machine learning to address optimization problems.",10.1145/3368926.3369720,many-objective optimization; objective reduction; clustering,,
Training feedforward neural network via multiobjective optimization model using non-smooth L-1/2 regularization,"Senhaji, K; Ramchoun, H; Ettaouil, M",NEUROCOMPUTING,2020.0,"The paper presents a new approach to optimize the Multilayer Perceptron Neural Network (MLPNN), to deal with the generalization problem. As known, most supervised learning algorithms aim to minimize the training error. However, the mentioned methods, based only on error minimizing, may generate a solution with an insufficient generalization performance. This present work proposes a multiobjective modelling problem involving two objectives: accuracy and complexity since the learning problem is multiobjective by nature. The learning task is carried on by minimizing both objectives simultaneously, according to Pareto domination concept, using NSGAII (Non-dominated Sorting Genetic Algorithm II) as a solver. This method leads us to a set of solutions called Pareto front, being the optimal solutions set, the adequate MLPNN need to be extracted. We show empirically that the proposed method is capable of reducing the neural networks topology and improved generalization performance, in addition to a good classification rate compared to different methods. (C) 2020 Published by Elsevier B.V.",10.1016/j.neucom.2020.05.066,Multiobjective optimization; NSGAII; Learning algorithm; L-1/2 regularization; Neural network,,
Image-Based Machine Learning for Reduction of User Fatigue in an Interactive Model Calibration System,"Singh, A; Minsker, BS; Bajcsy, P",JOURNAL OF COMPUTING IN CIVIL ENGINEERING,2010.0,"The interactive multiobjective genetic algorithm (IMOGA) is a promising new approach to calibrate models. The IMOGA combines traditional optimization with an interactive framework, thus allowing both quantitative calibration criteria as well as the subjective knowledge of experts to drive the search for model parameters. One of the major challenges in using such interactive systems is the burden they impose on the experts that interact with the system. This paper proposes the use of a novel image-based machine-learning (IBML) approach to reduce the number of user interactions required to identify promising calibration solutions involving spatially distributed parameter fields (e.g., hydraulic conductivity parameters in a groundwater model). The first step in the IBML approach involves selecting a few highly representative solutions for expert ranking. The selection is performed using unsupervised clustering approaches from the field of image processing, which group potential parameter fields based on their spatial similarities. The expert then ranks these representative solutions, after which a machine-learning model (augmented with the spatial information of the selected fields) is trained to learn user preferences and predict rankings for solutions not ranked by the expert. To better mimic the visual information processing of human experts, algorithms from the field of image processing are used to mine information about the spatial characteristics of parameter fields, thus improving the performance of the clustering and machine-learning algorithms. The IBML approach is tested and demonstrated on a groundwater calibration problem and is shown to lead to significant improvements, reducing the amount of user interaction by as much as half without compromising the solution quality of the IMOGA.",10.1061/(ASCE)CP.1943-5487.0000026,,,
A multiobjective weighted voting ensemble classifier based on differential evolution algorithm for text sentiment classification,"Onan, A; Korukoglu, S; Bulut, H",EXPERT SYSTEMS WITH APPLICATIONS,2016.0,"Typically performed by supervised machine learning algorithms, sentiment analysis is highly useful for extracting subjective information from text documents online. Most approaches that use ensemble learning paradigms toward sentiment analysis involve feature engineering in order to enhance the predictive performance. In response, we sought to develop a paradigm of a multiobjective, optimization-based weighted voting scheme to assign appropriate weight values to classifiers and each output class based on the predictive performance of classification algorithms, all to enhance the predictive performance of sentiment classification. The proposed ensemble method is based on static classifier selection involving majority voting error and forward search, as well as a multiobjective differential evolution algorithm. Based on the static classifier selection scheme, our proposed ensemble method incorporates Bayesian logistic regression, naive Bayes, linear discriminant analysis, logistic regression, and support vector machines as base learners, whose performance in terms of precision and recall values determines weight adjustment. Our experimental analysis of classification tasks, including sentiment analysis, software defect prediction, credit risk modeling, spam filtering, and semantic mapping, suggests that the proposed classification scheme can predict better than conventional ensemble learning methods such as AdaBoost, bagging, random subspace, and majority voting. Of all datasets examined, the laptop dataset showed the best classification accuracy (98.86%). (C) 2016 Elsevier Ltd. All rights reserved.",10.1016/j.eswa.2016.06.005,Sentiment analysis; Ensemble learning; Weighted majority voting; Multiobjective optimization,,
Failure estimation of the composite laminates using machine learning techniques,"Serban, A",STEEL AND COMPOSITE STRUCTURES,2017.0,"The problem of layup optimization of the composite laminates involves a very complex multidimensional solution space which is usually non-exhaustively explored using different heuristic computational methods such as genetic algorithms (GA). To ensure the convergence to the global optimum of the applied heuristic during the optimization process it is necessary to evaluate a lot of layup configurations. As a consequence the analysis of an individual layup configuration should be fast enough to maintain the convergence time range to an acceptable level. On the other hand the mechanical behavior analysis of composite laminates for any geometry and boundary condition is very convoluted and is performed by computational expensive numerical tools such as finite element analysis (FEA). In this respect some studies propose very fast FEA models used in layup optimization. However, the lower bound of the execution time of FEA models is determined by the global linear system solving which in some complex applications can be unacceptable. Moreover, in some situation it may be highly preferred to decrease the optimization time with the cost of a small reduction in the analysis accuracy. In this paper we explore some machine learning techniques in order to estimate the failure of a layup configuration. The estimated response can be qualitative (the configuration fails or not) or quantitative (the value of the failure factor). The procedure consists of generating a population of random observations (configurations) spread across solution space and evaluating using a FEA model. The machine learning method is then trained using this population and the trained model is then used to estimate failure in the optimization process. The results obtained are very promising as illustrated with an example where the misclassification rate of the qualitative response is smaller than 2%.",10.12989/scs.2017.25.6.663,failure estimation; layup optimization; machine learning; finite element analysis; numerical analysis,,
Machine Learning Supporting Experimental Design for Product Development in the Lab,"Babutzka, J; Bortz, M; Dinges, A; Foltin, G; Hajnal, D; Schultze, H; Weiss, H",CHEMIE INGENIEUR TECHNIK,2019.0,"An interactive decision support framework is presented that assists lab researchers in finding optimal product recipes. Within this framework, an approach for sequential experimental design for black box models in a multicriteria optimization context is introduced. An additional criterion involving the prediction error to design new experiments is used with the goal to get a reliable estimate of the Pareto frontier within a few experimental iterations. The resulting decision support approach accompanies the chemist through the whole workflow and supports the user via interactive, graphical elements.",10.1002/cite.201800089,Machine learning; Model selection; Multiobjective optimizations; Parameter estimation; Prediction error methods; Sequential experimental design,,
Recent Developments in Machine Learning for Energy Systems Reliability Management,"Duchesne, L; Karangelos, E; Wehenkel, L",PROCEEDINGS OF THE IEEE,2020.0,"This article reviews recent works applying machine learning (ML) techniques in the context of energy systems' reliability assessment and control. We showcase both the progress achieved to date as well as the important future directions for further research, while providing an adequate background in the fields of reliability management and of ML. The objective is to foster the synergy between these two fields and speed up the practical adoption of ML techniques for energy systems reliability management. We focus on bulk electric power systems and use them as an example, but we argue that the methods, tools, etc. can be extended to other similar systems, such as distribution systems, microgrids, and multienergy systems.",10.1109/JPROC.2020.2988715,Reliability engineering; Security; Power system reliability; Power system stability; Management; Power system dynamics; Machine learning; Power system control; Electric power systems (EPSs); machine learning (ML); reliability; security assessment; security control,,
Sepsis prediction in intensive care unit based on genetic feature optimization and stacked deep ensemble learning,"El-Rashidy, N; Abuhmed, T; Alarabi, L; El-Bakry, HM; Abdelrazek, S; Ali, F; El-Sappagh, S",NEURAL COMPUTING & APPLICATIONS,2022.0,"Sepsis is a life-threatening disease that is associated with organ dysfunction. It occurs due to the body's dysregulated response to infection. It is difficult to identify sepsis in its early stages, this delay in identification has a dramatic effect on mortality rate. Developing prognostic tools for sepsis prediction has been the focus of various studies over previous decades. However, most of these studies relied on tracking a limited number of features, as such, these approaches may not predict sepsis sufficiently accurately in many cases. Therefore, in this study, we concentrate on building a more accurate and medically relevant predictive model for identifying sepsis. First, both NSGA-II (a multi-objective genetic algorithm optimization approach) and artificial neural networks are used concurrently to extract the optimal feature subset from patient data. In the next stage, a deep learning model is built based on the selected optimal feature set. The proposed model has two layers. The first is a deep learning classification model used to predict sepsis. This is a stacking ensemble of neural network models that predicts which patients will develop sepsis. For patients who were predicted to have sepsis, data from their first six hours after admission to the ICU are retrieved, this data is then used for further model optimization. Optimization based on this small, recent timeframe leads to an increase in the effectiveness of our classification model compared to other models from previous works. In the second layer of our model, a multitask regression deep learning model is used to identify the onset time of sepsis and the blood pressure at that time in patients that were predicted to have sepsis by the first layer. Our study was performed using the medical information from the intensive care MIMIC III real-world dataset. The proposed classification model achieved 0.913, 0.921, 0.832, 0.906 for accuracy, specificity, sensitivity, and AUC, respectively. In addition, the multitask regression model obtained an RMSE of 10.26 and 9.22 for predicting the onset time of sepsis and the blood pressure at that time, respectively. There are no other studies in the literature that can accurately predict the status of sepsis in terms of its onset time and predict medically verifiable quantities like blood pressure to build confidence in the onset time prediction. The proposed model is medically intuitive and achieves superior performance when compared to all other current state-of-the-art approaches.",10.1007/s00521-021-06631-1,Ensemble classifier; Deep learning; Feature optimization; Multitask learning; Sepsis prediction,,
Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science,"Olson, RS; Bartley, N; Urbanowicz, RJ; Moore, JH",GECCO'16: PROCEEDINGS OF THE 2016 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE,2016.0,"As the field of data science continues to grow, there will be an ever-increasing demand for tools that make machine learning accessible to non-experts. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning-pipeline design. We implement an open source Tree-based Pipeline Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a series of simulated and real-world benchmark data sets. In particular, we show that TPOT can design machine learning pipelines that provide a significant improvement over a basic machine learning analysis while requiring little to no input nor prior knowledge from the user. We also address the tendency for TPOT to design overly complex pipelines by integrating Pareto optimization, which produces compact pipelines without sacrificing classification accuracy. As such, this work represents an important step toward fully automating machine learning pipeline design.",10.1145/2908812.2908918,pipeline optimization; hyperparameter optimization; data science; machine learning; genetic programming; Pareto optimization; Python,,
Bias free multiobjective active learning for materials design and discovery,"Jablonka, KM; Jothiappan, GM; Wang, SF; Smit, B; Yoo, B",NATURE COMMUNICATIONS,2021.0,"The design rules for materials are clear for applications with a single objective. For most applications, however, there are often multiple, sometimes competing objectives where there is no single best material and the design rules change to finding the set of Pareto optimal materials. In this work, we leverage an active learning algorithm that directly uses the Pareto dominance relation to compute the set of Pareto optimal materials with desirable accuracy. We apply our algorithm to de novo polymer design with a prohibitively large search space. Using molecular simulations, we compute key descriptors for dispersant applications and drastically reduce the number of materials that need to be evaluated to reconstruct the Pareto front with a desired confidence. This work showcases how simulation and machine learning techniques can be coupled to discover materials within a design space that would be intractable using conventional screening approaches.",10.1038/s41467-021-22437-0,,,
Evaluating urban flood risk using hybrid method of TOPSIS and machine learning,"Rafiei-Sardooi, E; Azareh, A; Choubin, B; Mosavi, AH; Clague, JJ",INTERNATIONAL JOURNAL OF DISASTER RISK REDUCTION,2021.0,"With the growth of cities, urban flooding has increasingly become an issue for regional and national governments. The destructive effects of floods are magnified in cities. Accurate models of urban flood susceptibility are required to mitigate this hazard mitigation and build resilience in cities. In this paper, we evaluate flood riskin Jiroft city, Iran, using a combination of machine learning and decision-making methods. Flood hazard maps were created using three state-of-the-art machine learning methods (support vector machine, random forest, and boosted regression tree). The metadata supporting our analysis comprises 218 flood inundation points and a variety of derived factors: slope aspect, elevation, slope angle, rainfall, distance to streets, distance to rivers, land use/land cover, distance to urban drainages, urban drainage density, and curve number. We then employed the TOPSIS decision-making tool for urban flood vulnerability analysis, which is based on socio-economic factors such as building density, population density, building history, and socio-economic conditions. Finally, we derived an urban flood risk map for Jiroft based on flood hazard and vulnerability maps. Of the three models tested, the random forest model yielded the most accurate map. The results indicate that urban drainage density and distance to urban drainages are the most important factors in urban flood hazard modeling. As might be expected, areas with a high or very high population density are most vulnerable to flooding. These results show that flood risk mapping provide insights for priority planning in flood risk management, especially in areas with limited hydrological data.",10.1016/j.ijdrr.2021.102614,Urban flooding; Hazard; Vulnerability; TOPSIS; Machine learning; Artificial intelligence,,
Optimization of Deep Learning Precipitation Models Using Categorical Binary Metrics,"Larraondo, PR; Renzullo, LJ; Van Dijk, AIJM; Inza, I; Lozano, JA",JOURNAL OF ADVANCES IN MODELING EARTH SYSTEMS,2020.0,"This work introduces a methodology for optimizing neural network models using a combination of continuous and categorical binary indices in the context of precipitation forecasting. Probability of detection and false alarm rate are popular metrics used in the verification of precipitation models. However, machine learning models trained using gradient descent cannot be optimized based on these metrics, as they are not differentiable. We propose an alternative formulation for these categorical indices that are differentiable and we demonstrate how they can be used to optimize the skill of precipitation neural network models defined as a multiobjective optimization problem. To our knowledge, this is the first proposal of a methodology for optimizing weather neural network models based on categorical indices.",10.1029/2019MS001909,precipitation verification; machine learning; categorical indexes; neural networks; modeling,,
Machine Learning Techniques Focusing on the Energy Performance of Buildings: A Dimensions and Methods Analysis,"Anastasiadou, M; Santos, V; Dias, MS",BUILDINGS,2022.0,"The problem of energy consumption and the importance of improving existing buildings' energy performance are notorious. This work aims to contribute to this improvement by identifying the latest and most appropriate machine learning or statistical techniques, which analyze this problem by looking at large quantities of building energy performance certification data and other data sources. PRISMA, a well-established systematic literature review and meta-analysis method, was used to detect specific factors that influence the energy performance of buildings, resulting in an analysis of 35 papers published between 2016 and April 2021, creating a baseline for further inquiry. Through this systematic literature review and bibliometric analysis, machine learning and statistical approaches primarily based on building energy certification data were identified and analyzed in two groups: (1) automatic evaluation of buildings' energy performance and, (2) prediction of energy-efficient retrofit measures. The main contribution of our study is a conceptual and theoretical framework applicable in the analysis of the energy performance of buildings with intelligent computational methods. With our framework, the reader can understand which approaches are most used and more appropriate for analyzing the energy performance of different types of buildings, discussing the dimensions that are better used in such approaches.",10.3390/buildings12010028,energy performance certificate (EPC); machine learning (ML); energy-efficient retrofitting measures (EERM); energy performance of buildings (EPB); energy efficiency (EE),,
Machine learning data-driven approaches for land use/cover mapping and trend analysis using Google Earth Engine,"Feizizadeh, B; Omarzadeh, D; Garajeh, MK; Lakes, T; Blaschke, T",JOURNAL OF ENVIRONMENTAL PLANNING AND MANAGEMENT,,"With the recent advances in earth observation technologies, the increasing availability of data from more and more different satellite sensors as well as progress in semi-automated and automated classification techniques enable the (semi-) automated remote monitoring and analysis of large areas. Online platforms such as Google Earth Engine (GEE) bring data-driven techniques to the desktops of researchers while changing workflows and making excessive data downloads redundant. We present a study that utilizes machine learning algorithms on the GEE cloud computing platform for land use/land cover (LULC) mapping and change detection analysis using a Landsat satellite image time series. We applied different machine learning techniques to data from an environmentally sensitive area in Northern Iran. We tested their efficiency for LULC mapping and change detection analysis using the support vector machine (SVM), random forest (RF) and classification and regression tree (CART). We obtained LULC maps for the years 2000, 2005, 2010, 2015 and 2020. Training data was collected from field operations and historical datasets, and the respective LULC maps were validated using ground control points. In addition, we validated the reliability of the results through a spatial uncertainty analysis using Dempster-Shafer Theory (DST). The resulting accuracies of the classification outcomes varied significantly. SVM performed best with accuracies of 90.25%, 91.84%, 89.02%, 93.35% and 95.65% for 2000, 2005, 2010, 2015 and 2020, respectively. The spatial uncertainty analysis also validated the efficiency of SVM compared to RF and CART. The results confirm the potential of machine learning techniques for time series LULC mapping on the GEE platform while lowering the barriers to analyzing large amounts of satellite data. The results are also critical for decision-makers and authorities for analyzing the LULC changes and developing the respective environmental protection and polices in Northern Iran.",10.1080/09640568.2021.2001317,Machine learning; spatial uncertainty analysis; comparative approach Google Earth Engine; land use; cover mapping; Urmia lake basin,,
Knowledge Transfer with Weighted Adversarial Network for Cold-Start Store Site Recommendation,"Liu, Y; Guo, B; Zhang, DQ; Zeghlache, D; Chen, JM; Hu, K; Zhang, SZ; Zhou, D; Yu, ZW",ACM TRANSACTIONS ON KNOWLEDGE DISCOVERY FROM DATA,2021.0,"Store site recommendation aims to predict the value of the store at candidate locations and then recommend the optimal location to the company for placing a new brick-and-mortar store. Most existing studies focus on learning machine learning or deep learning models based on large-scale training data of existing chain stores in the same city. However, the expansion of chain enterprises in new cities suffers from data scarcity issues, and these models do not work in the new city where no chain store has been placed (i.e., cold-start problem). In this article, we propose a unified approach for cold-start store site recommendation, Weighted Adversarial Network with Transferability weighting scheme (WANT), to transfer knowledge learned from a data-rich source city to a target city with no labeled data. In particular, to promote positive transfer, we develop a discriminator to diminish distribution discrepancy between source city and target city with different data distributions, which plays the minimax game with the feature extractor to learn transferable representations across cities by adversarial learning. In addition, to further reduce the risk of negative transfer, we design a transferability weighting scheme to quantify the transferability of examples in source city and reweight the contribution of relevant source examples to transfer useful knowledge. We validate WANT using a real-world dataset, and experimental results demonstrate the effectiveness of our proposed model over several state-of-the-art baseline models.",10.1145/3442203,Urban computing; cold-start problem; store site recommendation; transfer learning; neural networks,,
MODIS-FIRMS and ground-truthing-based wildfire likelihood mapping of Sikkim Himalaya using machine learning algorithms,"Banerjee, P",NATURAL HAZARDS,2022.0,"Wildfires in limited extent and intensity can be a boon for the forest ecosystem. However, recent episodes of wildfires of 2019 in Australia and Brazil are sad reminders of their heavy ecological and economical costs. Understanding the role of environmental factors in the likelihood of wildfires in a spatial context would be instrumental in mitigating it. In this study, 15 environmental features encompassing meteorological, topographical, ecological, in situ and anthropogenic factors have been considered for preparing the wildfire likelihood map of Sikkim Himalaya. A comparative study on the efficiency of machine learning methods like Generalized Linear Model, Support Vector Machine, Random Forest (RF) and Gradient Boosting Model (GBM) has been performed to identify the best performing algorithm in wildfire prediction. The study indicates that all the machine learning methods are good at predicting wildfires. However, RF has outperformed, followed by GBM in the prediction. Also, environmental features like average temperature, average wind speed, proximity to roadways and tree cover percentage are the most important determinants of wildfires in Sikkim Himalaya. This study can be considered as a decision support tool for preparedness, efficient resource allocation and sensitization of people towards mitigation of wildfires in Sikkim.",10.1007/s11069-021-04973-6,Forest fire; Prediction map; Algorithm; Statistical learning; GIS,,
Cropland Suitability Assessment Using Satellite-Based Biophysical Vegetation Properties and Machine Learning,"Radocaj, D; Jurisic, M; Gasparovic, M; Plascak, I; Antonic, O",AGRONOMY-BASEL,2021.0,"The determination of cropland suitability is a major step for adapting to the increased food demands caused by population growth, climate change and environmental contamination. This study presents a novel cropland suitability assessment approach based on machine learning, which overcomes the limitations of the conventional GIS-based multicriteria analysis by increasing computational efficiency, accuracy and objectivity of the prediction. The suitability assessment method was developed and evaluated for soybean cultivation within two 50 x 50 km subsets located in the continental biogeoregion of Croatia, in the four-year period during 2017-2020. Two biophysical vegetation properties, leaf area index (LAI) and a fraction of absorbed photosynthetically active radiation (FAPAR), were utilized to train and test machine learning models. The data derived from a medium-resolution satellite mission PROBA-V were prime indicators of cropland suitability, having a high correlation to crop health, yield and biomass in previous studies. A variety of climate, soil, topography and vegetation covariates were used to establish a relationship with the training samples, with a total of 119 covariates being utilized per yearly suitability assessment. Random forest (RF) produced a superior prediction accuracy compared to support vector machine (SVM), having the mean overall accuracy of 76.6% to 68.1% for Subset A and 80.6% to 79.5% for Subset B. The 6.1% of the highly suitable FAO suitability class for soybean cultivation was determined on the sparsely utilized Subset A, while the intensively cultivated agricultural land produced only 1.5% of the same suitability class in Subset B. The applicability of the proposed method for other crop types adjusted by their respective vegetation periods, as well as the upgrade to high-resolution Sentinel-2 images, will be a subject of future research.",10.3390/agronomy11081620,leaf area index (LAI); fraction of absorbed photosynthetically active radiation (FAPAR); random forest (RF); support vector machine (SVM); soybean; GIS-based multicriteria analysis; covariates,,
Modeling and Optimization of a Jackfruit Seed-Based Supercapacitor Electrode Using Machine Learning,"Mathew, S; Karandikar, PB; Kulkarni, NR",CHEMICAL ENGINEERING & TECHNOLOGY,2020.0,"Supercapacitors can be used for portable energy storage applications. In this study, machine learning techniques are applied to optimize the process of preparation of supercapacitor electrodes from chemically activated carbon made from jackfruit seeds. Experimental trials were carried out using statistical design of experiments. Artificial neural network was employed to generate the process model and a multiobjective optimization was attempted by means of swarm intelligence and the Derringer's desirability function. The optimized electrode demonstrated high capacitance and low resistance making it suitable for supercapacitors. The algorithm developed in the study can be adopted by process engineers for efficient optimization.",10.1002/ceat.201900616,Activated carbon; Artificial neural networks; Derringer's desirability function; Supercapacitors; Swarm intelligence,,
Data-Driven Selection of Land Product Validation Station Based on Machine Learning,"Li, RX; Tao, Z; Zhou, X; Lv, TT; Wang, J; Xie, FT; Zhai, MJ",REMOTE SENSING,2022.0,"Validation is a crucial technique used to strengthen the application capabilities of earthobservation satellite data and solve the quality problems of remote-sensing products. Observing land-surface parameters in the field is one of the key steps of validation. Therefore, the demand for long-term stable validation stations has gradually increased. However, the current location-selection procedure of validation stations lacks a systematic and objective evaluation system. In this research, a data-driven selection of a land product validation station (DSS-LPV) based on Machine Learning is proposed. Firstly, we construct an evaluation indicator system in which all factors affecting the location of validation stations are divided into surface characteristics, atmospheric conditions and the social environment. Then, multi-scale evaluation grids are constructed and indicators are allocated for spatial evaluation. Finally, four Machine Learning (ML) methods are used to learn the established reliable stations, and different data-driven scoring models are constructed to explore the intrinsic relationship between evaluation indicators and station locations. In this article, the reliability of DSS-LPV is effectively validated by the example of China using the national-level land product validation station that has been established. After a comparison between the four ML models, the random forest (RF) with the highest accuracy was selected as the modeling method of DSS-LPV. The correlation between the regression value of test stations and the target value is 0.9133. The average score of test stations is 0.8304. The test stations are generally located within the calculated hot-spot area of the score density map, which means that it is highly consistent with the location of the built stations. Research results indicate that DSS-LPV is an effective method that can provide a reasonable geographical distribution of the stations. The location-selection results can provide scientific decision-making support for the construction of land product validation stations.",10.3390/rs14040813,land product validation station; location selection; data-driven; machine learning,,
Metamodel-based design optimization of structural one-way slabs based on deep learning neural networks to reduce environmental impact,"Ferreiro-Cabello, J; Fraile-Garcia, E; Ascacibar, EMD; Martinez-de-Pison, FJ",ENGINEERING STRUCTURES,2018.0,"This article presents a methodology for the construction and use of metamodels with Deep Learning (DL) methods that are useful for making multi-criteria decisions in the design and optimization of one-way slabs. The main motivation behind this research has been to examine the possibilities of improving slab design by including this methodology in future tools, which is capable of calculating thousands of solutions in real time based on the designer's specifications. The process of creating these metamodels begins by developing a database of millions of combinations of slab designs. These combinations are calculated with a heuristic algorithm that provides the following results: rigidity, deflection, cost per square meter, CO2 emissions and embodied energy. Once a database including the entire universe of possible solutions has been created, a metamodel is developed that is capable of condensing the implicit knowledge contained in the database. This metamodel is included within a Decision Support System (DSS) that produces thousands of solutions for slabs that all comply with a range of specifications designated by the design plan. Furthermore, the methodology described herein proposes the use of Pareto-optimal solutions and graphic tools to help designers make multi-criteria decisions regarding the solutions that best fit their needs. A case study is presented to illustrate this proposal: optimizing slab design in two buildings according to technical, economic and sustainability criteria. The results indicate that the multi-criteria solutions obtained would entail a significant reduction in both emissions and embodied energy as compared to mono-criteria solutions, without significantly increasing costs.",10.1016/j.engstruct.2017.11.005,One-way slab; Deep learning; Multicriteria optimization; Metamodel; Structures; Reinforced concrete,,
Prediction of curing process for thermosetting prepreg compression molding process based on machine learning,"Hou, JT; You, B; Xu, JZ; Wang, T",POLYMER COMPOSITES,2022.0,"In the curing process of thermosetting prepreg compression molding (PCM), the distribution of the temperature field and the curing degree field have an important influence on the performance of composites. Therefore, the establishment of method to accurately predict the temperature difference and the degree of cure (DoC) difference during the curing process is significance for improving the performance of composites. In this paper, three kinds of machine learning models are studied: back propagation (BP) neural network, genetic algorithm-back propagation (GA-BP) neural network, radial basis function (RBF) neural network, then predictive models based on finite element method (FEM) and machine learning models are proposed. In the double-dwell curing curve, six typical parameters are selected as inputs; the maximum value of temperature, the maximum value of temperature overshoot, the maximum DoC difference, the curing time, these four parameters during the curing process are selected as outputs, then the rapid predictive model is established. Within the value range of the process parameters, the Latin hypercube sampling (LHS) method is used to select 100 sets of sample points, and after training on three predictive models, comparison, and verification are carried out. The results show that the predictive effect of the RBF model is the best. In these three models, the RBF model is more suitable for the performance prediction of composites PCM. In this article, the research provides the basis for the performance prediction of composites and the multiobjective optimization of the curing process.",10.1002/pc.26494,curing process; machine learning; prediction; prepreg compression molding; thermosetting composites,,
A Novel Method on Hydrographic Survey Technology Selection Based on the Decision Tree Supervised Learning,"Medvesek, IG; Vujovic, I; Soda, J; Krcum, M",APPLIED SCIENCES-BASEL,2021.0,"Featured Application If one wants to perform a hydrographic survey, many available types of equipment are presenting different technologies, which could (or not) be applicable in specific environments and conditions. The main question is how to choose the optimal available technology. In this paper, we develop a novel method for choosing the best-suited technology for a specific survey region, and it is applied on Kastela Bay as an application example. The example shows the step-by-step process of identifying appropriate technology. Hydrographic survey or seabed mapping plays an important role in achieving better maritime safety, especially in coastal waters. Due to advances in survey technologies, it becomes important to choose well-suited technology for a specific area. Moreover, various technologies have various ranges of equipment and manufacturers, as well as characteristics. Therefore, in this paper, a novel method of a hydrographic survey, i.e., identifying the appropriate technology, has been developed. The method is based on a reduced elimination matrix, decision tree supervised learning, and multicriteria decision methods. The available technologies were: remotely operated underwater vehicle (ROV), unmanned aerial vehicle (UAV), light detection and ranging (LIDAR), autonomous underwater vehicle (AUV), satellite-derived bathymetry (SDB), and multibeam echosounder (MBES), and they are applied as a case study of Kastela Bay. Results show, considering the specifics of the survey area, that UAV is the best-suited technology to be used for a hydrographic survey. However, some other technologies, such as SDB come close and can be considered an alternative for hydrographic surveys.",10.3390/app11114966,supervised learning; decision tree; hydrographic survey; weighted sum model,,
Design of Polymer Blend Electrolytes through a Machine Learning Approach,"Wheatle, BK; Fuentes, EF; Lynd, NA; Ganesan, V",MACROMOLECULES,2020.0,"We apply a machine learning (ML) technique to the multiobjective design of polymer blend electrolytes. In particular, we are interested in maximizing electrolyte performance measured by a combination of ionic transport (measured by ionic conductivity) and electrolyte mechanical properties (measured by viscosity) in a coarse-grained molecular dynamics framework. Recognizing the expense of evaluating each of these properties, we identify that the anionic mean-squared displacement and polymer relaxation time can serve as their proxies. By employing the ML approach known as Bayesian optimization, we identify a trade-off between ion transport and electrolyte mechanical properties as a function of varied design parameters, which include host molecular weight and polarity. Our results suggest that blend electrolytes whose hosts have unequal molecular weights, such as gel polymer electrolytes, rarely maximize electrolyte performance. Overall, our results suggest the potential of a framework to design high-performance electrolytes using a combination of molecular simulation and ML.",10.1021/acs.macromol.0c01547,,,
Solving Test Case Based Problems With Fuzzy Dominance,"Zutty, J; Rohling, G",PROCEEDINGS OF THE 2017 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE (GECCO'17),2017.0,"Genetic algorithms and genetic programming lend themselves well to the field of machine learning, which involves solving test case based problems. However, most traditional multi-objective selection methods work with scalar objectives, such as minimizing false negative and false positive rates, that are computed from underlying test cases. In this paper, we propose a new fuzzy selection operator that takes into account the statistical nature of machine learning problems based on test cases. Rather than use a Pareto rank or strength computed from scalar objectives, such as with NSGA2 or SPEA2, we will compute a probability of Pareto optimality. This will be accomplished through covariance estimation and Markov chain Monte Carlo simulation in order to generate probabilistic objective scores for each individual. We then compute a probability that each individual will generate a Pareto optimal solution. This probability is directly used with a roulette wheel selection technique. Our method's performance is evaluated on the evolution of a feature selection vector for a binary classification on each of eight different activities. Fuzzy selection performance varies, outperforming both NSGA2 and SPEA2 in both speed (measured in generations) and solution quality (measured by area under the curve) in some cases, while underperforming in others.",10.1145/3071178.3071234,Genetic Algorithms; Machine Learning; Markov Chain Monte Carlo; Pareto Dominance,,
A Novel Ensemble Machine Learning and an Evolutionary Algorithm in Modeling the COVID-19 Epidemic and Optimizing Government Policies,"Tayarani-Najaran, MH",IEEE TRANSACTIONS ON SYSTEMS MAN CYBERNETICS-SYSTEMS,,"The spread of the COVID-19 disease has prompted a need for immediate reaction by governments to curb the pandemic. Many countries have adopted different policies and studies are performed to understand the effect of each of the policies on the growth rate of the infected cases. In this article, the data about the policies taken by all countries at each date, and the effect of the policies on the growth rate of the pandemic are used to build a model of the pandemic's behavior. The model takes as input a set of policies and predicts the growth rate of the pandemic. Then, a population-based multiobjective optimization algorithm is developed, which uses the model to search through the policy space and finds a set of policies that minimize the cost induced to the society due to the policies and the growth rate of the pandemic. Because of the complexity of the modeling problem and the uncertainty in measuring the growth rate of the pandemic via the models, an ensemble learning algorithm is proposed in this article to improve the performance of individual learning algorithms. The ensemble consists of ten learning algorithms and a metamodel algorithm that is built to predict the accuracy of each learning algorithm for a given data record. The metamodel is a set of support vector machine (SVM) algorithms that is used in the aggregation phase of the ensemble algorithm. Because there is uncertainty in measuring the growth rate via the models, a landscape smoothing operator is proposed in the optimization process, which aims at reducing uncertainty. The algorithm is tested on open access data online and experiments on the ensemble learning and the policy optimization algorithms are performed.",10.1109/TSMC.2022.3143955,Prediction algorithms; Pandemics; Uncertainty; Optimization; Predictive models; COVID-19; Neural networks; Ensemble learning; epidemiology COVID-19; evolutionary algorithms; optimization; policy making,,
Multiobjective optimization of classifiers by means of 3D convex-hull-based evolutionary algorithms,"Zhao, JQ; Fernandes, VB; Jiao, LC; Yevseyeya, I; Maulana, A; Li, R; Back, T; Tang, K; Emmerich, MTM",INFORMATION SCIENCES,2016.0,"The receiver operating characteristic (ROC) and detection error tradeoff (DET) curves are frequently used in the machine learning community to analyze the performance of binary classifiers. Recently, the convex-hull-based multiobjective genetic programming algorithm was proposed and successfully applied to maximize the convex hull area for binary classification problems by minimizing false positive rate and maximizing true positive rate at the same time using indicator-based evolutionary algorithms. The area under the ROC curve was used for the performance assessment and to guide the search. Here we extend this research and propose two major advancements: Firstly we formulate the algorithm in detection error tradeoff space, minimizing false positives and false negatives, with the advantage that misclassification cost tradeoff can be assessed directly. Secondly, we add complexity as an objective function, which gives rise to a 3D objective space (as opposed to a 2D previous ROC space). A domain specific performance indicator for 3D Pareto front approximations, the volume above DET surface, is introduced, and used to guide the indicator -based evolutionary algorithm to find optimal approximation sets. We assess the performance of the new algorithm on designed theoretical problems with different geometries of Pareto fronts and DET surfaces, and two application-oriented benchmarks: (1) Designing spam filters with low numbers of false rejects, false accepts, and low computational cost using rule ensembles, and (2) finding sparse neural networks for binary classification of test data from the UCI machine learning benchmark. The results show a high performance of the new algorithm as compared to conventional methods for multicriteria optimization. (C) 2016 Elsevier Inc. All rights reserved.",10.1016/j.ins.2016.05.026,Convex hull; Classification; Evolutionary multiobjective optimization; Parsimony; ROC analysis; Anti-spam filters,,
Efficient multi-criteria optimization on noisy machine learning problems,"Koch, P; Wagner, T; Emmerich, MTM; Back, T; Konen, W",APPLIED SOFT COMPUTING,2015.0,"Recent research revealed that model-assisted parameter tuning can improve the quality of supervised machine learning (ML) models. The tuned models were especially found to generalize better and to be more robust compared to other optimization approaches. However, the advantages of the tuning often came along with high computation times, meaning a real burden for employing tuning algorithms. While the training with a reduced number of patterns can be a solution to this, it is often connected with decreasing model accuracies and increasing instabilities and noise. Hence, we propose a novel approach defined by a two criteria optimization task, where both the runtime and the quality of ML models are optimized. Because the budgets for this optimization task are usually very restricted in ML, the surrogate-assisted Efficient Global Optimization (EGO) algorithm is adapted. In order to cope with noisy experiments, we apply two hypervolume indicator based EGO algorithms with smoothing and reinterpolation of the surrogate models. The techniques do not need replicates. We find that these EGO techniques can outperform traditional approaches such as latin hypercube sampling (LHS), as well as EGO variants with replicates. (C) 2015 Elsevier B.V. All rights reserved.",10.1016/j.asoc.2015.01.005,Machine learning; Multi-criteria optimization; Efficient Global Optimization; Kriging; Hypervolume indicator,,
Real-time realization of Dynamic Programming using machine learning methods for IC engine waste heat recovery system power optimization,"Xu, B; Rathod, D; Yebi, A; Filipi, Z",APPLIED ENERGY,2020.0,"This study aims to present a method for real-time realization of Dynamic Programming algorithm for power optimization in an organic Rankine Cycle waste heat recovery system. Different from existing studies, for the first time machine learning algorithms are utilized to extract the rules from offline Dynamic Programming results for optimal power generation. In addition, for the first time a single state Proper Orthogonal Decomposition and Galerkin Projection based reduced order model is combined with Dynamic Programming for its high accuracy and low computation cost. For a transient driving cycle, Dynamic Programming algorithm is utilized to generate the optimal working fluid pump speed. A total of eleven state-of-art machine learning algorithms are screened to predict this pump speed. Random Forest algorithm is then selected for its best pump speed prediction accuracy. A rule-based method is added to the Random Forest model to improve energy recovery. As one of the main discoveries in this study, in the rule extraction process, the Random Forest model reveals that the time delayed exhaust gas mass flow rate and exhaust temperature improve the rule extraction accuracy. This observation points out the difference between steady state and transient optimization and that the steady state optimization results do not necessarily hold true in transient conditions. Another key observation is that Random Forest - rule-based method retrieves 97.2% of the energy recovered by offline Dynamic Programming in a validation driving cycle. In addition, the inclusion of rule-based method significantly increases the Random Forest model's energy recovery from 66.5% to 97.2%. This high accuracy means that the machine learning models can be used to extract Dynamic Programming rules for real-time application.",10.1016/j.apenergy.2020.114514,Waste heat recovery; Organic Rankine Cycle; Dynamic Programming; Transient optimization; Real-time implementation; Machine learning,,
Multiobjective evolution of deep learning parameters for robot manipulator object recognition and grasping,"Hossain, D; Capi, G",ADVANCED ROBOTICS,2018.0,"Deep Learning (DL) is currently very popular because of its similarity to the hierarchical architecture of human brain with multiple levels of abstraction. DL has many parameters that influence the network performance. In this paper, we introduce a multiobjective evolutionary algorithm (MOEA) to optimize the DBNN parameters subject to the error rate and the network training time as two conflicting objectives. To verify the effectiveness, the proposed method is applied to the robot object recognition and grasping task. We compare the performance of the optimized DBNN model with a) DBNN with arbitrarily selected parameters and b) Deep Belief Network-Deep Neural Network (DBN-DNN). The results show that optimized DL has a superior performance in terms of training time and recognition success rate. In addition, the optimized DBNN model is effective for real-time robotic implementations.",10.1080/01691864.2018.1529620,Deep learning; multiobjective evolution; object recognition; robot grasping; DBNN,,
Temporal difference learning applied to game playing and the results of application to shogi,"Beal, DF; Smith, MC",THEORETICAL COMPUTER SCIENCE,2001.0,"This paper describes the application of temporal difference (TD) learning to minimax searches in general, and presents results from shogi. TD learning is used to adjust the weights for evaluation features over the course of a series of games, starting from arbitrary initial values. For some games, to obtain weights accurate enough for high-performance play will require the TD learning phase to make use of minimax searches. A theoretical description of TD applied to minimax search is given, and we discuss how the theoretical characteristics of the method interact with practical considerations. These include the depth of search appropriate for successful learning and the use of self-play to enable the algorithm to be independent of human knowledge. We then report on experiments that obtained values for use in shogi-playing programs. Unlike chess, shogi has no generally agreed standardized set of values for pieces, so there is more need for machine learning. We compare our machine-learnt values, obtained without any human knowledge input, with hand-crafted values. TD learning was successful in obtaining values that performed well in matches against hand-crafted values. (C) 2001 Elsevier Science B.V. All rights reserved.",10.1016/S0304-3975(00)00078-5,learning; temporal difference; minimax; search; shogi,,
Machine learning to discover mineral trapping signatures due to CO2 injection,"Ahmmed, B; Karra, S; Vesselinov, VV; Mudunuru, MK",INTERNATIONAL JOURNAL OF GREENHOUSE GAS CONTROL,2021.0,"Mineral trapping is pursued as a geological CO2 sequestration (GCS) mechanism because it permanently stores CO2 in solid phases or minerals. However, CO2 mineral-trapping mechanisms are poorly understood due to (1) lack of sufficient field and laboratory data characterizing these complex processes, and (2) challenges to develop site-specific reactive-transport models coupling fluid flow and geochemical reactions occurring at various temporal (from milliseconds to years) and spatial (from pore (millimeters) to field (kilometers)) scales. Reactive transport with additional complexities such as heterogeneity can make the simulation outputs even more difficult to interpret because of complex nonlinearity and multi-scale interdependencies. Furthermore, the values of model outputs such as concentrations can vary by several orders of magnitude, making it harder to correlate and characterize the impact of the variables via traditional data interpretation techniques such as exploratory data analyses. Recently, machine learning (ML) has shown promise in feature discovery and in highlighting hidden mechanisms that cannot be obtained by existing data-analytics and statistical methods. In this study, we applied an unsupervised ML approach, non-negative matrix factorization with custom k-means clustering (NMFk) to the data generated by reactive-transport simulations of GCS. The reactive-transport data consisted of 19 attributes, including four physio-chemical variables (pH, porosity, aqueous CO2, and sequestered CO2), six chemical species (K+, Na+, HCO-3 , Ca2+, Mg2+, Fe2+), and four carbonate minerals (calcite, dolomite, siderite, and ankerite), a feldspar mineral (albite), and four clay minerals (illite, clinochlore, kaolinite, and smectite) over a period of 200 years of simulation time. The simulation adata used was for Morrow B sandstone at the Farnsworth hydrocarbon unit in Texas. Data are sampled at two locations within the model domain: (1) at the injection well and (2) 200 m west of the injection well. The injection was performed for a period of 10 years. Using NMFk, we estimated the temporal interdependencies among the 19 attributes over a span of 200 years. We found that NMFk was able to identify four reaction stages and their dominant attributes; these cannot be directly discerned through traditional visualization (e.g., line plots, Pareto analysis, Glyph-based visualization methods) or exploratory data analysis tools of the simulation data. The four stages were: reactions in the injection phase followed by short-, mid-, and long-term reactions. The NMFk analysis also revealed that 10 among the 19 attributes are dominant. These dominant attributes for mineral trapping include calcite, dolomite at injection well, siderite at 200 m away from the injection well, clinochlore, kaolinite, Na+, K+, Ca2+, Mg2+, pH, and aqeuous CO2. Finally, at late times (65-200 years), our results showed that calcite plays a major role in mineral trapping with insignificant contribution from siderite, ankerite, and clay minerals. These findings make the proposed unsupervised MLmodel attractive for reactive-transport sensing towards real-time GCS monitoring.",10.1016/j.ijggc.2021.103382,Unsupervised machine learning; Reactive-transport simulation; CO2 sequestration; Hidden features; Matrix factorization,,
Integrated and intelligent design framework for cemented paste backfill: A combination of robust machine learning modelling and multi-objective optimization,"Qi, CC; Chen, QS; Kim, SS",MINERALS ENGINEERING,2020.0,"Modern mining industry thrives for energy-efficient, clean and sustainable mining processes. The cemented paste backfill (CPB) technology, which may constitute 25-30% of the total mining cost, is no exception. One of the major bottlenecks for the current CPB design is that different steps were considered separately. No integrated design frameworks have been proposed, hindering the selection of the optimal CPB processing parameters. Towards this end, this study introduces an integrated and intelligent design framework for CPB (IIDF_CPB). The efficiency and accuracy of the proposed IIDF_CPB rely on two important parts. For one thing, robust machine learning (ML) modelling from constituent materials/processing parameters to performance indicators is established. Accurate ML modelling can save lots of time and substantially reduce the number of lab experiments. For another, IIDF_CPB is inherently a multi-objective optimization problem where two or more objectives need to be optimized simultaneously. The methodology of IIDF_CPB is presented and its feasibility is validated using a comprehensive case study. In the case study, ML modelling is conducted using a hybrid method that combines gradient boosting regression tree (GBRT) and particle swarm optimization (PSO). The non-dominated sorting genetic algorithm II (NSGA-II) is employed to maximize two conflicting performance indicators, namely slump and unconfined compressive strength at 28 days (28-UCS). The case study shows that the GBRT-PSO is robust in the slump and 28-UCS predictions. The average correlation coefficient between experimental and predicted outputs is 0.970 for slump and 0.991 for UCS. NSGA-II is effective in the concurrent optimization of slump and 28-UCS, which determines the Pareto front and maintains the diversity of non-dominated points.",10.1016/j.mineng.2020.106422,Cemented paste backfill; Integrated and intelligent design framework; Machine learning modelling; Multi-objective optimization,,
Statistical properties of the method of regularization with periodic Gaussian reproducing kernel,"Lin, Y; Brown, LD",ANNALS OF STATISTICS,2004.0,"The method of regularization with the Gaussian reproducing kernel is popular in the machine learning literature and successful in many practical applications. In this paper we consider the periodic version of the Gaussian kernel regularization. We show in the white noise model setting, that in function spaces of very smooth functions, such as the infinite-order Sobolev space and the space of analytic functions, the method under consideration is asymptotically minimax; in finite-order Sobolev spaces, the method is rate optimal, and the efficiency in terms of constant when compared with the minimax estimator is reasonably high. The smoothing parameters in the periodic Gaussian regularization can be chosen adaptively without loss of asymptotic efficiency. The results derived in this paper give a partial explanation of the success of the Gaussian reproducing kernel in practice. Simulations are carried out to study the finite sample properties of the periodic Gaussian regularization.",10.1214/009053604000000454,asymptotic minimax risk; Gaussian reproducing kernel; nonparametric estimation; rate of convergence; Sobolev spaces; white noise model,,
Intelligent System for Planning Group Actions of Unmanned Aircraft in Observing Mobile Objects on the Ground in the Specified Area,"Goncharenko, VI; Zheltov, SY; Knyaz, VA; Lebedev, GN; Mikhaylin, DA; Tsareva, OY",JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL,2021.0,"The multicriteria task of preflight and operational planning of group actions of unmanned aerial vehicles (UAVs), taking into account the required service schedule, is considered. A minimax criterion for the operational planning of group actions when the dynamic situation changes is proposed. The shape of the expert system for controlling the duration of observation during the search and detection of ground objects is formed. The obtained results of assessing the quality of the solution to the subproblem of neural network recognition of mobile objects based on deep learning confirm the effectiveness of the proposed approach in monitoring the controlled area.",10.1134/S1064230721030047,,,
A Protocol for the Diagnosis of Autism Spectrum Disorder Structured in Machine Learning and Verbal Decision Analysis,"Andrade, E; Portela, S; Pinheiro, PR; Nunes, LC; Simao, M; Costa, WS; Pinheiro, MCD",COMPUTATIONAL AND MATHEMATICAL METHODS IN MEDICINE,2021.0,"Autism Spectrum Disorder is a mental disorder that afflicts millions of people worldwide. It is estimated that one in 160 children has traces of autism, with five times the higher prevalence in boys. The protocols for detecting symptoms are diverse. However, the following are among the most used: the Diagnostic and Statistical Manual of Mental Disorders, 5th Edition (DSM-5), of the American Psychiatric Association; the Revised Autistic Diagnostic Observation Schedule (ADOS-R); the Autistic Diagnostic Interview (ADI); and the International Classification of Diseases, 10th edition (ICD-10), published by the World Health Organization (WHO) and adopted in Brazil by the Unified Health System (SUS). The application of machine learning models helps make the diagnostic process of Autism Spectrum Disorder more precise, reducing, in many cases, the number of criteria necessary for evaluation, denoting a form of attribute engineering (feature engineering) efficiency. This work proposes a hybrid approach based on machine learning algorithms' composition to discover knowledge and concepts associated with the multicriteria method of decision support based on Verbal Decision Analysis to refine the results. Therefore, the study has the general objective of evaluating how the mentioned hybrid methodology proposal can make the protocol derived from ICD-10 more efficient, providing agility to diagnosing Autism Spectrum Disorder by observing a minor symptom. The study database covers thousands of cases of people who, once diagnosed, obtained government assistance in Brazil.",10.1155/2021/1628959,,,
Machine learning modelling for predicting non-domestic buildings energy performance: A model to support deep energy retrofit decision-making,"Seyedzadeh, S; Rahimian, FP; Oliver, S; Rodriguez, S; Glesk, I",APPLIED ENERGY,2020.0,"Non-domestic buildings contribute 20% of the UK's annual carbon emissions. A contribution exacerbated by its ageing stock of which only 7% is considered new-build. Consequently, the government has set regulations to decrease the amount of energy take-up by buildings which currently favour deep energy retrofitting analysis for decision-making and demonstrating compliance. Due to the size and complexity of non-domestic buildings, identifying optimal retrofit packages can be very challenging. The need for effective decision-making has led to the wide adoption of artificial intelligence in the retrofit strategy design process. However, the vast retrofit solution space and high time-complexity of energy simulations inhibit artificial intelligence's application. This paper presents an energy performance prediction model for non-domestic buildings supported by machine learning. The aim of the model is to provide a rapid energy performance estimation engine for assisting multiobjective optimisation of non-domestic buildings energy retrofit planning. The study lays out the process of model development from the investigation of requirements and feature extraction to the application on a case study. It employs sensitivity analysis methods to evaluate the effectiveness of the feature set in covering retrofit technologies. The machine learning model which is optimised using advanced evolutionary algorithms provide a robust and reliable tool for building analysts enabling them to meaningfully explore the expanding solution space. The model is evaluated by assessing three thousand retrofit variations of a case study building, achieving a root mean square error of 1.02 kgCO(2)/m(2) x year equal to 1.7% of error.",10.1016/j.apenergy.2020.115908,Building energy performance; Data-driven model; Energy performance certificate; Machine learning; Non-domestic building emission rate,,
Three-objective genetics-based machine learning for linguistic rule extraction,"Ishibuchi, H; Nakashima, T; Murata, T",INFORMATION SCIENCES,2001.0,"This paper shows how a small number of linguistically interpretable fuzzy rules can be extracted from numerical data for high-dimensional pattern classification problems. One difficulty in the handling of high-dimensional problems by fuzzy rule-based systems is the exponential increase in the number of fuzzy rules with the number of input variables. Another difficulty is the deterioration in the comprehensibility of fuzzy rules when they involve many antecedent conditions. Our task is to design comprehensible fuzzy rule-based systems with high classification ability. This task is formulated as a combinatorial optimization problem with three objectives: to maximize the number of correctly classified training patterns, to minimize the number of fuzzy rules, and to minimize the total number of antecedent conditions. We show two genetic-algorithm-based approaches. One is rule selection where a small number of linguistically interpretable fuzzy rules are selected from a large number of prespecified candidate rules. The other is fuzzy genetics-based machine learning where rule sets are evolved by genetic operations. These two approaches search for non-dominated rule sets with respect to the three objectives. (C) 2001 Elsevier Science Inc. All rights reserved.",10.1016/S0020-0255(01)00144-X,pattern classification; fuzzy systems; genetic algorithms; rule extraction,,
"Analyzing, controlling, and optimizing Damavand power plant operating parameters using a synchronous parallel shuffling self-organized Pareto strategy and neural network: a survey","Mozaffari, A; Gorji-Bandpy, M; Samadian, P; Noudeh, SM",PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART A-JOURNAL OF POWER AND ENERGY,2012.0,"In recent decades, analyzing and optimizing thermal systems have become of great interest to researchers. Recently, the engineers concentrated on variant concepts of artificial intelligence such as machine learning, simulation, fuzzy logic, game theory, and evolutionary computing to deal with complicated barriers and obstacles. Artificial intelligence and expert system techniques play an important role for surveying and controlling mechanical systems such as power plants and reservoirs. This is because of their interdisciplinary applications and versatile servicing potential in mathematical modeling of industrial systems. In this article, a new method called synchronous parallel shuffling self-organized Pareto strategy algorithm is presented which synthesizes different artificial techniques, nominally evolutionary computing, swarm intelligence techniques, and time adaptive self-organizing map that apply simultaneously incorporating with a stochastic data sharing behavior. Thereafter, it is applied to verify the optimum operating parameter of Damavand power plant as the biggest constructed power plant in Middle East with the potential of producing about 2300MW electricity sited in Tehran, capital of Iran, as a multi-objective, multi-modal complex problem. It is also proved that implementing the governing equations of power plant leads to a multi-objective problem where some of these objectives are non-linear, non-convex, and multi-modal with different type of real-life engineering constraints. The results confirm the acceptable performance of proposed technique in optimizing the operating parameters of Damavand power plant.",10.1177/0957650912454822,Damavand power plant; multi-objective optimizing; artificial neural network; exergetic and exergoeconomic analyses,,
Understanding the Effect of Hyperparameter Optimization on Machine Learning Models for Structure Design Problems,"Du, XP; Xu, HY; Zhu, F",COMPUTER-AIDED DESIGN,2021.0,"To relieve the computational cost of design evaluations using expensive finite element (FE) simulations, surrogate models have been widely applied in computer-aided engineering design. Machine learning algorithms (MLAs) have been implemented as surrogate models due to their capability of learning the complex interrelations between the design variables and the response from big datasets. Typically, an MLA regression model contains model parameters and hyperparameters. The model parameters are obtained by fitting the training data. Hyperparameters, which govern the model structures and the training processes, are assigned by users before training. There is a lack of systematic studies on the effect of hyperparameters on the accuracy and robustness of the surrogate model. In this work, we proposed to establish a hyperparameter optimization framework to deepen our understanding of the effect. Based on the sequential model-based optimization method, the Pareto front is generated by running the optimal acquisition and updating the surrogate model iteratively. The optimum acquisition works by repeating a design space shrinking process. Using the acquired optimum, the surrogate model is updated, which describes the relationship between the hyperparameter combinations (inputs) generated by Latin hypercube sampling from the design space and structural response (outputs) to evaluate the modeling accuracy. The updated model will then be used for the next iteration of optimal acquisition until the termination criterion is met. Four frequently used MLAs, namely Gaussian Process Regression (GPR), Support Vector Machine (SVM), Random Forest Regression (RFR), and Artificial Neural Network (ANN), are tested on four benchmark examples of structure design optimization. For each MLA model, the model accuracy and robustness before and after the hyperparameters optimization (HOpt) are compared. The results show that HOpt can generally improve the performance of the MLA models in general with dependency on model complexity. HOpt leads to unstable improvements in the MLAs accuracy and robustness for complex problems, which are featured by high-dimensional mixed variable design space. We also investigated the additional computational costs incurred by HOpt. The training cost is closely related to the MLA architecture. After HOpt, the training cost of ANN and RFR is increased more than that of the GPR and SVM. In summary, this study benefits the selection of HOpt method for different types of design problems based on their complexity (i.e. design domain continuity and the number of design variables, etc.). (C) 2021 Elsevier Ltd. All rights reserved.",10.1016/j.cad.2021.103013,Structure design; Surrogate models; Machine learning; Hyperparameters optimization,,
Investigation of biocompatible implant material through WEDM process using RSM modeling hybrid with the machine learning algorithm,"Kumar, A; Sharma, R; Gupta, AK; Gujral, R",SADHANA-ACADEMY PROCEEDINGS IN ENGINEERING SCIENCES,2021.0,"CP-Ti-G2 (Commercially pure titanium grade-2) has become the preferred biocompatible material for various devices mainly used in orthopedic and dental implants and it is also used in aviation and aircraft. While CP-Ti deals with good ductility, higher stiffness, and fatigue resistance. The novelty of present research work was to create a rough surface on CP-Ti-G2 through the WEDM process. Further, this rough surface was used in the development of bone marrow cells on it. Propagation and diversity of bone marrow cells were applied in dental implant osteointegration applications. Six WEDM factors were analyzed through the BBD design of the experiment. 54 trial experiments were conducted to observe the MRR and SR output responses. After machining, surface topography was examined through SEM and EDX. ANOVA was applied to analyze the significance of factors. It was observed that POT (pulse on time), POFT (pulse off time), PC (peak current), and SGV (spark gap voltage) are the most significant factors. The WEDM factors have also been significantly deteriorating the microstructure of machined samples remarkably deeper, wider craters, globules of debris, and micro cracks. A multi-objective optimization 'desirability' function was applied to obtain the optimal solutions by numerical and supervised machine learning algorithms. They lead to the reflection of parametric machine learning algorithms to surmise about the effectiveness of WEDM process. The results show a good agreement between actual and predicted values.",10.1007/s12046-021-01676-3,WEDM; CP-Ti-G2; biocompatibility; MRR; SR; SEM; bone marrow cells; desirability function; machine learning,,
Machine learning for orders of magnitude speedup in multiobjective optimization of particle accelerator systems,"Edelen, A; Neveu, N; Frey, M; Huber, Y; Mayes, C; Adelmann, A",PHYSICAL REVIEW ACCELERATORS AND BEAMS,2020.0,"High-fidelity physics simulations are powerful tools in the design and optimization of charged particle accelerators. However, the computational burden of these simulations often limits their use in practice for design optimization and experiment planning. It also precludes their use as on-line models tied directly to accelerator operation. We introduce an approach based on machine learning to create nonlinear, fast-executing surrogate models that are informed by a sparse sampling of the physics simulation. The models are O(10(6))-O(10(7)) times more computationally efficient to execute. We also demonstrate that these models can be reliably used with multiobjective optimization to obtain orders-of-magnitude speedup in initial design studies and experiment planning. For example, we required 132 times fewer simulation evaluations to obtain an equivalent solution for our main test case, and initial studies suggest that between 330-550 times fewer simulation evaluations are needed when using an iterative retraining process. Our approach enables new ways for high-fidelity particle accelerator simulations to be used, at comparatively little computational cost.",10.1103/PhysRevAccelBeams.23.044601,,,
A Comparison of three evolutionary strategies for multiobjective genetic programming,"Zhang, Y; Rockett, P",ARTIFICIAL INTELLIGENCE REVIEW,2007.0,"We report what we believe to be the first comparative study of multi-objective genetic programming (GP) algorithms on benchmark symbolic regression and machine learning problems. We compare the Strength Pareto Evolutionary Algorithm (SPEA2), the Non-dominated Sorting Genetic Algorithm (NSGA-II) and the Pareto Converging Genetic Algorithm (PCGA) evolutionary paradigms. As well as comparing the quality of the final solutions, we also examine the speed of convergence of the three evolutionary algorithms. Based on our observations, the SPEA2-based algorithm appears to have problems controlling tree bloat-that is, the uncontrolled growth in the size of the chromosomal tree structures. The NSGA-II-based algorithm on the other hand seems to experience difficulties in locating low error solutions. Overall, the PCGA-based algorithm gives solutions with the lowest errors and the lowest mean complexity.",10.1007/s10462-008-9093-2,Genetic programming; Multiobjective optimisation; Symbolic regression; Machine learning,,
Pareto Optimal Swimmers,"Verma, S; Hadjidoukas, P; Wirth, P; Rossinelli, D; Koumoutsakos, P",PROCEEDINGS OF THE PLATFORM FOR ADVANCED SCIENTIFIC COMPUTING CONFERENCE (PASC17),2017.0,"A fundamental understanding of how various biological traits and features provide organisms with a competitive advantage can help us improve the design of a number of mechanical systems. Numerical optimization can play an invaluable role for this purpose, by allowing us to scrutinize the evolution of specific biological adaptations in nature. Importantly, the use of numerical optimization can help us overcome limiting constraints that restrict the evolutionary capability of biological species. We capitalize on these advantages by coupling high-fidelity simulations of self-propelled swimmers with evolutionary optimization algorithms, to examine peculiar swimming patterns observed in a number of fish species. More specifically, we investigate the intermittent form of locomotion referred to as 'burst-and-coast' swimming, which involves a few quick flicks of the fish's tail followed by a prolonged unpowered glide. This mode of swimming is believed to confer energetic benefits, in addition to several other advantages. We discover a range of intermittent-swimming patterns, the most efficient of which resembles the swimming behaviour observed in live fish. We also discover patterns which lead to a marked increase in swimming-speed, albeit with a significant increase in energy expenditure. Notably, the use of multi-objective optimization reveals locomotion patterns that strike the perfect balance between speed and efficiency, which can be invaluable for use in robotic applications. As an additional goal of the paper, we highlight the ease with which disparate codes can be coupled via the software framework used, without encumbering the user with the details of efficient parallelization and machine-learning based task-scheduling.",10.1145/3093172.3093232,Task-based Parallelism; Multi-objective Optimization; Burst-and-coast Swimming; Energy-efficient Locomotion,,
The MBPEP: a deep ensemble pruning algorithm providing high quality uncertainty prediction,"Hu, RH; Huang, QJ; Chang, S; Wang, H; He, J",APPLIED INTELLIGENCE,2019.0,"Machine learning algorithms have been effectively applied into various real world tasks. However, it is difficult to provide high-quality machine learning solutions to accommodate an unknown distribution of input datasets; this difficulty is called the uncertainty prediction problems. In this paper, a margin-based Pareto deep ensemble pruning (MBPEP) model is proposed. It achieves the high-quality uncertainty estimation with a small value of the prediction interval width (MPIW) and a high confidence of prediction interval coverage probability (PICP) by using deep ensemble networks. In addition to these networks, unique loss functions are proposed, and these functions make the sub-learners available for standard gradient descent learning. Furthermore, the margin criterion fine-tuning-based Pareto pruning method is introduced to optimize the ensembles. Several experiments including predicting uncertainties of classification and regression are conducted to analyze the performance of MBPEP. The experimental results show that MBPEP achieves a small interval width and a low learning error with an optimal number of ensembles. For the real-world problems, MBPEP performs well on input datasets with unknown distributions datasets incomings and improves learning performance on a multi task problem when compared to that of each single model.",10.1007/s10489-019-01421-8,Uncertainty prediction; Ensemble pruning; Loss function; Margin criterion tuning,,
Dual-convolutional neural network based aerodynamic prediction and multi-objective optimization of a compact turbine rotor,"Wang, YQ; Liu, TY; Zhang, D; Xie, YH",AEROSPACE SCIENCE AND TECHNOLOGY,2021.0,"With the development of neural network technology, surrogate models and dimensionality reduction strategies based on machine learning have become the research hotspots of aerodynamic shape optimization recently. In order to further improve the accuracy and interpretability of the traditional surrogate models, this research establishes a deep learning model, named Dual Convolutional Neural Network (Dual-CNN) for the aero-engine turbines. The aerodynamic performances are predicted and the pressure, temperature fields are reconstructed for multiple rotor profile conditions. The prediction of efficiency is compared with the accuracy of Gaussian Process Regression (GPR) and Artificial Neural Network (ANN) models. The results show that the proposed Dual-CNN model can accurately reconstruct the fields, thus interpreting the mechanism for the change of aerodynamic performance. Dual-CNN is more accurate than GPR and ANN in predicting efficiency and torque, whose error is within an acceptable range of optimization. Then, efficiency and torque are selected as the objective functions to perform a gradient-based multi-objective optimization by the automatic differentiation method and a Pareto solution is obtained. The trained Dual-CNN provides rapid and accurate prediction of performance without CFD calculation in the optimization. Finally, the sensitivity to train size is analyzed for the Dual CNN model, which indicates that the sampling of 1500 cases for eight design variables in this dataset enables Dual-CNN to achieve favorable effect of field reconstruction and performance prediction. (C) 2021 Elsevier Masson SAS. All rights reserved.",10.1016/j.ast.2021.106869,Aerodynamic prediction; Multi-objective optimization; Turbine; Deep learning; Convolution neural network,,
"Simulation, Optimization, and Machine Learning in Sustainable Transportation Systems: Models and Applications","de la Torre, R; Corlu, CG; Faulin, J; Onggo, BS; Juan, AA",SUSTAINABILITY,2021.0,"The need for effective freight and human transportation systems has consistently increased during the last decades, mainly due to factors such as globalization, e-commerce activities, and mobility requirements. Traditionally, transportation systems have been designed with the main goal of reducing their monetary cost while offering a specified quality of service. During the last decade, however, sustainability concepts are also being considered as a critical component of transportation systems, i.e., the environmental and social impact of transportation activities have to be taken into account when managers and policy makers design and operate modern transportation systems, whether these refer to long-distance carriers or to metropolitan areas. This paper reviews the existing work on different scientific methodologies that are being used to promote Sustainable Transportation Systems (STS), including simulation, optimization, machine learning, and fuzzy sets. This paper discusses how each of these methodologies have been employed to design and efficiently operate STS. In addition, the paper also provides a classification of common challenges, best practices, future trends, and open research lines that might be useful for both researchers and practitioners.",10.3390/su13031551,transportation systems; sustainability; simulation; optimization; machine learning,,
Self-Driving Platform for Metal Nanoparticle Synthesis: Combining Microfluidics and Machine Learning,"Tao, HC; Wu, TY; Kheiri, S; Aldeghi, M; Aspuru-Guzik, A; Kumacheva, E",ADVANCED FUNCTIONAL MATERIALS,2021.0,"Many applications of inorganic nanoparticles (NPs), including photocatalysis, photovoltaics, chemical and biochemical sensing, and theranostics, are governed by NP optical properties. Exploration and identification of reaction conditions for the synthesis of NPs with targeted spectroscopic characteristics is a time-, labor-, and resource-intensive task, as it involves the optimization of multiple interdependent reaction conditions. Integration of machine learning (ML) and microfluidics (MF) offers accelerated identification and optimization of reaction conditions for NP synthesis. Here, an autonomous ML-driven, oscillatory MF platform for the synthesis of NPs is reported. The platform utilized multiple recipes and reaction times for the synthesis of NPs with different dimensions, conducted spectroscopic NP characterization, and employed ML approaches to analyze multiple yet prioritized spectroscopic NP characteristics, and identified reaction conditions for the synthesis of NPs with targeted optical properties. The platform is also used to develop an understanding of the relationship between reaction conditions and NP properties. This study shows the strong potential of ML-driven oscillatory MF platforms in materials science and paves the way for automated NP development.",10.1002/adfm.202106725,machine learning; microfluidics; nanoparticles,,
PSGAN: A Minimax Game for Personalized Search with Limited and Noisy Click Data,"Lu, SQ; Dou, ZC; Xu, J; Nie, JY; Wen, JR",PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19),2019.0,"Personalized search aims to adapt document ranking to user's personal interests. Traditionally, this is done by extracting click and topical features from historical data in order to construct a user profile. In recent years, deep learning has been successfully used in personalized search due to its ability of automatic feature learning. However, the small amount of noisy personal data poses challenges to deep learning models to learn the personalized classification boundary between relevant and irrelevant results. In this paper, we propose PSGAN, a Generative Adversarial Network (GAN) framework for personalized search. By means of adversarial training, we enforce the model to pay more attention to training data that are difficult to distinguish. We use the discriminator to evaluate personalized relevance of documents and use the generator to learn the distribution of relevant documents. Two alternative ways to construct the generator in the framework are tested: based on the current query or based on a set of generated queries. Experiments on data from a commercial search engine show that our models can yield significant improvements over state-of-the-art models.",10.1145/3331184.3331218,personalized web search; generative adversarial network,,
Application of interpretable machine learning models for the intelligent decision,"Li, YW; Yang, L; Yang, BH; Wang, N; Wu, T",NEUROCOMPUTING,2019.0,"In this study, an interpretable machine learning algorithm is proposed for the issues of intelligent decision through predicting the firms' efficiency of innovation. Based on the unbalanced panel data collected in Zhongguancun Science Parks from year 2005 to 2015, the efficiency of over 10,000 firms have been analysed in this study, and the change and growth of these firms have been captured over time. The linear regression, decision tree, random forests, neural network and XGBoost models are applied to figure out the impact factors of innovation. After comparing the results of different models, it has been found that the accuracy of XGBoost for R&D efficiency labelled, commercial efficiency labelled and overall efficiency labelled classification problems are 73.65%, 70.02% and 70.09%, which outperform the other four models. Moreover, the interpretability of XGBoost is also better than other models. Thus, the XGBoost model makes it possible for managers to predict the firm's future innovation performance derived from their innovation strategies in the current stage. Furthermore, it helps firms to build an intelligent decision support system, which is of great importance for them to deal with complex decision environments, and to increase their efficiency of innovation in the long-term dynamic competition with other firms. (C) 2018 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2018.12.012,Machine learning; XGBoost; R&D investments; Firm size; Innovation performance,,
Ultra-Short-Term Building Cooling Load Prediction Model Based on Feature Set Construction and Ensemble Machine Learning,"Ding, Y; Su, H; Kong, XF; Zhang, ZQ",IEEE ACCESS,2020.0,"As the requirements for the optimal control of building systems increase, the accuracy and speed of load predictions should also increase. However, the accuracy of load predictions is related to not only the prediction algorithm, but also the feature set construction. Therefore, this study develops a short-term building cooling load prediction model based on feature set construction. The impacts of four different feature set construction methods-feature extraction, correlation analysis, K-means clustering, and discrete wavelet transform (DWT)-on the prediction accuracy are compared. To ensure that the effect of the feature set construction method is universal, three different prediction algorithms are used. The influences of the sample dimension and prediction time horizon on the prediction accuracy are also analysed. The prediction model is developed based on an ensemble learning algorithm utilising the cubist algorithm, and the performance of the prediction model is improved when DWT is used for constructing the feature set. Compared with other commonly used prediction models, the proposed model exhibits the best performance, with R-squared and CV-RMSE values of 99.8% and 1.5%, respectively.",10.1109/ACCESS.2020.3027061,Load modeling; Predictive models; Buildings; Feature extraction; Prediction algorithms; Computational modeling; Cooling; Cooling load prediction; feature extraction; ensemble learning algorithms; discrete wavelet transform,,
Machine Learning Based Antenna Design for Physical Layer Security in Ambient Backscatter Communications,"Hong, T; Liu, C; Kadoch, M",WIRELESS COMMUNICATIONS & MOBILE COMPUTING,2019.0,"Ambient backscatter employs existing radio frequency (RF) signals in the environment to support sustainable and independent communications, thereby providing a new set of applications that promote the Internet of Things (IoT). However, nondirectional forms of communication are prone to information leakage. In order to ensure the security of the IoT communication system, in this paper, we propose a machine learning based antenna design scheme, which achieves directional communication from the relay tag to the receiving reader by combining patch antenna with log-periodic dual-dipole antenna (LPDA). A multiobjective genetic algorithm optimizes the antenna side lobe, gain, standing wave ratio, and return loss, with a goal of limiting the number of large side lobes and reduce the side lobe level (SLL). The simulation results demonstrate that our proposed antenna design is well suited for practical applications in physical layer security communication, where signal-to-noise ratio of the wiretap channel is reduced, communication quality of the main channel is ensured, and information leakage is prevented.",10.1155/2019/4870656,,,
Optimal rates for the regularized least-squares algorithm,"Caponnetto, A; De Vito, E",FOUNDATIONS OF COMPUTATIONAL MATHEMATICS,2007.0,"We develop a theoretical analysis of the performance of the regularized least-square algorithm on a reproducing kernel Hilbert space in the supervised learning setting. The presented results hold in the general framework of vector-valued functions; therefore they can be applied to multitask problems. In particular, we observe that the concept of effective dimension plays a central role in the definition of a criterion for the choice of the regularization parameter as a function of the number of samples. Moreover, a complete minimax analysis of the problem is described, showing that the convergence rates obtained by regularized least-squares estimators are indeed optimal over a suitable class of priors defined by the considered kernel. Finally, we give an improved lower rate result describing worst asymptotic behavior on individual probability measures rather than over classes of priors.",10.1007/s10208-006-0196-8,learning theory; model selection; optimal rates; least squares,,
Deep Demosaicing for Edge Implementation,"Ramakrishnan, R; Jui, SL; Nia, VP","IMAGE ANALYSIS AND RECOGNITION, ICIAR 2019, PT I",2019.0,"Most digital cameras use sensors coated with a Color Filter Array (CFA) to capture channel components at every pixel location, resulting in a mosaic image that does not contain pixel values in all channels. Current research on reconstructing these missing channels, also known as demosaicing, introduces many artifacts, such as zipper effect and false color. Many deep learning demosaicing techniques outperform other classical techniques in reducing the impact of artifacts. However, most of these models tend to be over-parametrized. Consequently, edge implementation of the state-of-the-art deep learning-based demosaicing algorithms on low-end edge devices is a major challenge. We provide an exhaustive search of deep neural network architectures and obtain a Pareto front of Color Peak Signal to Noise Ratio (CPSNR) as the performance criterion versus the number of parameters as the model complexity that outperforms the state-of-the-art. Architectures on the Pareto front can then be used to choose the best architecture for a variety of resource constraints. Simple architecture search methods such as exhaustive search and grid search requires some conditions of the loss function to converge to the optimum. We clarify these conditions in a brief theoretical study.",10.1007/978-3-030-27202-9_25,Deep learning; Demosaicing; Edge computing; Network architecture search,,
Double decomposition and optimal combination ensemble learning approach for interval-valued AQI forecasting using streaming data,"Wang, ZC; Chen, LR; Zhu, JM; Chen, HY; Yuan, HJ",ENVIRONMENTAL SCIENCE AND POLLUTION RESEARCH,2020.0,"To forecast possible future environmental risks, numerous models are developed to predict the hourly values or daily averages of air pollutant concentrations using streaming data (a kind of big data collected from the Internet). On the one hand, real-time hourly data is massive and redundant, making it difficult to process. On the other hand, daily averages cannot reflect the fluctuations of air pollutant concentrations throughout the day. Therefore, a double decomposition and optimal combination ensemble learning approach is proposed for interval-valued AQI (air quality index) forecasting in this paper. In the first decomposition, considering the strong seasonal representation of AQI, the original data of each year is decomposed into four seasonal subseries on the basis of the Chinese calendar. Subsequently, we reconstruct the data of the same season in different years to get a new seasonal series to reduce the interference of seasonal changes on AQI forecasting. In the second decomposition, due to the nonlinearity and irregularity of interval-valued AQI time series, BEMD (bivariate empirical mode decomposition) is employed to decompose the interval-valued signals into a finite number of complex-valued IMF (intrinsic mode function) components and one complex-valued residue component with different frequencies to reduce the complexity of interval times series. Interval multilayer perceptron (iMLP) is utilized to model the lower bound and the upper bound simultaneously of the total components to obtain the corresponding forecasting results, which are merged to produce the final interval-valued output by an optimal combination ensemble method. Empirical study results show that the proposed model with different datasets and different forecasting horizons is significantly better than other considered models for its superior forecasting performances.",10.1007/s11356-020-09891-x,Air quality index; Interval forecasting; Bivariate empirical mode decomposition; Optimal combination ensemble; Seasonality,,
Generalized higher-level automated innovization with application to inventory management,"Bandaru, S; Aslam, T; Ng, AHC; Deb, K",EUROPEAN JOURNAL OF OPERATIONAL RESEARCH,2015.0,This paper generalizes the automated innovization framework using genetic programming in the context of higher-level innovization. Automated innovization is an unsupervised machine learning technique that can automatically extract significant mathematical relationships from Pareto-optimal solution sets. These resulting relationships describe the conditions for Pareto-optimality for the multi-objective problem under consideration and can be used by scientists and practitioners as thumb rules to understand the problem better and to innovate new problem solving techniques; hence the name innovization (innovation through optimization). Higher-level innovization involves performing automated innovization on multiple Pareto-optimal solution sets obtained by varying one or more problem parameters. The automated innovization framework was recently updated using genetic programming. We extend this generalization to perform higher-level automated innovization and demonstrate the methodology on a standard two-bar bi-objective truss design problem. The procedure is then applied to a classic case of inventory management with multi-objective optimization performed at both system and process levels. The applicability of automated innovization to this area should motivate its use in other avenues of operational research. (C) 2014 Elsevier B.V. All rights reserved.,10.1016/j.ejor.2014.11.015,Automated innovization; Higher-level innovization; Genetic programming; Inventory management; Knowledge discovery,,
Convergence analysis of sliding mode trajectories in multi-objective neural networks learning,"Costa, MA; Braga, AP; de Menezes, BR",NEURAL NETWORKS,2012.0,"The Pareto-optimality concept is used in this paper in order to represent a constrained set of solutions that are able to trade-off the two main objective functions involved in neural networks supervised learning: data-set error and network complexity. The neural network is described as a dynamic system having error and complexity as its state variables and learning is presented as a process of controlling a learning trajectory in the resulting state space. In order to control the trajectories, sliding mode dynamics is imposed to the network. It is shown that arbitrary learning trajectories can be achieved by maintaining the sliding mode gains within their convergence intervals. Formal proofs of convergence conditions are therefore presented. The concept of trajectory learning presented in this paper goes further beyond the selection of a final state in the Pareto set, since it can be reached through different trajectories and states in the trajectory can be assessed individually against an additional objective function. (c) 2012 Elsevier Ltd. All rights reserved.",10.1016/j.neunet.2012.04.006,Neural networks; Multi-objective learning; Sliding mode,,
Leveraging Indicator-based Ensemble Selection in Evolutionary Multiobjective Optimization Algorithms,"Phan, DH; Suzuki, J; Hayashi, I",PROCEEDINGS OF THE FOURTEENTH INTERNATIONAL CONFERENCE ON GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE,2012.0,"Various evolutionary multiobjective optimization algorithms (EMOAs) have replaced or augmented the notion of dominance with quality indicators and leveraged them in selection operators. Recent studies show that indicator-based EMOAs outperform traditional dominance-based EMOAs. This paper proposes and evaluates an ensemble learning method that constructs an ensemble of existing indicators with a novel boosting algorithm called Pdi-Boosting. The proposed method is carried out with a training problem in which Pareto-optimal solutions are known. It can work with a simple training problem, and an ensemble of indicators can effectively aid parent selection and environmental selection in order to solve harder problems. Experimental results show that the proposed method is efficient thanks to its dynamic adjustment of training data. An ensemble of indicators outperforms existing individual indicators in optimality, diversity and robustness. The proposed ensemble-based evolutionary algorithm outperforms a well-known dominance-based EMOA and existing indicator-based EMOAs.",10.1145/2330163.2330234,Evolutionary multiobjective optimization algorithms; Quality indicators; Indicator-based ensemble selection; Boosting,,
Adaptive Use of Innovization Principles for a Faster Convergence of Evolutionary Multi-Objective Optimization Algorithms,"Gaur, A; Deb, K",PROCEEDINGS OF THE 2016 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE (GECCO'16 COMPANION),2016.0,"Innovization is a task of learning common principles that exist among some or all of the Pareto-optimal solutions of a multi-objective optimization problem. Except a few earlier studies, most innovization related studies were performed on the final non-dominated solutions found by an EMO algorithm. Since the innovization principles are properties of good and near-optimal solutions, an early identification of them can help improve the evolving population to converge quicker to the Pareto-optimal set. This paper advocates the discovery of innovized principles through machine learning methods during an evolutionary multi-objective optimization run and then using these principles to repair the population adaptively to achieve a faster convergence. Implementing this idea with linear regression as the learning tool and applying it in a test problem with power-law rules existing among Pareto-optimal solutions yields encouraging results. The results show not only an improvement in convergence rate but also in the diversity of non-dominated solutions.",10.1145/2908961.2909019,Multi-objective Optimization; Innovization; Convergence,,
Analysis of interpretability-accuracy tradeoff of fuzzy systems by multiobjective fuzzy genetics-based machine learning,"Ishibuchi, H; Nojima, Y",INTERNATIONAL JOURNAL OF APPROXIMATE REASONING,2007.0,"This paper examines the interpretability-accuracy tradeoff in fuzzy rule-based classifiers using a multiobjective fuzzy genetics-based machine learning (GBML) algorithm. Our GBML algorithm is a hybrid version of Michigan and Pittsburgh approaches, which is implemented in the framework of evolutionary multiobjective optimization (EMO). Each fuzzy rule is represented by its antecedent fuzzy sets as an integer string of fixed length. Each fuzzy rule-based classifier, which is a set of fuzzy rules, is represented as a concatenated integer string of variable length. Our GBML algorithm simultaneously maximizes the accuracy of rule sets and minimizes their complexity. The accuracy is measured by the number of correctly classified training patterns while the complexity is measured by the number of fuzzy rules and/or the total number of antecedent conditions of fuzzy rules. We examine the in terpretability-accuracy tradeoff for training patterns through computational experiments on some benchmark data sets. A clear tradeoff structure is visualized for each data set. We also examine the interpretabitity-accuracy tradeoff for test patterns. Due to the overfitting to training patterns, a clear tradeoff structure is not always obtained in computational experiments for test patterns. (C) 2006 Elsevier Inc. All rights reserved.",10.1016/j.ijar.2006.01.004,classification; fuzzy systems; fuzzy data mining; multiobjective optimization; genetic algorithms; genetics-based machine learning,,
A novel two-phase cycle algorithm for effective cyber intrusion detection in edge computing,"Gong, YG; Liu, YP; Yin, CY",EURASIP JOURNAL ON WIRELESS COMMUNICATIONS AND NETWORKING,2021.0,"Edge computing extends traditional cloud services to the edge of the network, closer to users, and is suitable for network services with low latency requirements. With the rise of edge computing, its security issues have also received increasing attention. In this paper, a novel two-phase cycle algorithm is proposed for effective cyber intrusion detection in edge computing based on a multi-objective genetic algorithm (MOGA) and modified back-propagation neural network (MBPNN), namely TPC-MOGA-MBPNN. In the first phase, the MOGA is employed to build a multi-objective optimization model that tries to find the Pareto optimal parameter set for MBPNN. The Pareto optimal parameter set is applied for simultaneous minimization of the average false positive rate (Avg FPR), mean squared error (MSE) and negative average true positive rate (Avg TPR) in the dataset. In the second phase, some MBPNNs are created based on the parameter set obtained by MOGA and are trained to search for a more optimal parameter set locally. The parameter set obtained in the second phase is used as the input of the first phase, and the training process is repeated until the termination criteria are reached. A benchmark dataset, KDD cup 1999, is used to demonstrate and validate the performance of the proposed approach for intrusion detection. The proposed approach can discover a pool of MBPNN-based solutions. Combining these MBPNN solutions can significantly improve detection performance, and a GA is used to find the optimal MBPNN combination. The results show that the proposed approach achieves an accuracy of 98.81% and a detection rate of 98.23% and outperform most systems of previous works found in the literature. In addition, the proposed approach is a generalized classification approach that is applicable to the problem of any field having multiple conflicting objectives.",10.1186/s13638-021-02016-z,Two-phase cycle algorithm; Edge computing; Cyber intrusion detection,,
Machine-learning-accelerated multimodal characterization and multiobjective design optimization of natural porous materials,"Lo Dico, G; Nunez, AP; Carcelen, V; Haranczyk, M",CHEMICAL SCIENCE,2021.0,"Natural porous materials such as nanoporous clays are used as green and low-cost adsorbents and catalysts. The key factors determining their performance in these applications are the pore morphology and surface activity, which are typically represented by properties such as specific surface area, pore volume, micropore content and pH. The latter may be modified and tuned to specific applications through material processing and/or chemical treatment. Characterization of the material, raw or processed, is typically performed experimentally, which can become costly especially in the context of tuning of the properties towards specific application requirements and needing numerous experiments. In this work, we present an application of tree-based machine learning methods trained on experimental datasets to accelerate the characterization of natural porous materials. The resulting models allow reliable prediction of the outcomes of experimental characterization of processed materials (R-2 from 0.78 to 0.99) as well as identification of key factors contributing to those properties through feature importance analysis. Furthermore, the high throughput of the models enables exploration of processing parameter-property correlations and multiobjective optimization of prototype materials towards specific applications. We have applied these methodologies to pinpoint and rationalize optimal processing conditions for clays exploitable in acid catalysis. One of such identified materials was synthesized and tested revealing appreciable acid character improvement with respect to the pristine material. Specifically, it achieved 79% removal of chlorophyll-a in acid catalyzed degradation.",10.1039/d1sc00816a,,,
Optimal design for disc golf by computational fluid dynamics and machine learning,"Immonen, E",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,2022.0,"In this article, we introduce a computational methodology for golf disc shape optimization that employs a novel disc shape parameterization by cubic B-splines. Through application of batch Computational Fluid Dynamics simulations and Machine Learning, the disc parameterization yields functional relationships-so-called shape surrogate models-between the flying rotating disc shape and its flight characteristics. The shape surrogate models facilitate free and constrained optimization in both single- and multiobjective settings, such that both aerodynamic (drag and lift) and structural (mass and moment of inertia) features of the disc are addressed simultaneously. Further, the Professional Disc Golf Association rules for permissible golf discs can be cast as nonlinear constraints for the computational optimization problem. The proposed numerical optimization method yields disc drag coefficient values as low as 0.48 (unconstrained) and 0.52 (constrained) and lift coefficient values as high as 0.26 (unconstrained) and 0.19 (constrained). The presented numerical optimization results also describe the many design tradeoffs between the discs that target long flight range (so-called drivers) and the discs that target flight at low speeds (so-called putters). Moreover, novel optimal rule compliant designs are presented for driver-type and putter-type discs, as well as their compromise, the so-called mid-range discs.",10.1007/s00158-021-03107-7,Computational design; Computational fluid dynamics; Disc golf; Drag coefficient; Lift coefficient; Machine learning; Constrained optimization; Multiobjective optimization; Surrogate model,,
Machine Learning for Melting Temperature Predictions and Design in Polyhydroxyalkanoate-Based Biopolymers,"Bejagam, KK; Lalonde, J; Iverson, CN; Marrone, BL; Pilania, G",JOURNAL OF PHYSICAL CHEMISTRY B,2022.0,"Diminishing fossil fuel-based resources and ever-growing environmental concerns related to plastic pollution demand for the development of sustainable and biodegradable polymeric material alternatives. Polyhydroxyalkanoates (PHAs) represent an eco-friendly and economically viable class of polymers with a wide range of applications. However, the chemical diversity combined with tunable physical properties available within PHAs poses discovery and optimization challenges with respect to identifying optimal application-specific chemical compositions. Here we use an example of melting temperature (T-m) prediction to demonstrate the promise of machine learning (ML)-based techniques for establishing efficient structure-property mappings in PHA-based chemical space. We employ a manually curated data set of experimentally measured T-m values for a wide range of PHA homo- and copolymer chemistries along with their reported polymer molecular weights and polydispersity indices. Descriptors based on topology, shape, and charge/polarity of specific motifs forming the polymer backbone were then used to numerically represent the polymers. The ML models developed by using available data were used to rapidly predict the property of multicomponent PHA-based copolymers, while estimating uncertainties underlying the predictions. Combined with a previously developed glass transition temperature (T-g) prediction model and an evolutionary algorithm-based search strategy, the approach is demonstrated to address polymer design with multiobjective optimization challenges.",10.1021/acs.jpcb.1c08354,,,
"Identification of the most suitable afforestation sites by Juniperus excels specie using machine learning models: Firuzkuh semi-arid region, Iran","Yousefi, S; Avand, M; Yariyan, P; Goujani, HJ; Costache, R; Tavangar, S; Tiefenbacher, JP",ECOLOGICAL INFORMATICS,2021.0,"Choosing Selecting suitable sites for afforestation is a complex process that is influenced by various factors that require the use of new models and methods in order to create better results. The main purpose of this study is to investigate the use of a machine learning framework to map the best sites for afforestation with J. excelsa, an important species for soil and water conservation in Firuzkuh County, Tehran Province, Iran. Existing stands of J. excelsa were located. Measures of 14 environmental variables were compiled at each site. Three machine learning algorithms-Fuzzy ARTMAP (FAM), Multi-layers perceptron (MLP), and Classification tree analysis (CTA) - were used to model ideal locations for growing the tree. They were compared in terms of success rate. The best performance was achieved by CTA (area under curve (AUC) = 0.899). MLP (AUC = 0.892) was second best, and FAM (AUC = 0.835) had the lowest success. All three models achieved very good to excellent results; however, the CTA model was the most effective. Locations of high and very high favorability for J. excelsa comprise between 8% and 18% of the study area. The factors that are most important for the locations of replanting are those with bedrock of the Cl geological group and where rainfall ranges from 350 mm/year and 450 mm/year. This study offers support to decision makers for improving (lower cost and less time) selection of planting sites that are more likely to support tree survival to achieve natural restoration.",10.1016/j.ecoinf.2021.101427,Afforestation; Forest restoration; Tree planting; Site suitability; Firuzkuh region,,
Prediction and optimization of isentropic efficiency of vortex pump under full operating conditions in Organic Rankine Cycle waste heat recovery system based on deep learning and intelligent algorithm,"Ping, X; Yang, FB; Zhang, HG; Zhang, WJ; Song, GG; Yang, YX",SUSTAINABLE ENERGY TECHNOLOGIES AND ASSESSMENTS,2020.0,"The isentropic efficiency of vortex pump in Organic Rankine Cycle (ORC) system has an important influence on the performance of the system. In this paper, vortex pump experimental data and deep learning are combined to construct an experimental data-driven isentropic efficiency prediction model of vortex pump under full operating conditions. The activation function and the number of hidden layer nodes in the model are filtered by nested screening technique. Through bilinear interpolation algorithm, the influence of vortex pump operation parameters on the isentropic efficiency is analyzed. In addition, the optimization boundary is selected in the four-dimensional space. Finally, the deep learning prediction model is combined with Linear Decreasing Inertia Weight Particle Swarm Optimization (LDIWPSO) to predict and optimize the maximum isentropic efficiency of vortex pump under full operating conditions. Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), Root Mean Square Error (RMSE) and Coefficient of Determination (R-square) are combined to evaluate the prediction accuracy of the model. The prediction and optimization results show that the maximum isentropic efficiency of vortex pump can reach 22.89%. The combination of deep learning and LDIWPSO can predict and optimize the maximum isentropic efficiency for vortex pump with high precision. It also provides a reference for the maximum value of vortex pump isentropic efficiency in theoretical analysis and numerical simulation.",10.1016/j.seta.2020.100898,Organic Rankine cycle; Vortex pump; Isentropic efficiency; Deep learning; Particle swarm optimization,,
Generative Adversarial Network for Wireless Signal Spoofing,"Shi, Y; Davaslioglu, K; Sagduyu, YE",PROCEEDINGS OF THE 2019 ACM WORKSHOP ON WIRELESS SECURITY AND MACHINE LEARNING (WISEML '19),2019.0,"The paper presents a novel approach of spoofing wireless signals by using a general adversarial network (GAN) to generate and transmit synthetic signals that cannot be reliably distinguished from intended signals. It is of paramount importance to authenticate wireless signals at the PHY layer before they proceed through the receiver chain. For that purpose, various waveform, channel, and radio hardware features that are inherent to original wireless signals need to be captured. In the meantime, adversaries become sophisticated with the cognitive radio capability to record, analyze, and manipulate signals before spoofing. Building upon deep learning techniques, this paper introduces a spoofing attack by an adversary pair of a transmitter and a receiver that assume the generator and discriminator roles in the GAN and play a minimax game to generate the best spoofing signals that aim to fool the best trained defense mechanism. The output of this approach is two-fold. From the attacker point of view, a deep learning-based spoofing mechanism is trained to potentially fool a defense mechanism such as RF fingerprinting. From the defender point of view, a deep learning-based defense mechanism is trained against potential spoofing attacks when an adversary pair of a transmitter and a receiver cooperates. The probability that the spoofing signal is misclassified as the intended signal is measured for random signal, replay, and GAN-based spoofing attacks. Results show that the GAN-based spoofing attack provides a major increase in the success probability of wireless signal spoofing even when a deep learning classifier is used as the defense.",10.1145/3324921.3329695,adversarial machine learning; deep learning; general adversarial network (GAN); spoofing attack,,
Comparison of machine learning models for gully erosion susceptibility mapping,"Arabameri, A; Chen, W; Loche, M; Zhao, X; Li, Y; Lombardo, L; Cerda, A; Pradhan, B; Bui, DT",GEOSCIENCE FRONTIERS,2020.0,"Gully erosion is a disruptive phenomenon which extensively affects the Iranian territory, especially in the Northern provinces. A number of studies have been recently undertaken to study this process and to predict it over space and ultimately, in a broader national effort, to limit its negative effects on local communities. We focused on the Bastam watershed where 9.3% of its surface is currently affected by gullying. Machine learning algorithms are currently under the magnifying glass across the geomorphological community for their high predictive ability. However, unlike the bivariate statistical models, their structure does not provide intuitive and quantifiable measures of environmental preconditioning factors. To cope with such weakness, we interpret preconditioning causes on the basis of a bivariate approach namely, Index of Entropy. And, we performed the susceptibility mapping procedure by testing three extensions of a decision tree model namely, Alternating Decision Tree (ADTree), Naive-Bayes tree (NBTree), and Logistic Model Tree (LMT). We dichotomized the gully information over space into gully presence/absence conditions, which we further explored in their calibration and validation stages. Being the presence/absence information and associated factors identical, the resulting differences are only due to the algorithmic structures of the three models we chose. Such differences are not significant in terms of performances; in fact, the three models produce outstanding predictive AUC measures (ADTree = 0.922; NBTree = 0.939; LMT = 0.944). However, the associated mapping results depict very different patterns where only the LMT is associated with reasonable susceptibility patterns. This is a strong indication of what model combines best performance and mapping for any natural hazard - oriented application.",10.1016/j.gsf.2019.11.009,Oil erosion; GIS; Alternating decision tree model; Logistic model tree model,,
Deep-learning-based porous media microstructure quantitative characterization and reconstruction method,"Huang, YB; Xiang, Z; Qian, M",PHYSICAL REVIEW E,2022.0,"Microstructure characterization and reconstruction (MCR) is one of the most important components of discovering processing-structure-property relations of porous media behavior and inverse porous media design in computational materials science. Since the algorithms for describing and controlling the geometric configuration of microstructures need to solve a large number of variables and involve multiobjective conditions, the existing MCR methods have difficulty in gaining a perfect trade-off among the quantitative generation and characterization capability and the reconstruction quality. In this work, an improved 3D Porous Media Microstructure (3DPmmGAN) generative adversarial network based on deep-learning algorithm is demonstrated for high-quality microstructures generation with high controllability and high prediction accuracy. The proposed 3DPmmGAN allows the model to utilize unlabeled data for complex high-randomness microstructures end-toend training within an acceptable time consumption. Further analysis shows that the trained network has good adaptivity for microstructures with different random geometric configurations, and can quantitatively control the generated structure according to semantic conditions, and can also quantitatively predict complex microstructure features. The key results suggest the proposed 3DPmmGAN is a powerful tool to accelerate the preparation and the initial characterization of 3D porous media, and potentially maximize the design efficiency for porous media.",10.1103/PhysRevE.105.015308,,,
Uncertainty quantification and exploration-exploitation trade-off in humans,"Candelieri, A; Ponti, A; Archetti, F",JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING,,"The main objective of this paper is to outline a theoretical framework to analyse how humans' decision-making strategies under uncertainty manage the trade-off between information gathering (exploration) and reward seeking (exploitation). A key observation, motivating this line of research, is the awareness that human learners are amazingly fast and effective at adapting to unfamiliar environments and incorporating upcoming knowledge: this is an intriguing behaviour for cognitive sciences as well as an important challenge for Machine Learning. The target problem considered is active learning in a black-box optimization task and more specifically how the exploration/exploitation dilemma can be modelled within Gaussian Process based Bayesian Optimization framework, which is in turn based on uncertainty quantification. The main contribution is to analyse humans' decisions with respect to Pareto rationality where the two objectives are improvement expected and uncertainty quantification. According to this Pareto rationality model, if a decision set contains a Pareto efficient (dominant) strategy, a rational decision maker should always select the dominant strategy over its dominated alternatives. The distance from the Pareto frontier determines whether a choice is (Pareto) rational (i.e., lays on the frontier) or is associated to exasperate exploration. However, since the uncertainty is one of the two objectives defining the Pareto frontier, we have investigated three different uncertainty quantification measures and selected the one resulting more compliant with the Pareto rationality model proposed. The key result is an analytical framework to characterize how deviations from rationality depend on uncertainty quantifications and the evolution of the reward seeking process.",10.1007/s12652-021-03547-5,Active learning; Pareto analysis; Uncertainty quantification; Human learning; Exploration; exploitation dilemma,,
Fuzzy classifier identification using decision tree and multiobjective evolutionary algorithms,"Pulkkinen, P; Koivisto, H",INTERNATIONAL JOURNAL OF APPROXIMATE REASONING,2008.0,"This paper presents a hybrid method for identification of Pareto-optimal fuzzy classifiers (FCs). In contrast to many existing methods, the initial population for multiobjective evolutionary algorithms (MOEAs) is neither created randomly nor a priori knowledge is required. Instead, it is created by the proposed two-step initialization method. First, a decision tree (DT) created by C4.5 algorithm is transformed into an FC. Therefore, relevant variables are selected and initial partition of input space is performed. Then, the rest of the population is created by randomly replacing some parameters of the initial FC, such that, the initial population is widely spread. That improves the convergence of MOEAs into the correct Pareto front. The initial population is optimized by NSGA-II algorithm and a set of Pareto-optimal FCs representing the trade-off between accuracy and interpretability is obtained. The method does not require any a priori knowledge of the number of fuzzy sets, distribution of fuzzy sets or the number of relevant variables. They are all determined by it. Performance of the obtained FCs is validated by six benchmark data sets from the literature. The obtained results are compared to a recently published paper [H. Ishibuchi, Y. Nojima, Analysis of interpretability-accuracy tradeoff of fuzzy systems by multiobjective fuzzy genetics-based machine learning, International Journal of Approximate Reasoning 44 (1) (2007) 4-31] and the benefits of our method are clearly shown. (C) 2007 Elsevier Inc. All rights reserved.",10.1016/j.ijar.2007.10.004,fuzzy classifiers (FCs); multiobjective evolutionary algorithms (MOEAs); decision trees (DTs); initialization,,
Multi-objective optimization of interatomic potentials with application to MgO,"Ragasa, EJ; O'Brien, CJ; Hennig, RG; Foiles, SM; Phillipot, SR",MODELLING AND SIMULATION IN MATERIALS SCIENCE AND ENGINEERING,2019.0,"The parameterization of a functional form for an interatomic potential is treated as a problem in multi-objective optimization. An autonomous, machine-learning approach based on the identification of the Pareto hyper-surface of errors in predicted properties allows the development of an ensemble of parameterizations with high materials fidelity and robustness. The efficacy of this approach is illustrated for the simple example of a Buckingham potential for MgO. This approach also provides a strong foundation for uncertainty quantification of potential parameterizations.",10.1088/1361-651X/ab28d9,interatomic potential; atomic-level simulation; Pareto optimization; rational design,,
DEEP NEURAL NETWORKS FOR ESTIMATION AND INFERENCE,"Farrell, MH; Liang, TY; Misra, S",ECONOMETRICA,2021.0,"We study deep neural networks and their use in semiparametric inference. We establish novel nonasymptotic high probability bounds for deep feedforward neural nets. These deliver rates of convergence that are sufficiently fast (in some cases minimax optimal) to allow us to establish valid second-step inference after first-step estimation with deep learning, a result also new to the literature. Our nonasymptotic high probability bounds, and the subsequent semiparametric inference, treat the current standard architecture: fully connected feedforward neural networks (multilayer perceptrons), with the now-common rectified linear unit activation function, unbounded weights, and a depth explicitly diverging with the sample size. We discuss other architectures as well, including fixed-width, very deep networks. We establish the nonasymptotic bounds for these deep nets for a general class of nonparametric regression-type loss functions, which includes as special cases least squares, logistic regression, and other generalized linear models. We then apply our theory to develop semiparametric inference, focusing on causal parameters for concreteness, and demonstrate the effectiveness of deep learning with an empirical application to direct mail marketing.",10.3982/ECTA16901,Deep learning; neural networks; rectified linear unit; nonasymptotic bounds; convergence rates; semiparametric inference; treatment effects; program evaluation,,
A multiobjective optimization-based sparse extreme learning machine algorithm,"Wu, Y; Zhang, YS; Liu, XB; Cai, ZH; Cai, YM",NEUROCOMPUTING,2018.0,"Extreme Learning Machine (ELM) is a popular machine learning method and has been widely applied to real-world problems due to its fast training speed and good generalization performance. However, in ELM, the randomly assigned input weights and hidden biases usually degrade the generalization performance. Furthermore, ELM is considered as an empirical risk minimization model and easily leads to overfitting when dataset exists some outliers. In this paper, we proposed a novel algorithm named Multiobjective Optimization-based Sparse Extreme Learning Machine (MO-SELM), where parameter optimization and structure learning are integrated into the learning process to simultaneously enhance the generalization performance and alleviate the overfitting problem. In MO-SELM, the training error and the connecting sparsity are taken as two conflicting objectives of the multiobjective model, which aims to find sparse connecting structures with optimal weights and biases. Then, a hybrid encoding-based MOEA/D is used to optimize the multiobjective model. In addition, ensemble learning is embedded into this algorithm to make decisions after multiobjective optimization. Experimental results of several classification and regression applications demonstrate the effectiveness of the proposed MO-SELM. (C) 2018 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2018.07.060,Extreme learning machine; Sparse connecting structure; Parameter optimization; Structure learning; Multiobjective optimization,,
Using machine learning to link spatiotemporal information to biological processes in the ocean: a case study for North Sea cod recruitment,"Kuhn, B; Taylor, MH; Kempf, A",MARINE ECOLOGY PROGRESS SERIES,2021.0,"Marine organisms are subject to environmental variability on various temporal and spatial scales, which affect processes related to growth and mortality of different life stages. Marine scientists are often faced with the challenge of identifying environmental variables that best explain these processes, which, given the complexity of the interactions, can be like searching for a needle in the proverbial haystack. Even after initial hypothesis-based variable selection, a large number of potential candidate variables can remain if different lagged and seasonal influences are considered. To tackle this problem, we propose a machine learning framework that incorporates important steps in model building, ranging from environmental signal extraction to automated variable selection and model validation. Its modular structure allows for the inclusion of both parametric and machine learning models, like random forest. Unsupervised feature extractions via empirical orthogonal functions (EOFs) or self-organising maps (SOMs) are demonstrated as a way to summarize spatiotemporal fields for inclusion in predictive models. The proposed framework offers a robust way to reduce model complexity through a multi-objective genetic algorithm (NSGA-II) combined with rigorous cross-validation. We applied the framework to recruitment of the North Sea cod stock and investigated the effects of sea surface temperature (SST), salinity and currents on the stock via a modified version of random forest. The best model (5-fold CV r(2) = 0.69) incorporated spawning stock biomass and EOF-derived time series of SST and salinity anomalies acting through different seasons, likely relating to differing environmental effects on specific life-history stages during the recruitment year.",10.3354/meps13689,Machine learning; Multi-objective genetic algorithm; Empirical orthogonal function; EOF; Self-organising map; SOM; Random forest; Extreme randomized trees; Environmental stock-recruitment relationships; North Sea,,
p Finding the optimal multilayer network structure through reinforcement learning in fault diagnosis,"Cao, J; Ma, JL; Huang, DL; Yu, P",MEASUREMENT,2022.0,"Deep learning (DL) is an important method in industrial fault diagnosis. However, DL's network structure needs to be designed with experience. To simplify the design of network structures, we propose the neural architecture search network with Pareto efficiency reward and insert replay buffer (NAS-PERIRB) algorithm. In this paper, the early stopping and insert replay buffer (IRB) are used to improving the training efficiency of the samples. In addition, we design the Pareto efficiency reward function to optimize the goals and design a network search space to perform effective searches. What is more, we evaluate the NAS-PERIRB under two datasets. Results show that the two datasets have reached 99% accuracy in various situations, which means the NAS-PERIRB can achieve the purpose of designing the network structure independently.",10.1016/j.measurement.2021.110377,Neural architecture search; Pareto efficiency; Reinforcement learning; Fault diagnosis,,
A hybrid deep learning-based neural network for 24-h ahead wind power forecasting,"Hong, YY; Rioflorido, CLPP",APPLIED ENERGY,2019.0,"Wind power generation is always associated with uncertainties as a result of fluctuations of wind speed. Accurate predictions of wind power generation are important for the efficient operation of power systems. This paper presents a hybrid deep learning neural network for 24 h-ahead wind power generation forecasting. This novel method is based on a Convolutional Neural Network (CNN) that is cascaded with a Radial Basis Function Neural Network (RBFNN) with a double Gaussian function (DGF) as its activation function. The CNN is utilized to extract wind power characteristics by convolution, kernel and pooling operations. The supervised RBFNN, incorporating a DGF, deals with uncertain characteristics. Realistic wind power generations, measured on a wind farm, were used in simulations. The proposed method is implemented using TensorFlow and Keras Library. Comparative studies of different approaches are shown. Simulation results reveal that the proposed method is more accurate than traditional methods for 24 h-ahead wind power forecasting.",10.1016/j.apenergy.2019.05.044,Deep learning; Double Gaussian function; Feature extraction; Wind power forecasting,,
Dynamic Objective Sampling in Many-Objective Optimization,"Breaban, ME; Iftene, A","KNOWLEDGE-BASED AND INTELLIGENT INFORMATION & ENGINEERING SYSTEMS 19TH ANNUAL CONFERENCE, KES-2015",2015.0,"Given the poor convergence of multi-objective evolutionary algorithms (MOEAs) demonstrated in several studies that address many-objective optimization, we propose a simple objective sampling scheme that can be incorporated in any MOEA in order to enhance its convergence towards the Pareto front. An unsupervised clustering algorithm is applied in the space of objectives at various moments during the search process performed by the MOEA, and only representative objectives are used to guide the optimizer towards the Pareto front during next iterations. The effectiveness of the approach is experimentally demonstrated in the context of the NSGA-II optimizer. The redundant objectives are eliminated during search when the number of clusters (representative objectives) is automatically selected by an unsupervised standard procedure, popular in the field of unsupervised machine learning. Furthermore, if after eliminating all the redundant objectives the number of conflicting objectives is still high, continuing to eliminate objectives by imposing a lower number of clusters speeds-up the convergence towards the Pareto front. (C) 2015 The Authors. Published by Elsevier B.V.",10.1016/j.procs.2015.08.117,many-objective optimization; unsupervised objective selection; clustering; rank correlation,,
A comparative study of Gaussian process regression with other three machine learning approaches in the performance prediction of centrifugal pump,"Zhao, XT; Zhang, DS; Zhang, RH; Xu, B",PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART C-JOURNAL OF MECHANICAL ENGINEERING SCIENCE,,"Accurate prediction of performance indices using impeller parameters is of great importance for the initial and optimal design of centrifugal pump. In this study, a kernel-based non-parametric machine learning method named with Gaussian process regression (GPR) was proposed, with the purpose of predicting the performance of centrifugal pump with less effort based on available impeller parameters. Nine impeller parameters were defined as model inputs, and the pump performance indices, that is, the head and efficiency, were determined as model outputs. The applicability of three widely used nonlinear kernel functions of GPR including squared exponential (SE), rational quadratic (RQ) and Matern5/2 was investigated, and it was found by comparing with the experimental data that the SE kernel function is more suitable to capture the relationship between impeller parameters and performance indices because of the highest R square and the lowest values of max absolute relative error (MARE), mean absolute proportional error (MAPE), and root mean square error (RMSE). In addition, the results predicted by GPR with SE kernel function were compared with the results given by other three machine learning models. The comparison shows that the GPR with SE kernel function is more accurate and robust than other models in centrifugal pump performance prediction, and its prediction errors and uncertainties are both acceptable in terms of engineering applications. The GPR method is less costly in the performance prediction of centrifugal pump with sufficient accuracy, which can be further used to effectively assist the design and manufacture of centrifugal pump and to speed up the optimization design process of impeller coupled with stochastic optimization methods.",10.1177/09544062211050542,Centrifugal pump; impeller parameters; performance prediction; machine learning; Gaussian process regression,,
Defect learning with predictive sampling for process improvement,"Tolle, I; Lee, J; Salvador, D; Saville, B; Yong, PB; Marcuccilli, G","METROLOGY, INSPECTION, AND PROCESS CONTROL FOR MICROLITHOGRAPHY XXXIII",2019.0,"As technology nodes advance, the need for higher sensitivity optical inspection to identify critical defects has become extremely important for technology development. However, more sensitive optical inspection can induce more nuisance and hence more SEM non-visual (SNV) defects during review sampling. High SNV in the defect Pareto hinders the ability to get a true picture of the actual distribution of defect types on a wafer, and defect-of-interest (DOI) types that are crucial for process diagnostics can be missed. The culprit of this problem is the method of review sampling. Traditional review sampling consists of two parts: binning and defect selection. Binning is defined as a set of rules and conditions determined by human experience and judgment to categorize different DOI types. Then, defects are selected from each bin and reviewed by SEM. Due to the nature of high SNV from optical inspection, the random selection of defects will end up with high SNV in the defect Pareto. A defect Pareto with high SNV provides little value to yield learning. Because SEM review plus classification is limited by time and economic budget, improving the ability to predict whether a defect is DOI or SNV before SEM review is valuable. This paper introduces a machine learning based method suitable for high volume manufacturing that can increase the probability of finding DOIs during review sampling by integrating all available data sources, such as historical defect attributes from optical inspection, context information of the inspection recipe, design hotspots and metrology measurements. In addition to review sampling, this paper also illustrates other applications based on machine learning defect prediction, such as virtual process window discovery, and predicted defect types for trend monitoring. A predictive analytics platform was employed to allow defect type prediction based upon multiple inputs. [GRAPHICS] .",10.1117/12.2523963,predictive analytics platform; machine learning; SEM sample optimization; process window qualification,,
Kernel methods for short-term portfolio management,"Ince, H; Trafalis, TB",EXPERT SYSTEMS WITH APPLICATIONS,2006.0,"Portfolio optimization problem has been studied extensively. In this paper, we look at this problem from a different perspective. Several researchers argue that the USA equity market is efficient. Some of the studies show that the stock market is not efficient around the earning season. Based on these findings, we formulate the problem as a classification problem by using state of the art machine learning techniques such as minimax probability machine (MPM) and support vector machines (SVM). The MPM method finds a bound on the misclassification probabilities. On the other hand, SVM finds a hyperplane that maximizes the distance between two classes. Both methods prove similar results for short-term portfolio management. (c) 2005 Elsevier Ltd. All rights reserved.",10.1016/j.eswa.2005.10.008,support vector machines; minimax probability machine; kernel methods; portfolio management; earning announcements,,
Maxi-min margin machine: Learning large margin classifiers locally and globally,"Huang, KZ; Yang, HQ; King, I; Lyu, MR",IEEE TRANSACTIONS ON NEURAL NETWORKS,2008.0,"In this paper, we propose a novel large margin classifier, called the maxi-min margin machine (M-4). This model learns the decision boundary both locally and globally. In comparison, other large margin classifiers construct separating hyperplanes only either locally or globally. For example, a state-of-the-art large margin classifier, the support vector machine (SVM), considers data only locally, while another significant model, the minimax probability machine (MPM), focuses on building the decision hyperplane exclusively based on the global information. As a major contribution, we show that SVM yields the same solution as M-4 when data satisfy certain conditions, and MPM can be regarded as a relaxation model of M-4. Moreover, based on our proposed local and global view of data, another popular model, the linear discriminant analysis, can easily be interpreted and extended as well. We describe the M-4 model definition, provide a geometrical interpretation, present theoretical justifications, and propose a practical sequential conic programming method to solve the optimization problem. We also show how to exploit Mercer kernels to extend M-4 for nonlinear classifications. Furthermore, we perform a series of evaluations on both synthetic data sets and real-world benchmark data sets. Comparison with SVM and MPM demonstrates the advantages of our new model.",10.1109/TNN.2007.905855,classification; kernel methods; large margin; learning locally and globally; second-order cone programming,,
A Machine Learning Framework With an Intelligent Algorithm for Predicting the Isentropic Efficiency of a Hydraulic Diaphragm Metering Pump in the Organic Rankine Cycle System,"Ping, X; Yao, BF; Niu, K; Yuan, M",FRONTIERS IN ENERGY RESEARCH,2022.0,"The pump provides the necessary pressure and flow for the organic Rankine cycle (ORC) system. The traditional methods have obvious limitations when analyzing the time-varying characteristics of the key operating parameters of the pump. This study first introduces the scatter plot analysis method to analyze and evaluate the time-varying and coupling characteristics of the hydraulic diaphragm metering pump. Then, a machine learning-fitting algorithm hybrid model is constructed to solve and verify the actual matching correlation equation of the key operating parameters. In addition, the complicated non-linear relationship brings great challenges to obtaining the limit value of the pump isentropic efficiency. This study introduces the bilinear interpolation algorithm to systematically analyze the change trend between operating parameters and isentropic efficiency. Based on the wavelet neural network (WNN) with momentum term and particle swarm optimization-adaptive inertia weight adjusting (PSO-AIWA), a machine learning framework with an intelligent algorithm is constructed. Under this framework, the maximum isentropic efficiency of the pump can be stabilized at 70.22-74.67% under all working conditions. Through the theoretical analysis model, the effectiveness of this framework is evaluated. Finally, the optimal cycle parameters are evaluated. This study can provide direct significance for the analysis and optimization of the actual performance of the ORC system.",10.3389/fenrg.2022.851513,Organic Rankine cycle; Hydraulic diaphragm metering pump; Isentropic efficiency; Machine learning; Particle swarm optimization,,
A machine-learning approach to predicting the energy conversion performance of centrifugal pump impeller influenced by blade profile,"Wu, YZ; Tao, R; Zhu, D; Yao, ZF; Xiao, RF",PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART C-JOURNAL OF MECHANICAL ENGINEERING SCIENCE,2021.0,"Centrifugal pump is a kind of energy conversion machine for fluid delivering. It transfers the mechanical energy of impeller to the potential and kinetic energy of fluid. As a key factor in influencing the energy conversion performance of centrifugal pump, blade profile design is crucial. Traditional design concepts have ideal assumptions. To have a better design guidance, machine-learning based on neural network is used in this study. A typical centrifugal pump with simplified blade profile is numerically studied with experimental validation for a better discussion. Statistical results show that, for the high dimensional nonlinear relationship between blade angle and performance of centrifugal pump, neural network can adapt to this complex correlation better. The blade installation angle at leading-edge (beta(LE)') and trailing-edge (beta(TE)') and the wrap angle (Delta theta ') has significant correlation with the performance including pump head H, pump efficiency eta, impeller head H-imp, impeller efficiency eta(imp) and volute loss Delta H-vol. The influence level of blade angle follows the high-to-low order of Delta theta ', beta(LE)' and beta(TE)'. Determination of blade profile can be done for improving the energy conversion efficiency. Optimal blade profiles have higher beta(LE)' and Delta theta ' with better flow-control ability. Compared with the blade parameters of the initial pump, the blade profile with the best centrifugal pump efficiency is the best beta(LE)' increased by 1.926 degrees, Delta theta ' increased by 9.858 degrees, Optimization of impeller efficiency beta(LE)' increased by 1.855 degrees, Delta theta ' increased by 9.421 degrees. Computational fluid dynamics indicate the elimination of vortex in impeller after optimal selection. Then, beta(TE)' and Delta theta ' are found influential in aggravating the circumferential flow component in this special circular-volute with generating higher loss. beta(TE)' has a positive correlation with impeller head which suits traditional theory. In general, the machine-learning using neural network is effective in determining blade profiles for enhancing the performance of centrifugal pump.",10.1177/09544062211028264,Flow energy conversion; neural network; centrifugal pump; blade profile; computational fluid dynamics (CFD); machine learning,,
Smart adaptive run parameterization (SArP): enhancement of user manual selection of running parameters in fluid dynamic simulations using bio-inspired and machine-learning techniques,"Ghorbel, H; Zannini, N; Cherif, S; Sauser, F; Grunenwald, D; Droz, W; Baradji, M; Lakehal, D",SOFT COMPUTING,2019.0,"Computational fluid dynamic (CFD) simulations present numerous challenges in the domain of artificial intelligence. Computational time, resources and cost that can reach disproportional size before leading a simulation to its fully converged solution are one of the central issues in this domain. In this paper, we propose a novel algorithm that finds optimal parameter settings for the numerical solvers of CFD software. Indeed, this research proposes an alternative approach; rather than going deeper in reducing the mathematical complexity, it suggests taking advantage of the history of previous runs in order to estimate the best parameters for numerical equation resolution. In fact, our approach is bio-inspired and based on a genetic algorithm (GA) and evolutionary strategies enhanced with surrogate functions based on machine-learning meta-models. Our research method was tested on 11 different use cases using various configurations of the GA and algorithms of machine learning such as regression trees extra trees regressors and random forest regressors. Our approach has achieved better runtime performance and higher convergence quality (an improvement varying between 8 and 40%) in all of the test cases when compared to a basic approach which requires manually selecting the parameters. Moreover, our approach outperforms in some cases manual selection of parameters by reaching convergent solutions that couldn't otherwise be achieved manually.",10.1007/s00500-019-03761-6,Computational fluid dynamics; Genetic algorithms; Surrogate functions; Machine learning,,
Multi-objective Selection of Algorithm Portfolios: Experimental Validation,"Horn, D; Schork, K; Wagner, T",PARALLEL PROBLEM SOLVING FROM NATURE - PPSN XIV,2016.0,"The selection of algorithms to build portfolios represents a multi-objective problem. From a possibly large pool of algorithm candidates, a portfolio of limited size but good quality over a wide range of problems is desired. Possible applications can be found in the context of machine learning, where the accuracy and runtime of different learning techniques must be weighed. Each algorithm is represented by its Pareto front, which has been approximated in an a priori parameter tuning. Our approach for multi-objective selection of algorithm portfolios (MOSAP) is capable to trade-off the number of algorithm candidates and the respective quality of the portfolio. The quality of the portfolio is defined as the distance to the joint Pareto front of all algorithm candidates. By means of a decision tree, also the selection of the right algorithm is possible based on the characteristics of the problem. In this paper, we propose a validation framework to analyze the performance of our MOSAP approach. This framework is based on a parametrized generator of the algorithm candidate's Pareto front shapes. We discuss how to sample a landscape of multiple Pareto fronts with predefined intersections. The validation is performed by calculating discrete approximations for different landscapes and assessing the effect of the landscape parameters on the MOSAP approach.",10.1007/978-3-319-45823-6_39,Multi-objective optimization; Algorithm selection; Performance assessment; Benchmarking,,
A digital-twin and machine-learning framework for the design of multiobjective agrophotovoltaic solar farms,"Zohdi, TI",COMPUTATIONAL MECHANICS,2021.0,"This work develops a computational Digital-Twin framework to track and optimize the flow of solar power through complex, multipurpose, solar farm facilities, such as Agrophotovoltaic (APV) systems. APV systems symbiotically cohabitate power-generation facilities and agricultural production systems. In this work, solar power flow is rapidly computed with a reduced order model of Maxwell's equations, based on a high-frequency decomposition of the irradiance into multiple rays, which are propagated forward in time to ascertain multiple reflections and absorption for various source-system configurations, varying multi-panel inclination, panel refractive indices, sizes, shapes, heights, ground refractive properties, etc. The method allows for a solar installation to be tested from multiple source directions quickly and uses a genomic-based Machine-Learning Algorithm to optimize the system. This is particularly useful for planning of complex next-generation solar farm systems involving bifacial (double-sided) panelling, which are capable of capturing ground albedo reflection, exemplified by APV systems. Numerical examples are provided to illustrate the results, with the overall goal being to provide a computational framework to rapidly design and deploy complex APV systems.",10.1007/s00466-021-02035-z,Agrophotovoltaics; Digital-twin; Machine-learning,,
Multi-objective learning of hybrid classifiers,"Piltaver, R; Lustrek, M; Zupancic, J; Dzeroski, S; Gams, M",21ST EUROPEAN CONFERENCE ON ARTIFICIAL INTELLIGENCE (ECAI 2014),2014.0,"We propose a multi-objective machine learning approach guaranteed to find the Pareto optimal set of hybrid classification models consisting of comprehensible and incomprehensible submodels. The algorithm run-times are below 1 s for typical applications despite the exponential worst-case time complexity. The user chooses the model with the best comprehensibility-accuracy trade-off from the Pareto front which enables a well informed decision or repeats finding new Pareto fronts with modified seeds. For a classification trees as the comprehensible seed, the hybrids include single black-box model, invoked in hybrid leaves. The comprehensibility of such hybrid classifiers is measured with the proportion of examples classified by the regular leaves. We propose one simple and one computationally efficient algorithm for finding the Pareto optimal hybrid trees, starting from an initial classification tree and a black-box classifier. We evaluate the proposed algorithms empirically, comparing them to the baseline solution set, showing that they often provide valuable improvements. Furthermore, we show that the efficient algorithm outperforms the NSGA-II algorithm in terms of quality of the result set and efficiency (for this optimisation problem). Finally we show that the algorithm returns hybrid classifiers that reflect the expert's knowledge on activity recognition problem well.",10.3233/978-1-61499-419-0-717,,,
"Multi-zone optimisation of high-rise buildings using artificial intelligence for sustainable metropolises. Part 1: Background, methodology, setup, and machine learning results","Ekici, B; Kazanasmaz, ZT; Turrin, M; Tasgetiren, MF; Sariyildiz, IS",SOLAR ENERGY,2021.0,"Designing high-rise buildings is one of the complex tasks of architecture because it involves interdisciplinary performance aspects in the conceptual phase. The necessity for sustainable high-rise buildings has increased owing to the demand for metropolises based on population growth and urbanisation trends. Although artificial intelligence (AI) techniques support swift decision-making when addressing multiple performance aspects related to sustainable buildings, previous studies only examined single floors because modelling and optimising the entire building requires extensive computational time. However, different floor levels require various design decisions because of the performance variances between the ground and sky levels of high-rises in dense urban districts. This paper presents a multi-zone optimisation (MUZO) methodology to support decision-making for an entire high-rise building considering multiple floor levels and performance aspects. The proposed methodology includes parametric modelling and simulations of high-rise buildings, as well as machine learning and optimisation as AI methods. The specific setup focuses on the quad-grid and diagrid shading devices using two daylight metrics of LEED: spatial daylight autonomy and annual sunlight exposure. The parametric model generated samples to develop surrogate models using an artificial neural network. The results of 40 surrogate models indicated that the machine learning part of the MUZO methodology can report very high prediction accuracies for 31 models and high accuracies for six quad-grid and three diagrid models. The findings indicate that the MUZO can be an important part of designing high-rises in metropolises while predicting multiple performance aspects related to sustainable buildings during the conceptual design phase.",10.1016/j.solener.2021.05.083,Performance-based design; Building simulation; Sustainability; High-rise building; Machine learning; Optimization,,
Metaheuristic-based Deep COVID-19 Screening Model from Chest X-Ray Images,"Kaur, M; Kumar, V; Yadav, V; Singh, D; Kumar, N; Das, NN",JOURNAL OF HEALTHCARE ENGINEERING,2021.0,"COVID-19 has affected the whole world drastically. A huge number of people have lost their lives due to this pandemic. Early detection of COVID-19 infection is helpful for treatment and quarantine. Therefore, many researchers have designed a deep learning model for the early diagnosis of COVID-19-infected patients. However, deep learning models suffer from overfitting and hyperparameter-tuning issues. To overcome these issues, in this paper, a metaheuristic-based deep COVID-19 screening model is proposed for X-ray images. The modified AlexNet architecture is used for feature extraction and classification of the input images. Strength Pareto evolutionary algorithm-II (SPEA-II) is used to tune the hyperparameters of modified AlexNet. The proposed model is tested on a four-class (i.e., COVID-19, tuberculosis, pneumonia, or healthy) dataset. Finally, the comparisons are drawn among the existing and the proposed models.",10.1155/2021/8829829,,,
Conditional Wasserstein Generative Adversarial Networks for Rebalancing Iris Image Datasets,"Li, YH; Aslam, MS; Harfiya, LN; Chang, CC",IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS,2021.0,"The recent development of deep learning-based generative models has sharply intensified the interest in data synthesis and its applications. Data synthesis takes on an added importance especially for some pattern recognition tasks in which some classes of data are rare and difficult to collect. In an iris dataset, for instance, the minority class samples include images of eyes with glasses, oversized or undersized pupils, misaligned iris locations, and iris occluded or contaminated by eyelids, eyelashes, or lighting reflections. Such class-imbalanced datasets often result in biased classification performance. Generative adversarial networks (GANs) are one of the most promising frameworks that learn to generate synthetic data through a two-player minimax game between a generator and a discriminator. In this paper, we utilized the state-of-the-art conditional Wasserstein generative adversarial network with gradient penalty (CWGAN-GP) for generating the minority class of iris images which saves huge amount of cost of human labors for rare data collection. With our model, the researcher can generate as many iris images of rare cases as they want and it helps to develop any deep learning algorithm whenever large size of dataset is needed.",10.1587/transinf.2021EDP7079,deep learning; generative adversarial network; iris image generation; machine learning neural networks; signal synthesis,,
Robust multiobjective evolutionary feature subset selection algorithm for binary classification using machine learning techniques,"Deniz, A; Kiziloz, HE; Dokeroglu, T; Cosar, A",NEUROCOMPUTING,2017.0,"This study investigates the success of a multiobjective genetic algorithm (GA) combined with state-of-the-art machine learning (ML) techniques for the feature subset selection (FSS) in binary classification problem (BCP). Recent studies have focused on improving the accuracy of BCP by including all of the features, neglecting to determine the best performing subset of features. However, for some problems, the number of features may reach thousands, which will cause too much computation power to be consumed during the feature evaluation and classification phases, also possibly reducing the accuracy of the results. Therefore, selecting the minimum number of features while preserving and/or increasing the accuracy of the results at a high level becomes an important issue for achieving fast and accurate binary classification. Our multiobjective evolutionary algorithm includes two phases, FSS using a GA and applying ML techniques for the BCP. Since exhaustively investigating all of the feature subsets is intractable, a GA is preferred for the first phase of the algorithm for intelligently detecting the most appropriate feature subset. The GA uses multiobjective crossover and mutation operators to improve a population of individuals (each representing a selected feature subset) and obtain (near-) optimal solutions through generations. In the second phase of the algorithms, the fitness of the selected subset is decided by using state-of-the-art ML techniques; Logistic Regression, Support Vector Machines, Extreme Learning Machine, K-means, and Affinity Propagation. The performance of the multiobjective evolutionary algorithm (and the ML techniques) is evaluated with comprehensive experiments and compared with state-of-the-art algorithms, Greedy Search, Particle Swarm Optimization, Tabu Search, and Scatter Search. The proposed algorithm was observed to be robust and it performed better than the existing methods on most of the datasets. (C) 2017 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2017.02.033,Multiobjective feature selection; Evolutionary algorithm; Binary classification; Supervised/unsupervised machine learning,,
Machine Learning and Simulation-Optimization Coupling for Water Distribution Network Contamination Source Detection,"Grbcic, L; Kranjcevic, L; Druzeta, S",SENSORS,2021.0,"This paper presents and explores a novel methodology for solving the problem of a water distribution network contamination event, which includes determining the exact source of contamination, the contamination start and end times and the injected contaminant concentration. The methodology is based on coupling a machine learning algorithm for predicting the most probable contamination sources in a water distribution network with an optimization algorithm for determining the values of contamination start time, end time and injected contaminant concentration for each predicted node separately. Two slightly different algorithmic frameworks were constructed which are based on the mentioned methodology. Both algorithmic frameworks utilize the Random Forest algorithm for classification of top source contamination node candidates, with one of the frameworks directly using the stochastic fireworks optimization algorithm to determine the contamination start time, end time and injected contaminant concentration for each predicted node separately. The second framework uses the Random Forest algorithm for an additional regression prediction of each top node's start time, end time and contaminant concentration and is then coupled with the deterministic global search optimization algorithm MADS. Both a small sized (92 potential sources) network with perfect sensor measurements and a medium sized (865 potential sources) benchmark network with fuzzy sensor measurements were used to explore the proposed frameworks. Both algorithmic frameworks perform well and show robustness in determining the true source node, start and end times and contaminant concentration, with the second framework being extremely efficient on the fuzzy sensor measurement benchmark network.",10.3390/s21041157,random forests; water network contamination; simulation-optimization; machine learning; pollution source identification; fireworks algorithm; MADS,,
Machine learning predictive models for optimal design of building-integrated photovoltaic-thermal collectors,"Shahsavar, A; Moayedi, H; Al-Waeli, AHA; Sopian, K; Chelvanathan, P",INTERNATIONAL JOURNAL OF ENERGY RESEARCH,2020.0,"This research article aims to examine the feasibility of several machine learning techniques to forecast the exergetic performance of a building-integrated photovoltaic-thermal (BIPVT) collector. In this regard, it uses multiple linear regression, multilayer perceptron, radial basis function regressor, sequential minimal optimization improved support vector machine, lazy.IBK, random forest (RF), and random tree approaches. Moreover, it implements the performance evaluation criteria (PEC) to evaluate the system's performance from the perspective of exergy. The use of these approaches serves the identification process to realize the relationship between the input-output parameters of the BIPVT system. The novelty of this work is that it utilizes and compares multiple learning algorithms to predict the PEC of BIPVT through design parameters. Hence, the research considers the parameter (PEC) as the essential output of the BIPVT collector, while the input parameters are the length, width, and depth of the duct, located under the PV modules, as well as the air mass flow rate. The results of the research for the statistical indexes of mean absolute error, root mean square error, relative absolute error (%), and root relative squared error (%) show values of (0.2967, 0.3885, 1.8754, and 1.5237) and (0.4957, 0.8153, 2.9586, and 2.8289), respectively, for the training and testing datasets. While R-2 ranges (0.9997-0.9999) for those datasets. Therefore, to estimate the exergy performance of the BIPVT collector, the RF model is superior to other proposed models.",10.1002/er.5323,BIPVT; exergetic performance; machine learning; performance evaluation criteria; predictive model,,
"Multi-Hazard Exposure Mapping Using Machine Learning for the State of Salzburg, Austria","Nachappa, TG; Ghorbanzadeh, O; Gholamnia, K; Blaschke, T",REMOTE SENSING,2020.0,"We live in a sphere that has unpredictable and multifaceted landscapes that make the risk arising from several incidences that are omnipresent. Floods and landslides are widespread and recurring hazards occurring at an alarming rate in recent years. The importance of this study is to produce multi-hazard exposure maps for flooding and landslides for the federal State of Salzburg, Austria, using the selected machine learning (ML) approach of support vector machine (SVM) and random forest (RF). Multi-hazard exposure maps were established on thirteen influencing factors for flood and landslides such as elevation, slope, aspect, topographic wetness index (TWI), stream power index (SPI), normalized difference vegetation index (NDVI), geology, lithology, rainfall, land cover, distance to roads, distance to faults, and distance to drainage. We classified the inventory data for flood and landslide into training and validation with the widely used splitting ratio, where 70% of the locations are used for training, and 30% are used for validation. The accuracy assessment of the exposure maps was derived through ROC (receiver operating curve) and R-Index (relative density). RF yielded better results for both flood and landslide exposure with 0.87 for flood and 0.90 for landslides compared to 0.87 for flood and 0.89 for landslides using SVM. However, the multi-hazard exposure map for the State of Salzburg derived through RF and SVM provides the planners and managers to plan better for risk regions affected by both floods and landslides.",10.3390/rs12172757,multi-hazard; flood; landslide; random forest (RF); support vector machine (SVM); exposure mapping,,
ELECTRE tree: a machine learning approach to infer ELECTRE Tri-B parameters,"de Banos, GMM; Pereira, V; Roboredo, MC",DATA TECHNOLOGIES AND APPLICATIONS,2021.0,"Purpose This paper presents an algorithm that can elicitate (infer) all or any combination of elimination and choice expressing reality (ELECTRE) Tri-B parameters. For example, a decision maker can maintain the values for indifference, preference and veto thresholds, and the study's algorithm can find the criteria weights, reference profiles and the lambda cutting level. The study's approach is inspired by a machine learning ensemble technique, the random forest, and for that, the authors named the study's approach as ELECTRE tree algorithm. Design/methodology/approach First, the authors generate a set of ELECTRE Tri-B models, where each model solves a random sample of criteria and alternates. Each sample is made with replacement, having at least two criteria and between 10% and 25% of alternates. Each model has its parameters optimized by a genetic algorithm (GA) that can use an ordered cluster or an assignment example as a reference to the optimization. Finally, after the optimization phase, two procedures can be performed; the first one will merge all models, finding in this way the elicitated parameters and in the second procedure, each alternate is classified (voted) by each separated model, and the majority vote decides the final class. Findings The authors have noted that concerning the voting procedure, nonlinear decision boundaries are generated and they can be suitable in analyzing problems of the same nature. In contrast, the merged model generates linear decision boundaries. Originality/value The elicitation of ELECTRE Tri-B parameters is made by an ensemble technique that is composed of a set of multicriteria models that are engaged in generating robust solutions.",10.1108/DTA-10-2020-0256,ELECTRE Tri-B; Parameters elicitation; Machine learning,,
Digital Recognition of Street View House Numbers Based on DCGAN,"Zhong, JP; Gao, J; Chen, RJ; Li, J",ICIGP 2019: PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON IMAGE AND GRAPHICS PROCESSING / 2019 5TH INTERNATIONAL CONFERENCE ON VIRTUAL REALITY,2019.0,"Deep learning algorithms have surpassed human resolution in applications such as face recognition and object classification. However, it can only produce very blurred, lack of details of the image. Generative Adversarial Network is a game training of minimax antagonism between generator G and discriminator D, and ultimately achieves Nash equilibrium. We use deep convolutional GAN that recognizes sequence numbers and without split characters. First we use convolution network to extract character features. Second we construct a convolution neural network to recognize digits of natural scene house number. DCGAN is used to improve the resolution of the number of fuzzy houses, so as to extract more abundant data features in data set training. It can better recognize the numbers in the natural street.",10.1145/3313950.3313963,Image processing; machine learning; deep convolutional generative adversarial networks; street view house numbers,,
Machine learning approaches to characterize the obesogenic urban exposome,"Ohanyan, H; Portengen, L; Huss, A; Traini, E; Beulens, JWJ; Hoek, G; Lakerveld, J; Vermeulen, R",ENVIRONMENT INTERNATIONAL,2022.0,"Background: Characteristics of the urban environment may contain upstream drivers of obesity. However, research is lacking that considers the combination of environmental factors simultaneously. Objectives: We aimed to explore what environmental factors of the urban exposome are related to body mass index (BMI), and evaluated the consistency of findings across multiple statistical approaches. Methods: A cross-sectional analysis was conducted using baseline data from 14,829 participants of the Occupational and Environmental Health Cohort study. BMI was obtained from self-reported height and weight. Geocoded exposures linked to individual home addresses (using 6-digit postcode) of 86 environmental factors were estimated, including air pollution, traffic noise, green-space, built environmental and neighborhood sociodemographic characteristics. Exposure-obesity associations were identified using the following approaches: sparse group Partial Least Squares, Bayesian Model Averaging, penalized regression using the Minimax Concave Penalty, Generalized Additive Model-based boosting Random Forest, Extreme Gradient Boosting, and Multiple Linear Regression, as the most conventional approach. The models were adjusted for individual sociodemographic variables. Environmental factors were ranked according to variable importance scores attributed by each approach and median ranks were calculated across these scores to identify the most consistent associations. Results: The most consistent environmental factors associated with BMI were the average neighborhood value of the homes, oxidative potential of particulate matter air pollution (OP), healthy food outlets in the neighborhood (5 km buffer), low-income neighborhoods, and one-person households in the neighborhood. Higher BMI levels were observed in low-income neighborhoods, with lower average house values, lower share of one-person households and smaller amount of healthy food retailers. Higher BMI levels were observed in low-income neighborhoods, with lower average house values, lower share of one-person households, smaller amounts of healthy food retailers and higher OP levels. Across the approaches, we observed consistent patterns of results based on model's capacity to incorporate linear or nonlinear associations. Discussion: The pluralistic analysis on environmental obesogens strengthens the existing evidence on the role of neighborhood socioeconomic position, urbanicity and air pollution.",10.1016/j.envint.2021.107015,Exposome; Random forest; Extreme gradient boosting (XGBoost); Shapley values; Socioeconomic position; Air pollution,,
Detection of areas prone to flood risk using state-of-the-art machine learning models,"Costache, R; Arabameri, A; Elkhrachy, I; Ghorbanzadeh, O; Pham, QB",GEOMATICS NATURAL HAZARDS & RISK,2021.0,"The present study aims to evaluate the susceptibility to floods in the river basin of Buzau in Romania through the following 6 machine learning models: Support Vector Machine (SVM), J48 decision tree, Adaptive Neuro-Fuzzy Inference System (ANFIS), Random Forest (RF), Artificial Neural Network (ANN) and Alternating Decision Tree (ADT). In the first stage of the study, an inventory of the areas affected by floods was made in the study area, and a number of 205 flood points were identified. Further, 12 flood predictors were selected to be used for final susceptibility mapping. The six models' training was performed by using 70% of the total flood points that have been associated with the values of flood predictors. The highest accuracy (0.973) was obtained by the RF model, while J48 had the lowest performance (0.825). Besides, by classifying flood predictors' values in flood and non-flood pixels, the six flood susceptibility maps were made. High and very high flood susceptibility values cover between 17.71% (MLP) and 27.93% (ANFIS) of the study area. The validation of the results, performed using the ROC Curve, shows that the most accurate flood susceptibility values are also assigned to the RF model (AUC = 0.996).",10.1080/19475705.2021.1920480,Buzau catchment; flood susceptibility; machine learning; Romania; GIS,,
Artificial chicken swarm algorithm for multi-objective optimization with deep learning,"Wei, QZ; Huang, DR; Zhang, Y",JOURNAL OF SUPERCOMPUTING,2021.0,"With the rapid development of computer hardware in the past three decades, various classic algorithms such as neural computing and bionic optimization computing have been widely used in practical problems. This paper extended the new bionic algorithm-flock algorithm proposed in 2014 and obtained a multi-objective flock algorithm to solve the multi-objective problem. This study used aggregate functions to define social ranks, and simulated the foraging behavior of chickens in the process of searching for food in the objective space and found the balance between diversity and convergence when looking for the best Pareto solution. The algorithm took five types of bi-objective functions and four types of three-objective functions as objects and compared it with four more widely used algorithms in multi-objective problems. The results demonstrate that the MOCSO (multi-objective chicken swarm optimization) algorithm shows better results in the optimization of multi-objective problems.",10.1007/s11227-021-03770-z,Neural computing; Bionic optimization calculation; Multi-objective optimization; Chicken swarm optimization algorithm; Pareto solution set; Deep learning,,
LEARNING MODELS WITH UNIFORM PERFORMANCE VIA DISTRIBUTIONALLY ROBUST OPTIMIZATION,"Duchi, JC; Namkoong, H",ANNALS OF STATISTICS,2021.0,"A common goal in statistics and machine learning is to learn models that can perform well against distributional shifts, such as latent heterogeneous subpopulations, unknown covariate shifts or unmodeled temporal effects. We develop and analyze a distributionally robust stochastic optimization (DRO) framework that learns a model providing good performance against perturbations to the data-generating distribution. We give a convex formulation for the problem, providing several convergence guarantees. We prove finite-sample minimax upper and lower bounds, showing that distributional robustness sometimes comes at a cost in convergence rates. We give limit theorems for the learned parameters, where we fully specify the limiting distribution so that confidence intervals can be computed. On real tasks including generalizing to unknown subpopulations, fine-grained recognition and providing good tail performance, the distributionally robust approach often exhibits improved performance.",10.1214/20-AOS2004,Robust optimization; minimax optimality; risk-averse learning,,
Many-Objective Quantum-Inspired Particle Swarm Optimization Algorithm for Placement of Virtual Machines in Smart Computing Cloud,"Balicki, J",ENTROPY,2022.0,"Particle swarm optimization algorithm (PSO) is an effective metaheuristic that can determine Pareto-optimal solutions. We propose an extended PSO by introducing quantum gates in order to ensure the diversity of particle populations that are looking for efficient alternatives. The quality of solutions was verified in the issue of assignment of resources in the computing cloud to improve the live migration of virtual machines. We consider the multi-criteria optimization problem of deep learning-based models embedded into virtual machines. Computing clouds with deep learning agents can support several areas of education, smart city or economy. Because deep learning agents require lots of computer resources, seven criteria are studied such as electric power of hosts, reliability of cloud, CPU workload of the bottleneck host, communication capacity of the critical node, a free RAM capacity of the most loaded memory, a free disc memory capacity of the most busy storage, and overall computer costs. Quantum gates modify an accepted position for the current location of a particle. To verify the above concept, various simulations have been carried out on the laboratory cloud based on the OpenStack platform. Numerical experiments have confirmed that multi-objective quantum-inspired particle swarm optimization algorithm provides better solutions than the other metaheuristics.",10.3390/e24010058,particle swarm optimization; quantum gates; virtual machines; computing cloud; many-objective optimization,,
Global and local two-sample tests via regression,"Kim, I; Lee, AB; Lei, J",ELECTRONIC JOURNAL OF STATISTICS,2019.0,"Two-sample testing is a fundamental problem in statistics. Despite its long history, there has been renewed interest in this problem with the advent of high-dimensional and complex data. Specifically, in the machine learning literature, there have been recent methodological developments such as classification accuracy tests. The goal of this work is to present a regression approach to comparing multivariate distributions of complex data. Depending on the chosen regression model, our framework can efficiently handle different types of variables and various structures in the data, with competitive power under many practical scenarios. Whereas previous work has been largely limited to global tests which conceal much of the local information, our approach naturally leads to a local two-sample testing framework in which we identify local differences between multivariate distributions with statistical confidence. We demonstrate the efficacy of our approach both theoretically and empirically, under some well-known parametric and nonparametric regression methods. Our proposed methods are applied to simulated data as well as a challenging astronomy data set to assess their practical usefulness.",10.1214/19-EJS1648,Galaxy morphology; intrinsic dimension; kernel regression; nearest neighbor regression; permutation test; random forests,,
Intelligent sensitivity analysis framework based on machine learning for spacecraft thermal design,"Xiong, Y; Guo, L; Yang, YT; Wang, HL",AEROSPACE SCIENCE AND TECHNOLOGY,2021.0,"The variance-based method of global sensitivity analysis (GSA) has been widely applied in spacecraft thermal design, which is typically calculated using Monte Carlo estimations. However, such estimations require a large number of samples to ensure sufficient accuracy, which makes GSA expensive to perform when modeling is difficult. Moreover, multimodal or highly skewed output distributions may result in the use of variance as an uncertain agent that generates contradictory results. Therefore, an intelligent density-based GSA framework based on machine learning and multi-fidelity metamodels called IDGSA-3M is proposed. An intelligent batch processing system based on a real-time data interaction between MATLAB and NX/TMG was designed that uses many cheap low-fidelity sample points to reduce the cost of model evaluation while using a small number of expensive high-fidelity sample points to maintain high accuracy, thus achieving trade-offs between high accuracy and low computational cost. A radial basis function (RBF) neural network based on an improved mind evolutionary algorithm was applied to approximate the multi-fidelity metamodel of a spacecraft thermophysical model calculated using a batch processing system, which had a computational speed that was 1000+ times faster than that of the traditional thermophysical model and a high computational accuracy of 99%+. The output distributions of the RBF were then characterized by its cumulative distribution functions to obtain density-based sensitivity indices. Both the theoretical and experimental results of GSA for the thermal design parameters of the extreme ultraviolet radiation detector on the space-based Lyman-Alpha Solar Telescope, developed in China, demonstrated that the convergence rate of IDGSA-3M can be improved up to 10-fold for a fixed convergence level in comparison with two other GSA methods, thereby verifying its superiority. (C) 2021 Elsevier Masson SAS. All rights reserved.",10.1016/j.ast.2021.106927,Global sensitivity analysis; Spacecraft thermal design; Machine learning; Multi-fidelity,,
Systematic method for a deep learning-based prediction model for gamma evaluation in patient-specific quality assurance of volumetric modulated arc therapy,"Tomori, S; Kadoya, N; Kajikawa, T; Kimura, Y; Narazaki, K; Ochi, T; Jingu, K",MEDICAL PHYSICS,2021.0,"Purpose This study aimed to develop and evaluate a novel strategy for establishing a deep learning-based gamma passing rate (GPR) prediction model for volumetric modulated arc therapy (VMAT) using dummy target plan data, one measurement process, and a multicriteria prediction method. Methods A total of 147 VMAT plans were used for the training set (two sets of 48 dummy target plans) and test set (51 clinical target plans). The dummy plans were measured using a diode array detector. We developed an original convolutional neural network that accepts coronal and sagittal dose distributions to predict the GPRs of 36 pairs of gamma criteria from 0.5%/0.5 mm to 3%/3 mm. Sixfold cross-validation and model averaging were performed, and the mean training result and mean test result were derived from six trained models that were produced during cross-validation. Results Strong or moderate correlations were observed between the measured and predicted GPRs in all criteria. The mean absolute errors and root mean squared errors of the test set (clinical target plan) were 0.63 and 1.11 in 3%/3 mm, 1.16 and 1.73 in 3%/2 mm, 1.96 and 2.66 in 2%/2 mm, 5.00 and 6.35 in 1%/1 mm, and 5.42 and 6.78 in 0.5%/1 mm, respectively. The Pearson correlation coefficients were 0.80 in the training set and 0.68 in the test set at the 0.5%/1 mm criterion. Conclusion Our results suggest that the training of the deep learning-based quality assurance model can be performed using a dummy target plan.",10.1002/mp.14682,deep learning; IMRT; quality assurance; radiation therapy; VMAT,,
Bayesian optimization of nanoporous materials,"Deshwal, A; Simon, CM; Doppa, JR",MOLECULAR SYSTEMS DESIGN & ENGINEERING,2021.0,"Nanoporous materials (NPMs) could be used to store, capture, and sense many different gases. Given an adsorption task, we often wish to search a library of NPMs for the one with the optimal adsorption property. The high cost of NPM synthesis and gas adsorption measurements, whether these experiments are in the lab or in a simulation, often precludes exhaustive search. We explain, demonstrate, and advocate Bayesian optimization (BO) to actively search for the optimal NPM in a library of NPMs-and find it using the fewest experiments. The two ingredients of BO are a surrogate model and an acquisition function. The surrogate model is a probabilistic model reflecting our beliefs about the NPM-structure-property relationship based on observations from past experiments. The acquisition function uses the surrogate model to score each NPM according to the utility of picking it for the next experiment. It balances two competing goals: (a) exploitation of our current approximation of the structure-property relationship to pick the NPM we believe [under uncertainty] will be the highest-performing, and (b) exploration of regions of NPM space we have not visited, to pick an NPM we are uncertain about and improve our approximation of the structure-property relationship. We demonstrate BO by searching an open database of similar to 70 000 hypothetical covalent organic frameworks (COFs) for the COF with the highest simulated methane deliverable capacity (pertinent for vehicular adsorbed natural gas storage). BO finds the optimal COF and acquires similar to 30% of the top 100 highest-ranked COFs after evaluating only similar to 140 COFs. More, BO searches more efficiently than evolutionary and one-shot supervised machine learning approaches.",10.1039/d1me00093d,,,
Slope stability prediction using integrated metaheuristic and machine learning approaches: A comparative study,"Qi, CC; Tang, XL",COMPUTERS & INDUSTRIAL ENGINEERING,2018.0,"Advances in dataset collection and machine learning (ML) algorithms are important contributors to the stability analysis in industrial engineering, especially to slope stability analysis. In the past decade, various ML algorithms have been used to estimate slope stability on different datasets, and yet a comprehensive comparative study of the most advanced ML algorithms is lacking. In this article, we proposed and compared six integrated artificial intelligence (AI) approaches for slope stability prediction based on metaheuristic and ML algorithms. Six ML algorithms, including logistic regression, decision tree, random forest, gradient boosting machine, support vector machine, and multilayer perceptron neural network, were used for the relationship modelling and firefly algorithm (FA) was used for the hyper-parameters tuning. Three performance measures, namely confusion matrices, the receiver operating characteristic (ROC) curve, and the area under the ROC curve (AUC), were used to evaluate the predictive performance of AI approaches. We first demonstrated that integrated AI approaches had great potential to predict slope stability and FA was efficient in the hyper-parameter tunning. The AUC values of all AI approaches on the testing set were between 0.822 and 0.967, denoting excellent performance was achieved. The optimum support vector machine model with the Youden's cutoff was recommended in terms of the AUC value, the accuracy, and the true negative rate. We also investigated the relative importance of influencing variables and found that cohesion was the most influential variable for slope stability with an importance score of 0.310. This research provides useful recommendations for future slope stability analysis and can be used for a wider application in the rest of industrial engineering.",10.1016/j.cie.2018.02.028,Slope stability prediction; Integrated AI approaches; Machine learning algorithms; Firefly algorithm; Variable importance,,
Robust randomized optimization with k nearest neighbors,"Reeve, HWJ; Kaban, A",ANALYSIS AND APPLICATIONS,2019.0,"Modern applications of machine learning typically require the tuning of a multitude of hyperparameters. With this motivation in mind, we consider the problem of optimization given a set of noisy function evaluations. We focus on robust optimization in which the goal is to find a point in the input space such that the function remains high when perturbed by an adversary within a given radius. Here we identify the minimax optimal rate for this problem, which turns out to be of order O(n(-lambda/(2 lambda+ 1))), where n is the sample size and lambda quantifies the smoothness of the function for a broad class of problems, including situations where the metric space is unbounded. The optimal rate is achieved (up to logarithmic factors) by a conceptually simple algorithm based on k-nearest neighbor regression.",10.1142/S0219530519400086,Optimization for machine learning; metric spaces; non-parametric methods,,
A multi-objective evolutionary approach to training set selection for support vector machine,"Acampora, G; Herrera, F; Tortora, G; Vitiello, A",KNOWLEDGE-BASED SYSTEMS,2018.0,"The Support Vector Machine (SVM) is one of the most powerful algorithms for machine learning and data mining in numerous and heterogenous application domains. However, in spite of its competitiveness, SVM suffers from scalability problems which drastically worsens its performance in terms of memory requirements and execution time. As a consequence, there is a strong emergence of approaches for supporting SVM in efficiently addressing the aforementioned problems without affecting its classification capabilities. In this scenario, methods for Training Set Selection (TSS) represent a suitable and consolidated pre-processing technique to compute a reduced but representative training dataset, and improve SVM's scalability without deprecating its classification accuracy. Recently, TSS has been formulated as an optimization problem characterized by two objectives (the classification accuracy and the reduction rate) and solved through the application of evolutionary algorithms. However, so far, all the evolutionary approaches for TSS have been based on a so-called multi-objective a priori technique, where multiple objectives are aggregated together into a single objective through a weighted combination. This paper proposes to apply, for the first time, a Pareto-based multi-objective optimization approach to the TSS problem in order to explicitly deal with both its objectives and offer a better trade-off between SVM's classification and reduction performance. The benefits of the proposed approach are validated by a set of experiments involving well-known datasets taken from the UCI Machine Learning Database Repository. As shown by statistical tests, the application of a Pareto-based multi-objective optimization approach improves on state-of-the-art TSS techniques and enhances SVM efficiency. (c) 2018 Elsevier B.V. All rights reserved.",10.1016/j.knosys.2018.02.022,Training set selection; Multi-objective optimization; Support vector machine,,
Does Physical Activity Predict Obesity-A Machine Learning and Statistical Method-Based Analysis,"Cheng, XL; Lin, SY; Liu, J; Liu, SY; Zhang, J; Nie, P; Fuemmeler, BF; Wang, YF; Xue, H",INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH AND PUBLIC HEALTH,2021.0,"Background: Obesity prevalence has become one of the most prominent issues in global public health. Physical activity has been recognized as a key player in the obesity epidemic. Objectives: The objectives of this study are to (1) examine the relationship between physical activity and weight status and (2) assess the performance and predictive power of a set of popular machine learning and traditional statistical methods. Methods: National Health and Nutrition Examination Survey (NHANES, 2003 to 2006) data were used. A total of 7162 participants met our inclusion criteria (3682 males and 3480 females), with average age ranging from 48.6 (normal weight) to 52.1 years old (overweight). Eleven classifying algorithms-including logistic regression, naive Bayes, Radial Basis Function (RBF), local k-nearest neighbors (k-NN), classification via regression (CVR), random subspace, decision table, multiobjective evolutionary fuzzy classifier, random tree, J48, and multilayer perceptron-were implemented and evaluated, and they were compared with traditional logistic regression model estimates. Results: With physical activity and basic demographic status, of all methods analyzed, the random subspace classifier algorithm achieved the highest overall accuracy and area under the receiver operating characteristic (ROC) curve (AUC). The duration of vigorous-intensity activity in one week and the duration of moderate-intensity activity in one week were important attributes. In general, most algorithms showed similar performance. Logistic regression was middle-ranking in terms of overall accuracy, sensitivity, specificity, and AUC among all methods. Conclusions: Physical activity was an important factor in predicting weight status, with gender, age, and race/ethnicity being less but still essential factors associated with weight outcomes. Tailored intervention policies and programs should target the differences rooted in these demographic factors to curb the increase in the prevalence of obesity and reduce disparities among sub-demographic populations.",10.3390/ijerph18083966,physical activity; obesity; machine learning; disparity,,
Multi-Objective Optimization of a Maneuvering Small Aircraft Turbine Engine Rotor System,"Shibu, KJ; Shankar, K; Babu, CK; Degaonkar, GK",JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS,2021.0,"This paper presents the multi-objective optimization of a small aircraft turbine engine rotor system subjected to maneuver loads. Application of a clustering algorithm, an unsupervised machine learning technique, to the Pareto front developed from multi-objective optimization of maneuvering aircraft rotor system is the novelty of the present work. An in-house finite element code is developed using MATLAB for the analysis of rotor system. Hybrid Genetic Algorithm is employed to simultaneously minimize the rotor response at maximum speed during maneuver and rotor response at critical speed with restrictions imposed on critical speed. Shaft diameters and pedestal stiffness at both the bearing locations are identified as design variables. Pareto optimal solutions are generated, clustering is carried out in both objective space and decision space and the solution close to the utopia point is selected as final compromise solution. The average of the values of design variables for the selected cluster is compared with final compromise solution and is found in good agreement. The response of the rotor system and the critical speeds are verified by carrying out tests on ground.",10.1007/s10846-021-01511-1,Multi-objective optimization; Genetic algorithm; Clustering; Pareto front; Maneuvering,,
"Flood susceptibility mapping in Brahmaputra floodplain of Bangladesh using deep boost, deep learning neural network, and artificial neural network","Ahmed, N; Hoque, MAA; Arabameri, A; Pal, SC; Chakrabortty, R; Jui, J",GEOCARTO INTERNATIONAL,,"Floods are considered one of the most destructive natural hydro-meteorological disasters across the world. The present study attempts to assess flood susceptibility of the Brahmaputra floodplain of Bangladesh using Deep Boost, Deep Learning Neural Network, and Artificial Neural Network. Primarily, flood inventory maps were prepared from fieldworks and satellite image classification. Consequently, the flood locations were segregated into 70% training and 30% validation samples randomly for running the models and validating the models, respectively. The complete procedure is designed to be considered 12 flood conditioning criteria under four relevant components. The efficiency assessment of DLNN, DB, and ANN models using validation data through the area under the curve (AUC) reveals that DB demonstrates higher accuracy (AUC= .917) than DLNN (AUC = .901) and ANN (AUC= .895) approaches. Therefore, proposed susceptibility mapping approaches are efficient in assessing flood susceptibility accurately and can be implemented in other flood-affected regions.",10.1080/10106049.2021.2005698,Flood susceptibility; deep boost; deep learning; ANN; Brahmaputra flood-plain,,
MCR SVM classifier with group sparsity,"Liu, JW; Cui, LP; Luo, XL",OPTIK,2016.0,"Classification and dimensionality reduction of high-dimensional data are two important topics in bioinformatics, data mining and machine learning. We propose a novel sparse minimax concave ridge support vector machine (MCR SVM) classifier that simultaneously performs classification and dimensionality reduction. The MCR SVM classifier proposed in this study combines the advantages of the unbiasedness of the estimators of the SCAD SVM and the ability of feature group selection of HHSVM to overcome the disadvantages. We also provide a theoretical justification for the group sparsity of the selected features. The experiments on artificial highly correlated data and high-dimensional real-world data with a small sample size show that the MCR SVM classifier is a attractive technique of classification and dimensionality reduction and its performance is better than the other sparse SVMs. (C) 2016 Elsevier GmbH. All rights reserved.",10.1016/j.ijleo.2016.03.060,Sparsity; Feature selection; Group feature selection; MCR penalty; MCR SVM,,
Learning comprehensible and accurate hybrid trees,"Piltaver, R; Lustrek, M; Dzeroski, S; Gjoreski, M; Gams, M",EXPERT SYSTEMS WITH APPLICATIONS,2021.0,"Finding the best classifiers according to different criteria is often performed by a multi-objective machine learning algorithm. This study considers two criteria that are usually treated as the most important when deciding which classifier to apply in practice: comprehensibility and accuracy. A model that offers a broad range of trade-offs between the two criteria is introduced because they conflict; i.e., increasing one decreases the other. The choice of the model is motivated by the fact that domain experts often formalize decisions based on knowledge that can be represented by comprehensible rules and some tacit knowledge. This approach is mimicked by a hybrid tree that consists of comprehensible parts that originate from a regular classification tree and incomprehensible parts that originate from an accurate black-box classifier. An empirical evaluation on 23 UCI datasets shows that the hybrid trees provide trade-offs between the accuracy and comprehensibility that are not possible using traditional machine learning models. A corresponding hybrid-tree comprehensibility metric is also proposed. Furthermore, the paper presents a novel algorithm for learning MAchine LeArning Classifiers with HybrId TrEes (MALACHITE), and it proves that the algorithm finds a complete set of nondominated hybrid trees with regard to their accuracy and comprehensibility. The algorithm is shown to be faster than the well-known multi-objective evolutionary optimization algorithm NSGA-II for trees with moderate size, which is a prerequisite for comprehensibility. On the other hand, the MALACHITE algorithm can generate considerably larger hybrid-trees than a naive exhaustive search algorithm in a reasonable amount of time. In addition, an interactive iterative data mining process based on the algorithm is proposed that enables inspection of the Pareto set of hybrid trees. In each iteration, the domain expert analyzes the current set of nondominated hybrid trees, infers domain relations, and sets the parameters for the next machine learning step accordingly.",10.1016/j.eswa.2020.113980,Hybrid tree; Multi-objective learning; Comprehensibility; Accuracy; Classification,,
Linear-superelastic Ti-Nb nanocomposite alloys with ultralow modulus via high-throughput phase-field design and machine learning,"Zhu, YQ; Xu, T; Wei, QH; Mai, JW; Yang, HX; Zhang, HR; Shimada, T; Kitamura, T; Zhang, TY",NPJ COMPUTATIONAL MATERIALS,2021.0,"The optimal design of shape memory alloys (SMAs) with specific properties is crucial for the innovative application in advanced technologies. Herein, inspired by the recently proposed design concept of concentration modulation, we explore martensitic transformation (MT) in and design the mechanical properties of Ti-Nb nanocomposites by combining high-throughput phase-field simulations and machine learning (ML) approaches. Systematic phase-field simulations generate data of the mechanical properties for various nanocomposites constructed by four macroscopic degrees of freedom. An ML-assisted strategy is adopted to perform multiobjective optimization of the mechanical properties, through which promising nanocomposite configurations are prescreened for the next set of phase-field simulations. The ML-guided simulations discover an optimized nanocomposite, composed of Nb-rich matrix and Nb-lean nanofillers, that exhibits a combination of mechanical properties, including ultralow modulus, linear super-elasticity, and near-hysteresis-free in a loading-unloading cycle. The exceptional mechanical properties in the nanocomposite originate from optimized continuous MT rather than a sharp first-order transition, which is common in typical SMAs. This work demonstrates the great potential of ML-guided phase-field simulations in the design of advanced materials with extraordinary properties.",10.1038/s41524-021-00674-7,,,
An analytical approach for big social data analysis for customer decision-making in eco-friendly hotels,"Nilashi, M; Minaei-Bidgoli, B; Alrizq, M; Alghamdi, A; Alsulami, AA; Samad, S; Mohd, S",EXPERT SYSTEMS WITH APPLICATIONS,2021.0,"Sustainable tourism is an emerging trend around the world. Eco-friendly (green) hotels are environmentally friendly properties that are becoming more popular among green travellers. Electronic Word-of-Mouth (e-WOM) is a method of communicating with customers to share their experiences and is a powerful marketing tool for hotel marketing. This paper investigates the role of online reviews of eco-friendly hotels for preference learning using multi-criteria decision-making and machine learning techniques. We develop a new method using multicriteria decision making, supervised and unsupervised learning techniques. The Expectation-Maximization (EM) algorithm is used as an unsupervised learning technique to cluster travellers' online reviews. We use the Higher-Order Singular-Value Decomposition technique along with a similarity measure to find the most similar customers based on their preference. To predict travellers' preference for eco-friendly hotels, we employ a neurofuzzy system, the Adaptive Neuro-Fuzzy Inference System, as a supervised learning technique. To select the most important criteria, we use the entropy-weight approach in each segment. Several experiments were performed on the collected data from the Czech Republic's eco-friendly hotels on the TripAdvisor platform. The results demonstrated that the hybrid approach is effective for customers' segmentation, and preference learning and prediction in eco-friendly hotels.",10.1016/j.eswa.2021.115722,Big social data; Eco-friendly hotels; Sustainable tourism; Electronic word-of-mouth; Machine learning techniques,,
High Dimensional Restrictive Federated Model Selection with Multi-objective Bayesian Optimization over Shifted Distributions,"Sun, XD; Bommert, A; Pfisterer, F; Rahenfurher, J; Lang, M; Bischl, B","INTELLIGENT SYSTEMS AND APPLICATIONS, VOL 1",2020.0,"A novel machine learning optimization process coined Restrictive Federated Model Selection (RFMS) is proposed under the scenario, for example, when data from healthcare units can not leave the site it is situated on and it is forbidden to carry out training algorithms on remote data sites due to either technical or privacy and trust concerns. To carry out a clinical research in this scenario, an analyst could train a machine learning model only on local data site, but it is still possible to execute a statistical query at a certain cost in the form of sending a machine learning model to some of the remote data sites and get the performance measures as feedback, maybe due to prediction being usually much cheaper. Compared to federated learning, which is optimizing the model parameters directly by carrying out training across all data sites, RFMS trains model parameters only on one local data site but optimizes hyper parameters across other data sites jointly since hyper-parameters play an important role in machine learning performance. The aim is to get a Pareto optimal model with respective to both local and remote unseen prediction losses, which could generalize well across data sites. In this work, we specifically consider high dimensional data with different distributions over data sites. As an initial investigation, Bayesian Optimization especially multi-objective Bayesian Optimization is used to guide an adaptive hyper-parameter optimization process to select models under the RFMS scenario. Empirical results shows that solely using the local data site to tune hyper-parameters generalizes poorly across data sites, compared to methods that utilize the local and remote performances. Furthermore, in terms of hypervolumes, multi-objective Bayesian Optimization algorithms show increased performance across multiple data sites among other candidates.",10.1007/978-3-030-29516-5_48,Federated learning; Multi-objective Bayesian Optimization; High dimensional data; Differential privacy; Distribution shift; Model selection,,
SPRINT Multi-Objective Model Racing,"Zhang, TT; Georgiopoulos, M; Anagnostopoulos, GC",GECCO'15: PROCEEDINGS OF THE 2015 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE,2015.0,"Multi-objective model selection, which is an important aspect of Machine Learning, refers to the problem of identifying a set of Pareto optimal models from a given ensemble of models. This paper proposes SPRINT-Race, a multi-objective racing algorithm based on the Sequential Probability Ratio Test with an Indifference Zone. In SPRINT-Race, a non-parametric ternary-decision sequential analogue of the sign test is adopted to identify pair-wise dominance and non-dominance relationship. In addition, a Bonferroni approach is employed to control the overall probability of any erroneous decisions. In the fixed confidence setting, SPRINT-Race tries to minimize the computational effort needed to achieve a predefined confidence about the quality of the returned models. The efficiency of SPRTNT-Race is analyzed on artificially-constructed multi-objective model selection problems with known ground-truth. Moreover, SPRINT-Race is applied to identifying the Pareto optimal parameter settings of Ant Colony Optimization algorithms in the context of solving Traveling Salesman Problems. The experimental results confirm the advantages of SPRINT-Race for multi-objective model selection.",10.1145/2739480.2754791,Racing Algorithm; Model Selection; Multi-objective Optimization; Sequential Probability Ratio Test,,
"Utilization of machine-learning algorithms for wind turbine site suitability modeling in Iowa, USA","Petrov, AN; Wessling, JM",WIND ENERGY,2015.0,"Because of the current shift away from fossil fuels and toward renewable energy sources, it is necessary to plan for the installation of new infrastructure to meet the demand for clean energy. Traditional methods for determining wind turbine site suitability suffer from the selection of arbitrary criteria and model parameters by experts, which may lead to a degree of uncertainty in the models produced. An alternative empirically based methodology for building a wind turbine siting model for the state of Iowa is presented in the study. We employ ecological niche' principles traditionally utilized to model species allocation to develop a new multicriteria, spatially explicit framework for wind turbine placement. Using information on suitability conditions at existing turbine locations, we incorporate seven variables (wind speed, elevation, slope, land cover, distance of infrastructure and settlements, and population density) into two machine-learning algorithms [maximum entropy method (Maxent) and Genetic Algorithm for Rule Set Prediction (GARP)] to model suitable areas for installation of wind turbines. The performance of this method is tested at the statewide level and a six-county region in western Iowa. Maxent and GARP identified areas in the Northwest and North Central regions of Iowa as the optimum location for new wind turbines. Information on variable contributions from Maxent illuminates the relative importance of environmental variables and its scale-dependent nature. It also allows validating existing assumptions about the relationship between variables and wind turbine suitability. The resultant models demonstrate high levels of accuracy and suggest that the presented approach is a possible methodology for developing wind turbine siting applications. Copyright (c) 2014 John Wiley & Sons, Ltd.",10.1002/we.1723,wind energy; turbine site suitability; machine-learning; multicriteria decision making; spatial modeling,,
A Method for Entity Resolution in High Dimensional Data Using Ensemble Classifiers,"Liu, Y; Diao, XC; Cao, JJ; Zhou, X; Shang, YL",MATHEMATICAL PROBLEMS IN ENGINEERING,2017.0,"In order to improve utilization rate of high dimensional data features, an ensemble learning method based on feature selection for entity resolution is developed. Entity resolution is regarded as a binary classification problem, an optimization model is designed to maximize each classifier's classification accuracy and dissimilarity between classifiers and minimize cardinality of features. A modified multiobjective ant colony optimization algorithm is employed to solve the model for each base classifier, two pheromone matrices are set up, weighted product method is applied to aggregate values of two pheromone matrices, and feature's Fisher discriminant rate of records' similarity vector is calculated as heuristic information. A solution which is called complementary subset is selected from Pareto archive according to the descending order of three objectives to train the given base classifier. After training all base classifiers, their classification outputs are aggregated by max-wins voting method to obtain the ensemble classifiers' final result. A simulation experiment is carried out on three classical datasets. The results show the effectiveness of our method, as well as a better performance compared with the other two methods.",10.1155/2017/4953280,,,
A projection multi-objective SVM method for multi-class classification,"Liu, L; Martin-Barragan, B; Prieto, FJ",COMPUTERS & INDUSTRIAL ENGINEERING,2021.0,"Support Vector Machines (SVMs), originally proposed for classifications of two classes, have become a very popular technique in the machine learning field. For multi-class classifications, various single-objective models and multi-objective ones have been proposed. However,in most single-objective models, neither the different costs of different misclassifications nor the users' preferences were considered. This drawback has been taken into account in multi-objective models. In these models, large and hard second-order cone programs(SOCPs) were constructed ane weakly Pareto-optimal solutions were offered. In this paper, we propose a Projected Multi-objective SVM (PM), which is a multi-objective technique that works in a higher dimensional space than the object space. For PM, we can characterize the associated Pareto-optimal solutions. Additionally, it significantly alleviates the computational bottlenecks for classifications with large numbers of classes. From our experimental results, we can see PM outperforms the single-objective multi-class SVMs (based on an all-together method, one-against-all method and one-against-one method) and other multi-objective SVMs. Compared to the single-objective multi-class SVMs, PM provides a wider set of options designed for different misclassifications, without sacrificing training time. Compared to other multi-objective methods, PM promises the out-of-sample quality of the approximation of the Pareto frontier, with a considerable reduction of the computational burden.",10.1016/j.cie.2021.107425,Multiple objective programming; Support vector machine; Multi-class multi-objective SVM; Pareto-optimal solution,,
Sustainability assessment and modeling based on supervised machine learning techniques: The case for food consumption,"Abdella, GM; Kucukvar, M; Onat, NC; Al-Yafay, HM; Bulak, ME",JOURNAL OF CLEANER PRODUCTION,2020.0,"Sustainability of food consumption requires the understanding of multi-dimensional environmental, economic and social impacts using a holistic and integrated sustainability assessment and modeling framework. This article presents a novel method on the assessment and modeling of sustainability impacts of food consumption. First, sustainability impacts of food consumption categories are quantified using high sector resolution input-output tables of U.S. economy. Later, an integrated sustainability modeling framework based on two supervised machine-learning techniques such as k-means clustering and logistics regression is presented. The proposed framework involves five steps: (1) economic input-output life cycle sustainability assessment, (2) non-dimensional normalization, (3) sustainability performance evaluation, (4) centroid-based clustering analysis, and (5) sustainability impact modeling. The findings show that the supply chains of food production sectors are accounted for major environmental impacts with higher than 80% of portions for total carbon footprints. Animal slaughtering, rendering, and processing is found as the most dominant sector in most of the environmental impact categories. The logistic model results revealed an overall model accuracy equal to 91.67%. Furthermore, among all the environmental sustainability indicators, it has found that CO and SO2 are the most significant contributors. The results also show that 13.7% of the food and beverage sectors are clustered as high, in which the bread and bakery product manufacturing is the central sector. The large value of the variance (5.24) is attributed to the large total weighted impact value of the animal (except poultry) slaughtering, rendering, and processing cluster. (C) 2019 Elsevier Ltd. All rights reserved.",10.1016/j.jclepro.2019.119661,Input-output analysis; Sustainability indicators; Sustainability assessment and modeling; Supervised machine learning; Food consumption,,
"Data-driven lithology prediction for tight sandstone reservoirs based on new ensemble learning of conventional logs: A demonstration of a Yanchang member, Ordos Basin","Gu, YF; Zhang, DY; Lin, YB; Ruan, JF; Bao, ZD",JOURNAL OF PETROLEUM SCIENCE AND ENGINEERING,2021.0,"Lithologies are significant indicators to get deep insight of depositional and mineralogical properties of target formations, and the classic approach of achieving them is crossplot. Nonetheless, crossplot presents ineffectively when addressing classification of tight sandstone reservoirs, since most primary lithological components are characterized by similar logging responses. LightGBM (light gradient boosting machine) has been proved powerful to produce a remarkable classification, while its performance is seriously limited by the setting of hyper-parameters. LD-AFSA (linear decreasing-artificial fish swarm algorithm), an excellent solver for multiobjective optimization, then is introduced to modify the setting in a best circumstance. Besides, another integration for LightGBM is CRBM (continuous restricted Boltzmann machine), which specializes in generating less variables to speed up calculation. Consequently, a data-driven lithology predictor based on new ensemble learning is proposed, named CRBM-LD-AFSA-LightGBM. Data for validation of new predictor is cored by wells of Chang 4 + 5 member, Jiyuan Oilfield, Ordos Basin, and accordingly four experiments are designed to make a comprehensive evaluation. To highlight validating effect, SVM (support vector machine) and XGBoost (extreme gradient boosting) are adopted as competitors. Through comparison of experimental results, including prediction accuracy, F1-score, and AUC (area under curve), it is figured out that XGBoost-cored and LightGBM-cored predictors have capabilities to produce similar while more reliable results, meanwhile also exhibiting better generalization on prediction, but the computing time of latter predictor is only 1/25 shorter than that of the former. The results well demonstrate the proposed predictor plays a real high-efficient role in predicting lithologies and is deserved to receive a widespread employment in the field of logging interpretation because of its greater applicability.",10.1016/j.petrol.2021.109292,Lithology prediction; Tight sandstone reservoirs; Light gradient boosting machine; Continuous restricted Boltzmann machine; Artificial fish swarm algorithm; Linear decreasing,,
GGADN: Guided generative adversarial dehazing network,"Zhang, J; Dong, QQ; Song, WJ",SOFT COMPUTING,,"Image dehazing has always been a challenging topic in image processing. The development of deep learning methods, especially the generative adversarial networks (GAN), provides a new way for image dehazing. In recent years, many deep learning methods based on GAN have been applied to image dehazing. However, GAN has two problems in image dehazing. Firstly, For haze image, haze not only reduces the quality of the image but also blurs the details of the image. For GAN network, it is difficult for the generator to restore the details of the whole image while removing the haze. Secondly, GAN model is defined as a minimax problem, which weakens the loss function. It is difficult to distinguish whether GAN is making progress in the training process. Therefore, we propose a guided generative adversarial dehazing network (GGADN). Different from other generation adversarial networks, GGADN adds a guided module on the generator. The guided module verifies the network of each layer of the generator. At the same time, the details of the map generated by each layer are strengthened. Network training is based on the pre-trained VGG feature model and L1-regularized gradient prior which is developed by new loss function parameters. From the dehazing results of synthetic images and real images, the proposed method is better than the state-of-the-art dehazing methods.",10.1007/s00500-021-06049-w,Dehazing; Generative adversarial networks; Guidance; Loss function,,
Machine learning-assisted CO2 utilization in the catalytic dry reforming of hydrocarbons: Reaction pathways and multicriteria optimization analyses,"Lim, JY; Loy, ACM; Alhazmi, H; Fui, BCL; Cheah, KW; Taylor, MJ; Kyriakou, G; Yoo, CK",INTERNATIONAL JOURNAL OF ENERGY RESEARCH,,"The catalytic dry reforming (DR) process is a clean approach to transform CO2 into H-2 and CO-rich synthetic gas that can be used for various energy applications such as Fischer-Tropsch fuels production. A novel framework is proposed to determine the optimum reaction configurations and reaction pathways for DR of C-1-C-4 hydrocarbons via a reaction mechanism generator (RMG). With the aid of machine learning, the variation of thermodynamic and microkinetic parameters based on different reaction temperatures, pressures, CH4/CO2 ratios and catalytic surface, Pt(111), and Ni(111), were successfully elucidated. As a result, a promising multicriteria decision-making process, TOPSIS, was employed to identify the optimum reaction configuration with the trade-off between H-2 yield and CO2 reduction. Notably, the optimum conditions for the DR of C-1 and C-2 hydrocarbons were 800 degrees C at 3 atm on Pt(111); whereas C-3 and C-4 hydrocarbons found favor at 800 degrees C and 2 atm on Ni(111) to attain the highest H-2 yield and CO2 conversion. Based on the RMG-Cat (first-principle microkinetic database), the energy profile of the most selective reaction pathway network for the DR of CH4 on Pt(111) at 3 atm and 800 degrees C was deducted. The activation energy (E-a) for C-H bond dissociation via dehydrogenation on the Pt(111) was found to be 0.60 eV, lower than that reported previously for Ni(111), Cu(111), and Co(111) surfaces. The most endothermic reaction of the CH4 reforming process was found to be C3H3* + H2O* <-> OH* + C3H4 (218.74 kJ/mol).",10.1002/er.7565,catalytic dry reforming; CO2 utilization; density functional theory; hydrogen productionmachine learningreaction mechanism network,,
Investigating the impact of data normalization on classification performance,"Singh, D; Singh, B",APPLIED SOFT COMPUTING,2020.0,"Data normalization is one of the pre-processing approaches where the data is either scaled or transformed to make an equal contribution of each feature. The success of machine learning algorithms depends upon the quality of the data to obtain a generalized predictive model of the classification problem. The importance of data normalization for improving data quality and subsequently the performance of machine learning algorithms has been presented in many studies. But, the work lacks for the feature selection and feature weighting approaches, a current research trend in machine learning for improving performance. Therefore, this study aims to investigate the impact of fourteen data normalization methods on classification performance considering full feature set, feature selection, and feature weighting. In this paper, we also present a modified Ant Lion optimization that search feature subsets and the best feature weights along with the parameter of Nearest Neighbor Classifier. Experiments are performed on 21 publicly available real and synthetic datasets, and results are analyzed based on the accuracy, the percentage of feature reduced and runtime. It has been observed from the results that no single method outperforms others. Therefore, we have suggested a set of the best and the worst methods combining the normalization procedure and empirical analysis of results. The better performers are z-Score and Pareto Scaling for the full feature set and feature selection, and tanh and its variant for feature weighting. The worst performers are Mean Centered, Variable Stability Scaling and Median and Median Absolute Deviation methods along with un-normalized data. (C) 2019 Published by Elsevier B.V.",10.1016/j.asoc.2019.105524,Ant lion optimization; Data normalization; Feature selection; Feature weighting; k-NN classifier,,
Testing the Methodology Multicriteria Decision Aid - Constructivist (MCDA-C) in the construction of judicial decisions stability support algorithms,"Mendes, AJ; da Rosa, AM; da Rosa, IO",REVISTA BRASILEIRA DE DIREITO,2019.0,"The present article examine UNIVALI's PPGD and Neoway Informatica Ltda's Methodology Multicriteria Decision Aid - Constructivist or MCDA-C test partial results. Under MDCA-C perspective while logic-algorithmics base apply to judicial decisions, the central hypothesis is that MDCA-C is capable of transcending limitations of methodologic-judicial-algorithm. For than, this article seeks to test the capability of MDCA-C methodology of incorporating a decision maker's subjectivities, meanwhile keep the coherence and integrity necessary to replicate a judicial decision, in a multidisciplinary approach. Under an inductive procedural method, and also an interventive method of the MDCA-C, the present article combines big data, machine learning and deep learning techniques in order to propose a system calibration done by the magistrate itself. In this context, the application of MDCA-C to judicial decisions pursuit to reach a final product of such precision on wich a distinction of the human or machine decision isn't possible. Owing to national and international repercussion of this kind of application, also thru the possibility of revolutionizing the whole method and actuation of the Judiciary Power, the relevance of the theme becomes clear.",10.18256/2238-0604.2019.v15i2.3650,Methodology Multicriteria Decision Aid - Constructivist; Judiciary Power; Artificial Intelligence; Decision,,
Optimizing logistic regression coefficients for discrimination and calibration using estimation of distribution algorithms,"Robles, V; Bielza, C; Larranaga, P; Gonzalez, S; Ohno-Machado, L",TOP,2008.0,"Logistic regression is a simple and efficient supervised learning algorithm for estimating the probability of an outcome or class variable. In spite of its simplicity, logistic regression has shown very good performance in a range of fields. It is widely accepted in a range of fields because its results are easy to interpret. Fitting the logistic regression model usually involves using the principle of maximum likelihood. The Newton-Raphson algorithm is the most common numerical approach for obtaining the coefficients maximizing the likelihood of the data. This work presents a novel approach for fitting the logistic regression model based on estimation of distribution algorithms (EDAs), a tool for evolutionary computation. EDAs are suitable not only for maximizing the likelihood, but also for maximizing the area under the receiver operating characteristic curve (AUC). Thus, we tackle the logistic regression problem from a double perspective: likelihood-based to calibrate the model and AUC-based to discriminate between the different classes. Under these two objectives of calibration and discrimination, the Pareto front can be obtained in our EDA framework. These fronts are compared with those yielded by a multiobjective EDA recently introduced in the literature.",10.1007/s11750-008-0054-3,Logistic regression; Evolutionary algorithms; Estimation of distribution algorithms; Calibration and discrimination; 62J12; 90C59; 90C29,,
Models for predicting objective function weights in prostate cancer IMRT,"Boutilier, JJ; Lee, T; Craig, T; Sharpe, MB; Chan, TCY",MEDICAL PHYSICS,2015.0,"Purpose: To develop and evaluate the clinical applicability of advanced machine learning models that simultaneously predict multiple optimization objective function weights from patient geometry for intensity-modulated radiation therapy of prostate cancer. Methods: A previously developed inverse optimization method was applied retrospectively to determine optimal objective function weights for 315 treated patients. The authors used an overlap volume ratio (OV) of bladder and rectum for different PTV expansions and overlap volume histogram slopes (OVSR and OVSB for the rectum and bladder, respectively) as explanatory variables that quantify patient geometry. Using the optimal weights as ground truth, the authors trained and applied three prediction models: logistic regression (LR), multinomial logistic regression (MLR), and weighted K-nearest neighbor (KNN). The population average of the optimal objective function weights was also calculated. Results: The OV at 0.4 cm and OVSR at 0.1 cm features were found to be the most predictive of the weights. The authors observed comparable performance (i.e., no statistically significant difference) between LR, MLR, and KNN methodologies, with LR appearing to perform the best. All three machine learning models outperformed the population average by a statistically significant amount over a range of clinical metrics including bladder/rectum V53Gy, bladder/rectum V70Gy, and dose to the bladder, rectum, CTV, and PTV. When comparing the weights directly, the LR model predicted bladder and rectum weights that had, on average, a 73% and 74% relative improvement over the population average weights, respectively. The treatment plans resulting from the LR weights had, on average, a rectum V70Gy that was 35% closer to the clinical plan and a bladder V70Gy that was 29% closer, compared to the population average weights. Similar results were observed for all other clinical metrics. Conclusions: The authors demonstrated that the KNN and MLR weight prediction methodologies perform comparably to the LR model and can produce clinical quality treatment plans by simultaneously predicting multiple weights that capture trade-offs associated with sparing multiple OARs. (C) 2015 American Association of Physicists in Medicine.",10.1118/1.4914140,knowledge-based treatment planning; machine learning; inverse optimization,,
Accurate Energy and Performance Prediction for Frequency-Scaled GPU Kernels,"Fan, KJ; Cosenza, B; Juurlink, B",COMPUTATION,2020.0,"Energy optimization is an increasingly important aspect of today's high-performance computing applications. In particular, dynamic voltage and frequency scaling (DVFS) has become a widely adopted solution to balance performance and energy consumption, and hardware vendors provide management libraries that allow the programmer to change both memory and core frequencies manually to minimize energy consumption while maximizing performance. This article focuses on modeling the energy consumption and speedup of GPU applications while using different frequency configurations. The task is not straightforward, because of the large set of possible and uniformly distributed configurations and because of the multi-objective nature of the problem, which minimizes energy consumption and maximizes performance. This article proposes a machine learning-based method to predict the best core and memory frequency configurations on GPUs for an input OpenCL kernel. The method is based on two models for speedup and normalized energy predictions over the default frequency configuration. Those are later combined into a multi-objective approach that predicts a Pareto-set of frequency configurations. Results show that our approach is very accurate at predicting extema and the Pareto set, and finds frequency configurations that dominate the default configuration in either energy or performance.",10.3390/computation8020037,frequency scaling; energy efficiency; GPU; modeling,,
Multi-stage optimal scheduling of multi-microgrids using deep-learning artificial neural network and cooperative game approach,"Bidgoli, MA; Ahmadian, A",ENERGY,2022.0,"This article proposes a two-stage system for the daily energy management of micro-grids (MGs) in the presence of wind turbines, photovoltaic (PV) panels, and electrical energy storage systems (ESSs). Each MG uses historical data to predict its consumers' load demand, wind speed, and solar irradiance in the first stage. In the second stage, the cooperative game method is used to determine the MG's daily dispatch and energy transaction. The paper develops a prediction model using artificial neural network (ANN) and rough neuron water cycle (RNWC) algorithms, called deep learning artificial neural network (DLANN), which is a combination of technology from the artificial neural network and WC algorithm in order to predict uncertain parameters. The above model is implemented in the 33bus power distribution system; the simulation results show that the DLANN method provides more accurate predictions than the ANN method. The results also show that a MG can achieve energy cost savings through an alliance of MGs using the cooperative game approach. Furthermore, analysis of the impact of the ESS on the operation of the MG shows that the absence of the ESS will reduce the power output of the wind turbine. (c) 2021 Elsevier Ltd. All rights reserved.",10.1016/j.energy.2021.122036,Neural network; Game theory; Energy management of micro-grids; Energy storage system; Demand response; Forecasting,,
Application of Machine-Learning in Network-Level Road Maintenance Policy-Making: The Case of Iran,"Mahpour, A; El-Diraby, T",EXPERT SYSTEMS WITH APPLICATIONS,2022.0,"A framework was proposed to find optimal maintenance policies in a road network. The framework included: identifying factors that contribute to policy-making; clustering the network based on these factors; identifying criteria that impact optimal policies; and determining optimal policies and periods using these criteria. To test the framework applicability, it was applied to Iran roads network step-by-step. First, road functional class and climate were identified as factors that contribute to policy-making. Second, the network was clustered into six sub-networks based on two road functional classes and three climates. Third; maintenance cost, network condition, and maintenance period were identified as criteria that impact optimal policies. Fourth, 96 maintenance policies were applied to each sub-network considering two-year, four-year, six-year, and twelve-year maintenance periods. To quantify policies cost, seven machine-learning algorithms including Gradient Boosting Regression, Lasso, Ridge, Random Forest Regression, Elastic Net, Neural Network, and Multiple Linear Regression were tested. Using the coefficient of determination (R-2) as the accuracy metric, it was found that in all sub-networks the Gradient Boosting Regression had the highest accuracy on testing set (greater than 90%) while that of other algorithms was between 50% and 90%. Sub-networks condition was modeled using the Markov Chain model and was measured by the average Pavement Condition Index (PCI). Having policies cost and sub-networks PCI, the optimal policy was selected using the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS). It was concluded that in all sub-networks, the four-year maintenance period was optimal. Roads in warm zone demanded the most intense policies, followed by those in cold and humid zones. The same applied to arterial roads followed by local ones.",10.1016/j.eswa.2021.116283,Road maintenance policy-making; Maintenance cost optimization; Network PCI; Gradient boosting regression; Markov Chain model; TOPSIS,,
Understanding and Improving Fairness-Accuracy Trade-offs in Multi-Task Learning,"Wang, YY; Wang, XZ; Beutel, A; Prost, F; Chen, JL; Chi, EH",KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING,2021.0,"As multi-task models gain popularity in a wider range of machine learning applications, it is becoming increasingly important for practitioners to understand the fairness implications associated with those models. Most existing fairness literature focuses on learning a single task more fairly, while how ML fairness interacts with multiple tasks in the joint learning setting is largely under-explored. In this paper, we are concerned with how group fairness (e.g., equal opportunity, equalized odds) as an ML fairness concept plays out in the multi-task scenario. In multi-task learning, several tasks are learned jointly to exploit task correlations for a more efficient inductive transfer. This presents a multi-dimensional Pareto frontier on (1) the trade-off between group fairness and accuracy with respect to each task, as well as (2) the trade-offs across multiple tasks. We aim to provide a deeper understanding on how group fairness interacts with accuracy in multi-task learning, and we show that traditional approaches that mainly focus on optimizing the Pareto frontier of multi-task accuracy might not perform well on fairness goals. We propose a new set of metrics to better capture the multi-dimensional Pareto frontier of fairness-accuracy tradeoffs uniquely presented in a multi-task learning setting. We further propose a Multi-Task-Aware Fairness (MTA-F) approach to improve fairness in multi-task learning. Experiments on several real-world datasets demonstrate the effectiveness of our proposed approach.",10.1145/3447548.3467326,fairness; multi-task learning; Pareto frontier; multi-task-aware fairness treatment,,
"Predicting the performance of polyvinylidene fluoride, polyethersulfone and polysulfone filtration membranes using machine learning","Liu, TL; Liu, LY; Cui, FC; Ding, F; Zhang, QF; Li, YQ",JOURNAL OF MATERIALS CHEMISTRY A,2020.0,"Micro/ultra/nano-filtration membranes based on polyvinylidene fluoride, polyethersulfone and polysulfone are advancing steadily in laboratory research and scalable production. Variables associated with the composition, fabrication, and operation are highly diverse, and their quantitative correlations with the core performance in permeability, selectivity and their trade-off are still elusive. To predict the performance based on a comprehensive dataset with 1895 vectors, the coefficient of determination spans from 0.79 to 0.85 for regression models, and the area under the receiver operating characteristic curve (AUC) reaches from 0.94 to 0.97 for classification models to distinguish the top 20% (Pareto set) and top 50% (balanced set) membranes with superior performance in the separation of macromolecules and salts from water. Further including experimental structural information (porosity, thickness, surface contact angle and roughness) brings significant improvement for regression models, while filling with predicted values only shows marginal improvement. A standalone algorithm that integrated the predictive models was released at https://github.com/polySML/polySML to facilitate the development of advanced filtration membranes through virtual experiments.",10.1039/d0ta07607d,,,
Game playing,"Rosin, CD",WILEY INTERDISCIPLINARY REVIEWS-COGNITIVE SCIENCE,2014.0,"Game playing has been a core domain of artificial intelligence research since the beginnings of the field. Game playing provides clearly defined arenas within which computational approaches can be readily compared to human expertise through head-to-head competition and other benchmarks. Game playing research has identified several simple core algorithms that provide successful foundations, with development focused on the challenges of defeating human experts in specific games. Key developments include minimax search in chess, machine learning from self-play in backgammon, and Monte Carlo tree search in Go. These approaches have generalized successfully to additional games. While computers have surpassed human expertise in a wide variety of games, open challenges remain and research focuses on identifying and developing new successful algorithmic foundations. WIREs Cogn Sci 2014, 5:193-205. doi: 10.1002/wcs.1278 Conflict of interest: The author has declared no conflicts of interest for this article. For further resources related to this article, please visit the .",10.1002/wcs.1278,,,
Strong Metric (Sub)regularity of Karush-Kuhn-Tucker Mappings for Piecewise Linear-Quadratic Convex-Composite Optimization and the Quadratic Convergence of Newton's Method,"Burke, JV; Engle, A",MATHEMATICS OF OPERATIONS RESEARCH,2020.0,"This work concerns the local convergence theory of Newton and quasi-Newton methods for convex composite optimization: where one minimizes an objective that can be written as the composition of a convex function with one that is continuiously differentiable. We focus on the case in which the convex function is a potentially infinite-valued piecewise linear-quadratic function. Such problems include nonlinear programming, minimax optimization, and estimation of nonlinear dynamics with non-Gaussian noise as well as many modem approaches to large-scale data analysis and machine learning. Our approach embeds the optimality conditions for convex-composite optimization problems into a generalized equation. We establish conditions for strong metric subregularity and strong metric regularity of the corresponding set-valued mappings. This allows us to extend classical convergence of Newton and quasi-Newton methods to the broader class of nonfinite valued piecewise linear quadratic convex-composite optimization problems. In particular, we establish local quadratic convergence of the Newton method under conditions that parallel those in nonlinear programming.",10.1287/moor.2019.1027,convex-composite optimization; generalized equations; Newton's method; quasi-Newton methods; partial smoothness; piecewise linear-quadratic; strong metric subregularity; strong metric regularity; quadratic convergence,,
Multi-objective evolutionary algorithms for fuzzy classification in survival prediction,"Jimenez, F; Sanchez, G; Juarez, JM",ARTIFICIAL INTELLIGENCE IN MEDICINE,2014.0,"Objective: This paper presents a novel rule-based fuzzy classification methodology for survival/mortality prediction in severe burnt patients. Due to the ethical aspects involved in this medical scenario, physicians tend not to accept a computer-based evaluation unless they understand why and how such a recommendation is given. Therefore, any fuzzy classifier model must be both accurate and interpretable. Methods and materials: The proposed methodology is a three-step process: (1) multi-objective constrained optimization of a patient's data set, using Pareto-based elitist multi-objective evolutionary algorithms to maximize accuracy and minimize the complexity (number of rules) of classifiers, subject to interpretability constraints; this step produces a set of alternative (Pareto) classifiers; (2) linguistic labeling, which assigns a linguistic label to each fuzzy set of the classifiers; this step is essential to the interpretability of the classifiers; (3) decision making, whereby a classifier is chosen, if it is satisfactory, according to the preferences of the decision maker. If no classifier is satisfactory for the decision maker, the process starts again in step (1) with a different input parameter set. Results: The performance of three multi-objective evolutionary algorithms, niched pre-selection multi-objective algorithm, elitist Pareto-based multi-objective evolutionary algorithm for diversity reinforcement (ENORA) and the non-dominated sorting genetic algorithm (NSGA-II), was tested using a patient's data set from an intensive care burn unit and a standard machine learning data set from an standard machine learning repository. The results are compared using the hypervolume multi-objective metric. Besides, the results have been compared with other non-evolutionary techniques and validated with a multi-objective cross-validation technique. Our proposal improves the classification rate obtained by other non-evolutionary techniques (decision trees, artificial neural networks, Naive Bayes, and case-based reasoning) obtaining with ENORA a classification rate of 0.9298, specificity of 0.9385, and sensitivity of 0.9364, with 14.2 interpretable fuzzy rules on average. Conclusions: Our proposal improves the accuracy and interpretability of the classifiers, compared with other non-evolutionary techniques. We also conclude that ENORA outperforms niched pre-selection and NSGA-II algorithms. Moreover, given that our multi-objective evolutionary methodology is non-combinational based on real parameter optimization, the time cost is significantly reduced compared with other evolutionary approaches existing in literature based on combinational optimization. (C) 2014 Elsevier B.V. All rights reserved.",10.1016/j.artmed.2013.12.006,Fuzzy classification; Multi-objective evolutionary computation; Severity scores; Intensive care burns unit,,
A Reinforcement Learning based evolutionary multi-objective optimization algorithm for spectrum allocation in Cognitive Radio networks,"Kaur, A; Kumar, K",PHYSICAL COMMUNICATION,2020.0,"To cope up with drastically increasing demand for radio resources lead to raise a challenge to the wireless community. The limited radio spectrum and fixed spectrum allocation strategy have become a bottleneck for various wireless communication. Cognitive Radio (CR) technology along with potential benefits of machine learning has attracted substantial research interest especially in the context of spectrum management. However, a variety of performance attributes as objectives draw attention during the technological preparations for spectrum management such as higher spectral efficiency, lower latency, higher network capacity, and better energy efficiency as these objectives are often conflicting with each other. Hence, this paper addresses the spectrum allocation problem concerning network capacity and spectrum efficiency as conflicting objectives and model the scenario as a multi objective optimization problem in CR networks. An improved version of the Non-dominated Sorting Genetic Algorithm-II (NSGA-II) which combines the feature of evolutionary algorithm and machine learning called Non-dominated Sorting Genetic Algorithm based on Reinforcement Learning (NSGARL) is proposed which incorporates a self-tuning parameter approach to handle multiple conflicting objectives. The numerical findings validate the effectiveness of the proposed algorithm through the Pareto optimal set and obtain optimal solution efficiently to satisfy various requirements of spectrum allocation in CR networks. (c) 2020 Elsevier B.V. All rights reserved.",10.1016/j.phycom.2020.101196,Cognitive Radio (CR) networks; Multi-objective optimization; NSGA-II; NSGA-RL; Reinforcement learning; Spectrum allocation,,
Bayesian rank penalization,"Tang, KW; Su, ZX; Zhang, J; Cui, LH; Jiang, W; Luo, XN; Sun, XY",NEURAL NETWORKS,2019.0,"Rank minimization is a key component of many computer vision and machine learning methods, including robust principal component analysis (RPCA) and low-rank representations (LRR). However, usual methods rely on optimization to produce a point estimate without characterizing uncertainty in this estimate, and also face difficulties in tuning parameter choice. Both of these limitations are potentially overcome with Bayesian methods, but there is currently a lack of general purpose Bayesian approaches for rank penalization. We address this gap using a positive generalized double Pareto prior, illustrating the approach in RPCA and LRR. Posterior computation relies on hybrid Gibbs sampling and geodesic Monte Carlo algorithms. We assess performance in simulation examples, and benchmark data sets. (C) 2019 Elsevier Ltd. All rights reserved.",10.1016/j.neunet.2019.04.018,Bayesian model; Generalized double Pareto; LRR; Low-rank; RPCA,,
An improved steganography without embedding based on attention GAN,"Yu, C; Hu, DH; Zheng, SL; Jiang, WJ; Li, M; Zhao, ZQ",PEER-TO-PEER NETWORKING AND APPLICATIONS,2021.0,"Steganography is an art to hide information in the carriers to prevent from being detected, while steganalysis is the opposite art to detect the presence of the hidden information. With the development of deep learning, several state-of-the-art steganography and steganalysis based on deep learning techniques have been proposed to improve hiding or detection capabilities. Generative Adversarial Networks (GANs) based steganography directly uses the minimax game between the generator and discriminator, to automatically generate steganography algorithms resisting being detected by powerful steganalysis. The steganography without embedding (SwE) based on GANs, where the generated cover images themselves are stego images carrying secret information has shown its state-of-the-art steganography performance. However, SwE based on GANs has serious weaknesses, such as low information recovery accuracy, low steganography capacity and poor natural showing. To solve these problems, this paper proposes a new SwE based on attention-GAN model, with carefully designed generator, discriminator and extractor, as well as their loss functions and optimized training mode. The generative model utilizes the attention method to improve the correlation among pixels and to correct errors such as image distortion and background abnormality. The soft margin discriminator is used to improve the compatibility of information recovery and fault tolerance of image generation. Experimental evaluations show that our method can achieve a very high information recovery accuracy (100% in some cases), and at the same time improve the steganography capacity and image quality.",10.1007/s12083-020-01033-x,Steganography; Steganography without embedding; Generative adversarial networks; Attention; Soft margin,,
Predictive Maintenance: An Autoencoder Anomaly-Based Approach for a 3 DoF Delta Robot,"Fathi, K; van de Venn, HW; Honegger, M",SENSORS,2021.0,"Performing predictive maintenance (PdM) is challenging for many reasons. Dealing with large datasets which may not contain run-to-failure data (R2F) complicates PdM even more. When no R2F data are available, identifying condition indicators (CIs), estimating the health index (HI), and thereafter, calculating a degradation model for predicting the remaining useful lifetime (RUL) are merely impossible using supervised learning. In this paper, a 3 DoF delta robot used for pick and place task is studied. In the proposed method, autoencoders (AEs) are used to predict when maintenance is required based on the signal sequence distribution and anomaly detection, which is vital when no R2F data are available. Due to the sequential nature of the data, nonlinearity of the system, and correlations between parameter time-series, convolutional layers are used for feature extraction. Thereafter, a sigmoid function is used to predict the probability of having an anomaly given CIs acquired from AEs. This function can be manually tuned given the sensitivity of the system or optimized by solving a minimax problem. Moreover, the proposed architecture can be used for fault localization for the specified system. Additionally, the proposed method can calculate RUL using Gaussian process (GP), as a degradation model, given HI values as its input.",10.3390/s21216979,predictive maintenance; anomaly detection; autoencoder; gaussian processes; deep learning; data-driven maintenance,,
"Assessing the frontier: Active learning, model accuracy, and multi-objective candidate discovery and optimization","del Rosario, Z; Rupp, M; Kim, Y; Antono, E; Ling, J",JOURNAL OF CHEMICAL PHYSICS,2020.0,"Discovering novel chemicals and materials can be greatly accelerated by iterative machine learning-informed proposal of candidates-active learning. However, standard global error metrics for model quality are not predictive of discovery performance and can be misleading. We introduce the notion of Pareto shell error to help judge the suitability of a model for proposing candidates. Furthermore, through synthetic cases, an experimental thermoelectric dataset and a computational organic molecule dataset, we probe the relation between acquisition function fidelity and active learning performance. Results suggest novel diagnostic tools, as well as new insights for the acquisition function design. (c) 2020 Author(s). All article content, except where otherwise noted, is licensed under a Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).",10.1063/5.0006124,,,
Discovery of Energy Storage Molecular Materials Using Quantum Chemistry-Guided Multiobjective Bayesian Optimization,"Agarwal, G; Doan, HA; Robertson, LA; Zhang, L; Assary, RS",CHEMISTRY OF MATERIALS,2021.0,"Redox flow batteries (RFBs) are a promising technology for stationary energy storage applications due to their flexible design, scalability, and low cost. In RFBs, energy is carried in flowable redoxactive materials (redoxmers) which are stored externally and pumped to the cell during operation. Further improvements in the energy density of RFBs necessitates redoxmer designs with wider redox potential windows and higher solubility. Additionally, designing redoxmers with a fluorescence-enabled self-reporting functionality allows monitoring of the state of health of RFBs. To accelerate the discovery of redoxmers with desired properties, state-of-the- art machine learning ( ML) methods, such as multiobjective Bayesian optimization (MBO), are useful. Here, we first employed density functional theory calculations to generate a database of reduction potentials, solvation free energies, and absorption wavelengths for 1400 redoxmer molecules based on a 2,1,3-benzothiadiazole (BzNSN) core structure. From the computed properties, we identified 22 Pareto-optimal molecules that represent best trade-off among all of the desired properties. We further utilized these data to develop and benchmark an MBO approach to identify candidates quickly and efficiently with multiple targeted properties. With MBO, optimal candidates from the 1400-molecule data set can be identified at least 15 times more efficiently compared to the brute force or random selection approach. Importantly, we utilized this approach for discovering promising redoxmers from an unseen database of 1 million BzNSN-based molecules, where we discovered 16 new Pareto-optimal molecules with significant improvements in properties over the initial 1400 molecules. We anticipate that this active learning technique is general and can be utilized for the discovery of any class of functional materials that satisfies multiple desired property criteria.",10.1021/acs.chemmater.1c02040,,,
A robust method for safety evaluation of steel trusses using Gradient Tree Boosting algorithm,"Truong, VH; Vu, QV; Thai, HT; Ha, MH",ADVANCES IN ENGINEERING SOFTWARE,2020.0,"In this study, an efficient method is proposed for the safety evaluation of steel trusses using the gradient tree boosting (GTB) algorithm, one of the most powerful techniques in machine learning (ML). Datasets are first generated using the advanced analysis to consider both geometric and material nonlinearities of the structure. Four GTB models are then proposed to predict the ultimate load-carrying capacity and displacement of the structure for safety evaluation of strength and serviceability. Both continuous and discrete input variables are considered. To demonstrate the efficiency of the proposed method, four popular ML methods including support vector machines (SVM), decision tree (DT), random forest (RF), and deep learning (DL) are refereed in a comparison study. Three numerical examples of steel truss structures including a planar truss, a spatial truss, and a case study of planar truss bridge are considered in the comparative study. The numerical results show that the developed GTB models provide high accurate (more than 90%) regardless of the number of training data and design variable types and have the best performance in most considered cases.",10.1016/j.advengsoft.2020.102825,Gradient Tree Boosting algorithm; Machine learning; Deep learning; Advanced analysis; Nonlinear inelastic analysis; Steel truss,,
"Newly explored machine learning model for river flow time series forecasting at Mary River, Australia","Cui, F; Salih, SQ; Choubin, B; Bhagat, SK; Samui, P; Yaseen, ZM",ENVIRONMENTAL MONITORING AND ASSESSMENT,2020.0,"Hourly river flow pattern monitoring and simulation is the indispensable precautionary task for river engineering sustainability, water resource management, flood risk mitigation, and impact reduction. Reliable river flow forecasting is highly emphasized to support major decision-makers. This research paper adopts a new implementation approach for the application of a river flow prediction model for hourly prediction of the flow of Mary River in Australia; a novel data-intelligent model called emotional neural network (ENN) was used for this purpose. A historical dataset measured over a 4-year period (2011-2014) at hourly timescale was used in building the ENN-based predictive model. The results of the ENN model were validated against the existing approaches such as the minimax probability machine regression (MPMR), relevance vector machine (RVM), and multivariate adaptive regression splines (MARS) models. The developed models are evaluated against each other for validation purposes. Various numerical and graphical performance evaluators are conducted to assess the predictability of the proposed ENN and the competitive benchmark models. The ENN model, used as an objective simulation tool, revealed an outstanding performance when applied for hourly river flow prediction in comparison with the other benchmark models. However, the order of the model, performance wise, is ENN > MARS > RVM > MPMR. In general, the present results of the proposed ENN model reveal a promising modeling strategy for the hourly simulation of river flow, and such a model can be explored further for its ability to contribute to the state-of-the-art of river engineering and water resources monitoring and future prediction at near real-time forecast horizons.",10.1007/s10661-020-08724-1,River flow monitoring; Water resources management; Emotional neural network; Machine learning,,
Effect of Deterministic and Continuous Design Space Resolution on Multiple-Objective Combustor Optimization,"Briones, AM; Rumpfkeil, MP; Thomas, NR; Rankin, BA",JOURNAL OF ENGINEERING FOR GAS TURBINES AND POWER-TRANSACTIONS OF THE ASME,2019.0,"Supervised machine learning is used to classify a continuous and deterministic design space into a nondominated Pareto frontier and dominated design points. The effect of the initial training data quantity on the Pareto frontier and output parameter sensitivity is explored. The study is performed with the optimization of a subsonic small-scale cavity-stabilized combustor. A 3D geometry is created and parameterized using computer aided design (CAD) that is combined with a software for meshing, which automatically transfers grids and boundary conditions to the solver and postprocessing tool. Steady, compressible three-dimensional simulations are conducted employing a multiphase Realizable k-epsilon Reynolds-averaged Navier-Stokes (RANS) physics with an adiabatic flamelet progress variable (FPV) model. The near-wall turbulence modeling is computed with scalable wall functions (SWFs). For each computational fluid dynamics (CFD) simulation, four levels of adaptive mesh refinement (AMR) are utilized on the original cut-cell grid. There are 15 geometrical input parameters and three output parameters, viz., a pattern factor proxy, a combustion efficiency proxy, and total pressure loss (TPL). Three times the number of input parameters plus one (48) is necessary to yield an optimization independent of the initial sampling. This conclusion is drawn by examining and comparing the Pareto frontiers and global sensitivities. However, the latter provides a better metric. The relative influence of the input parameters on the outputs is assessed by Spearman's order-rank correlation and an active subspace analysis. Some persistent geometric features for nondominated designs are also discussed.",10.1115/1.4045284,,,
A systematic comparison of machine learning methods for modeling of dynamic processes applied to combustion emission rate modeling,"Tuttle, JF; Blackburn, LD; Andersson, K; Powell, KM",APPLIED ENERGY,2021.0,"Ten established, data-driven dynamic algorithms are surveyed and a practical guide for understanding these methods generated. Existing Python programming packages for implementing each algorithm are acknowledged, and the model equations necessary for prediction are presented. A case study on a coal-fired power plant's NOx emission rates is performed, directly comparing each modeling method's performance on a mutual system. Each model is evaluated by its root mean squared error (RMSE) on out-of-sample future horizon predictions. Optimal hyperparameters are identified using either an exhaustive search or genetic algorithm. The top five model structures of each method are used to recursively predict future NOx emission rates over a 60-step time horizon. The RMSE at each future timestep is determined, and the recursive output prediction trends compared against measurements in time. The GRU neural network is identified as the best candidate for representing the system, demonstrating accurate and stable predictions across the future horizon by all considered models, while satisfactory performance was observed in several of the ARX/NARX formulations. These efforts have contributed 1) a concise resource of multiple proven dynamic machine learning methods, 2) a practical guide explaining the use of these methods, effectively lowering the barrier-to-entry of deploying such models in control systems, 3) a comparison study evaluating each method's performance on a mutual system, 4) demonstration of accurate multi-timestep emissions modeling suitable for systems-level control, and 5) generalizable results demonstrating the suitability of each method for prediction over a multi-step future horizon to other complex dynamic systems.",10.1016/j.apenergy.2021.116886,Nonlinear dynamical systems; Intelligent systems; Computational intelligence; Recurrent neural networks; Combustion modeling & optimization; NOx emissions,,
Improved PSO for optimizing the performance of intrusion detection systems,"Dickson, A; Thomas, C",JOURNAL OF INTELLIGENT & FUZZY SYSTEMS,2020.0,"Intrusion detection system is a second layer of defence in a secured network environment. When comes to an IoT platform, the role of IDS is very critical since it is highly vulnerable to security threats. For a trustworthy intrusion detection system in a network, it is necessary to improve the true positives with minimum false positives. Research reveals that the true positive and false positive are conflicting objectives that are to be simultaneously optimized and hence their trade-off always exists as a major challenge. This paper presents a method to solve the tradeoff among these conflicting objectives using multi-objective particle swarm optimization approach. We conducted empirical analysis of the system with multiple machine learning classifiers. Experimental results reveals that this technique with J48 classifier gives the highest gbest value 10.77 with minimum optimum value of false positive 0.02 and maximum true positive 0.995. Empirical evaluation shows an incredible improvement in Pareto set in the objective function space by attaining an optimum point.",10.3233/JIFS-179734,Intrusion detection system; receiver operating characteristics; particle swarm optimization; pareto front; multi-objective optimization; non-linear programming,,
The new optimization algorithm for the vehicle routing problem with time windows using multi-objective discrete learnable evolution model,"Moradi, B",SOFT COMPUTING,2020.0,"This paper presents a new multi-objective discreet learnable evolution model (MODLEM) to address the vehicle routing problem with time windows (VRPTW). Learnable evolution model (LEM) includes a machine learning algorithm, like the decision trees, that can discover the correct directions of the evolution leading to significant improvements in the fitness of the individuals. We incorporate a robust strength Pareto evolutionary algorithm in the LEM presented here to govern the multi-objective property of this approach. A new priority-based encoding scheme for chromosome representation in the LEM as well as corresponding routing scheme is introduced. To improve the quality and the diversity of the initial population, we propose a novel heuristic manner which leads to a good approximation of the Pareto fronts within a reasonable computational time. Moreover, a new heuristic operator is employed in the instantiating process to confront incomplete chromosome formation. Our proposed MODLEM is tested on the problem instances of Solomon's VRPTW benchmark. The performance of this proposed MODLEM for the VRPTW is assessed against the state-of-the-art approaches in terms of both the quality of solutions and the computational time. Experimental results and comparisons indicate the effectiveness and efficiency of our proposed intelligent routing approach.",10.1007/s00500-019-04312-9,Vehicle routing problem with time windows (VRPTW); Learnable evolution model (LEM); Multi-objective combinatorial optimization (MOCO); Strength Pareto evolutionary algorithm (SPEA),,
Predictable GPUs Frequency Scaling for Energy and Performance,"Fan, KJ; Cosenza, B; Juurlink, B",PROCEEDINGS OF THE 48TH INTERNATIONAL CONFERENCE ON PARALLEL PROCESSING (ICPP 2019),2019.0,"Dynamic voltage and frequency scaling (DVFS) is an important solution to balance performance and energy consumption, and hardware vendors provide management libraries that allow the programmer to change both memory and core frequencies. The possibility to manually set these frequencies is a great opportunity for application tuning, which can focus on the best application-dependent setting. However, this task is not straightforward because of the large set of possible configurations and because of the multi-objective nature of the problem, which minimizes energy consumption and maximizes performance. This paper proposes a method to predict the best core and memory frequency configurations on GPUs for an input OpenCL kernel. Our modeling approach, based on machine learning, first predicts speedup and normalized energy over the default frequency configuration. Then, it combines the two models into a multi-objective one that predicts a Pareto-set of frequency configurations. The approach uses static code features, is built on a set of carefully designed micro-benchmarks, and can predict the best frequency settings of a new kernel without executing it. Test results show that our modeling approach is very accurate on predicting extrema points and Pareto set for ten out of twelve test benchmarks, and discover frequency configurations that dominate the default configuration in either energy or performance.",10.1145/3337821.3337833,Frequency scaling; Energy efficiency; GPUs; Modeling,,
Multi-objective online optimization of a marine diesel engine using NSGA-II coupled with enhancing trained support vector machine,"Niu, XX; Wang, HC; Hu, S; Yang, CL; Wang, YY",APPLIED THERMAL ENGINEERING,2018.0,"The multi-objective optimization problems of diesel engines are always challenging for the engineers, especially with the application of new technologies that aim at improving the engine performance and emissions. This paper proposed a novel online optimization approach using NSGA-II coupled with a machine learning method (SVM). The proposed online optimization approach was conducted based on an engine physical model, which was calibrated and validated carefully using experimental data. In the optimization process, the engine physical model is used as a substitute of real engine to generate training data for the SVM and validate the accuracy of the optimization results; SVM, with fast computing speed, undertakes the massive calculating workloads of fitness evaluation on searching the Pareto optimal solutions. Moreover, this paper proposed an enhancing training method to guarantee the accuracy of SVM model. When applying on a marine diesel engine, the proposed online optimization approach has demonstrated its reliability and high efficiency. In addition, with fast computing speed, the well trained SVM model can develop the engine responses maps rapidly. Eventually, based on the Pareto-optimal solutions obtained by the proposed optimization approach, combining with the maps, the solving of multi-objective optimization problems will be significantly facilitated.",10.1016/j.applthermaleng.2018.03.080,Multi-objective optimization; Marine diesel engine; NSGA-II; SVM; Pareto-optimal solution,,
Investigation of a Bridge Pier Scour Prediction Model for Safe Design and Inspection,"Kim, I; Fard, MY; Chattopadhyay, A",JOURNAL OF BRIDGE ENGINEERING,2015.0,"A novel bridge scour estimation approach that comprises advantages of both empirical and data-driven models is developed here. Results from the new approach are compared with existing approaches. Two field datasets from the literature are used in this study. Support vector machine (SVM), which is a machine-learning algorithm, is used to increase the pool of field data samples. For a comprehensive understanding of bridge-pier-scour modeling, a model evaluation function is suggested using an orthogonal projection method on a model performance plot. A fast nondominated sorting genetic algorithm (NSGA-II) is evaluated on the model performance objective functions to search for Pareto optimal fronts. The proposed formulation is compared with two selected empirical models [Hydraulic Engineering Circular No. 18 (HEC-18) and Froehlich equation] and a recently developed data-driven model (gene expression programming model). Results show that the proposed model improves the estimation of critical scour depth compared with the other models.",10.1061/(ASCE)BE.1943-5592.0000677,Bridges; Scour; Support vector machine (SVM); Multiobjective optimization,,
OPTIMAL ASSEMBLY OF SUPPORT VECTOR REGRESSORS WITH APPLICATION TO SYSTEM MONITORING,"Alamaniotis, M; Ikonomopoulos, A; Tsoukalas, LH",INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS,2012.0,Power plants are high complexity systems running risks of low frequency but high consequence. The field of machine learning appears to offer the necessary tools for developing automated instrument surveillance systems supporting decision-making in critical systems such as power stations. A novel prediction method is presented with the aim to enhance system safety and performance by making an ahead-of-time prediction of the status of fundamental system components and subsequent detection of abnormalities. The utilization of a linear assembly of support vector regressors employing unique kernels is proposed in a hybrid computational scheme that encompasses the formulation of a multi-objective optimization problem addressed with an evolutionary algorithm that employs Pareto theory to identify an optimal solution. The approach is tested on the ahead of time prediction of the crack length in power plant turbine blades utilizing historical data. The results obtained highlight the efficiency of the proposed methodology since better performance over the standalone support vector regressors is observed.,10.1142/S0218213012500340,SVR; multi-objective optimization; Pareto optimal; parameter monitoring,,
A dual-objective evolutionary algorithm for rules extraction in data mining,"Tan, KC; Yu, Q; Ang, JH",COMPUTATIONAL OPTIMIZATION AND APPLICATIONS,2006.0,"This paper presents a dual-objective evolutionary algorithm (DOEA) for extracting multiple decision rule lists in data mining, which aims at satisfying the classification criteria of high accuracy and ease of user comprehension. Unlike existing approaches, the algorithm incorporates the concept of Pareto dominance to evolve a set of non-dominated decision rule lists each having different classification accuracy and number of rules over a specified range. The classification results of DOEA are analyzed and compared with existing rule-based and non-rule based classifiers based upon 8 test problems obtained from UCI Machine Learning Repository. It is shown that the DOEA produces comprehensible rules with competitive classification accuracy as compared to many methods in literature. Results obtained from box plots and t-tests further examine its invariance to random partition of datasets.",10.1007/s10589-005-3907-9,data mining; evolutionary algorithm; classification; rules extraction,,
Performant implementation of the atomic cluster expansion (PACE) and application to copper and silicon,"Lysogorskiy, Y; van der Oord, C; Bochkarev, A; Menon, S; Rinaldi, M; Hammerschmidt, T; Mrovec, M; Thompson, A; Csanyi, G; Ortner, C; Drautz, R",NPJ COMPUTATIONAL MATERIALS,2021.0,"The atomic cluster expansion is a general polynomial expansion of the atomic energy in multi-atom basis functions. Here we implement the atomic cluster expansion in the performant C++ code PACE that is suitable for use in large-scale atomistic simulations. We briefly review the atomic cluster expansion and give detailed expressions for energies and forces as well as efficient algorithms for their evaluation. We demonstrate that the atomic cluster expansion as implemented in PACE shifts a previously established Pareto front for machine learning interatomic potentials toward faster and more accurate calculations. Moreover, general purpose parameterizations are presented for copper and silicon and evaluated in detail. We show that the Cu and Si potentials significantly improve on the best available potentials for highly accurate large-scale atomistic simulations.",10.1038/s41524-021-00559-9,,,
Ensemble multi-objective biogeography-based optimization with application to automated warehouse scheduling,"Ma, HP; Su, SF; Simon, D; Fei, MR",ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE,2015.0,"This paper proposes an ensemble multi-objective biogeography-based optimization (EMBBO) algorithm, which is inspired by ensemble learning, to solve the automated warehouse scheduling problem. First, a real-world automated warehouse scheduling problem is formulated as a constrained multi-objective optimization problem. Then EMBBO is formulated as a combination of several multi-objective biogeography-based optimization (MBBO) algorithms, including vector evaluated biogeography-based optimization (VEBBO), non-dominated sorting biogeography-based optimization (NSBBO), and niched Pareto biogeography-based optimization (NPBBO). Performance is tested on a set of 10 unconstrained multi-objective benchmark functions and 10 constrained multi-objective benchmark functions from the 2009 Congress on Evolutionary Computation (CEC), and compared with single constituent MBBO and CEC competition algorithms. Results show that EMBBO is better than its constituent algorithms, and among the best CEC competition algorithms, for the benchmark functions studied in this paper. Finally, EMBBO is successfully applied to the automated warehouse scheduling problem, and the results show that EMBBO is a competitive algorithm for automated warehouse scheduling. (C) 2015 Elsevier Ltd. All rights reserved.",10.1016/j.engappai.2015.05.009,Automated warehousing; Travel time analysis; Multi-objective optimization; Simulation; Performance analysis,,
Quantum Prisoner's Dilemma and High Frequency Trading on the Quantum Cloud,"Khan, FS; Bao, N",FRONTIERS IN ARTIFICIAL INTELLIGENCE,2021.0,"High-frequency trading (HFT) offers an excellent use case and a potential killer application of the commercially available, first generation quasi-quantum computers. To this end, we offer here a simple game-theoretic model of HFT as the famous two player game, Prisoner's Dilemma. We explore the implementation of HFT as an instance of Prisoner's Dilemma on the (quasi) quantum cloud using the Eisert, Wilkens, and Lewenstein quantum mediated communication protocol, and how this implementation can not only increase transaction speed but also improve the lot of the players in HFT. Using cooperative game-theoretic reasoning, we also note that in the near future when the internet is properly quantum, players will be able to achieve Pareto-optimality in HFT as an instance of reinforced machine learning.",10.3389/frai.2021.769392,quantum games; high-frequency trading (HFT); Pareto optimal; Nash equilbrium; quantum computing (QC),,
Vector optimization of laser solid freeform fabrication system using a hierarchical mutable smart bee-fuzzy inference system and hybrid NSGA-II/self-organizing map,"Fathi, A; Mozaffari, A",JOURNAL OF INTELLIGENT MANUFACTURING,2014.0,"The purpose of current investigation is to develop a robust intelligent framework to achieve efficient and reliable operating process parameters for laser solid freeform fabrication (LSFF) process as a recent and ongoing topic of investigation. Firstly, based on mutable smart bee algorithm (MSBA) and fuzzy inference system (FIS) two models are developed to identify the clad hight (deposited layer thickness) and the melt pool depth as functions of scanning speed, laser power and mass powder. Using the obtained model, the well-known multiobjective evolutionary algorithm called non-dominated sorting genetic algorithm (NSGA-II) is used for multi-criterion optimization of LSFF process. According to the available reported information and also the author's experiments, it is observed that the obtained Pareto front is not justifiable since it fails to cover the entire Pareto hyper-volume due to the lack of intensified exploration. To tackle this deficiency, authors execute a post optimization process through utilizing a competitive unsupervised machine learning approach known as self-organizing map (SOM) with cubic spatial topology. Achieved results indicate that this grid based network is capable of enhancing the intensification of Pareto solutions since its synaptic weights successfully imitate the characteristics of non-dominated solutions (optimal values of mass powder, laser power and scanning speed). For extracting the corresponding objective functions of these non-dominated synaptic weights, MSBA-FIS is used again to map the operating parameters to objective functions space. After the termination of abovementioned procedures, a valuable archive, containing a set of non-dominated solutions, is obtained which lets the authors to make a deliberate engineering trade-off. Simulation experiments reveal that the proposed intelligent framework is highly capable to cope with complex engineering systems. Besides, it is observed that MSBA is more efficient in evolving the structure of hierarchical fuzzy inference system in comparison with classic hierarchical GA-FIS model. This rises from the simple structure of MSBA that turns it into a fast and robust algorithm for handling constraint distributed systems (i.e. hierarchical FIS in current investigation). The obtained results also indicate that the introduced intelligent framework is applicable for optimal design of complex engineering systems where there exists no analytical formulation that describes the phenomenon as well as information of optimal operating parameters.",10.1007/s10845-012-0718-6,Laser solid freeform fabrication (LSFF); Mutable smart bee algorithm (MSBA); Vector optimization; Intelligent post optimization; Supervised hierarchical fuzzy training,,
Trees on the move: using decision theory to compensate for climate change at the regional scale in forest social-ecological systems,"Benito-Garzon, M; Fady, B; Davi, H; Vizcaino-Palomar, N; Fernandez-Manjarres, J",REGIONAL ENVIRONMENTAL CHANGE,2018.0,"The adaptation of social-ecological systems such as managed forests depends largely on decisions taken by forest managers who must choose among a wide range of possible futures to spread risks. We used robust decision theory to guide management decisions on the translocation of tree populations to compensate for climate change. We calibrated machine learning correlational models using tree height data collected from five common garden tests in France where Abies alba provenances from 11 European countries are planted. Resulting models were used to simulate tree height in the planting sites under current and 2050 climates (regional concentration pathway scenarios (RCPs) 2.6, 4.5, 6.0 and 8.5). Our results suggest an overall increase in tree height by 2050, but with large variation among the predictions depending on the provenance and the RCPs. We applied maximin, maximax and minimax decision rules to address outcomes under five uncertain states of the world represented by the four RCPs and the present climate (baseline). The maximin rule indicated that for 2050, the best translocation option for maximising tree height would be the use of provenances from Northwest France into all target zones. The maximax and minimax regret rules pointed out the same result for all target zones except for the 'Les Chauvets' trial, where the East provenance was selected. Our results show that decision theory can help management by reducing the number of options if most decision rules converge. Interestingly, the commonly suggested recommendation of using multiple provenances to mitigate long-term maladaptation risks or from 'pre-adapted' populations from the south was not supported by our approach.",10.1007/s10113-018-1277-y,Assisted migration; Decision theory; Forests; Phenotypic variation; Social-ecological systems; Uncertainty,,
The geometric mean?,"Vogel, RM",COMMUNICATIONS IN STATISTICS-THEORY AND METHODS,2022.0,"The sample geometric mean (SGM) introduced by Cauchy in 1821, is a measure of central tendency with many applications in the natural and social sciences including environmental monitoring, scientometrics, nuclear medicine, infometrics, economics, finance, ecology, surface and groundwater hydrology, geoscience, geomechanics, machine learning, chemical engineering, poverty and human development, to name a few. Remarkably, it was not until 2013 that a theoretical definition of the population geometric mean (GM) was introduced. Analytic expressions for the GM are derived for many common probability distributions, including: lognormal, Gamma, exponential, uniform, Chi-square, F, Beta, Weibull, Power law, Pareto, generalized Pareto and Rayleigh. Many previous applications of SGM assumed lognormal data, though investigators were unaware that for that case, the GM is the median and SGM is a maximum likelihood estimator of the median. Unlike other measures of central tendency such as the mean, median, and mode, the GM lacks a clear physical interpretation and its estimator SGM exhibits considerable bias and mean square error, which depends significantly on sample size, pd, and skewness. A review of the literature reveals that there is little justification for use of the GM in many applications. Recommendations for future research and application of the GM are provided.",10.1080/03610926.2020.1743313,Central tendency; arithmetic mean; median; log transformation; lognormal; multiplicative aggregation; effective,,
Performance analysis of a degraded PEM fuel cell stack for hydrogen passenger vehicles based on machine learning algorithms in real driving conditions,"Raeesi, M; Changizian, S; Ahmadi, P; Khoshnevisan, A",ENERGY CONVERSION AND MANAGEMENT,2021.0,"Fuel cell degradation is one of the main challenges of hydrogen fuel cell vehicles, which can be solved by robust prediction techniques like machine learning. In this research, a specific Proton-exchange membrane fuel cell stack is considered, and the experimental data are imported to predict the future behavior of the stack. Besides, four different prediction neural network algorithms are considered, and Deep Neural Network is selected. Furthermore, Simcenter Amesim software is used with the ability of dynamic simulation to calculate real-time fuel consumption, fuel cell degradation, and engine performance. Finally, to better understand how fuel cell degradation affects fuel consumption and life cycle emission, lifecycle assessment as a potential tool is carried out using GREET software. The results show that a degraded Proton-exchange membrane fuel cell stack can result in an increase in fuel consumption by 14.32 % in the New European driving cycle and 13.9 % in the FTP-75 driving cycle. The Life Cycle Assessment analysis results show that fuel cell degradation has a significant effect on fuel consumption and total emission. The results show that a fuel cell with a predicted degradation will emit 26.4 % more CO2 emissions than a Proton-exchange membrane fuel cell without degradation.",10.1016/j.enconman.2021.114793,Proton-exchange membrane fuel cell; Machine learning; Deep neural network algorithms; Life cycle assessment; Degradation effects,,
Ensemble of heterogeneous flexible neural trees using multiobjective genetic programming,"Ojha, VK; Abraham, A; Snasel, V",APPLIED SOFT COMPUTING,2017.0,"Machine learning algorithms are inherently multiobjective in nature, where approximation error minimization and models complexity simplification are two conflicting objectives. We proposed a multiobjective genetic programming (MOGP) for creating a heterogeneous flexible neural tree (HFNT), tree-like flexible feedforward neural network model. The functional heterogeneity in neural tree nodes was introduced to capture a better insight of data during learning because each input in a dataset possess different features. MOGP guided an initial HFNT population towards Pareto-optimal solutions, where the final population was used for making an ensemble system. A diversity index measure along with approximation error and complexity was introduced to maintain diversity among the candidates in the population. Hence, the ensemble was created by using accurate, structurally simple, and diverse candidates from MOGP final population. Differential evolution algorithm was applied to fine-tune the underlying parameters of the selected candidates. A comprehensive test over classification, regression, and time-series datasets proved the efficiency of the proposed algorithm over other available prediction methods. Moreover, the heterogeneous creation of HFNT proved to be efficient in making ensemble system from the final population. (C) 2016 Elsevier B.V. All rights reserved.",10.1016/j.asoc.2016.09.035,Pareto-based multiobjectives; Flexible neural tree; Ensemble; Approximation; Feature selection,,
Feature weighting and selection with a Pareto-optimal trade-off between relevancy and redundancy,"Das, A; Das, S",PATTERN RECOGNITION LETTERS,2017.0,"Feature Selection (FS) is an important pre-processing step in machine learning and it reduces the number of features/variables used to describe each member of a dataset. Such reduction occurs by eliminating some of the non-discriminating and redundant features and selecting a subset of the existing features with higher discriminating power among various classes in the data. In this paper, we formulate the feature selection as a bi-objective optimization problem of some real-valued weights corresponding to each feature. A subset of the weighted features is thus selected as the best subset for subsequent classification of the data. Two information theoretic measures, known as 'relevancy' and 'redundancy' are chosen for designing the objective functions for a very competitive Multi-Objective Optimization (MOO) algorithm called 'Multi-Objective Evolutionary Algorithm based on Decomposition (MOEA/D)'. We experimentally determine the best possible constraints on the weights to be optimized. We evaluate the proposed bi-objective feature selection and weighting framework on a set of 15 standard datasets by using the popular k-Nearest Neighbor (k-NN) classifier. As is evident from the experimental results, our method appears to be quite competitive to some of the state-of-the-art FS methods of current interest. We further demonstrate the effectiveness of our framework by changing the choices of the optimization scheme and the classifier to Non-dominated Sorting Genetic Algorithm (NSGA)-II and Support Vector Machines (SVMs) respectively. (C) 2017 Elsevier B.V. All rights reserved.",10.1016/j.patrec.2017.01.004,Feature selection; Feature weighting; Multi-objective optimization; Information measure; Classification,,
A symbolic fault-prediction model based on multiobjective particle swarm optimization,"de Carvalho, AB; Pozo, A; Vergilio, SR",JOURNAL OF SYSTEMS AND SOFTWARE,2010.0,"In the literature the fault-proneness of classes or methods has been used to devise strategies for reducing testing costs and efforts. In general, fault-proneness is predicted through a set of design metrics and, most recently, by using Machine Learning (ML) techniques. However, some ML techniques cannot deal with unbalanced data, characteristic very common of the fault datasets and, their produced results are not easily interpreted by most programmers and testers. Considering these facts, this paper introduces a novel fault-prediction approach based on Multiobjective Particle Swarm Optimization (MOPSO). Exploring Pareto dominance concepts, the approach generates a model composed by rules with specific properties. These rules can be used as an unordered classifier, and because of this, they are more intuitive and comprehensible. Two experiments were accomplished, considering, respectively, fault-proneness of classes and methods. The results show interesting relationships between the studied metrics and fault prediction. In addition to this, the performance of the introduced MOPSO approach is compared with other ML algorithms by using several measures including the area under the ROC curve, which is a relevant criterion to deal with unbalanced data. (C) 2010 Elsevier Inc. All rights reserved.",10.1016/j.jss.2009.12.023,Fault prediction; Particle swarm optimization; Multiobjective; Rule learning algorithm,,
Applying machine learning to AHP multicriteria decision making method to assets prioritization in the context of industrial maintenance 4.0,"Lima, E; Gorski, E; Loures, EFR; Santos, EAP; Deschamps, F",IFAC PAPERSONLINE,2019.0,"The increasing competition among industries has led to the emergence of numerous tools and methods to support decision making focused on assets maintenance in a company, since ensuring good maintenance is directly linked with greater reliability and uptime for equipment, reducing losses in production processes and consequently increasing profitability. This work aims to use Machine Learning (ML) algorithms - Bayesian Networks (BN) and attribute relevance analysis (ARA), implemented in the Weka (R) platform, to process a dataset of event logs failure of industrial machine components. The approach aims to use the conditional probability relations generated by the BN and the ranking of criteria relevance for the design of an AHP decision-making model within the scope of industrial maintenance to prioritize which components of a specific machine are more susceptible to failures. The proposed integration mechanism aims to bring greater reliability to the weights assigned to the criteria of the AHP model, and consequently, a more accurate decision support. The results showed that the AHP model generated from a Bayesian Network is consistent with the conditional probabilities estimated by the BN, giving robustness to the decision sphere in the context of industrial maintenance. This AHP model can serve as a basis to be complemented by qualitative analysis criteria according to the need of the individual specialist, allowing the construction of strategic maintenance action plans. (C) 2019, IFAC (International Federation of Automatic Control) Hosting by Elsevier Ltd. All rights reserved.",10.1016/j.ifacol.2019.11.524,AHP; Industrial Maintenance; Bayesian Networks; ARA - Attribute Relevance Analysis algorithms,,
THEORETICAL AND COMPUTATIONAL GUARANTEES OF MEAN FIELD VARIATIONAL INFERENCE FOR COMMUNITY DETECTION,"Zhang, AY; Zhou, HH",ANNALS OF STATISTICS,2020.0,"The mean field variational Bayes method is becoming increasingly popular in statistics and machine learning. Its iterative coordinate ascent variational inference algorithm has been widely applied to large scale Bayesian inference. See Blei et al. (2017) for a recent comprehensive review. Despite the popularity of the mean field method, there exist remarkably little fundamental theoretical justifications. To the best of our knowledge, the iterative algorithm has never been investigated for any high-dimensional and complex model. In this paper, we study the mean field method for community detection under the stochastic block model. For an iterative batch coordinate ascent variational inference algorithm, we show that it has a linear convergence rate and converges to the minimax rate within log n iterations. This complements the results of Bickel et al. (2013) which studied the global minimum of the mean field variational Bayes and obtained asymptotic normal estimation of global model parameters. In addition, we obtain similar optimality results for Gibbs sampling and an iterative procedure to calculate maximum likelihood estimation, which can be of independent interest.",10.1214/19-AOS1898,Mean field; variational inference; Bayesian; community detection; stochastic block model,,
Boosting with structural sparsity: A differential inclusion approach,"Huang, CD; Sun, XW; Xiong, JC; Yao, Y",APPLIED AND COMPUTATIONAL HARMONIC ANALYSIS,2020.0,"Boosting as gradient descent algorithms is one popular method in machine learning. In this paper a novel Boosting-type algorithm is proposed based on restricted gradient descent with structural sparsity control whose underlying dynamics are governed by differential inclusions. In particular, we present an iterative regularization path with structural sparsity where the parameter is sparse under some linear transforms, based on variable splitting and the Linearized Bregman Iteration. Hence it is called Split LBI. Despite its simplicity, Split LBI outperforms the popular generalized Lasso in both theory and experiments. A theory of path consistency is presented that equipped with a proper early stopping, Split LBI may achieve model selection consistency under a family of Irrepresentable Conditions which can be weaker than the necessary and sufficient condition for generalized Lasso. Furthermore, some l(2) error bounds are also given at the minimax optimal rates. The utility and benefit of the algorithm are illustrated by several applications including image denoising, partial order ranking of sport teams, and world university grouping with crowdsourced ranking data. (C) 2018 Elsevier Inc. All rights reserved.",10.1016/j.acha.2017.12.004,Boosting; Differential inclusions; Structural sparsity; Linearized Bregman iteration; Variable splitting; Generalized Lasso; Model selection; Consistency,,
Pareto optimization of deep networks for COVID-19 diagnosis from chest X-rays,"Guarrasi, V; D'Amico, NC; Sicilia, R; Cordelli, E; Soda, P",PATTERN RECOGNITION,2022.0,"The year 2020 was characterized by the COVID-19 pandemic that has caused, by the end of March 2021, more than 2.5 million deaths worldwide. Since the beginning, besides the laboratory test, used as the gold standard, many applications have been applying deep learning algorithms to chest X-ray images to recognize COVID-19 infected patients. In this context, we found out that convolutional neural networks perform well on a single dataset but struggle to generalize to other data sources. To overcome this limita-tion, we propose a late fusion approach where we combine the outputs of several state-of-the-art CNNs, introducing a novel method that allows us to construct an optimum ensemble determining which and how many base learners should be aggregated. This choice is driven by a two-objective function that maximizes, on a validation set, the accuracy and the diversity of the ensemble itself. A wide set of ex-periments on several publicly available datasets, accounting for more than 92,0 0 0 images, shows that the proposed approach provides average recognition rates up to 93.54% when tested on external datasets. (c) 2021 Elsevier Ltd. All rights reserved.",10.1016/j.patcog.2021.108242,COVID-19; X-ray; Deep-learning; Multi-expert systems; Optimization; Convolutional neural networks,,
Robust multi-objective visual bayesian personalized ranking for multimedia recommendation,"Paul, A; Wu, ZF; Liu, K; Gong, SF",APPLIED INTELLIGENCE,2022.0,"Machine learning classifiers are susceptible to adversarial perturbations, and their existence raises security concerns with a focus on recommendation systems. While there is a substantial effort to investigate attacks and defensive techniques in recommendation systems, Basic Iterative perturbation strategies (BIM) have been under-researched in multimedia recommendation. In this work, we adapt the iterative approach for multimedia recommendation. We proposed a novel Dynamic Collaborative Filtering with Aesthetic (DCFA) approach which leverages aesthetic features of clothing images into a multi-objective pairwise ranking to capture consumer aesthetic taste at a specific time through adversarial training (ADCFA). The DCFA method extends visual recommendation to make three key contributions: (1) incorporate aesthetic features into multimedia recommender system to model consumers' preferences in the aesthetic aspect. (2) Design a multi-objective personalized ranking for the visual recommendation. (3) Use the aesthetic features to optimize the learning strategy to capture the temporal dynamics of image aesthetic preferences. To reduce the impact of perturbation, we train a DCFA objective function using minimax adversarial training. Extensive experiments on three datasets demonstrate the effectiveness of our method.",10.1007/s10489-021-02355-w,Multimedia recommendation; Aesthetic features; Temporal dynamics; Adversarial perturbation; Adversarial training,,
CHIME: CLUSTERING OF HIGH-DIMENSIONAL GAUSSIAN MIXTURES WITH EM ALGORITHM AND ITS OPTIMALITY,"Cai, TT; Ma, J; Zhang, LJ",ANNALS OF STATISTICS,2019.0,"Unsupervised learning is an important problem in statistics and machine learning with a wide range of applications. In this paper, we study clustering of high-dimensional Gaussian mixtures and propose a procedure, called CHIME, that is based on the EM algorithm and a direct estimation method for the sparse discriminant vector. Both theoretical and numerical properties of CHIME are investigated. We establish the optimal rate of convergence for the excess misclustering error and show that CHIME is minimax rate optimal. In addition, the optimality of the proposed estimator of the discriminant vector is also established. Simulation studies show that CHIME outperforms the existing methods under a variety of settings. The proposed CHIME procedure is also illustrated in an analysis of a glioblastoma gene expression data set and shown to have superior performance. Clustering of Gaussian mixtures in the conventional low-dimensional setting is also considered. The technical tools developed for the high-dimensional setting are used to establish the optimality of the clustering procedure that is based on the classical EM algorithm.",10.1214/18-AOS1711,High-dimensional data; unsupervised learning; Gaussian mixture model; EM algorithm; misclustering error; Minimax optimality,,
A New optimized sequential method for lung tumor diagnosis based on deep learning and converged search and rescue algorithm,"Tian, QJ; Wu, YT; Ren, XJ; Razmjooy, N",BIOMEDICAL SIGNAL PROCESSING AND CONTROL,2021.0,"Because the diagnosis of lung cancer and malignancy using imaging techniques such as CT-Scan without the need for sampling reduces the risk of cancer nodules spreading, the development of a computer diagnostic system to process images and lungs and then classify them into two classes of benign and malignant groups plays an important role in the early diagnosis of lung cancer and saving the lives of patients. This study aimed to achieve higher classification accuracy and consequently higher detection accuracy of malignant and benign glands based on deep learning and metaheuristics. In this study, first, the CT scan images of the lung are pre-processed and then the pattern segmented area is achieved by an optimized version of the new fuzzy possibilistic c-ordered mean based on a new version of a metaheuristic, called Converged Search and Rescue (CSAR) algorithm. Then, Enhanced Capsule Networks (ECN) is used for the final diagnosis. To validate the method, it is accomplished to the Lung CT-Diagnosis database and is analyzed based on four indicators including precision, accuracy, recall, and F1-score. The final results of the method are compared with three state-of-the-art methods, including ResNet, KE-CNN, and CNN. The results showed that the suggested method with 96.35 % precision, 96.07 % recall, 96.41 % F1-score, and 96.65 % accuracy has the best results against the compared methods.",10.1016/j.bspc.2021.102761,Lung tumor diagnosis; Fuzzy C-Ordered means; Enhanced capsule networks; Converged search and rescue algorithm (CSAR),,
Maximum relevance minimum common redundancy feature selection for nonlinear data,"Che, JX; Yang, YL; Li, L; Bai, XY; Zhang, SH; Deng, CZ",INFORMATION SCIENCES,2017.0,"In recent years, feature selection based on relevance redundancy trade-off criteria has become a very promising and popular approach in the field of machine learning. However, the existing algorithmic frameworks of mutual information feature selection have certain limitations for the common feature selection problems in practice. To overcome these limitations, the idea of a new framework is developed by introducing a novel maximum relevance and minimum common redundancy criterion and a minimax nonlinear optimization approach. In particular, a novel mutual information feature selection method based on the normalization of the maximum relevance and minimum common redundancy (N-MRMCR-MI) is presented, which produces a normalized value in the range [0, 1] and results in a regression problem. We perform extensive experimental comparisons over numerous state-of-art algorithms using different forecasts (Bayesian Additive Regression tree, treed Gaussian process, k-NN, and SVM) and different data sets (two simulated and five real datasets). The results show that the proposed algorithm outperforms the others in terms of feature selection and forecasting accuracy. (C) 2017 Elsevier Inc. All rights reserved.",10.1016/j.ins.2017.05.013,Feature selection; Mutual information; Normalization; Minimal common redundancy; Maximal relevance,,
Open Data Science to Fight COVID-19: Winning the 500k XPRIZE Pandemic Response Challenge,"Lozano, MA; Orts, OGI; Pinol, E; Rebollo, M; Polotskaya, K; Garcia-March, MA; Conejero, JA; Escolano, F; Oliver, N","MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES, ECML PKDD 2021: APPLIED DATA SCIENCE TRACK, PT IV",2021.0,"In this paper, we describe the deep learning-based COVID-19 cases predictor and the Pareto-optimal Non-Pharmaceutical Intervention (NPI) prescriptor developed by the winning team of the 500k XPRIZE Pandemic Response Challenge, a four-month global competition organized by the XPRIZE Foundation. The competition aimed at developing data-driven AI models to predict COVID-19 infection rates and to prescribe NPI Plans that governments, business leaders and organizations could implement to minimize harm when reopening their economies. In addition to the validation performed by XPRIZE with real data, the winning models were validated in a real-world scenario thanks to an ongoing collaboration with the Valencian Government in Spain. We believe that this experience contributes to the necessary transition to more evidence-driven policy-making, particularly during a pandemic.",10.1007/978-3-030-86514-6_24,SARS-CoV-2; Computational epidemiology; Data science for public health; Recurrent neural networks; Non-pharmaceutical interventions; Pareto-front optimization,,
Multi-Objective Optimization Design through Machine Learning for Drop-on-Demand Bioprinting,"Shi, J; Song, JC; Song, B; Lu, WF",ENGINEERING,2019.0,"Drop-on-demand (DOD) bioprinting has been widely used in tissue engineering due to its high-throughput efficiency and cost effectiveness. However, this type of bioprinting involves challenges such as satellite generation, too-large droplet generation, and too-low droplet speed. These challenges reduce the stability and precision of DOD printing, disorder cell arrays, and hence generate further structural errors. In this paper, a multi-objective optimization (MOO) design method for DOD printing parameters through fully connected neural networks (FCNNs) is proposed in order to solve these challenges. The MOO problem comprises two objective functions: to develop the satellite formation model with FCNNs; and to decrease droplet diameter and increase droplet speed. A hybrid multi-subgradient descent bundle method with an adaptive learning rate algorithm (HMSGDBA), which combines the multisubgradient descent bundle (MSGDB) method with Adam algorithm, is introduced in order to search for the Pareto-optimal set for the MOO problem. The superiority of HMSGDBA is demonstrated through comparative studies with the MSGDB method. The experimental results show that a single droplet can be printed stably and the droplet speed can be increased from 0.88 to 2.08 m.s(-1) after optimization with the proposed method. The proposed method can improve both printing precision and stability, and is useful in realizing precise cell arrays and complex biological functions. Furthermore, it can be used to obtain guidelines for the setup of cell-printing experimental platforms. (C) 2019 THE AUTHORS. Published by Elsevier LTD on behalf of Chinese Academy of Engineering and Higher Education Press Limited Company. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).",10.1016/j.eng.2018.12.009,Drop-on-demand printing; Inkjet printing; Gradient descent multi-objective optimization; Fully connected neural networks,,
A machine learning-based clinical decision support system to identify prescriptions with a high risk of medication error,"Corny, J; Rajkumar, A; Martin, O; Dode, X; Lajonchere, JP; Billuart, O; Bezie, Y; Buronfosse, A",JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION,2020.0,"Objective: To improve patient safety and clinical outcomes by reducing the risk of prescribing errors, we tested the accuracy of a hybrid clinical decision support system in prioritizing prescription checks. Materials and Methods: Data from electronic health records were collated over a period of 18 months. Inferred scores at a patient level (probability of a patient's set of active orders to require a pharmacist review) were calculated using a hybrid approach (machine learning and a rule-based expert system). A clinical pharmacist analyzed randomly selected prescription orders over a 2-week period to corroborate our findings. Predicted scores were compared with the pharmacist's review using the area under the receiving-operating characteristic curve and area under the precision-recall curve. These metrics were compared with existing tools: computerized alerts generated by a clinical decision support (CDS) system and a literature-based multicriteria query prioritization technique. Data from 10 716 individual patients (133 179 prescription orders) were used to train the algorithm on the basis of 25 features in a development dataset. Results: While the pharmacist analyzed 412 individual patients (3364 prescription orders) in an independent validation dataset, the areas under the receiving-operating characteristic and precision-recall curves of our digital system were 0.81 and 0.75, respectively, thus demonstrating greater accuracy than the CDS system (0.65 and 0.56, respectively) and multicriteria query techniques (0.68 and 0.56, respectively). Discussion: Our innovative digital tool was notably more accurate than existing techniques (CDS system and multicriteria query) at intercepting potential prescription errors. Conclusions: By primarily targeting high-risk patients, this novel hybrid decision support system improved the accuracy and reliability of prescription checks in a hospital setting.",10.1093/jamia/ocaa154,supervised machine learning; electronic prescribing; clinical pharmacy information systems; medication errors; decision support systems; clinical,,
EPAI-NC: Enhanced prediction of adenosine to inosine RNA editing sites using nucleotide compositions,"Ahmad, A; Shatabda, S",ANALYTICAL BIOCHEMISTRY,2019.0,"RNA editing process like Adenosine to Intosine (A-to-I) often influences basic functions like splicing stability and most importantly the translation. Thus knowledge about editing sites is of great importance in molecular biology. With the growth of known editing sites, machine learning or data centric approaches are now being applied to solve this problem of prediction of RNA editing sites. In this paper, we propose EPAI-NC, a novel method for prediction of RNA editing sites. We have used l-mer composition and n-gapped l-mer composition as features and used Pearson Correlation Coefficient to select features according to Pareto Principle. Locally deep support vector machines were used to train the classification model of EPAI-NC. EPAI-NC significantly enhances the prediction accuracy compared to the previous state-of-the-art methods when tested on standard benchmark and independent dataset.",10.1016/j.ab.2019.01.002,RNA editing sites; Nucleotide compositions; Feature selection; Machine learning; Classification; Web application,,
Numerical Study on the Thermal and Optical Performances of an Aerogel Glazing System with the Multivariable Optimization Using an Advanced Machine Learning Algorithm,"Zheng, SQ; Zhou, YK",ADVANCED THEORY AND SIMULATIONS,2019.0,"The implementation of advanced materials in high-efficient glazing system is important for green buildings. In this study, aerogel granules are implemented in the glazing system to form a translucent window with super-insulating performance. An experimentally validated numerical modeling integrating both heat transfer model and optical model is developed to characterize the sophisticated heat transfer and solar radiation transmission mechanisms. Sensitivity analysis is presented with quantifiable contribution ratio of each parameter to the total heat gain. Instead of returning back to numerical modeling repeatedly, an advanced optimization engine implemented with a generic optimization methodology with competitive computational efficiency and accuracy is proposed by implementing the supervised machine learning and advanced optimization algorithms. The research results show that the developed artificial neural network modeling is more accurate and computational-efficient than the traditional lsqcurvefit fitting methodology. In addition, the optimal case through the teaching-learning-based optimization is more robust and competitive than the optimal case through the particle swarm optimization in terms of the total heat gain. This study presents an in-depth understanding of heat transfer and solar radiation transmission of nanoporous aerogel granules together with a robust optimal design, which is important for the promotion of green buildings with high-energy performance.",10.1002/adts.201900092,aerogel; machine learning; optimization function; particle swarm optimization; teaching-learning-based optimization; transmittance,,
Conflict resolution in the multi-stakeholder stepped spillway design under uncertainty by machine learning techniques,"Mooselu, MG; Nikoo, MR; Bakhtiari, PH; Rayani, NB; Izady, A",APPLIED SOFT COMPUTING,2021.0,"The optimal spillway design is of great significance since these structures can reduce erosion downstream of the dams. This study proposes a risk-based optimization framework for a stepped spillway to achieve an economical design scenario with the minimum loss in hydraulic performance. Accordingly, the stepped spillway was simulated in the FLOW-3D (R) model, and the validated model was repeatedly performed for various geometric states. The results were used to form a Multilayer Perceptron artificial neural network (MLP-ANN) surrogate model. Then, a risk-based optimization model was formed by coupling the MLP-ANN and NSGA-II. The concept of conditional value at risk (CVaR) was utilized to reduce the risk of the designed spillway malfunctions in high flood flow rates, while minimizing the construction cost and the loss in hydraulic performance. Lastly, given the conflicting objectives of stakeholders, the non-cooperative graph model for conflict resolution (GMCR) was applied to achieve a compromise on the Pareto optimal solutions. Applicability of the suggested approach in the Jarreh Dam, Iran, resulted in a practical design scenario, which simultaneously minimizes the loss in hydraulic performance and the project cost and satisfies the priorities of decision-makers. (C) 2021 Elsevier B.V. All rights reserved.",10.1016/j.asoc.2021.107721,Stepped spillway; FLOW-3D (R); CVaR-based optimization model; GMCR-plus; NSGA-II,,
Experiments with Kemeny ranking: What works when?,"Ali, A; Meila, M",MATHEMATICAL SOCIAL SCIENCES,2012.0,"This paper performs a comparison of several methods for Kemeny rank aggregation (104 algorithms and combinations thereof in total) originating in social choice theory, machine learning, and theoretical computer science, with the goal of establishing the best trade-offs between search time and performance. We find that, for this theoretically NP-hard task, in practice the problems span three regimes: strong consensus, weak consensus, and no consensus. We make specific recommendations for each, and propose a computationally fast test to distinguish between the regimes. In spite of the great variety of algorithms, there are few classes that are consistently Pareto optimal. In the most interesting regime, the integer program exact formulation, local search algorithms and the approximate version of a theoretically exact branch and bound algorithm arise as strong contenders. (C) 2011 Elsevier B.V. All rights reserved.",10.1016/j.mathsocsci.2011.08.008,,,
Research on a Novel Combination System on the Basis of Deep Learning and Swarm Intelligence Optimization Algorithm for Wind Speed Forecasting,"He, XH; Nie, Y; Guo, HL; Wang, JZ",IEEE ACCESS,2020.0,"Wind speed forecasting takes a significant place in electric system owing to the fact that it has significant influence on operation efficiency and economic benefits. Aimming at improving forecast performance, a substantial number of wind speed prediction models have been proposed. However, these models have disregarded the limits of individual prediction models and the necessity of data preprocessing, resulting in poor prediction accuracy. In this study, a novel forecasting system is proposed consisting of three modules: data preprocessing module, individual forecasting module and weight optimization module, which effectively achieve better forecasting ability. For data preprocessing and individual forecasting module, more regular sequences are obtained by decomposition technology, and association features are extracted by deep learning algorithm in prediction module. In the weight optimized module, the combination method base on the multi-objective optimization algorithm and nonnegative constraint theory are used to improve the prediction effectiveness. The combination model successfully exceeds the limits of individual predicton models and comparatively improves prediction accuracy. The effectiveness of the developed combination system is evaluated by 10-min wind speed in Penglai, China. The experiment results indicate that proposed forecasting system is better than other traditional forecasting models on three real wind speed datasets indeed.",10.1109/ACCESS.2020.2980562,Wind speed forecasting; deep learning; multi-objective optimization algorithm; combination system,,
A Time-Saving Methodology for Optimizing a Compression Ignition Engine to Reduce Fuel Consumption through Machine Learning,"Rahnama, P; Arab, M; Reitz, RD",SAE INTERNATIONAL JOURNAL OF ENGINES,2020.0,"Applying a suitable design optimization technique is a crucial task for optimizing compression ignition engines because of the time-consuming process of optimization even with advanced supercomputers. Traditional computational fluid dynamics (CFD) used in conjunction with design of experiment (DOE) methods requires executing the CFD model several times. A response surface is usually fitted to relate the inputs to the outputs, which is often created based on linear regression. This method is not well suited to capture interaction effects between inputs and nonlinearities existing during engine combustion. A combination of genetic algorithm (GA) and CFD tools usually eventuates better optimum results. However, the CFD simulations must be executed sequentially, resulting in extremely high computational times, which makes it impossible to apply an optimization study using a single desktop computer. The current study examines a novel approach, which combines CFD, GA, and a type of machine learning approach, namely artificial neural networks (ANNs), in order to optimize a compression ignition engine to achieve its minimum indicated specific fuel consumption (ISFC). Start of injection (SOD timing and input pressure were selected as the optimization variables in order to investigate improvement in ISFC without any hardware modifications of the engine. Maximum in-cylinder peak pressure and ringing intensity (pressure rise rate) were chosen as the optimization constraints. Conducting a reliable optimization study with a single desktop computer in a shorter time can be achieved by using the proposed methodology. The results indicate that a 97% decrease in the estimated number of days to achieve the final results was obtained, compared to the traditional CFD-GA approach. Furthermore, adopting this methodology eliminates the necessity for additional response surface fitting to GA data. Therefore, it facilitates an examination of design parameter effects on the engine outputs, doing sensitivity analysis, post-processing the optimization results, and providing a powerful tool to gain optimum designs. The final optimum point illustrates a 10% improvement in ISFC, while avoiding sensitive regions and without exceeding optimization constraints.",10.4271/03-13-02-0019,Compression ignition engine; Optimization; Machine learning; Artificial neural networks; Computational fluid dynamics; Genetic algorithm,,
An improved optimal trigonometric ELM algorithm for numerical solution to ruin probability of Erlang(2) risk model,"Cheng, YJ; Hou, MZ; Wang, J",MULTIMEDIA TOOLS AND APPLICATIONS,2020.0,"In this paper, we focus on accurately calculating the numerical solution of the integral-differential equation for ruin probability in Erlang(2) renewal risk model with arbitrary claim distribution. Because the analytical solutions of the equation do not usually exist, firstly, using machine learning method in modern artificial intelligence, the activation functions in the ELM model are changed to trigonometric function, the initial conditions in the integral-differential equation are added to the ELM linear solver to get the ITELM model, and the steps and feasibility of the algorithm are strictly deduced in theory. As the analytic solution for the integral-differential equation only exists when the claim is subject to exponential distribution, and the numerical solution can be gotten with the pareto distribution. And, since the number of hidden neurons in the ITELM model is uncertain, a good numerical value of hidden neurons can only be determined through a large number of iterative tests and comparisons in the actual calculation. Then, we construct a multi-objective optimization model and algorithm, which can get the optimal number of hidden neurons to obtain the IOTELM model and algorithm. Then, in the above two cases for exponential distribution and pareto distribution, the optimal number of hidden neurons is calculated by IOTELM model and algorithm, and then corresponding ITELM models and algorithms are constructed to calculate the corresponding ruin probability. Compared with the previous numerical experiments, it can be seen that the numerical accuracy is greatly improved, which verified the versatility, feasibility and superiority of the proposed IOTELM model and algorithm.",10.1007/s11042-020-09382-8,Erlang(2) risk model; Ruin probability; Renewal integral-differential equation; IOTELM algorithm,,
Dynamic Bicycle Dispatching of Dockless Public Bicycle-sharing Systems Using Multi-objective Reinforcement Learning,"Chen, JG; Li, KL; Li, KQ; Yu, PS; Zeng, Z",ACM TRANSACTIONS ON CYBER-PHYSICAL SYSTEMS,2021.0,"As a new generation of Public Bicycle-sharing Systems (PBS), the Dockless PBS (DL-PBS) is an important application of cyber-physical systems and intelligent transportation. How to use artificial intelligence to provide efficient bicycle dispatching solutions based on dynamic bicycle rental demand is an essential issue for DL-PBS. In this article, we propose MORL-BD, a dynamic bicycle dispatching algorithm based on multi-objective reinforcement learning to provide the optimal bicycle dispatching solution for DL-PBS. We model the DL-PBS system from the perspective of cyber-physical systems and use deep learning to predict the layout of bicycle parking spots and the dynamic demand of bicycle dispatching. We define the multi-route bicycle dispatching problem as a multi-objective optimization problem by considering the optimization objectives of dispatching costs, dispatch truck's initial load, workload balance among the trucks, and the dynamic balance of bicycle supply and demand. On this basis, the collaborative multi-route bicycle dispatching problem among multiple dispatch trucks is modeled as a multi-agent and multi-objective reinforcement learning model. All dispatch paths between parking spots are defined as state spaces, and the reciprocal of dispatching costs is defined as a reward. Each dispatch truck is equipped with an agent to learn the optimal dispatch path in the dynamic DL-PBS network. We create an elite list to store the Pareto optimal solutions of bicycle dispatch paths found in each action, and finally get the Pareto frontier. Experimental results on the actual DL-PBS show that compared with existing methods, MORL-BD can find a higher quality Pareto frontier with less execution time.",10.1145/3447623,Bicycle-sharing systems; bicycle dispatching; intelligent transportation; multi-objective reinforcement learning; Pareto optimality,,
GIS-based landslide susceptibility mapping using numerical risk factor bivariate model and its ensemble with linear multivariate regression and boosted regression tree algorithms,"Arabameri, A; Pradhan, B; Rezaei, K; Sohrabi, M; Kalantari, Z",JOURNAL OF MOUNTAIN SCIENCE,2019.0,"In this study, a novel approach of the landslide numerical risk factor (LNRF) bivariate model was used in ensemble with linear multivariate regression (LMR) and boosted regression tree (BRT) models, coupled with radar remote sensing data and geographic information system (GIS), for landslide susceptibility mapping (LSM) in the Gorganroud watershed, Iran. Fifteen topographic, hydrological, geological and environmental conditioning factors and a landslide inventory (70%, or 298 landslides) were used in mapping. Phased array-type L-band synthetic aperture radar data were used to extract topographic parameters. Coefficients of tolerance and variance inflation factor were used to determine the coherence among conditioning factors. Data for the landslide inventory map were obtained from various resources, such as Iranian Landslide Working Party (ILWP), Forestry, Rangeland and Watershed Organisation (FRWO), extensive field surveys, interpretation of aerial photos and satellite images, and radar data. Of the total data, 30% were used to validate LSMs, using area under the curve (AUC), frequency ratio (FR) and seed cell area index (SCAI). Normalised difference vegetation index, land use/ land cover and slope degree in BRT model elevation, rainfall and distance from stream were found to be important factors and were given the highest weightage in modelling. Validation results using AUC showed that the ensemble LNRF-BRT and LNRFLMR models (AUC = 0.912 (91.2%) and 0.907 (90.7%), respectively) had high predictive accuracy than the LNRF model alone (AUC = 0.855 (85.5%)). The FR and SCAI analyses showed that all models divided the parameter classes with high precision. Overall, our novel approach of combining multivariate and machine learning methods with bivariate models, radar remote sensing data and GIS proved to be a powerful tool for landslide susceptibility mapping.",10.1007/s11629-018-5168-y,Landslide susceptibility; GIS; Remote sensing; Bivariate model; Multivariate model; Machine learning model,,
Ecosystem based multi-species management using Empirical Dynamic Programming,"Brias, A; Munch, SB",ECOLOGICAL MODELLING,2021.0,"Control theory and stochastic dynamic programming have long been used to develop optimal single-species management policies. However, most species interact with others through competition and predation as parts of complex ecosystems. As a consequence, it is unclear how far from optimal the single species policies currently in use actually are. Moreover, there are as yet no scalable algorithms for optimal ecosystem management. Here, we merge recently developed tools from machine learning and nonlinear dynamics to construct and evaluate near-optimal policies in multi-species systems. Specifically, a non-parametric model for the dynamics is estimated from time series data using Gaussian process-based dynamic modeling. A policy is then derived from the inferred dynamics using a temporal difference learning algorithm. Policy performance is benchmarked against single-species policies and the ad hoc ecosystem policies that have been previously offered. We found that EDP policies are closer to the true optimal policies than single-species policies in multi-species systems with two controls and three objectives. The Pareto fronts illustrate the flexibility of EDP policies compared with single-species policies.",10.1016/j.ecolmodel.2020.109423,Multi-species management; Multi-objectives management; Non-linear methods; Approximate dynamic programming; Gaussian process regression; Temporal difference learning,,
A multiobjective framework for wind speed prediction interval forecasts,"Shrivastava, NA; Lohia, K; Panigrahi, BK",RENEWABLE ENERGY,2016.0,"Wind energy is rapidly emerging as a potential and viable replacement for fossil fuels owing to its clean way of power production. However, integration of this abundantly available renewable energy into the power system is constrained by its intermittent nature and unpredictability. Efforts to improve the prediction accuracy of wind speed is therefore imperative for its successful integration into the grid. The uncertainty associated with the prediction is also an important information needed by the system operators for reliable and economic operations. This paper presents the implementation of a multi-objective differential evolution (MODE) algorithm for generation of prediction intervals (PIs) for capturing the uncertainty related to forecasts. Support vector machine (SVM) is used as the machine learning technique and its parameters are tuned such that multiple contradictory objectives are satisfied to generate Pareto-optimal solutions. Several case studies are performed for data from wind farms located in the eastern region of United States. The obtained results prove the successful implementation of the methodology and generation of high quality Pis. (C) 2015 Elsevier Ltd. All rights reserved.",10.1016/j.renene.2015.08.038,Differential evolution; Renewable energy; Multi-objective; Prediction interval; Support vector machines; Wind speed,,
Speeding up prediction performance of BDT-based models,"Khairullin, E; Ustyuzhanin, A",18TH INTERNATIONAL WORKSHOP ON ADVANCED COMPUTING AND ANALYSIS TECHNIQUES IN PHYSICS RESEARCH (ACAT2017),2018.0,"The outcome of a machine learning algorithm is a prediction model. Typically, these models are computationally expensive, where improving of the quality the prediction leads to a decrease in the inference speed. However it is not always tradeoff between quality and speed. In this paper we show it is possible to speed up the model by using additional memory without losing significat prediction quality for a novel boosted trees algorithm called CatBoost. The idea is to combine two approaches: training fewer trees and merging trees into a kind of hashmaps called DecisionTensors. The proposed method allows for pareto-optimal reduction of the computational complexity of the decision tree model with regard to the quality of the model. In the considered example the number of lookups was decreased from 5000 to only 6 (speedup factor of 1000) while AUC score of the model was reduced by less than 10(-3).",10.1088/1742-6596/1085/4/042009,,,
Optimal decision of multiobjective and multiperiod anticipatory shipping under uncertain demand: A data-driven framework,"Chen, C; Xu, XH; Zou, BP; Peng, HX; Li, ZW",COMPUTERS & INDUSTRIAL ENGINEERING,2021.0,"Anticipatory shipping helps to reduce the waiting time of online customers to receive their products. Present studies on anticipatory shipping mainly consider a single period and ignore the waiting time saved for customers. This paper develops a data-driven framework to investigate the multiperiod anticipatory shipping problem with the objectives of minimizing the cost of online retailers and maximizing the saved waiting time of customers. First, we propose a dual-process sales forecasting framework that employs five machine learning algorithms to forecast online retailers' daily sales using clickstream data and historical sales data. Then, we build a multiperiod and multiobjective integer programming model based on the forecasting sales and forecasting errors to explore the optimal quantity and time of products of anticipatory shipping. Finally, using TOPSIS, Shannon entropy, and LINMAP decision-making methods, the final optimal solution is selected from the Pareto set obtained by the NSGA-II algorithm. The case study results show that anticipatory shipping saves 5.96% cost for the online retailer and 1.69 days of waiting time on average for the customer compared with non-anticipatory shipping. Moreover, performing anticipatory shipping is especially beneficial in the case of high inventory holding cost, large distribution center capacity, and long transportation duration.",10.1016/j.cie.2021.107445,Anticipatory shipping; Multiobjective; Data-driven; Integer programming model; Online retail,,
Featureless adaptive optimization accelerates functional electronic materials design,"Wang, YQ; Iyer, A; Chen, W; Rondinelli, JM",APPLIED PHYSICS REVIEWS,2020.0,"Electronic materials that exhibit phase transitions between metastable states (e.g., metal-insulator transition materials with abrupt electrical resistivity transformations) are challenging to decode. For these materials, conventional machine learning methods display limited predictive capability due to data scarcity and the absence of features that impede model training. In this article, we demonstrate a discovery strategy based on multi-objective Bayesian optimization to directly circumvent these bottlenecks by utilizing latent variable Gaussian processes combined with high-fidelity electronic structure calculations for validation in the chalcogenide lacunar spinel family. We directly and simultaneously learn phase stability and bandgap tunability from chemical composition alone to efficiently discover all superior compositions on the design Pareto front. Previously unidentified electronic transitions also emerge from our featureless adaptive optimization engine. Our methodology readily generalizes to optimization of multiple properties, enabling co-design of complex multifunctional materials, especially where prior data is sparse.",10.1063/5.0018811,,,
Development and application of a machine learning based multi-objective optimization workflow for CO2-EOR projects,"You, JY; Ampomah, W; Sun, Q",FUEL,2020.0,"Carbon dioxide-Enhanced Oil Recovery (CO2-EOR) is known as one of techniques for hydrocarbon production improvement as wells as an important candidate to reduce greenhouse gas emissions. Thus, an ideal development strategy for a CO2-EOR project would consider multiple objectives including to maximize oil recovery, CO2 storage volume and project economic outcomes. This work proposes a robust computational framework that couples artificial neural network (ANN) and multi-objective optimizers to optimize the aforementioned objectives in CO2-EOR processes simultaneously. Expert ANN systems are trained and employed as surrogate models of the high-fidelity compositional simulator in the optimization workflow. The robustness of the development optimization protocol is confirmed via a synthetic injection-pattern-base case study. Afterward a field implementation to Morrow-B formation to optimize the tertiary recovery stage of the field development is discussed. This work compares the optimum solution found using an aggregate objective function and the solution repository (Pareto front) generated by the multi-objective optimization process. The comparison indicates the existence of potential multi-solutions satisfying certain criteria in a CO2-EOR project designing, which cannot be found using traditional weighted sum method. The optimization results provide significant insight into the decision-making process of CO2-EOR project when multiple objective functions are considered.",10.1016/j.fuel.2019.116758,Carbon dioxide sequestration; CO2-EOR; Multi-objective optimization; Artificial neural network,,
Hybrid ensemble soft computing approach for predicting penetration rate of tunnel boring machine in a rock environment,"Bardhan, A; Kardani, N; GuhaRay, A; Burman, A; Samui, P; Zhang, YM",JOURNAL OF ROCK MECHANICS AND GEOTECHNICAL ENGINEERING,2021.0,"This study implements a hybrid ensemble machine learning method for forecasting the rate of penetration (ROP) of tunnel boring machine (TBM), which is becoming a prerequisite for reliable cost assessment and project scheduling in tunnelling and underground projects in a rock environment. For this purpose, a sum of 185 datasets was collected from the literature and used to predict the ROP of TBM. Initially, the main dataset was utilised to construct and validate four conventional soft computing (CSC) models, i.e. minimax probability machine regression, relevance vector machine, extreme learning machine, and functional network. Consequently, the estimated outputs of CSC models were united and trained using an artificial neural network (ANN) to construct a hybrid ensemble model (HENSM). The outcomes of the proposed HENSM are superior to other CSC models employed in this study. Based on the experimental results (training RMSE = 0.0283 and testing RMSE = 0.0418), the newly proposed HENSM is potential to assist engineers in predicting ROP of TBM in the design phase of tunnelling and underground projects. (C) 2021 Institute of Rock and Soil Mechanics, Chinese Academy of Sciences. Production and hosting by Elsevier B.V.",10.1016/j.jrmge.2021.06.015,Tunnel boring machine (TBM); Rate of penetration (ROP); Artificial intelligence; Artificial neural network (ANN); Ensemble modelling,,
Hybrid Approach of SVM and Feature Selection Based Optimization Algorithm for Big Data Security,"Duhan, B; Dhankhar, N",PROCEEDINGS OF ICETIT 2019: EMERGING TRENDS IN INFORMATION TECHNOLOGY,2020.0,"As internet is growing at a fast rate due to which the cyber attacks have also increased. The type and rate of occurrence of these attacks is increasing rapidly. There are many traditional security solutions existing but these solutions do not perform well in case of Big Data. Securing Big Data from attacks needs a different approach rather than traditional solutions. In this paper spark tool is used. Spark tool has various advanced features like parallel processing of data. Its library provides some inbuilt machine learning algorithms. The dataset used is NSL KDDCUP which has size in MB's. This dataset is best suited for Big Data as a test case. An approach is proposed for intrusion detection i.e., svm classifier is used and feature selection is done by using a new kind of approach named pareto fronts multi objective based genetic algorithm.",10.1007/978-3-030-30577-2_62,Big data; Intrusion Detection System; SVM; Pareto fronts; Genetic algorithm,,
Multi-objective cost-sensitive attribute reduction on data with error ranges,"Fang, Y; Liu, ZH; Min, F",INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS,2016.0,"In current supervised machine learning research spectrum, there are several attribute reduction methodologies to acquire reducts with low test cost. They can deal with symbolic data, or numeric data with error ranges. In many cases, they consider the situation with only one type of cost; therefore the problem is single-objective. This paper addresses the attribute reduction problem on data with multi-type-costs and error ranges. First, we define the multi-objective attribute reduction problem where multi-type-costs are involved. Second, we propose three metrics to evaluate the quality of a reduct set. Third, we design a backtrack algorithm to compute the Pareto optimal set, and a heuristic algorithm to find a sub-optimal reduct set. Finally, we compare these algorithms on seven UCI (University of California-Irvine) datasets. Experimental results indicate that our heuristic algorithm has good capability of tackling the proposed problem.",10.1007/s13042-014-0296-3,Cost-sensitive learning; Attribute reduction; Test cost; Error range,,
Multi-objective optimization for limiting tunnel-induced damages considering uncertainties,"Zhang, LM; Lin, PH",RELIABILITY ENGINEERING & SYSTEM SAFETY,2021.0,"Due to the rapid development of the urban metro system, the situation of new excavation work being conducted adjacent to existing tunnels is quite common and becomes prime hazards in the tunnel design stage, together with uncertainties from the ground condition. To solve this problem, this paper develops a hybrid approach that integrates ensemble learning and non-dominant sorting genetic algorithm-II (NSGA-II) to mitigate the limit support pressure (LSP) and the ground surface deformation (GSD) during the tunnel excavation for improved design. The extreme gradient boosting (XGBoost) algorithm is used to establish ensemble learning models predicting LSP and GSD, where the new tunnel is constructed in parallel to an existing tunnel. NSGA-II is further used to optimize the two targets (i.e., LSP and GSD), considering the uncertainties from geotechnical conditions and errors from the meta-model. With the Monte-Carlo simulation, probability constraints are established to conduct the multi-objective optimization (MOO). Finally, the Pareto front is generated to obtain the best location of the new tunnel, and a comparison is made between MOO with and without considering uncertainties. The best solution is selected by the criterion of the point with the shortest distance from the ideal point. It is found that after considering uncertainties: (1) The improvement percentage of LSP is increased from 9.67% to 11.03%, and that of GSD drops from 2.39% to 0.9%; (2) A higher stability of improvement from optimization is achieved with the standard deviation of improvement percentage drops from 0.310 to 0.298 for LSP and 0.024 to 0.020 for GSD; (3) With a weaker confidence on the meta-model, a higher degree of sacrifice on GSD is observed. The novelty of the proposed approach lies in its capability to not only predict and optimize the damage from excavation adjacent to an existing tunnel, but also consider various types of uncertainties from geological conditions and meta-models to guarantee reliability.",10.1016/j.ress.2021.107945,Multi-objective optimization; Probability constraints; Ensemble learning; Tunnel alignment,,
Accurate Multiobjective Design in a Space of Millions of Transition Metal Complexes with Neural-Network-Driven Efficient Global Optimization,"Janet, JP; Ramesh, S; Duan, C; Kulik, HJ",ACS CENTRAL SCIENCE,2020.0,"The accelerated discovery of materials for real world applications requires the achievement of multiple design objectives. The multidimensional nature of the search necessitates exploration of multimillion compound libraries over which even density functional theory (DFT) screening is intractable. Machine learning (e.g., artificial neural network, ANN, or Gaussian process, GP) models for this task are limited by training data availability and predictive uncertainty quantification (UQ). We overcome such limitations by using efficient global optimization (EGO) with the multidimensional expected improvement (EI) criterion. EGO balances exploitation of a trained model with acquisition of new DFT data at the Pareto front, the region of chemical space that contains the optimal trade-off between multiple design criteria. We demonstrate this approach for the simultaneous optimization of redox potential and solubility in candidate M(II)/M(III) redox couples for redox flow batteries from a space of 2.8 M transition metal complexes designed for stability in practical redox flow battery (RFB) applications. We show that a multitask ANN with latent-distance-based UQ surpasses the generalization performance of a GP in this space. With this approach, ANN prediction and EI scoring of the full space are achieved in minutes. Starting from ca. 100 representative points, EGO improves both properties by over 3 standard deviations in only five generations. Analysis of lookahead errors confirms rapid ANN model improvement during the EGO process, achieving suitable accuracy for predictive design in the space of transition metal complexes. The ANN-driven EI approach achieves at least 500-fold acceleration over random search, identifying a Pareto-optimal design in around 5 weeks instead of 50 years.",10.1021/acscentsci.0c00026,,,
Tornado Detection with Kernel-Based Classifiers from WSR-88D Radar Data,"Trafalis, TB; Santosa, B; Richman, MB","DYNAMICS OF DISASTERS-KEY CONCEPTS, MODELS, ALGORITHMS, AND INSIGHTS",2016.0,"Detection of tornadoes that provides warning times sufficient for evasive action prior to a tornado strike has been a well-established objective of weather forecasters. With modern technology, progress has been made on increasing the average lead time of such warnings, which translates into a number of lives saved. Recently, machine learning (e.g., kernel methods) has been added to the collection of techniques brought to bear on severe weather prediction. In this chapter, we seek to extend this innovation by introducing and applying two types of kernel-based methods, support vector machines and minimax probability machines to detect tornadoes, using attributes from radar derived velocity data. These two approaches utilize kernel methods to address nonlinearity of the data in the input space. The approaches are based on maximizing the margin between two different classes: tornado and no tornado. The use of the Weather Surveillance Radar 1988 Doppler, with continuous data streaming every 6 min, presents a source for a dynamic data driven application system. The results are compared to those produced by neural networks (NN). Findings indicate that these kernel approaches are significantly more accurate than NN for the tornado detection problem.",10.1007/978-3-319-43709-5_16,Tornado detection; Dynamic data driven application; Generalization error; Feedforward neural networks; Kernel methods,,
Predicting flood susceptibility using LSTM neural networks,"Fang, ZC; Wang, Y; Peng, L; Hong, HY",JOURNAL OF HYDROLOGY,2021.0,"Identifying floods and producing flood susceptibility maps are crucial steps for decision-makers to prevent and manage disasters. Plenty of studies have used machine learning models to produce reliable susceptibility maps. Nevertheless, most research ignores the importance of developing appropriate feature engineering methods. In this study, we propose a local spatial sequential long short-term memory neural network (LSS-LSTM) for flood susceptibility prediction in Shangyou County, China. The three main contributions of this study are summarized below. First of all, it is a new perspective to use the deep learning technique of LSTM for flood susceptibility prediction. Second, we integrate an appropriate feature engineering method with LSTM to predict flood susceptibility. Third, we implement two optimization techniques of data augmentation and batch normalization to further improve the performance of the proposed method. The LSS-LSTM method can not only capture the attribution information of flood conditioning factors and the local spatial information of flood data, but also has powerful sequential modelling capabilities to deal with the spatial relationship of floods. The experimental results demonstrate that the LSS-LSTM method achieves satisfactory prediction performance (93.75% and 0.965) in terms of accuracy and area under the receiver operating characteristic (ROC) curve.",10.1016/j.jhydrol.2020.125734,Flood susceptibility prediction; Long short-term memory neural network; Deep learning; Feature engineering,,
Discovering optimal strategies for mitigating COVID-19 spread using machine learning: Experience from Asia,"Pan, Y; Zhang, LM; Yan, ZZ; Lwin, MO; Skibniewski, MJ",SUSTAINABLE CITIES AND SOCIETY,2021.0,"To inform data-driven decisions in fighting the global pandemic caused by COVID-19, this research develops a spatiotemporal analysis framework under the combination of an ensemble model (random forest regression) and a multi-objective optimization algorithm (NSGA-II). It has been verified for four Asian countries, including Japan, South Korea, Pakistan, and Nepal. Accordingly, we can gain some valuable experience to better understand the disease evolution, forecast the prevalence of the disease, which can provide sustainable evidence to guide further intervention and management. Random forest with a proper rolling time-window can learn the combined effects of environmental and social factors to accurately predict the daily growth of confirmed cases and daily death rate on a national scale, which is followed by NSGA-II to find a range of Pareto optimal solutions for ensuring the minimization of the infection rate and mortality at the same time. Experimental results demonstrate that the predictive model can alert the local government in advance, allowing the accused time to put forward relevant measures. The temperature in the category of environment and the stringency index belonging to the social factor are identified as the top 2 important features to exert a greater impact on the virus transmission. Moreover, optimal solutions provide references to design the best control strategies towards pandemic containment and prevention that can accommodate the country-specific circumstance, which are possible to decrease the two objectives by more than 95%. In particular, appropriate adjustment of social-related features needs to take priority over others, since it can bring about at least 1.47% average improvement of two objectives compared to environmental factors.",10.1016/j.scs.2021.103254,COVID-19; Random forest regression; Feature importance analysis; Multi-objective optimization; Sustainability,,
A multi-objective optimization algorithm for feature selection problems,"Abdollahzadeh, B; Gharehchopogh, FS",ENGINEERING WITH COMPUTERS,,"Feature selection (FS) is a critical step in data mining, and machine learning algorithms play a crucial role in algorithms performance. It reduces the processing time and accuracy of the categories. In this paper, three different solutions are proposed to FS. In the first solution, the Harris Hawks Optimization (HHO) algorithm has been multiplied, and in the second solution, the Fruitfly Optimization Algorithm (FOA) has been multiplied, and in the third solution, these two solutions are hydride and are named MOHHOFOA. The results were tested with MOPSO, NSGA-II, BGWOPSOFS and B-MOABC algorithms for FS on 15 standard data sets with mean, best, worst, standard deviation (STD) criteria. The Wilcoxon statistical test was also used with a significance level of 5% and the Bonferroni-Holm method to control the family-wise error rate. The results are shown in the Pareto front charts, indicating that the proposed solutions' performance on the data set is promising.",10.1007/s00366-021-01369-9,Feature selection; Harris hawks optimization; Fruitfly optimization algorithm; Multiobjective; Bonferroni&#8211; Holm; Family-wise error rate,,
An innovative hybrid model based on outlier detection and correction algorithm and heuristic intelligent optimization algorithm for daily air quality index forecasting,"Wang, JZ; Du, P; Hao, Y; Ma, X; Niu, T; Yang, WD",JOURNAL OF ENVIRONMENTAL MANAGEMENT,2020.0,"Air pollution forecasting plays an important role in helping reduce air pollutant emission and guiding people's daily activities and warning the public in advance. Nevertheless, previous articles still have many shortcomings, such as ignoring the importance of outlier point detection and correction of original time series, and random initial parameters of models, and so on. A new hybrid model using outlier detection and correction algorithm and heuristic intelligent optimization algorithm is proposed in this study to address the above mentioned problems. First, data preprocessing algorithms are conducted to detect and correct outliers, excavate the main characteristics of the original time series; second, a widely used heuristic intelligent optimization algorithm is adopted to optimize the parameters of extreme learning machine to obtain the forecasting results of each subseries with improvement in accuracy; finally, experimental results and analysis show that the presented hybrid model provides accurate prediction, outperforming other comparison models, which emphasize the importance of outlier point detection and correction and optimization parameters of models, it also give a new feasible method for air pollution prediction, and contribute to make effective plans for air pollutant emissions.",10.1016/j.jenvman.2019.109855,Air quality index; Sine cosine algorithm; Hybrid forecasting model; Outlier detection and correction,,
A hybrid ensemble pruning approach based on consensus clustering and multi-objective evolutionary algorithm for sentiment classification,"Onan, A; Korukoglu, S; Bulut, H",INFORMATION PROCESSING & MANAGEMENT,2017.0,"Sentiment analysis is a critical task of extracting subjective information from online text documents. Ensemble learning can be employed to obtain more robust classification schemes. However, most approaches in the field incorporated feature engineering to build efficient sentiment classifiers. The purpose of our research is to establish an effective sentiment classification scheme by pursuing the paradigm of ensemble pruning. Ensemble pruning is a crucial method to build classifier ensembles with high predictive accuracy and efficiency. Previous studies employed exponential search, randomized search, sequential search, ranking based pruning and clustering based pruning. However, there are tradeoffs in selecting the ensemble pruning methods. In this regard, hybrid ensemble pruning schemes can be more promising. In this study, we propose a hybrid ensemble pruning scheme based on clustering and randomized search for text sentiment classification. Furthermore, a consensus clustering scheme is presented to deal with the instability of clustering results. The classifiers of the ensemble are initially clustered into groups according to their predictive characteristics. Then, two classifiers from each cluster are selected as candidate classifiers based on their pairwise diversity. The search space of candidate classifiers is explored by the elitist Pareto-based multi-objective evolutionary algorithm. For the evaluation task, the proposed scheme is tested on twelve balanced and unbalanced benchmark text classification tasks. In addition, the proposed approach is experimentally compared with three ensemble methods (AdaBoost, Bagging and Random Subspace) and three ensemble pruning algorithms (ensemble selection from libraries of models, Bagging ensemble selection and LibD3C algorithm). Results demonstrate that the consensus clustering and the elitist pareto-based multi-objective evolutionary algorithm can be effectively used in ensemble pruning. The experimental analysis with conventional ensemble methods and pruning algorithms indicates the validity and effectiveness of the proposed scheme. (C) 2017 Elsevier Ltd. All rights reserved.",10.1016/j.ipm.2017.02.008,Ensemble pruning; Consensus clustering; Multi-objective evolutionary algorithm; Sentiment classification,,
Instance Selection Using Multi-objective CHC Evolutionary Algorithm,"Rathee, S; Ratnoo, S; Ahuja, J",INFORMATION AND COMMUNICATION TECHNOLOGY FOR COMPETITIVE STRATEGIES,2019.0,"Data reduction has always been an important field of research to enhance the performance of data mining algorithms. Instance selection, a data reduction technique, relates to selecting a subset of informative and non-redundant examples from data. This paper deals with the problem of instance selection in a multi-objective perspective and, hence, proposes a multi-objective cross-generational elitist selection, heterogeneous recombination, and cataclysmic mutation (CHC) for discovering a set of Pareto-optimal solutions. The suggested MOCHC algorithm integrates the concept of non-dominating sorting with CHC. The algorithm has been employed to eight datasets available from UCI machine learning repository. The MOCHC has been successful in finding a range of multiple optimal solutions instead of yielding a single solution. These solutions provide a user with several choices of reduced datasets. Further, the solutions may be combined into a single instance subset by exploiting the promising characteristics across the potentially good solutions based on some user-defined criteria.",10.1007/978-981-13-0586-3_48,Multi-objective optimization; CHC algorithm; Instance selection; KNN,,
Comparative analysis of image projection-based descriptors in Siamese neural networks,"Kertesz, G; Szenasi, S; Vamossy, Z",ADVANCES IN ENGINEERING SOFTWARE,2021.0,"Low-level object matching can be done using projection signatures. In case of a large number of projections, the matching algorithm has to deal with less significant slices. A trivial approach would be to do statistical analysis or apply machine learning to determine the significant features. To take adjacent values of the projection matrices into account, a convolutional neural network should be used. To compare two matrices, a Siamese structure of convolutional heads can be applied. In this paper, an experiment is designed and implemented to analyze the object matching performance of Siamese Convolutional Neural Networks based on multi-directional image projection data. A backtracking search-based Neural Architecture Generation method is used to create convolutional architectures, and a Master/Worker structured distributed processing with highly efficient scheduling based on the Longest Processing Times-heuristics is used for parallel training and evaluation of the models. Results show that the projection-based methods are Pareto optimal in terms of one-shot classification accuracy and memory consumption.",10.1016/j.advengsoft.2020.102963,Siamese neural networks; Multi-directional image projections; Neural architecture generation; one-shot classification accuracy; Vehicle re-identification,,
Combining Outcome-Based and Preference-Based Matching: A Constrained Priority Mechanism,"Acharya, A; Bansak, K; Hainmueller, J",POLITICAL ANALYSIS,2022.0,"We introduce a constrained priority mechanism that combines outcome-based matching from machine learning with preference-based allocation schemes common in market design. Using real-world data, we illustrate how our mechanism could be applied to the assignment of refugee families to host country locations, and kindergarteners to schools. Our mechanism allows a planner to first specify a threshold g for the minimum acceptable average outcome score that should be achieved by the assignment. In the refugee matching context, this score corresponds to the probability of employment, whereas in the student assignment context, it corresponds to standardized test scores. The mechanism is a priority mechanism that considers both outcomes and preferences by assigning agents (refugee families and students) based on their preferences, but subject to meeting the planner's specified threshold. The mechanism is both strategy-proof and constrained efficient in that it always generates a matching that is not Pareto dominated by any other matching that respects the planner's threshold.",10.1017/pan.2020.48,game theory; machine learning; matching; political market design; social choice,,
Multi-objective algorithm for the design of prediction intervals for wind power forecasting model,"Jiang, P; Li, RR; Li, HM",APPLIED MATHEMATICAL MODELLING,2019.0,"composite forecasting framework is designed and implemented successfully to estimate the prediction intervals of wind speed time series simultaneously through machine learning method embedding a newly proposed optimization method (multi-objective salp swarm algorithm). In this study, data pre-process strategy based on feature extraction is served for reducing the fluctuations of wind power generation and select appropriate input forms of wind speed datasets for the sake of improving the overall performance. Besides, fuzzy set theory selection technique is used to determine the best compromise solutions from Pareto front set deriving from the optimization phase. To test the effectiveness of the proposed composite forecasting framework, several case studies based on different time scale wind speed datasets are conducted. The corresponding results present that the proposed framework significantly outperforms other benchmark methods, and it can provide very satisfactory results in both goals between high coverage and small width. (C) 2018 Elsevier Inc. All rights reserved.",10.1016/j.apm.2018.10.019,Interval forecasting; Multi-objective salp swarm algorithm; Fuzzy set theory; Best compromise solution; Least square support vector machine,,
Gully Head-Cut Distribution Modeling Using Machine Learning Methods-A Case Study of NW Iran,"Arabameri, A; Chen, W; Blaschke, T; Tiefenbacher, JP; Pradhan, B; Bui, DT",WATER,2020.0,"To more effectively prevent and manage the scourge of gully erosion in arid and semi-arid regions, we present a novel-ensemble intelligence approach-bagging-based alternating decision-tree classifier (bagging-ADTree)-and use it to model a landscape's susceptibility to gully erosion based on 18 gully-erosion conditioning factors. The model's goodness-of-fit and prediction performance are compared to three other machine learning algorithms (single alternating decision tree, rotational-forest-based alternating decision tree (RF-ADTree), and benchmark logistic regression). To achieve this, a gully-erosion inventory was created for the study area, the Chah Mousi watershed, Iran by combining archival records containing reports of gully erosion, remotely sensed data from Google Earth, and geolocated sites of gully head-cuts gathered in a field survey. A total of 119 gully head-cuts were identified and mapped. To train the models' analysis and prediction capabilities, 83 head-cuts (70% of the total) and the corresponding measures of the conditioning factors were input into each model. The results from the models were validated using the data pertaining to the remaining 36 gully locations (30%). Next, the frequency ratio is used to identify which conditioning-factor classes have the strongest correlation with gully erosion. Using random-forest modeling, the relative importance of each of the conditioning factors was determined. Based on the random-forest results, the top eight factors in this study area are distance-to-road, drainage density, distance-to-stream, LU/LC, annual precipitation, topographic wetness index, NDVI, and elevation. Finally, based on goodness-of-fit and AUROC of the success rate curve (SRC) and prediction rate curve (PRC), the results indicate that the bagging-ADTree ensemble model had the best performance, with SRC (0.964) and PRC (0.978). RF-ADTree (SRC = 0.952 and PRC = 0.971), ADTree (SRC = 0.926 and PRC = 0.965), and LR (SRC = 0.867 and PRC = 0.870) were the subsequent best performers. The results also indicate that bagging and RF, as meta-classifiers, improved the performance of the ADTree model as a base classifier. The bagging-ADTree model's results indicate that 24.28% of the study area is classified as having high and very high susceptibility to gully erosion. The new ensemble model accurately identified the areas that are susceptible to gully erosion based on the past patterns of formation, but it also provides highly accurate predictions of future gully development. The novel ensemble method introduced in this research is recommended for use to evaluate the patterns of gullying in arid and semi-arid environments and can effectively identify the most salient conditioning factors that promote the development and expansion of gullies in erosion-susceptible environments.",10.3390/w12010016,gully head-cuts; machine learning modeling; soil erosion; Iran,,
Parsimonious neural networks learn interpretable physical laws,"Desai, S; Strachan, A",SCIENTIFIC REPORTS,2021.0,"Machine learning is playing an increasing role in the physical sciences and significant progress has been made towards embedding domain knowledge into models. Less explored is its use to discover interpretable physical laws from data. We propose parsimonious neural networks (PNNs) that combine neural networks with evolutionary optimization to find models that balance accuracy with parsimony. The power and versatility of the approach is demonstrated by developing models for classical mechanics and to predict the melting temperature of materials from fundamental properties. In the first example, the resulting PNNs are easily interpretable as Newton's second law, expressed as a non-trivial time integrator that exhibits time-reversibility and conserves energy, where the parsimony is critical to extract underlying symmetries from the data. In the second case, the PNNs not only find the celebrated Lindemann melting law, but also new relationships that outperform it in the pareto sense of parsimony vs. accuracy.",10.1038/s41598-021-92278-w,,,
KNOWLEDGE EXTRACTION FOR PRODUCTION MANAGEMENT,"KARNI, R; FOURNIER, F",JOURNAL OF INTELLIGENT MANUFACTURING,1994.0,"A procedure and underlying algorithm for extracting knowledge from production and inventory databases to support engineering management activities is described. The process searches for, detects and isolates behaviour patterns inherent in the data. It relates these pattems to production irregularities, suggests connections with specific causes and helps propose possible corrective or preventive actions. The approach is based on a four-phase procedure: (1) the decision-maker focuses on the subject or difficulty at issue, represented by a target concept; (2) the KEDB algorithm, based on a machine learning approach, processes the relevant database and provides knowledge characterizing and classifying the target concept; (3) the output is interpreted in Pareto fashion as a series of possible circumstances explaining the target concept behaviour; and (4) based on these causes, the decision-maker decides on possible corrective actions to improve the situation, or preventive actions to forestall unfavourable conditions. A case study based on an actual quality control database is detailed.",10.1007/BF00123921,PRODUCTION MANAGEMENT; KNOWLEDGE EXTRACTION; MACHINE LEARNING; INDUSTRIAL DATABASES,,
Directional Adversarial Training for Recommender Systems,"Xu, YJ; Chen, L; Xie, FF; Hu, WB; Zhu, JM; Chen, CA; Zheng, ZB",ECAI 2020: 24TH EUROPEAN CONFERENCE ON ARTIFICIAL INTELLIGENCE,2020.0,"Adversarial training is shown as an effective method to improve the generalization ability of deep learning models by making random perturbations in the input space during model training. A recent study has successfully applied adversarial training into recommender systems by perturbing the embeddings of users and items through a minimax game. However, this method ignores the collaborative signal in recommender systems and fails to capture the smoothness in data distribution. We argue that the collaborative signal, which reveals the behavioural similarity between users and items, is critical to modeling recommender systems. In this work, we develop the Directional Adversarial Training (DAT) strategy by explicitly injecting the collaborative signal into the perturbation process. That is, both users and items are perturbed towards their similar neighbours in the embedding space with proper restriction. To verify its effectiveness, we demonstrate the use of DAT on Generalized Matrix Factorization (GMF), one of the most representative collaborative filtering methods. Our experimental results on three public datasets show that our method (called DAGMF) achieves a significant accuracy improvement over GMF and meanwhile, it is less prone to overfitting than GMF.",10.3233/FAIA200138,,,
A novel adversarial learning framework in deep convolutional neural network for intelligent diagnosis of mechanical faults,"Han, T; Liu, C; Yang, WG; Jiang, DX",KNOWLEDGE-BASED SYSTEMS,2019.0,"In recent years, deep learning has become an emerging research orientation in the field of intelligent monitoring and fault diagnosis for industry equipment. Generally, the success of supervised deep models is largely attributed to a mass of typically labeled data, while it is often limited in real diagnosis tasks. In addition, the diagnostic model trained with data from limited conditions may generalize poorly for conditions not observed during training. To tackle these challenges, adversarial learning is introduced as a regularization into the convolutional neural network (CNN), and a novel deep adversarial convolutional neural network (DACNN) is accordingly proposed in this paper. By adding an additional discriminative classifier, an adversarial learning framework can be developed to train the convolutional blocks with the split data subsets, leading to a minimax two-player game. This process contributes to making the feature representation robust, boosting the generalization ability of the trained model as well as avoiding overfitting with a small size of labeled samples. The comparison studies with respect to conventional deep models on two fault datasets demonstrate the applicability and superiority of proposed method. (C) 2018 Elsevier B.V. All rights reserved.",10.1016/j.knosys.2018.12.019,Adversarial training; Generative adversarial network; Deep convolutional neural network; Intelligent fault diagnosis; Rotating machinery,,
Developing Automatic Multi-Objective Optimization Methods for Complex Actuators,"Chis, R; Vintan, L",ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING,2017.0,"This paper presents the analysis and multiobjective optimization of a magnetic actuator. By varying just 8 parameters of the magnetic actuator's model the design space grows to more than 6 million configurations. Much more, the 8 objectives that must be optimized are conflicting and generate a huge objectives space, too. To cope with this complexity, we use advanced heuristic methods for Automatic Design Space Exploration. FADSE tool is one Automatic Design Space Exploration framework including different state of the art multi-objective meta-heuristics for solving NP-hard problems, which we used for the analysis and optimization of the COMSOL and MATLAB model of the magnetic actuator. We show that using a state of the art genetic multi-objective algorithm, response surface modelling methods and some machine learning techniques, the timing complexity of the design space exploration can be reduced, while still taking into consideration objective constraints so that various Pareto optimal configurations can be found. Using our developed approach, we were able to decrease the simulation time by at least a factor of 10, compared to a run that does all the simulations, while keeping prediction errors to around 1%.",10.4316/AECE.2017.04011,actuators; computer aided engineering; machine learning; pareto optimization; response surface methodology,,
"Multi-objective optimization of PEM fuel cell by coupled significant variables recognition, surrogate models and a multi-objective genetic algorithm","Li, HW; Xu, BS; Lu, GL; Du, CH; Huang, N",ENERGY CONVERSION AND MANAGEMENT,2021.0,"This paper aims to present a fast and systematic optimization approach for proton exchange membrane fuel cell (PEMFC) by combining variance analysis, surrogate models and non-dominated sorting genetic algorithm (NSGA-II). First, a three-dimensional steady-state PEMFC computational fluid dynamics (CFD) model is developed as the base model for optimization. Second, six variables that have significant effect on PEMFC performance are selected from numerious common parameters using variance analysis, reducing the number of decision variables from 11 to 6. Then, three data-driven ensemble learning models are trained as surrogate models to accelerate the fitness values evaluation of the optimization algorithm. Finally, three PEMFC performance indexes, including power density, system efficiency and oxygen distribution uniformity on cathode catalyst layer are optimized simultaneously based on NSGA-II. Using the NSGA-II combined with surrogate models, a set of Pareto solutions is obtained in a short time. The results indicate that PEMFCs with optimized parameters perform better than the base model in terms of all three performance indexes, demonstrating the success of this approach in solving time-consuming multi-optimization problems. This study provides a fast and systematic approach for PEMFC multi-objective optimization and can be a guide for engineering applications.",10.1016/j.enconman.2021.114063,Proton exchange membrane fuel cell (PEMFC); Multi-objective optimization; Significant variables recognition; NSGA-II; Ensemble learning model,,
Conceptual structural system layouts via design response grammars and evolutionary algorithms,"Boonstra, S; van der Blom, K; Hofmeyer, H; Emmerich, MTM",AUTOMATION IN CONSTRUCTION,2020.0,"Two new methods to generate structural system layouts for conceptual building spatial designs are presented. The first method, the design response grammar, uses design rules-configurable by parameters-to develop a structural system layout step by step as a function of a building spatial design's geometry and preliminary assessments of the structural system under development. The second method, design via optimizer assignment, uses an evolutionary algorithm to assign structural components to a building spatial design's geometry. In this work, the methods are demonstrated for two objectives: minimal strain energy (a commonly used objective for structural topology optimization) and minimal structural volume. In a first case study three building spatial designs have been subjected to the methods: Design via optimizer assignment yields a uniformly distributed Pareto front approximation, which incorporates the best performing layouts among both methods. On the other hand, results of the design response grammar show that layouts that correspond to specific positions on the Pareto front (e.g. layouts that perform well for strain energy), share the same parameter configurations among the three different building spatial designs. By generalizing, specific points on the Pareto front approximation have been expressed in terms of parameter configurations. A second case study addresses the use of a generic material and generic dimensions in the assessment of structural system layouts. The study applies a technique similar to topology optimization to optimize the material density distribution of each individual structural component, which can be regarded as a part of determining materials and dimensions in more advanced stages of the design of a system layout. This optimization approach is applied to the layouts that are part of the Pareto front approximations as found by the evolutionary algorithm in the first case study, the study shows that-after optimization-the fronts remain the same qualitatively, suggesting that the methods produce results that are also useful in more advanced design stages. A final case study tests the generalization that is established in the first case study by using the found configurations for the design response grammar, and it is shown that the generated layouts indeed are positioned near the desired positions on the Pareto front approximation found by the evolutionary algorithm. Although the evolutionary algorithm can find better performing solutions among a better distributed Pareto front approximation, the design response grammar uses only a fraction of the computational cost. As such it is concluded that the design response grammar is a promising support tool for the exploration and structural assessment of conceptual building spatial designs. Future research should focus on more types of structural elements; more objectives; new constraints to ensure feasible solutions, especially stress constraints; and the application of state-of-the-art techniques like machine learning to find more generalizations.",10.1016/j.autcon.2019.103009,Building spatial design; Multi-disciplinary design; Design grammar; Structural design; Automated design; Design optimization; Conceptual design,,
A multi-objective predictive energy management strategy for residential grid-connected PV-battery hybrid systems based on machine learning technique,"Shivam, K; Tzou, JC; Wu, SC",ENERGY CONVERSION AND MANAGEMENT,2021.0,"This paper proposes a multi-objective predictive energy management strategy based on machine learning technique for residential grid-connected hybrid energy systems. The hybrid system considered in this study comprise three principal components: a photovoltaic array as a renewable energy source, a battery bank as an energy storage system, and residential building as an electric load. The proposed strategy comprises three levels of controls: a logical level to manage the computational load and accuracy, a dual prediction model based on residual causal dilated convolutional networks for energy production and electric load on system, and a multiobjective optimization for efficient trade of energy with the utility grid by battery charge scheduling. The prediction model used in this study can provide one-step ahead photovoltaic energy production and load forecast with sufficient accuracy using a sliding window training technique and can be implemented on an average personal computer. The energy management problem comprises multiple objectives that include minimization of energy bought from utility grid, maximization the battery bank?s state-of-charge and reduction of carbon dioxide emission. The optimization problem is constrained to the maximum allowed carbon dioxide production and battery bank?s state-of-charge limits. The proposed strategy is tested for static and dynamic electricity prices using hourly energy and load data. Simulation results show a high coefficient of determination of 93.08% for energy production predictions and 97.25% for electric load predictions using proposed dual prediction model. The proposed prediction model is benchmarked against na?ve prediction, support vector machine and artificial neural network models using several metrics and shows noticeable improvements in prediction accuracy. Not only the proposed strategy combined with the proposed prediction model can handle over 50% of the total yearly load requirement but also shows a significant decrease in electricity bill and carbon dioxide compared to residential buildings without hybrid energy systems and hybrid energy system without energy management strategy.",10.1016/j.enconman.2021.114103,Energy management strategy; Hybrid energy systems; Machine learning; Multiobjective evolutionary algorithm; Predictive control; Time-series forecasting,,
floodGAN: Using Deep Adversarial Learning to Predict Pluvial Flooding in Real Time,"Hofmann, J; Schuttrumpf, H",WATER,2021.0,"Using machine learning for pluvial flood prediction tasks has gained growing attention in the past years. In particular, data-driven models using artificial neuronal networks show promising results, shortening the computation times of physically based simulations. However, recent approaches have used mainly conventional fully connected neural networks which were (a) restricted to spatially uniform precipitation events and (b) limited to a small amount of input data. In this work, a deep convolutional generative adversarial network has been developed to predict pluvial flooding caused by nonlinear spatial heterogeny rainfall events. The model developed, floodGAN, is based on an image-to-image translation approach whereby the model learns to generate 2D inundation predictions conditioned by heterogenous rainfall distributions-through the minimax game of two adversarial networks. The training data for the floodGAN model was generated using a physically based hydrodynamic model. To evaluate the performance and accuracy of the floodGAN, model multiple tests were conducted using both synthetic events and a historic rainfall event. The results demonstrate that the proposed floodGAN model is up to 10(6) times faster than the hydrodynamic model and promising in terms of accuracy and generalizability. Therefore, it bridges the gap between detailed flood modelling and real-time applications such as end-to-end early warning systems.",10.3390/w13162255,flood modelling; machine learning; deep learning; generative adversarial networks; real-time flood forecasting,,
Fair Classification and Social Welfare,"Hu, L; Chen, YL","FAT* '20: PROCEEDINGS OF THE 2020 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY",2020.0,"Now that machine learning algorithms lie at the center of many important resource allocation pipelines, computer scientists have been unwittingly cast as partial social planners. Given this state of affairs, important questions follow. How do leading notions of fairness as defined by computer scientists map onto longer-standing notions of social welfare? In this paper, we present a welfare-based analysis of fair classification regimes. Our main findings assess the welfare impact of fairness-constrained empirical risk minimization programs on the individuals and groups who are subject to their outputs. We fully characterize the ranges of Delta epsilon perturbations to a fairness parameter epsilon in a fair Soft Margin SVM problem that yield better, worse, and neutral outcomes in utility for individuals and by extension, groups. Our method of analysis allows for fast and efficient computation of fairness-to-welfare solution paths, thereby allowing practitioners to easily assess whether and which fair learning procedures result in classification outcomes that make groups better-off. Our analyses show that applying stricter fairness criteria codified as parity constraints can worsen welfare outcomes for both groups. More generally, always preferring more fair classifiers does not abide by the Pareto Principle-a fundamental axiom of social choice theory and welfare economics. Recent work in machine learning has rallied around these notions of fairness as critical to ensuring that algorithmic systems do not have disparate negative impact on disadvantaged social groups. By showing that these constraints often fail to translate into improved outcomes for these groups, we cast doubt on their effectiveness as a means to ensure fairness and justice.",10.1145/3351095.3372857,,,
Stronger data poisoning attacks break data sanitization defenses,"Koh, PW; Steinhardt, J; Liang, P",MACHINE LEARNING,2022.0,"Machine learning models trained on data from the outside world can be corrupted by data poisoning attacks that inject malicious points into the models' training sets. A common defense against these attacks is data sanitization: first filter out anomalous training points before training the model. In this paper, we develop three attacks that can bypass a broad range of common data sanitization defenses, including anomaly detectors based on nearest neighbors, training loss, and singular-value decomposition. By adding just 3% poisoned data, our attacks successfully increase test error on the Enron spam detection dataset from 3 to 24% and on the IMDB sentiment classification dataset from 12 to 29%. In contrast, existing attacks which do not explicitly account for these data sanitization defenses are defeated by them. Our attacks are based on two ideas: (i) we coordinate our attacks to place poisoned points near one another, and (ii) we formulate each attack as a constrained optimization problem, with constraints designed to ensure that the poisoned points evade detection. As this optimization involves solving an expensive bilevel problem, our three attacks correspond to different ways of approximating this problem, based on influence functions; minimax duality; and the Karush-Kuhn-Tucker (KKT) conditions. Our results underscore the need to develop more robust defenses against data poisoning attacks.",10.1007/s10994-021-06119-y,Data poisoning; Data sanitization; Anomaly detection; Security,,
Prenaut: Design space exploration for embedded symmetric multiprocessing with various on-chip architectures,"Malazgirt, GA; Yurdakul, A",JOURNAL OF SYSTEMS ARCHITECTURE,2017.0,"As embedded systems have evolved to appear in many different domains, symmetric multiprocessing (SMP) has been the design choice from low-end to high-end devices. In this paper we present Prenaut, a design space exploration method for finding the best on-chip SMP architectures given processor cores, Level 1, Level 2, and Level 3 caches. finlike traditional design space exploration tools that are majorly concerned with optimizations in processor, memory and cache structures with a fixed on-chip architecture, Prenaut explores architectures that have not been considered in symmetric multiprocessing domain. These architectures consist of shared instruction caches between cores and heterogeneous cache topologies that feature bypassing a level in the cache hierarchy. The design idea behind Prenaut is to build a data oriented design space exploration method that exploits simulation data to its full extent rather than discarding it. Therefore, Prenaut uses simulation data and applies machine learning methods for estimating design parameters. This provides very rapid estimation of the Pareto set and guides designers through the overall system design process. The design space is pruned by topological clustering of design points which groups similar topologies and new simulation points are selected via an ordered look up table that prevents infeasible random jumps in the design space. For the selected benchmarks, Prenaut can estimate the Pareto set up to 147x faster and the clustering information can reduce the design space up to 82% in comparison with a state-of-the-art evolutionary algorithm. (C) 2016 Elsevier B.V. All rights reserved.",10.1016/j.sysarc.2016.07.004,Design space exploration; Symmetric multiprocessing; Machine learning; Clustering,,
Ownership Recommendation via Iterative Adversarial Training,"Paul, A; Zhao, XM; Fang, LP; Wu, ZF",NEURAL PROCESSING LETTERS,2022.0,"Machine learning classifiers are vulnerable to adversarial perturbation, and their presence raises security concerns, especially in recommendation systems. While attacks and defense mechanisms in recommendation systems have received significant attention, Basic Iterative Method (BIM), which has been shown in Computer Vision to increase attack effectiveness by more than 60%, has received little attention in ownership recommendation. As a result, ownership recommender systems may be more sensitive to iterative perturbations, resulting in significant generalization errors. Adversarial Training, a regularization strategy that can withstand worst-case iterative perturbations, could be a viable option for improving model robustness and generalization. In this paper, we implement BIM for ownership recommendations. Through adversarial training, we propose the Adversarial Consumer and Producer Recommendation (ACPR) approach that integrates ownership features into a multi-objective pairwise ranking to capture the user's preferences. The ACPR method learns a core embedding for each user and two transformation matrices that project the user's core embedding into two role embeddings (i.e., a producer and consumer role) using an extension of matrix factorization. To minimize the impact of iterative perturbation, we train a consumer and producer recommender objective function using minimax adversarial training. Empirical studies on two Large-scale applications show that our method outperforms standard recommendation methods and recent methods that model ownership information.",10.1007/s11063-021-10647-y,Ownership recommendation; Sharing platforms; Iterative perturbation; Adversarial training,,
Multi-objective approach based on grammar-guided genetic programming for solving multiple instance problems,"Zafra, A; Ventura, S",SOFT COMPUTING,2012.0,"Multiple instance learning (MIL) is considered a generalization of traditional supervised learning which deals with uncertainty in the information. Together with the fact that, as in any other learning framework, the classifier performance evaluation maintains a trade-off relationship between different conflicting objectives, this makes the classification task less straightforward. This paper introduces a multi-objective proposal that works in a MIL scenario to obtain well-distributed Pareto solutions to multi-instance problems. The algorithm developed, Multi-Objective Grammar Guided Genetic Programming for Multiple Instances (MOG3P-MI), is based on grammar-guided genetic programming, which is a robust tool for classification. Thus, this proposal combines the advantages of the grammar-guided genetic programming with benefits provided by multi-objective approaches. First, a study of multi-objective optimization for MIL is carried out. To do this, three different extensions of MOG3P-MI are designed and implemented and their performance is compared. This study allows us on the one hand, to check the performance of multi-objective techniques in this learning paradigm and on the other hand, to determine the most appropriate evolutionary process for MOG3P-MI. Then, MOG3P-MI is compared with some of the most significant proposals developed throughout the years in MIL. Computational experiments show that MOG3P-MI often obtains consistently better results than the other algorithms, achieving the most accurate models. Moreover, the classifiers obtained are very comprehensible.",10.1007/s00500-011-0794-0,Multiple instance learning; Multiple objective learning; Grammar guided genetic programming; Evolutionary rule learning,,
Multifaceted radiomics for distant metastasis prediction in head & neck cancer,"Zhou, ZG; Wang, K; Folkert, M; Liu, H; Jiang, S; Sher, D; Wang, J",PHYSICS IN MEDICINE AND BIOLOGY,2020.0,"Accurately predicting distant metastasis in head & neck cancer has the potential to improve patient survival by allowing early treatment intensification with systemic therapy for high-risk patients. By extracting large amounts of quantitative features and mining them, radiomics has achieved success in predicting treatment outcomes for various diseases. However, there are several challenges associated with conventional radiomic approaches, including: (1) how to optimally combine information extracted from multiple modalities; (2) how to construct models emphasizing different objectives for different clinical applications; and (3) how to utilize and fuse output obtained by multiple classifiers. To overcome these challenges, we propose a unified model termed as multifaceted radiomics (M-radiomics). In M-radiomics, a deep learning with stacked sparse autoencoder is first utilized to fuse features extracted from different modalities into one representation feature set. A multi-objective optimization model is then introduced into M-radiomics where probability-based objective functions are designed to maximize the similarity between the probability output and the true label vector. Finally, M-radiomics employs multiple base classifiers to get a diverse Pareto-optimal model set and then fuses the output probabilities of all the Pareto-optimal models through an evidential reasoning rule fusion (ERRF) strategy in the testing stage to obtain the final output probability. Experimental results show that M-radiomics with the stacked autoencoder outperforms the model without the autoencoder. M-radiomics obtained more accurate results with a better balance between sensitivity and specificity than other single-objective or single-classifier-based models.",10.1088/1361-6560/ab8956,distant metastasis prediction; head & neck cancer; radiomics; stacked autoencoder; multi-objective optimization; evidential reasoning rule,,
A decision support system for usability evaluation of web-based information systems,"Oztekin, A",EXPERT SYSTEMS WITH APPLICATIONS,2011.0,"In this study, a decision support system (DSS) for usability assessment and design of web-based information systems (WIS) is proposed. It employs three machine learning methods (support vector machines, neural networks, and decision trees) and a statistical technique (multiple linear regression) to reveal the underlying relationships between the overall WIS usability and its determinative factors. A sensitivity analysis on the predictive models is performed and a new metric, criticality index, is devised to identify the importance ranking of the determinative factors. Checklist items with the highest and the lowest contribution to the usability performance of the WIS are specified by means of the criticality index. The most important usability problems for the WIS are determined with the help of a pseudo-Pareto analysis. A case study through a student information system at Fatih University is carried out to validate the proposed DSS. The proposed DSS can be used to decide which usability problems to focus on so as to improve the usability and quality of WIS. (C) 2010 Elsevier Ltd. All rights reserved.",10.1016/j.eswa.2010.07.151,Web-based information system; Usability engineering; Machine learning; Sensitivity analysis; Criticality index,,
CommunityGAN: Community Detection with Generative Adversarial Nets,"Jia, YT; Zhang, QQ; Zhang, WN; Wang, XB",WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019),2019.0,"Community detection refers to the task of discovering groups of vertices sharing similar properties or functions so as to understand the network data. With the recent development of deep learning, graph representation learning techniques are also utilized for community detection. However, the communities can only be inferred by applying clustering algorithms based on learned vertex embeddings. These general cluster algorithms like K-means and Gaussian Mixture Model cannot output much overlapped communities, which have been proved to be very common in many real-world networks. In this paper, we propose CommunityGAN, a novel community detection framework that jointly solves overlapping community detection and graph representation learning. First, unlike the embedding of conventional graph representation learning algorithms where the vector entry values have no specific meanings, the embedding of CommunityGAN indicates the membership strength of vertices to communities. Second, a specifically designed Generative Adversarial Net (GAN) is adopted to optimize such embedding. Through the minimax competition between the motif-level generator and discriminator, both of them can alternatively and iteratively boost their performance and finally output a better community structure. Extensive experiments on synthetic data and real-world tasks demonstrate that CommunityGAN achieves substantial community detection performance gains over the state-of-the-art methods.",10.1145/3308558.3313564,Community Detection; Graph Representation Learning; Generative Adversarial Nets,,
Simultaneous feature and parameter selection using multiobjective optimization: application to named entity recognition,"Ekbal, A; Saha, S",INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS,2016.0,"In this paper, we propose an efficient algorithm based on the concept of multiobjective optimization (MOO) for performing feature selection and parameter optimization of any machine learning technique. Feature and parameter combinations have significant effect to the accuracy of the classifier. We perform feature selection and parameter optimization for four different classifiers, namely conditional random field, support vector machine, memory based learner and maximum entropy. The proposed algorithms are evaluated for solving the problems of named entity recognition, an important component in many text processing applications. Currently we experiment with four different languages, namely Bengali, Hindi, Telugu and English. At first the proposed MOO based technique is used to determine the appropriate features and parameters. For each of the classifiers, the algorithm produces a set of solutions on the final Pareto optimal front. Each solution represents a classifier with a particular feature and parameter combination. All these solutions are thereafter combined using a MOO based classifier ensemble technique. Evaluation results show that the proposed approach attains the F-measure (harmonic mean of recall and precision) values of 90.48, 90.44, 78.71 and 88.68 % for Bengali, Hindi, Telugu and English, respectively. We also show that for all the experimental settings the proposed feature and parameter optimization technique performs reasonably better than the baseline systems, developed with random feature subsets. Comparisons with the existing works also show the efficacy of our proposed algorithm.",10.1007/s13042-014-0268-7,Named entity recognition (NER); Feature selection; Parameter selection; Machine learning; Multiobjective optimization,,
"Mixtures, envelopes and hierarchical duality","Polson, NG; Scott, JG",JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,2016.0,"We develop a connection between mixture and envelope representations of objective functions that arise frequently in statistics. We refer to this connection by using the term hierarchical duality'. Our results suggest an interesting and previously underexploited relationship between marginalization and profiling, or equivalently between the Fenchel-Moreau theorem for convex functions and the Bernstein-Widder theorem for Laplace transforms. We give several different sets of conditions under which such a duality result obtains. We then extend existing work on envelope representations in several ways, including novel generalizations to variance-mean models and to multivariate Gaussian location models. This turns out to provide an elegant missing data interpretation of the proximal gradient method, which is a widely used algorithm in machine learning. We show several statistical applications in which the framework proposed leads to easily implemented algorithms, including a robust version of the fused lasso, non-linear quantile regression via trend filtering and the binomial fused double-Pareto model. Code for the examples is available on GitHub at .https://github.com/jgscott/hierduals.",10.1111/rssb.12130,Bayesian inference; Convex duality; Envelopes; Gaussian mixtures; Maximum a posteriori estimation; Penalized likelihood; Variational methods,,
Flood Susceptibility Modeling in a Subtropical Humid Low-Relief Alluvial Plain Environment: Application of Novel Ensemble Machine Learning Approach,"Pandey, M; Arora, A; Arabameri, A; Costache, R; Kumar, N; Mishra, VN; Nguyen, H; Mishra, J; Siddiqui, MA; Ray, Y; Soni, S; Shukla, UK",FRONTIERS IN EARTH SCIENCE,2021.0,"This study has developed a new ensemble model and tested another ensemble model for flood susceptibility mapping in the Middle Ganga Plain (MGP). The results of these two models have been quantitatively compared for performance analysis in zoning flood susceptible areas of low altitudinal range, humid subtropical fluvial floodplain environment of the Middle Ganga Plain (MGP). This part of the MGP, which is in the central Ganga River Basin (GRB), is experiencing worse floods in the changing climatic scenario causing an increased level of loss of life and property. The MGP experiencing monsoonal subtropical humid climate, active tectonics induced ground subsidence, increasing population, and shifting landuse/landcover trends and pattern, is the best natural laboratory to test all the susceptibility prediction genre of models to achieve the choice of best performing model with the constant number of input parameters for this type of topoclimatic environmental setting. This will help in achieving the goal of model universality, i.e., finding out the best performing susceptibility prediction model for this type of topoclimatic setting with the similar number and type of input variables. Based on the highly accurate flood inventory and using 12 flood predictors (FPs) (selected using field experience of the study area and literature survey), two machine learning (ML) ensemble models developed by bagging frequency ratio (FR) and evidential belief function (EBF) with classification and regression tree (CART), CART-FR and CART-EBF, were applied for flood susceptibility zonation mapping. Flood and non-flood points randomly generated using flood inventory have been apportioned in 70:30 ratio for training and validation of the ensembles. Based on the evaluation performance using threshold-independent evaluation statistic, area under receiver operating characteristic (AUROC) curve, 14 threshold-dependent evaluation metrices, and seed cell area index (SCAI) meant for assessing different aspects of ensembles, the study suggests that CART-EBF (AUC(SR) = 0.843; AUC(PR) = 0.819) was a better performant than CART-FR (AUC(SR) = 0.828; AUC(PR) = 0.802). The variability in performances of these novel-advanced ensembles and their comparison with results of other published models espouse the need of testing these as well as other genres of susceptibility models in other topoclimatic environments also. Results of this study are important for natural hazard managers and can be used to compute the damages through risk analysis.",10.3389/feart.2021.659296,CART; FR; EBF; ensembles; Middle Ganga Plain; Ganga Foreland Basin,,
Support Vector Machine Applied to the Optimal Design of Composite Wing Panels,"dos Santos, RR; Machado, TGD; Castro, SGP",AEROSPACE,2021.0,"One of the core technologies in lightweight structures is the optimal design of laminated composite stiffened panels. The increasing tailoring potential of new materials added to the simultaneous optimization of various design regions, leading to design spaces that are vast and non-convex. In order to find an optimal design using limited information, this paper proposes a workflow consisting of design of experiments, metamodeling and optimization phases. A machine learning strategy based on support vector machine (SVM) is used for data classification and interpolation. The combination of mass minimization and buckling evaluation under combined load is handled by a multi-objective formulation. The choice of a deterministic algorithm for the optimization cycle accelerates the convergence towards an optimal design. The analysis of the Pareto frontier illustrates the compromise between conflicting objectives. As a result, a balance is found between the exploration of new design regions and the optimal design refinement. Numerical experiments evaluating the design of a representative upper skin wing panel are used to show the viability of the proposed methodology.",10.3390/aerospace8110328,multi-objective optimization; stiffened panels; composite wing; layout optimization; sizing optimization; buckling,,
A PSO algorithm for multi-objective cost-sensitive attribute reduction on numeric data with error ranges,"Fang, Y; Liu, ZH; Min, F",SOFT COMPUTING,2017.0,"Multi-objective cost-sensitive attribute reduction is an attractive problem in supervised machine learning. Most research has focused on single-objective minimal test cost reduction or dealt with symbolic data. In this paper, we propose a particle swarm optimization algorithm for the attribute reduction problem on numeric data with multiple costs and error ranges and use three metrics with which to evaluate the performance of the algorithm. The proposed algorithm benefits from a fitness function based on the positive region, the selected n types of the test cost, a set of constant weight values , and a designated non-positive exponent . We design a learning strategy by setting dominance principles, which ensures the preservation of Pareto-optimal solutions and the rejection of redundant solutions. With different parameter settings, our PSO algorithm searches for a sub-optimal reduct set. Finally, we test our algorithm on seven UCI (University of California, Irvine) datasets. Comparisons with alternative approaches including the -weighted method and exhaustive calculation method of reduction are analyzed. Experimental results indicate that our heuristic algorithm outperforms existing algorithms.",10.1007/s00500-016-2260-5,Attribute reduction; Cost-sensitive learning; Particle swarm optimization; Rough sets,,
Multi-objective selection for collecting cluster alternatives,"Kraus, JM; Mussel, C; Palm, G; Kestler, HA",COMPUTATIONAL STATISTICS,2011.0,"Grouping objects into different categories is a basic means of cognition. In the fields of machine learning and statistics, this subject is addressed by cluster analysis. Yet, it is still controversially discussed how to assess the reliability and quality of clusterings. In particular, it is hard to determine the optimal number of clusters inherent in the underlying data. Running different cluster algorithms and cluster validation methods usually yields different optimal clusterings. In fact, several clusterings with different numbers of clusters are plausible in many situations, as different methods are specialized on diverse structural properties. To account for the possibility of multiple plausible clusterings, we employ a multi-objective approach for collecting cluster alternatives (MOCCA) from a combination of cluster algorithms and validation measures. In an application to artificial data as well as microarray data sets, we demonstrate that exploring a Pareto set of optimal partitions rather than a single solution can identify alternative solutions that are overlooked by conventional clustering strategies. Competitive solutions are hereby ranked following an impartial criterion, while the ultimate judgement is left to the investigator.",10.1007/s00180-011-0244-6,Cluster analysis; Multi-objective optimization; Cluster number estimation; Cluster validation,,
A Probabilistic Graphical Model-based Approach for Minimizing Energy Under Performance Constraints,"Mishra, N; Zhang, HZ; Lafferty, JD; Hoffmann, H",ACM SIGPLAN NOTICES,2015.0,"In many deployments, computer systems are underutilized - meaning that applications have performance requirements that demand less than full system capacity. Ideally, we would take advantage of this under-utilization by allocating system resources so that the performance requirements are met and energy is minimized. This optimization problem is complicated by the fact that the performance and power consumption of various system configurations are often application or even input - dependent. Thus, practically, minimizing energy for a performance constraint requires fast, accurate estimations of application-dependent performance and power tradeoffs. This paper investigates machine learning techniques that enable energy savings by learning Pareto-optimal power and performance tradeoffs. Specifically, we propose LEO, a probabilistic graphical model-based learning system that provides accurate online estimates of an application's power and performance as a function of system configuration. We compare LEO to (1) offline learning, (2) online learning, (3) a heuristic approach, and (4) the true optimal solution. We find that LEO produces the most accurate estimates and near optimal energy savings.",10.1145/2775054.2694373,Design; Experimentation; Measurement; Performance; Adaptation; Dynamic systems; Statistics; Probabilistic Graphical models; Energy minimization,,
Computational fluid dynamics analysis and optimisation of polymerase chain reaction thermal flow systems,"Hamad, HS; Kapur, N; Khatir, Z; Querin, OM; Thompson, HM; Wang, YX; Wilson, MCT",APPLIED THERMAL ENGINEERING,2021.0,"A novel Computational Fluid Dynamics-enabled multi-objective optimisation methodology for Polymerase Chain Reaction flow systems is proposed and used to explore the effect of geometry, material and flow variables on the temperature uniformity, pressure drop and heating power requirements, in a prototype three-zone thermal flow system. A conjugate heat transfer model for the three-dimensional flow and heat transfer is developed and solved numerically using COMSOL Multiphysics (R) and the solutions obtained demonstrate how the design variables affect each of the three performance parameters. These show that choosing a substrate with high conductivity and small thickness, together with a small channel area, generally improves the temperature uniformity in each zone, while channel area and substrate conductivity have the key influences on pressure drop and heating power respectively. The multi-objective optimisation methodology employs accurate surrogate modelling facilitated by Machine Learning via fully-connected Neural Networks to create Pareto curves which demonstrate clearly the compromises that can be struck between temperature uniformity throughout the three zones and the pressure drop and heating power required.",10.1016/j.applthermaleng.2020.116122,PCR; Computational fluid dynamics; Machine learning; Multi-objective optimisation,,
Semi-supervised Generative Adversarial Hashing for Image Retrieval,"Wang, GA; Hu, QH; Cheng, J; Hou, ZG","COMPUTER VISION - ECCV 2018, PT 15",2018.0,"With explosive growth of image and video data on the Internet, hashing technique has been extensively studied for large-scale visual search. Benefiting from the advance of deep learning, deep hashing methods have achieved promising performance. However, those deep hashing models are usually trained with supervised information, which is rare and expensive in practice, especially class labels. In this paper, inspired by the idea of generative models and the minimax two-player game, we propose a novel semi-supervised generative adversarial hashing (SSGAH) approach. Firstly, we unify a generative model, a discriminative model and a deep hashing model in a framework for making use of triplet-wise information and unlabeled data. Secondly, we design novel structure of the generative model and the discriminative model to learn the distribution of triplet-wise information in a semi-supervised way. In addition, we propose a semi-supervised ranking loss and an adversary ranking loss to learn binary codes which preserve semantic similarity for both labeled data and unlabeled data. Finally, by optimizing the whole model in an adversary training way, the learned binary codes can capture better semantic information of all data. Extensive empirical evaluations on two widely-used benchmark datasets show that our proposed approach significantly outperforms state-of-the-art hashing methods.",10.1007/978-3-030-01267-0_29,Information retrieval; Hashing; Deep learning; GANs,,
"Using GIS, Remote Sensing, and Machine Learning to Highlight the Correlation between the Land-Use/Land-Cover Changes and Flash-Flood Potential","Costache, R; Pham, QB; Corodescu-Rosca, E; Cimpianu, C; Hong, HY; Linh, NTT; Fai, CM; Ahmed, AN; Vojtek, M; Pandhiani, SM; Minea, G; Ciobotaru, N; Popa, MC; Diaconu, DC; Pham, BT",REMOTE SENSING,2020.0,"The aim of the present study was to explore the correlation between the land-use/land cover change and the flash-flood potential changes in Zbala catchment (Romania) between 1989 and 2019. In this regard, the efficiency of GIS, remote sensing and machine learning techniques in detecting spatial patterns of the relationship between the two variables was tested. The paper elaborated upon an answer to the increase in flash flooding frequency across the study area and across the earth due to the occurred land-use/land-cover changes, as well as due to the present climate change, which determined the multiplication of extreme meteorological phenomena. In order to reach the above-mentioned purpose, two land-uses/land-covers (for 1989 and 2019) were obtained using Landsat image processing and were included in a relative evolution indicator (total relative difference-synthetic dynamic land-use index), aggregated at a grid-cell level of 1 km(2). The assessment of runoff potential was made with a multilayer perceptron (MLP) neural network, which was trained for 1989 and 2019 with the help of 10 flash-flood predictors, 127 flash-flood locations, and 127 non-flash-flood locations. For the year 1989, the high and very high surface runoff potential covered around 34% of the study area, while for 2019, the same values accounted for approximately 46%. The MLP models performed very well, the area under curve (AUC) values being higher than 0.837. Finally, the land-use/land-cover change indicator, as well as the relative evolution of the flash flood potential index, was included in a geographically weighted regression (GWR). The results of the GWR highlights that high values of the Pearson coefficient (r) occupied around 17.4% of the study area. Therefore, in these areas of the Zbala river catchment, the land-use/land-cover changes were highly correlated with the changes that occurred in flash-flood potential.",10.3390/rs12091422,Zbala; Landsat images; multilayer perceptron; total relative difference-synthetic dynamic land-use index; flash-flood potential index; geographically weighted regression,,
An effective multiobjective approach for hard partitional clustering,"Prakash, J; Singh, PK",MEMETIC COMPUTING,2015.0,"Clustering is an unsupervised classification method in the field of data mining. Many population based evolutionary and swarm intelligence optimization methods are proposed to optimize clustering solutions globally based on a single selected objective function which lead to produce a single best solution. In this sense, optimized solution is biased towards a single objective, hence it is not equally well to the data set having clusters of different geometrical properties. Thus, clustering having multiple objectives should be naturally optimized through multiobjective optimization methods for capturing different properties of the data set. To achieve this clustering goal, many multiobjective population based optimization methods, e.g., multiobjective genetic algorithm, mutiobjective particle swarm optimization (MOPSO), are proposed to obtain diverse tradeoff solutions in the pareto-front. As single directional diversity mechanism in particle swarm optimization converges prematurely to local optima, this paper presents a two-stage diversity mechanism in MOPSO to improve its exploratory capabilities by incorporating crossover operator of the genetic algorithm. External archive is used to store non-dominated solutions, which is further utilized to find one best solution having highest F-measure value at the end of the run. Two conceptually orthogonal internal measures SSE and connectedness are used to estimate the clustering quality. Results demonstrate effectiveness of the proposed method over its competitors MOPSO, non-dominated sorting genetic algorithm, and multiobjective artificial bee colony on seven real data sets from UCI machine learning repository.",10.1007/s12293-014-0147-5,Data clustering; Multiobjective optimization; Evolutionary and swarm intelligence; Mutiobjective particle swarm optimization,,
A bi-objective hybrid algorithm for the classification of imbalanced noisy and borderline data sets,"Saeed, S; Ong, HC",PATTERN ANALYSIS AND APPLICATIONS,2019.0,"Classification of imbalanced data sets is one of the significant problems of machine learning and data mining. Traditional classifiers usually produced suboptimal results for imbalanced data sets. This study proposed an idea of using a newly proposed bi-objective hybrid algorithm for the given classification task of binary imbalanced noisy and borderline data sets. The bi-objective hybrid algorithm was based on the hybridization of two metaheuristics, namely cuckoo search and covariance matrix adaptation evolution strategy. The validation of this proposed hybrid algorithm was confirmed in terms of the Pareto fronts. Thereafter, this algorithm was used in a methodology proposed for the classification task of the binary imbalanced data sets. The proposed methodology was based on an idea of estimating the probabilities from both classes (majority and minority) of a data set, using normal distribution. Optimization of parameters of the normal distribution was done with the help of the proposed algorithm. Different data sets (simulated, noisy borderline and real) were used. Four well-known classifiers with a preprocessing algorithm were cast-off for the comparison purpose. Performances of all classifiers were evaluated using three evaluation measures, sensitivity, G mean and F measure. A promising performance of proposed methodology was observed.",10.1007/s10044-018-0693-4,Data mining; Classification; Imbalance data sets; Bi-objective hybrid algorithm; Metaheuristics,,
A comprehensive techno-economic assessment of alkali-surfactant-polymer flooding processes using data-driven approaches,"Sun, Q; Ertekin, T; Zhang, M; On, T",ENERGY REPORTS,2021.0,"The objective of this work is to propose a machine-learning assisted multi-objective optimization protocol to design and optimize alkali-surfactant-polymer flooding processes in the presence of multiple technical and economic objective functions. Several universal multi-layer neural networks are trained, and they act as universal surrogate models of high-fidelity numerical simulator to evaluate the objective functions involved in the optimization workflow. The HLD-NAC equation is employed to model the microemulsion phase behavior of the crude oil/brine/surfactant system with the presence of alkali agents. The validity of the expert systems is confirmed via extensive blind testing applications with error margins of 9%, which includes the predictions of fluid production and pressure responses. A multi-objective optimization workflow is structured by coupling the expert systems with particle swarm optimizer, which employs Pareto optimum theory to carry comprehensive ASP injection assessments considering various technical and economic objective functions. Moreover, the proposed workflow enables the decision makers to comprehend the project risks by taking the inherent uncertainties from crude oil market into accounts. The robustness of the optimization protocol is verified by introducing a synthetic field case study, which validates the competence of optimizing ASP injection design considering project net present value as an objective function. A series of Pareto front solutions are generated when more objective functions such as chemical efficacies and water cut reductions are included. Subsequently, the workflow evaluates the project economic uncertainties from the fluctuation of oil price, which helps the decision makers investigate the project risks from technical and economic perspectives. (C) 2021 The Author(s). Published by Elsevier Ltd.",10.1016/j.egyr.2021.05.003,,,
ASIC Clouds: Specializing the Datacenter for Planet-Scale Applications,"Taylor, MB; Vega, L; Khazraee, M; Magaki, I; Davidson, S; Richmond, D",COMMUNICATIONS OF THE ACM,2020.0,"Planet-scale applications are driving the exponential growth of the Cloud, and datacenter specialization is the key enabler of this trend. GPU- and FPGA-based clouds have already been deployed to accelerate compute-intensive workloads. ASIC-based clouds are a natural evolution as cloud services expand across the planet. ASIC Clouds are purpose-built datacenters comprised of large arrays of ASIC accelerators that optimize the total cost of ownership (TCO) of large, high-volume scale-out computations. On the surface, ASIC Clouds may seem improbable due to high NREs and ASIC inflexibility, but large-scale ASIC Clouds have already been deployed for the Bitcoin cryptocurrency system. This paper distills lessons from these Bitcoin ASIC Clouds and applies them to other large scale workloads such as YouTube-style video-transcoding and Deep Learning, showing superior TCO versus CPU and GPU. It derives Pareto-optimal ASIC Cloud servers based on accelerator properties, by jointly optimizing ASIC architecture, DRAM, motherboard, power delivery, cooling, and operating voltage. Finally, the authors examine the impact of ASIC NRE and when it makes sense to build an ASIC Cloud.",10.1145/3399734,,,
Multi-object optimization of Navy-blue anodic oxidation via response surface models assisted with statistical and machine learning techniques,"Khan, H; Wahab, F; Hussain, S; Khan, S; Rashid, M",CHEMOSPHERE,2022.0,"This study aims to model, analyze, and compare the electrochemical removal of Navy-blue dye (NB, %) and subsequent energy consumption (EC, Wh) using the integrated response surface modelling and optimization approaches. The Box-Behnken experimental design was exercised using current density, electrolyte concentra-tion, pH and oxidation time as inputs, while NB removal and EC were recorded as responses for the imple-mentation and analysis of multiple linear regression, support vector regression and artificial neural network models. The dual-response optimization using genetic algorithm generated multi-Pareto solutions for maximized NB removal at minimum energy cost, which were further ranked by employing the desirability function approach. The optimal parametric solution having total desirability of 0.804 is found when pH, current density, Na2SO4 concentration and electrolysis time were 6.4, 11.89 mA cm(-2), 0.055 M and 21.5 min, respectively. At these conditions, NB degradation and EC were 83.23% and 3.64 Wh, respectively. Sensitivity analyses revealed the influential patterns of variables on simultaneous optimization of NB removal and EC to be current density fol-lowed by treatment time and finally supporting electrolyte concentration. Statistical metrics of modeling and validation confirmed the accuracy of artificial neural network model followed by support vector regression and multiple linear regression anlaysis. The results revealed that statistical and computational modeling is an effective approach for the optimization of process variables of an electrochemical degradation process.",10.1016/j.chemosphere.2021.132818,Electrochemical degradation; Nb/BDD; MLR; SVR; ANN; Navy blue,,
Statistical Analysis and Machine Learning Prediction of Fog-Caused Low-Visibility Events at A-8 Motor-Road in Spain,"Cornejo-Bueno, S; Casillas-Perez, D; Cornejo-Bueno, L; Chidean, MI; Caamano, AJ; Cerro-Prada, E; Casanova-Mateo, C; Salcedo-Sanz, S",ATMOSPHERE,2021.0,"This work presents a full statistical analysis and accurate prediction of low-visibility events due to fog, at the A-8 motor-road in Mondonedo (Galicia, Spain). The present analysis covers two years of study, considering visibility time series and exogenous variables collected in the zone affected the most by extreme low-visibility events. This paper has then a two-fold objective: first, we carry out a statistical analysis for estimating the fittest probability distributions to the fog event duration, using the Maximum Likelihood method and an alternative method known as the L-moments method. This statistical study allows association of the low-visibility depth with the event duration, showing a clear relationship, which can be modeled with distributions for extremes such as Generalized Extreme Value and Generalized Pareto distributions. Second, we apply a neural network approach, trained by means of the ELM (Extreme Learning Machine) algorithm, to predict the occurrence of low-visibility events due to fog, from atmospheric predictive variables. This study provides a full characterization of fog events at this motor-road, in which orographic fog is predominant, causing important traffic problems during all year. We also show how the ELM approach is able to obtain highly accurate low-visibility events predictions, with a Pearson correlation coefficient of 0.8, within a half-hour time horizon, enough to initialize some protocols aiming at reducing the impact of these extreme events in the traffic of the A-8 motor road.",10.3390/atmos12060679,low-visibility events; orographic and hill-fogs; extreme learning machines; prediction problems; machine learning algorithms,,
Learning representations from dendrograms,"Chehreghani, MH; Chehreghani, MH",MACHINE LEARNING,2020.0,"We propose unsupervised representation learning and feature extraction from dendrograms. The commonly used Minimax distance measures correspond to building a dendrogram with single linkage criterion, with defining specific forms of a level function and a distance function over that. Therefore, we extend this method to arbitrary dendrograms. We develop a generalized framework wherein different distance measures and representations can be inferred from different types of dendrograms, level functions and distance functions. Via an appropriate embedding, we compute a vector-based representation of the inferred distances, in order to enable many numerical machine learning algorithms to employ such distances. Then, to address the model selection problem, we study the aggregation of different dendrogram-based distances respectively in solution space and in representation space in the spirit of deep representations. In the first approach, for example for the clustering problem, we build a graph with positive and negative edge weights according to the consistency of the clustering labels of different objects among different solutions, in the context of ensemble methods. Then, we use an efficient variant of correlation clustering to produce the final clusters. In the second approach, we investigate the combination of different distances and features sequentially in the spirit of multi-layered architectures to obtain the final features. Finally, we demonstrate the effectiveness of our approach via several numerical studies.",10.1007/s10994-020-05895-3,Representation learning; Unsupervised learning; Ensemble method; Feature extraction; Dendrogram,,
Iterative learning-based many-objective history matching using deep neural network with stacked autoencoder,"Kim, J; Park, C; Ahn, S; Kang, B; Jung, H; Jang, I",PETROLEUM SCIENCE,2021.0,"This paper presents an innovative data-integration that uses an iterative-learning method, a deep neural network (DNN) coupled with a stacked autoencoder (SAE) to solve issues encountered with many objective history matching. The proposed method consists of a DNN-based inverse model with SAE encoded static data and iterative updates of supervised-learning data are based on distance-based clustering schemes. DNN functions as an inverse model and results in encoded flattened data, while SAE, as a pre-trained neural network, successfully reduces dimensionality and reliably reconstructs geomodels. The iterative-learning method can improve the training data for DNN by showing the error reduction achieved with each iteration step. The proposed workflow shows the small mean absolute percentage error below 4% for all objective functions, while a typical multi-objective evolutionary algorithm fails to significantly reduce the initial population uncertainty. Iterative learning-based many objective history matching estimates the trends in water cuts that are not reliably included in dynamic data matching. This confirms the proposed workflow constructs more plausible geo-models. The workflow would be a reliable alternative to overcome the less-convergent Pareto-based multi-objective evolutionary algorithm in the presence of geological uncertainty and varying objective functions. (c) 2021 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/ 4.0/).",10.1016/j.petsci.2021.08.001,Deep neural network; Stacked autoencoder; History matching; Iterative learning; Clustering; Many-objective,,
Constraint Programming for Multi-criteria Conceptual Clustering,"Chabert, M; Solnon, C",PRINCIPLES AND PRACTICE OF CONSTRAINT PROGRAMMING (CP 2017),2017.0,"A conceptual clustering is a set of formal concepts (i.e., closed itemsets) that defines a partition of a set of transactions. Finding a conceptual clustering is an NP-complete problem for which Constraint Programming (CP) and Integer Linear Programming (ILP) approaches have been recently proposed. We introduce new CP models to solve this problem: a pure CP model that uses set constraints, and an hybrid model that uses a data mining tool to extract formal concepts in a preprocessing step and then uses CP to select a subset of formal concepts that defines a partition. We compare our new models with recent CP and ILP approaches on classical machine learning instances. We also introduce a new set of instances coming from a real application case, which aims at extracting setting concepts from an Enterprise Resource Planning (ERP) software. We consider two classic criteria to optimize, i.e., the frequency and the size. We show that these criteria lead to extreme solutions with either very few small formal concepts or many large formal concepts, and that compromise clusterings may be obtained by computing the Pareto front of non dominated clusterings.",10.1007/978-3-319-66158-2_30,,,
Novel elegant fuzzy genetic algorithms in classification problems,"Venkatanareshbabu, K; Nisheel, S; Sakthivel, R; Muralitharan, K",SOFT COMPUTING,2019.0,"In this paper, we propose three novel algorithms such as Novel genetic algorithm complex-valued backpropagation neural network (GA-CVBNN), Novel elegant fuzzy genetic algorithm (EFGA) and elegant fuzzy genetic algorithm-based complex-valued backpropagation neural network (EFGA-CVBNN) for classification of accuracy in datasets. In GA-CVBNN, classical Genetic Algorithm has been used for selecting appropriate initial weights for CVBNN. The EFGA is developed to resolve the drawback of classical GA by employing fuzzy logic to control parameters and selective pressure of GA. The EFGA uses a Min-Heap data structure and Pareto principle to improve the classical genetic algorithm. The EFGA-CVBNN resolves the drawbacks of classical CVBNN by employing EFGA at the time of initial weight selection. From the simulation result, the GA-CVBNN performs better than existing CVBNN and it is not efficient. To enhance the performance of GA-CVBNN, we have developed EFGA-CVBNN. Experimental results on various synthetic datasets and benchmark datasets taken from UCI machine learning repository shows that EFGA-CVBNN outperforms PSO-CVBNN in terms of classification accuracy and time. Statistical t test has been used to validate the obtained results.",10.1007/s00500-018-3216-8,Classification; Complex number; Fuzzy logic; Genetic algorithm; Neural network; Optimization,,
A cross-benchmark comparison of 87 learning to rank methods,"Tax, N; Bockting, S; Hiemstra, D",INFORMATION PROCESSING & MANAGEMENT,2015.0,"Learning to rank is an increasingly important scientific field that comprises the use of machine learning for the ranking task. New learning to rank methods are generally evaluated on benchmark test collections. However, comparison of learning to rank methods based on evaluation results is hindered by the absence of a standard set of evaluation benchmark collections. In this paper we propose a way to compare learning to rank methods based on a sparse set of evaluation results on a set of benchmark datasets. Our comparison methodology consists of two components: (1) Normalized Winning Number, which gives insight in the ranking accuracy of the learning to rank method, and (2) Ideal Winning Number, which gives insight in the degree of certainty concerning its ranking accuracy. Evaluation results of 87 learning to rank methods on 20 well-known benchmark datasets are collected through a structured literature search. ListNet, SmoothRank, FenchelRank, FSMRank, LRUF and LARF are Pareto optimal learning to rank methods in the Normalized Winning Number and Ideal Winning Number dimensions, listed in increasing order of Normalized Winning Number and decreasing order of Ideal Winning Number. (C) 2015 Elsevier Ltd. All rights reserved.",10.1016/j.ipm.2015.07.002,Learning to rank; Information retrieval; Evaluation metric,,
Data-based melody generation through multi-objective evolutionary computation,"de Leon, PJP; Inesta, JM; Calvo-Zaragoza, J; Rizo, D",JOURNAL OF MATHEMATICS AND MUSIC,2016.0,"Genetic-based composition algorithms are able to explore an immense space of possibilities, but the main difficulty has always been the implementation of the selection process. In this work, sets of melodies are utilized for training a machine learning approach to compute fitness, based on different metrics. The fitness of a candidate is provided by combining the metrics, but their values can range through different orders of magnitude and evolve in different ways, which makes it hard to combine these criteria. In order to solve this problem, a multi-objective fitness approach is proposed, in which the best individuals are those in the Pareto front of the multi-dimensional fitness space. Melodic trees are also proposed as a data structure for chromosomic representation of melodies and genetic operators are adapted to them. Some experiments have been carried out using a graphical interface prototype that allows one to explore the creative capabilities of the proposed system. An Online Supplement is provided and can be accessed at http://dx.doi.org/10.1080/17459737.2016.1188171, where the reader can find some technical details, information about the data used, generated melodies, and additional information about the developed prototype and its performance.",10.1080/17459737.2016.1188171,machine learning; evolutionary algorithms; composition; melody; tree representation; multi-objective optimization; 68T05; applied computing; sound and music computing; computing methodologies; learning from critiques; computing methodologies; genetic programming,,
An improved learnable evolution model for solving multi-objective vehicle routing problem with stochastic demand,"Niu, YY; Kong, DT; Wen, R; Cao, ZG; Xiao, JH",KNOWLEDGE-BASED SYSTEMS,2021.0,"The multi-objective vehicle routing problem with stochastic demand (MO-VRPSD) is much harder to tackle than other traditional vehicle routing problems (VRPs), due to the uncertainty in customer demands and potentially conflicted objectives. In this paper, we present an improved multi-objective learnable evolution model (IMOLEM) to solve MO-VRPSD with three objectives of travel distance, driver remuneration and number of vehicles. In our method, a machine learning algorithm, i.e., decision tree, is exploited to help find and guide the desirable direction of evolution process. To cope with the key issue of route failure caused due to stochastic customer demands, we propose a novel chromosome representation based on priority with bubbles. Moreover, an efficient nondominated sort using a sequential search strategy (ENS-SS) in conjunction with some heuristic operations are leveraged to handle the multi-objective property of the problem. Our algorithm is evaluated on the instances of modified Solomon VRP benchmark. Experimental results show that the proposed IMOLEM is capable to find better Pareto front of solutions and also deliver superior performance to other evolutionary algorithms. (C) 2021 Elsevier B.V. All rights reserved.",10.1016/j.knosys.2021.107378,Vehicle routing problems; Stochastic demand; Learnable evolution model; Multi-objective evolutionary algorithm,,
Multi-objective mobile robot path planning problem through learnable evolution model,"Moradi, B",JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE,2019.0,"A new multi-objective non-Darwinian-type evolutionary computation approach based on learnable evolution model (LEM) is proposed for solving the robot path planning problem. The multi-objective property of this approach is governed by a robust strength Pareto evolutionary algorithm (SPEA) incorporated in the LEM algorithm presented here. Learnable evolution model includes a machine learning method, like the decision trees, that can detect the right directions of the evolution and leads to large improvements in the fitness of the individuals. Several new refiner operators are proposed to improve the objectives of the individuals in the evolutionary process. These objectives are: the path length, the path safety and the path smoothness. A modified integer coding path representation scheme is proposed where the edge-fixing and top-row fixing procedures are performed implicitly. This proposed robot path planning problem solving approach is assessed on eight realistic scenarios in order to verify the performance thereof. Computer simulations reveal that this proposed approach exhibits much higher hypervolume and set coverage in comparison with other similar approaches. The experimental results confirm that the proposed approach performs in the workspaces with a dense set of obstacles in a significant manner.",10.1080/0952813X.2018.1549107,Robotics; path planning; multi-objective optimisation; learnable evolution model; strength Pareto evolutionary algorithm,,
Bi-level multi-objective evolution of a Multi-Layered Echo-State Network Autoencoder for data representations,"Chouikhi, N; Ammar, B; Hussain, A; Alimi, AM",NEUROCOMPUTING,2019.0,"The Multi-Layered Echo-State Network (ML-ESN) is a recently developed, highly powerful type of recurrent neural network. It has succeeded in dealing with several non-linear benchmark problems. On account of its rich dynamics, ML-ESN is exploited in this paper, for the first time, as a recurrent Autoencoder (ML-ESNAE) to extract new features from original data representations. Further, the challenging and crucial task of optimally determining the ML-ESNAE architecture and training parameters is addressed, in order to extract more efficient features from the data. Traditionally, in a ML-ESN, the number of parameters (hidden neurons, sparsity rates, weights) are randomly chosen and manually altered to achieve a minimum learning error. On one hand, this random setting may not guarantee best generalization results. On the other, it can increase the network's complexity. In this paper, a novel bi-level evolutionary optimization approach is thus proposed for the ML-ESNAE, to deal with these challenges. The first level offers Pareto multi-objective architecture optimization, providing maximum learning accuracy while maintaining a reduced complexity target. Next, every Pareto optimal solution obtained from the first level undergoes a mono-objective weights optimization at the second level. Particle Swarm Optimization (PSO) is used as an evolutionary tool for both levels 1 and 2. An empirical study shows that the evolved ML-ESNAE produces a noticeable improvement in extracting new, more expressive data features from original ones. A number of application case studies, using a range of benchmark datasets, show that the extracted features produce excellent results in terms of classification accuracy. The effectiveness of the evolved ML-ESNAE is demonstrated for both noisy and noise-free data. In conclusion, the evolutionary ML-ESNAE is proposed as a new benchmark for the evolutionary AI and machine learning research community. (C) 2019 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2019.03.012,Multi-Layered Echo State Network; Autoencoder; Data representation; PSO; Multi-objective optimization; Architecture optimization; Weights optimization,,
Policy invariance under reward transformations for multi-objective reinforcement learning,"Mannion, P; Devlin, S; Mason, K; Duggan, J; Howley, E",NEUROCOMPUTING,2017.0,"Reinforcement Learning (RL) is a powerful and well-studied Machine Learning paradigm, where an agent learns to improve its performance in an environment by maximising a reward signal. In multi-objective Reinforcement Learning (MORL) the reward signal is a vector, where each component represents the performance on a different objective. Reward shaping is a well-established family of techniques that have been successfully used to improve the performance and learning speed of RL agents in single-objective problems. The basic premise of reward shaping is to add an additional shaping reward to the reward naturally received from the environment, to incorporate domain knowledge and guide an agent's exploration. Potential-Based Reward Shaping (PBRS) is a specific form of reward shaping that offers additional guarantees. In this paper, we extend the theoretical guarantees of PBRS to MORL problems. Specifically, we provide theoretical proof that PBRS does not alter the true Pareto front in both single- and multi-agent MORL. We also contribute the first published empirical studies of the effect of PBRS in single- and multi-agent MORL problems. (C) 2017 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2017.05.090,Reinforcement learning; Multi-objective; Potential-based; Reward shaping; Multi-agent systems,,
Active Fairness in Algorithmic Decision Making,"Noriega-Campero, A; Bakker, MA; Garcia-Bulle, B; Pentland, A","AIES '19: PROCEEDINGS OF THE 2019 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY",2019.0,"Society increasingly relies on machine learning models for automated decision making. Yet, efficiency gains from automation have come paired with concern for algorithmic discrimination that can systematize inequality. Recent work has proposed optimal post processing methods that randomize classification decisions for a fraction of individuals, in order to achieve fairness measures related to parity in errors and calibration. These methods, however, have raised concern due to the information inefficiency intra-group unfairness, and Pareto sub-optimality they entail. The present work proposes an alternative active framework for fair classification, where, in deployment, a decision-maker adaptively acquires information according to the needs of different groups or individuals, towards balancing disparities in classification performance. We propose two such methods, where information collection is adapted to group- and individual-level needs respectively. We show on real-world datasets that these can achieve: 1) calibration and single error parity (e.g., equal opportunity); and 2) parity in both false positive and false negative rates (i.e., equal odds). Moreover, we show that by leveraging their additional degree of freedom, active approaches can substantially outperform randomization-based classifiers previously considered optimal, while avoiding limitations such as intra-group unfairness.",10.1145/3306618.3314277,algorithmic fairness; adaptive inquiry; active feature acquisition,,
Reference-point-based multi-objective optimization algorithm with opposition-based voting scheme for multi-label feature selection,"Bidgoli, AA; Ebrahimpour-Komleh, H; Rahnamayan, S",INFORMATION SCIENCES,2021.0,"Multi-label classification is a machine learning task to construct a model for assigning an entity in the dataset to two or more class labels. In order to improve the performance of multi-label classification, a multi-objective feature selection algorithm has been proposed in this paper. Feature selection as a preprocessing task for Multi-label classification problems aims to choose a subset of relevant features. Selecting a small number of high-quality features decreases the computational cost and at the same time maximizes the classification performance. However extreme decreasing the number of features causes the failure of classification. As a result, feature selection has two conflicting objectives, namely, minimizing the classification error and minimizing the number of selected features. This paper proposes a multi-objective optimization algorithm to tackle the multi-label feature selection. The task is to find a set of solutions (a subset of features) in a sophisticated large-scale search space using a reference-based multi-objective optimization method. The proposed algorithm utilizes an opposition-based binary operator to generate more diverse solutions. Injection of extreme point of the Pareto-front is another component of the algorithm which aims to find feature subsets with less classification error. The proposed method is compared with two other existing methods on eight multi-label benchmark datasets. The experimental results show that the proposed method outperforms existing algorithms in terms of various multi-objective evaluation measures, such as Hyper-volume indicator, Pure diversity, Two-set coverage, and Pareto-front proportional contribution. The proposed method leads to get a set of well-distributed trade-off solutions which reach less classification error in comparing with competitors, even with the fewer number of features. (C) 2020 Elsevier Inc. All rights reserved.",10.1016/j.ins.2020.08.004,Multi-label classification; Feature selection; Multi-objective optimization; Evolutionary algorithm; Opposition-based computation,,
"High-quality academic teachers in business school. The case of The University of Gdansk, Poland","Wisniewska, M; Grudowski, P",TOTAL QUALITY MANAGEMENT & BUSINESS EXCELLENCE,2016.0,"The Bologna process, the increasing number of higher education institutions, the mass education and the demographic problems make the quality of education and quality of the academic teachers a subject of wide public debate and concern. The aim of the paper is to identify the most preferred characteristics of a teacher working at a business school. The research problem was: What should a high-quality business school academic teacher be like? During the research, a six-stage qualitative survey design was proposed, and a letter questionnaire was applied as a free writing instrument and sent to second-year bachelor students of the Faculty of Management at The University of Gdansk, Poland. To identify the most preferred characteristics, a content analysis and Pareto analysis were used. As a result, 32 characteristics were proposed and grouped into 5 categories, namely tangibles (T), reliability (Rel), responsiveness (Res), assurance (A) and empathy (E). Based on this, several proposals and recommendations for the future were specified. The results obtained help not only to understand the needs of students, but also to prepare the most desired teaching environment in which deep learning outcomes are made possible for future managers in the context of modern economy.",10.1080/14783363.2015.1064766,academic teacher; business school; quality; management education,,
"Optimization of state-of-the-art fuzzy-metaheuristic ANFIS-based machine learning models for flood susceptibility prediction mapping in the Middle Ganga Plain, India","Arora, A; Arabameri, A; Pandey, M; Siddiqui, MA; Shukla, UK; Bui, DT; Mishra, VN; Bhardwaj, A",SCIENCE OF THE TOTAL ENVIRONMENT,2021.0,"This study is an attempt to quantitatively test and compare novel advanced-machine learning algorithms in terms of their performance in achieving the goal of predicting flood susceptible areas in a low altitudinal range, sub-tropical floodplain environmental setting, like that prevailing in the Middle Ganga Plain (MGP), India. This part of the Ganga floodplain region, which under the influence of undergoing active tectonic regime related subsidence, is the hotbed of annual flood disaster. This makes the region one of the best natural laboratories to test the flood susceptibility models for establishing a universalization of such models in low relief highly flood prone areas. Based on highly sophisticated flood inventory archived for this region, and 12 flood conditioning factors viz. annual rainfall, soil type, stream density, distance from stream, distance from road, Topographic Wetness Index (TWI), altitude, slope aspect, slope, curvature, land use/land cover, and geomorphology, an advanced novel hybrid model Adaptive Neuro Fuzzy Inference System (ANFIS), and three metaheuristic models-based ensembles with ANFIS namely ANFIS-GA (Genetic Algorithm), ANFIS-DE (Differential Evolution), and ANFIS-PSO (Particle Swarm Optimization), have been applied for zonation of the flood susceptible areas. The flood inventory dataset, prepared by collected flood samples, were apportioned into 70:30 classes to prepare training and validation datasets. One independent validation method, the Area-Under Receiver Operating Characteristic (AUROC) Curve, and other 11 cut-off-dependent model evaluation metrices have helped to conclude that the ANIFS-GA has outperformed other three models with highest success rate AUC = 0.922 and prediction rate AUC = 0.924. The accuracy was also found to be highest for ANFIS-GA during training (0.886) & validation (0.883). Better performance of ANIFS-GA than the individual models as well as some ensemble models suggests andwarrants further study in this topoclimatic environment using other classes of susceptibility models. This will further help establishing a benchmark model with capability of highest accuracy and sensitivity performance in the similar topographic and climatic setting taking assumption of the quality of input parameters as constant. (C) 2020 Elsevier B.V. All rights reserved.",10.1016/j.scitotenv.2020.141565,Flood susceptibility mapping; ANFIS; Genetic algorithm (GA); Differential evolution (DE); Particle swarm optimization (PSO); Metaheuristic optimization; Middle ganga plain,,
Forecasting heating and cooling loads of buildings: a comparative performance analysis,"Roy, SS; Samui, P; Nagtode, I; Jain, H; Shivaramakrishnan, V; Mohammadi-ivatloo, B",JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING,2020.0,"Heating load and cooling load forecasting are crucial for estimating energy consumption and improvement of energy performance during the design phase of buildings. Since the capacity of cooling ventilation and air-conditioning system of the building contributes to the operation cost, it is ideal to develop accurate models for heating and cooling load forecasting of buildings. This paper proposes a machine-learning technique for prediction of heating load and cooling load of residential buildings. The proposed model is deep neural network (DNN), which presents a category of learning algorithms that adopt nonlinear extraction of information in several steps within a hierarchical framework, primarily applied for learning and pattern classification. The output of DNN has been compared with other proposed methods such as gradient boosted machine (GBM), Gaussian process regression (GPR) and minimax probability machine regression (MPMR). To develop DNN model, the energy data set has been divided into training (70%) and testing (30%) sets. The performance of proposed model was benchmarked by statistical performance metrics such as variance accounted for (VAF), relative average absolute error (RAAE), root means absolute error (RMAE), coefficient of determination (R-2), standard deviation ratio (RSR), mean absolute percentage error (MAPE), Nash-Sutcliffe coefficient (NS), root means squared error (RMSE), weighted mean absolute percent error (WMAPE) and mean absolute percentage Error (MAPE). DNN and GPR have produced best-predicted VAF for cooling load and heating load of 99.76% and 99.84% respectively.",10.1007/s12652-019-01317-y,Heating load; Cooling load; Building; Deep neural network; Gradient boosted machine; Gaussian process regression; Minimax probability machine regression,,
Differential Evolution and Multiclass Support Vector Machine for Alzheimer's Classification,"Kaka, JR; Prasad, KS",SECURITY AND COMMUNICATION NETWORKS,2022.0,"Early diagnosis of Alzheimer's helps a doctor to decide the treatment for the patient based on the stages. The existing methods involve applying the deep learning methods for Alzheimer's classification and have the limitations of overfitting problems. Some researchers were involved in applying the feature selection based on the optimization method, having limitations of easily trapping into local optima and poor convergence. In this research, Differential Evolution-Multiclass Support Vector Machine (DE-MSVM) is proposed to increase the performance of Alzheimer's classification. The image normalization method is applied to enhance the quality of the image and represent the features effectively. The AlexNet model is applied to the normalized images to extract the features and also applied for feature selection. The Differential Evolution method applies Pareto Optimal Front for nondominated feature selection. This helps to select the feature that represents the characteristics of the input images. The selected features are applied in the MSVM method to represent in high dimension and classify Alzheimer's. The DE-MSVM method has accuracy of 98.13% in the axial slice, and the existing whale optimization with MSVM has 95.23% accuracy.",10.1155/2022/7275433,,,
Multiobjective evolutionary RBF networks and its application to ensemble learning,"Kondo, N; Hatanaka, T; Uosaki, K",FRONTIERS OF COMPUTATIONAL SCIENCE,2007.0,"This paper considers a pattern classification by the ensemble of evolutionary RBF networks. Mathematical models generally have a dilemma about model complexity, so the structure determination of RBF network can be considered as the multi-objective optimization problem concerning with accuracy, complexity, and smoothness of the model. The set of RBF network are obtained by multi-objective evolutionary computation, and then RBF network ensemble is constructed of all or some RBF networks at the final generation. Some experiments on the benchmark problem of the pattern classification demonstrate that the RBF network ensemble has comparable generalization ability to conventional ensemble methods.",10.1007/978-3-540-46375-7_50,,,
Novel multiobjective TLBO algorithms for the feature subset selection problem,"Kiziloz, HE; Deniz, A; Dokeroglu, T; Cosar, A",NEUROCOMPUTING,2018.0,"Teaching Learning Based Optimization (TLBO) is a new metaheuristic that has been successfully applied to several intractable optimization problems in recent years. In this study, we propose a set of novel multiobjective TLBO algorithms combined with supervised machine learning techniques for the solution of Feature Subset Selection (FSS) in Binary Classification Problems (FSS-BCP). Selecting the minimum number of features while not compromising the accuracy of the results in FSS-BCP is a multiobjective optimization problem. We propose TLBO as a FSS mechanism and utilize its algorithm-specific parameterless concept that does not require any parameters to be tuned during the optimization. Most of the classical metaheuristics such as Genetic and Particle Swarm Optimization algorithms need additional efforts for tuning their parameters (crossover ratio, mutation ratio, velocity of particle, inertia weight, etc.), which may have an adverse influence on their performance. Comprehensive experiments are carried out on the well-known machine learning datasets of UCI Machine Learning Repository and significant improvements have been observed when the proposed multiobjective TLBO algorithms are compared with state-of-the-art NSGA-II, Particle Swarm Optimization, Tabu Search, Greedy Search, and Scatter Search algorithms. (C) 2018 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2018.04.020,Teaching learning based optimization; Multiobjective feature selection; Supervised learning,,
Multicriteria assignment method PROAFTN: Methodology and medical application,"Belacel, N",EUROPEAN JOURNAL OF OPERATIONAL RESEARCH,2000.0,"This paper presents a new fuzzy multicriteria classification method, called PROAFTN, for assigning alternatives to predefined categories. This method belongs to the class of supervised learning algorithms and enables to determine the fuzzy indifference relations by generalising the indices (concordance and discordance) used in the ELECTRE III method. Then, it assigns the fuzzy belonging degree of the alternatives to the categories. We also present a clinical application of the proposed method in the cytopathological diagnosis of acute leukaemia. (C) 2000 Elsevier Science B.V. All rights reserved.",10.1016/S0377-2217(99)00192-7,multicriteria decision analysis; classification; fuzzy assignment; medical diagnosis,,
Multi-objective Optimization for Materials Discovery via Adaptive Design,"Gopakumar, AM; Balachandran, PV; Xue, DZ; Gubernatis, JE; Lookman, T",SCIENTIFIC REPORTS,2018.0,"Guiding experiments to find materials with targeted properties is a crucial aspect of materials discovery and design, and typically multiple properties, which often compete, are involved. In the case of two properties, new compounds are sought that will provide improvement to existing data points lying on the Pareto front (PF) in as few experiments or calculations as possible. Here we address this problem by using the concept and methods of optimal learning to determine their suitability and performance on three materials data sets; an experimental data set of over 100 shape memory alloys, a data set of 223 M(2)AX phases obtained from density functional theory calculations, and a computational data set of 704 piezoelectric compounds. We show that the Maximin and Centroid design strategies, based on value of information criteria, are more efficient in determining points on the PF from the data than random selection, pure exploitation of the surrogate model prediction or pure exploration by maximum uncertainty from the learning model. Although the datasets varied in size and source, the Maximin algorithm showed superior performance across all the data sets, particularly when the accuracy of the machine learning model fits were not high, emphasizing that the design appears to be quite forgiving of relatively poor surrogate models.",10.1038/s41598-018-21936-3,,,
An Effective Metaheuristic for Bi-objective Feature Selection in Two-Class Classification Problem,"Lyubchenko, AA; Pacheco, JA; Casado, S; Nunez, L",XII INTERNATIONAL SCIENTIFIC AND TECHNICAL CONFERENCE APPLIED MECHANICS AND SYSTEMS DYNAMICS,2019.0,"Feature selection is known as a very useful technique in machine learning practice as it may result in the development of more straightforward models with better accuracy. Traditionally, feature selection is considered as a single-objective problem, however, it can be easily formulated in terms of two objectives. The solving of such problems requires the application of appropriate multi-objective optimization methods that do not always offer equally good solutions even under the same conditions. This paper focuses on the development of a metaheuristic optimization approach for bi-objective feature selection problem in two-class classification. We consider the solving of this problem in terms of minimization of both misclassification error and feature subset size. For solving the considered problem, an adaptation of the Multi-Objective Adaptive Memory Programming (MOAMP) metaheuristic based on the tabu search strategy is proposed. Our MOAMP adaption has been utilized to obtain the sets of most relevant features for two real classification problems with two classes. Finally, using popular Pareto front quality indicators, the obtained results have been compared with the sets of non-dominated solutions derived by the well-known NSGA2 algorithm. The conducted research allows concluding about the ability of the MOAMP adaptation to get a better efficient frontier for the same number of objective function calls.",10.1088/1742-6596/1210/1/012086,bi-objective feature selection; classification; tabu search; MOAMP,,
Joint Analysis of Multiple Algorithms and Performance Measures,"de Campos, CP; Benavoli, A",NEW GENERATION COMPUTING,2017.0,"There has been an increasing interest in the development of new methods using Pareto optimality to deal with multi-objective criteria (for example, accuracy and time complexity). Once one has developed an approach to a problem of interest, the problem is then how to compare it with the state of art. In machine learning, algorithms are typically evaluated by comparing their performance on different data sets by means of statistical tests. Standard tests used for this purpose are able to consider jointly neither performance measures nor multiple competitors at once. The aim of this paper is to resolve these issues by developing statistical procedures that are able to account for multiple competing measures at the same time and to compare multiple algorithms altogether. In particular, we develop two tests: a frequentist procedure based on the generalized likelihood ratio test and a Bayesian procedure based on a multinomial-Dirichlet conjugate model. We further extend them by discovering conditional independences among measures to reduce the number of parameters of such models, as usually the number of studied cases is very reduced in such comparisons. Data from a comparison among general purpose classifiers are used to show a practical application of our tests.",10.1007/s00354-016-0005-8,,,
Multi-Objective Modeling of Leading-Edge Serrations Applied to Low-Pressure Axial Fans,"Biedermann, TM; Reich, M; Paschereit, CO",JOURNAL OF ENGINEERING FOR GAS TURBINES AND POWER-TRANSACTIONS OF THE ASME,2020.0,"A novel modeling strategy is proposed which allows high-accuracy predictions of aerodynamic and aeroacoustic target values for a low-pressure axial fan, equipped with serrated leading edges. Inspired by machine learning processes, the sampling of the experimental space is realized by use of a Latin hypercube design plus a factorial design, providing highly diverse information on the analyzed system. The effects of four influencing parameters (IP) are tested, characterizing the inflow conditions as well as the serration geometry. A total of 65 target values in the time and frequency domains are defined and can be approximated with high accuracy by individual artificial neural networks. Furthermore, the validation of the model against fully independent test points within the experimental space yields a remarkable fit, even for the spectral distribution in 1/3-octave bands, proving the ability of the model to generalize. A metaheuristic multi-objective optimization approach provides two-dimensional Pareto optimal solutions for selected pairs of target values. This is particularly important for reconciling opposing trends, such as the noise reduction capability and aerodynamic performance. The chosen optimization strategy also allows for a customized design of serrated leading edges, tailored to the specific operating conditions of the axial fan.",10.1115/1.4048599,,,
Unsupervised feature construction for improving data representation and semantics,"Rizoiu, MA; Velcin, J; Lallich, S",JOURNAL OF INTELLIGENT INFORMATION SYSTEMS,2013.0,"Attribute-based format is the main data representation format used by machine learning algorithms. When the attributes do not properly describe the initial data, performance starts to degrade. Some algorithms address this problem by internally changing the representation space, but the newly constructed features rarely have any meaning. We seek to construct, in an unsupervised way, new attributes that are more appropriate for describing a given dataset and, at the same time, comprehensible for a human user. We propose two algorithms that construct the new attributes as conjunctions of the initial primitive attributes or their negations. The generated feature sets have reduced correlations between features and succeed in catching some of the hidden relations between individuals in a dataset. For example, a feature like would be true for non-urban images and is more informative than simple features expressing the presence or the absence of an object. The notion of Pareto optimality is used to evaluate feature sets and to obtain a balance between total correlation and the complexity of the resulted feature set. Statistical hypothesis testing is employed in order to automatically determine the values of the parameters used for constructing a data-dependent feature set. We experimentally show that our approaches achieve the construction of informative feature sets for multiple datasets.",10.1007/s10844-013-0235-x,Unsupervised feature construction; Feature evaluation; Nonparametric statistics; Data mining; Clustering; Representations; Algorithms for data and knowledge management; Heuristic methods; Pattern analysis,,
Equation Discovery Using Fast Function Extraction: a Deterministic Symbolic Regression Approach,"Vaddireddy, H; San, O",FLUIDS,2019.0,"Advances in machine learning (ML) coupled with increased computational power have enabled identification of patterns in data extracted from complex systems. ML algorithms are actively being sought in recovering physical models or mathematical equations from data. This is a highly valuable technique where models cannot be built using physical reasoning alone. In this paper, we investigate the application of fast function extraction (FFX), a fast, scalable, deterministic symbolic regression algorithm to recover partial differential equations (PDEs). FFX identifies active bases among a huge set of candidate basis functions and their corresponding coefficients from recorded snapshot data. This approach uses a sparsity-promoting technique from compressive sensing and sparse optimization called pathwise regularized learning to perform feature selection and parameter estimation. Furthermore, it recovers several models of varying complexity (number of basis terms). FFX finally filters out many identified models using non-dominated sorting and forms a Pareto front consisting of optimal models with respect to minimizing complexity and test accuracy. Numerical experiments are carried out to recover several ubiquitous PDEs such as wave and heat equations among linear PDEs and Burgers, Korteweg-de Vries (KdV), and Kawahara equations among higher-order nonlinear PDEs. Additional simulations are conducted on the same PDEs under noisy conditions to test the robustness of the proposed approach.",10.3390/fluids4020111,deterministic symbolic regression; fast function extraction; compressive sensing; pathwise regularized learning; non-dominated sorting,,
Computational Intelligence Modeling of the Macromolecules Release from PLGA Microspheres-Focus on Feature Selection,"Zawbaa, HM; Szlek, J; Grosan, C; Jachowicz, R; Mendyk, A",PLOS ONE,2016.0,"Poly-lactide-co-glycolide (PLGA) is a copolymer of lactic and glycolic acid. Drug release from PLGA microspheres depends not only on polymer properties but also on drug type, particle size, morphology of microspheres, release conditions, etc. Selecting a subset of relevant properties for PLGA is a challenging machine learning task as there are over three hundred features to consider. In this work, we formulate the selection of critical attributes for PLGA as a multiobjective optimization problem with the aim of minimizing the error of predicting the dissolution profile while reducing the number of attributes selected. Four bioinspired optimization algorithms: antlion optimization, binary version of antlion optimization, grey wolf optimization, and social spider optimization are used to select the optimal feature set for predicting the dissolution profile of PLGA. Besides these, LASSO algorithm is also used for comparisons. Selection of crucial variables is performed under the assumption that both predictability and model simplicity are of equal importance to the final result. During the feature selection process, a set of input variables is employed to find minimum generalization error across different predictive models and their settings/architectures. The methodology is evaluated using predictive modeling for which various tools are chosen, such as Cubist, random forests, artificial neural networks (monotonic MLP, deep learning MLP), multivariate adaptive regression splines, classification and regression tree, and hybrid systems of fuzzy logic and evolutionary computations (fugeR). The experimental results are compared with the results reported by Szl.k. We obtain a normalized root mean square error (NRMSE) of 15.97% versus 15.4%, and the number of selected input features is smaller, nine versus eleven.",10.1371/journal.pone.0157610,,,
Meta-Analysis Based on Nonconvex Regularization,"Zhang, H; Li, SJ; Zhang, H; Yang, ZY; Ren, YQ; Xia, LY; Liang, Y",SCIENTIFIC REPORTS,2020.0,"The widespread applications of high-throughput sequencing technology have produced a large number of publicly available gene expression datasets. However, due to the gene expression datasets have the characteristics of small sample size, high dimensionality and high noise, the application of biostatistics and machine learning methods to analyze gene expression data is a challenging task, such as the low reproducibility of important biomarkers in different studies. Meta-analysis is an effective approach to deal with these problems, but the current methods have some limitations. In this paper, we propose the meta-analysis based on three nonconvex regularization methods, which are L-1/2 regularization (meta-Half), Minimax Concave Penalty regularization (meta-MCP) and Smoothly Clipped Absolute Deviation regularization (meta-SCAD). The three nonconvex regularization methods are effective approaches for variable selection developed in recent years. Through the hierarchical decomposition of coefficients, our methods not only maintain the flexibility of variable selection and improve the efficiency of selecting important biomarkers, but also summarize and synthesize scientific evidence from multiple studies to consider the relationship between different datasets. We give the efficient algorithms and the theoretical property for our methods. Furthermore, we apply our methods to the simulation data and three publicly available lung cancer gene expression datasets, and compare the performance with state-of-the-art methods. Our methods have good performance in simulation studies, and the analysis results on the three publicly available lung cancer gene expression datasets are clinically meaningful. Our methods can also be extended to other areas where datasets are heterogeneous.",10.1038/s41598-020-62473-2,,,
Improving Constrained Clustering Via Decomposition-based Multiobjective Optimization with Memetic Elitism,"Gonzalez-Almagro, G; Rosales-Perez, A; Luengo, J; Cano, JR; Garcia, S",GECCO'20: PROCEEDINGS OF THE 2020 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE,2020.0,"Clustering has always been a topic of interest in knowledge discovery, it is able to provide us with valuable information within the unsupervised machine learning framework. It received renewed attention when it was shown to produce better results in environments where partial information about how to solve the problem is available, thus leading to a new machine learning paradigm: semi-supervised machine learning. This new type of information can be given in the form of constraints, which guide the clustering process towards quality solutions. In particular, this study considers the pairwise instance-level must-link and cannot-link constraints. Given the ill-posed nature of the constrained clustering problem, we approach it from the multiobjective optimization point of view. Our proposal consists in a memetic elitist evolutionary strategy that favors exploitation by applying a local search procedure to the elite of the population and transferring its results only to the external population, which will also be used to generate new individuals. We show the capability of this method to produce quality results for the constrained clustering problem when considering incremental levels of constraint-based information. For the comparison with state-of-the-art methods, we include previous multiobjective approaches, single-objective genetic algorithms and classic constrained clustering methods.",10.1145/3377930.3390187,Semi-supervised learning; constrained clustering; pairwise instance-level constraints; multiobjective optimization; memetic elitis MOEA,,
Wealth Flow Model: Online Portfolio Selection Based on Learning Wealth Flow Matrices,"Yin, JF; Wang, RL; Guo, YQ; Bai, YZ; Ju, SD; Liu, WL; Huang, JZX",ACM TRANSACTIONS ON KNOWLEDGE DISCOVERY FROM DATA,2021.0,"This article proposes a deep learning solution to the online portfolio selection problem based on learning a latent structure directly from a price time series. It introduces a novel wealth flow matrix for representing a latent structure that has special regular conditions to encode the knowledge about the relative strengths of assets in portfolios. Therefore, a wealth flow model (WFM) is proposed to learn wealth flow matrices and maximize portfolio wealth simultaneously. Compared with existing approaches, our work has several distinctive benefits: (1) the learning of wealth flow matrices makes our model more generalizable than models that only predict wealth proportion vectors, and (2) the exploitation of wealth flow matrices and the exploration of wealth growth are integrated into our deep reinforcement algorithm for the WFM. These benefits, in combination, lead to a highly-effective approach for generating reasonable investment behavior, including short-term trend following, the following of a few losers, no self-investment, and sparse portfolios. Extensive experiments on five benchmark datasets from real-world stock markets confirm the theoretical advantage of the WFM, which achieves the Pareto improvements in terms of multiple performance indicators and the steady growth of wealth over the state-of-the-art algorithms.",10.1145/3464308,Online portfolio selection; wealth flow matrix; deep reinforcement learning regret bound,,
SA-CGAN: An oversampling method based on single attribute guided conditional GAN for multi-class imbalanced learning,"Dong, YF; Xiao, HX; Dong, Y",NEUROCOMPUTING,2022.0,"Imbalanced data can always be observed in our daily life and various practical tasks. A lot of well constructed machine learning methodologies may produce ineffective performance, when conducted on this kind of data. This originates from the produced high training biases that towards the majority class instances. Among all the solutions of this problem, data generation of the minority class is always considered the most effective approach. However, in all the previous works, data are always processed sample-wisely and the distribution of each single data attribute is never noticed. So, in this paper, to estimate the mechanism of how each attribute contributes to its label, we explore the potential connection between the two items by Conditional Generative Adversarial Networks (CGAN) separately and individually. Then, the constructed new instances are purified by a designed attribute-based minimax filter and the survivors are concatenated to form the eventual generated data. In other words, different from the CGAN based data generation way, the proposed approach improves it by additionally considering all the single attribute patterns of the data that to construct new instances. In addition, we extend the binary class imbalanced learning framework to multiple class one. In the experimental part, the improved model is compared against GAN, CGAN and some other standard multiple-class oversampling algorithms on several widely used datasets. Results, in terms of four common measurements, have shown that the proposed approach can produce comparable and always superior performance when compared with the competitors. (c) 2021 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2021.04.135,Generative adversarial network; Multi-class uneven; imbalanced data; Data generation; Attribute; feature pattern learning,,
Data centric nanocomposites design via mixed-lvariable Bayesian optimization,"Iyer, A; Zhang, YC; Prasad, A; Gupta, P; Tao, SY; Wang, YX; Prabhune, P; Schadler, LS; Brinson, LC; Chen, W",MOLECULAR SYSTEMS DESIGN & ENGINEERING,2020.0,"With an unprecedented combination of mechanical and electrical properties, polymer nanocomposites have the potential to be widely used across multiple industries. Tailoring nanocomposites to meet application specific requirements remains a challenging task, owing to the vast, mixed-variable design space that includes composition (i.e. choice of polymer, nanoparticle, and surface modification) and microstructures (i.e. dispersion and geometric arrangement of particles) of the nanocomposite material. Modeling properties of the interphase, the region surrounding a nanoparticle, introduces additional complexity to the design process and requires computationally expensive simulations. As a result, previous attempts at designing polymer nanocomposites have focused on finding the optimal microstructure for only a fixed combination of constituents. In this article, we propose a data centric design framework to concurrently identify optimal composition and microstructure using mixed-variable Bayesian optimization. This framework integrates experimental data with state-of-the-art techniques in interphase modeling, microstructure characterization and reconstructions and machine learning. Latent variable Gaussian processes (LVGPs) quantifies the lack-of-data uncertainty over the mixed-variable design space that consists of qualitative and quantitative material design variables. The design of electrically insulating nanocomposites is cast as a multicriteria optimization problem with the goal of maximizing dielectric breakdown strength while minimizing dielectric permittivity and dielectric loss. Within tens of simulations, our method identifies a diverse set of designs on the Pareto frontier indicating the tradeoff between dielectric properties. These findings project data centric design, effectively integrating experimental data with simulations for Bayesian Optimization, as an effective approach for design of engineered material systems.",10.1039/d0me00079e,,,
Predicting cloud performance for HPC applications before deployment,"Mariani, G; Anghel, A; Jongerius, R; Dittmann, G",FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE,2018.0,"To reduce the capital investment required to acquire and maintain a high performance computing cluster, today many HPC users are moving to cloud. When deploying an application in the cloud, the users may (a) fail to understand the interactions of the application with the software layers implementing the cloud system, (b) be unaware of some hardware details of the cloud system, and (c) fail to understand how sharing part of the cloud system with other users might degrade application performance. These misunderstandings may lead the users to select suboptimal cloud configurations in terms of cost or performance. In this work we propose a machine-learning methodology to support the user in the selection of the best cloud configuration to run the target workload before deploying it in the cloud. This enables the user to decide if and what to buy before facing the cost of porting and analyzing the application in the cloud. We couple a cloud-performance-prediction model (CP) on the cloud-provider side with a hardware independent profile-prediction model (PP) on the user-side. PP captures the application-specific scaling behavior. The user profiles the target application while processing small datasets on small machines she (or he) owns, and applies machine learning to generate PP to predict the profiles for larger datasets to be processed in the cloud. CP is generated by the cloud provider to learn the relationships between the hardware-independent profile and cloud performance starting from the observations gathered by executing a set of training applications on a set of training cloud configurations. Since the profile data in use is hardware-independent the user and the provider can generate the prediction models independently possibly on heterogeneous machines. We apply the prediction models to Fortran-MPI benchmarks. The resulting relative error is below 12% for CP and 30% for PP. The optimal Pareto front of cloud configurations finally found when maximizing performance and minimizing execution cost on the prediction models is at most 25% away from the actual optimal solutions. (C) 2017 Elsevier B.V. All rights reserved.",10.1016/j.future.2017.10.048,Cloud computing; Performance prediction; Random forest,,
Predicting patient survival after liver transplantation using evolutionary multi-objective artificial neural networks,"Cruz-Ramirez, M; Hervas-Martinez, C; Fernandez, JC; Briceno, J; de la Mata, M",ARTIFICIAL INTELLIGENCE IN MEDICINE,2013.0,"Objective: The optimal allocation of organs in liver transplantation is a problem that can be resolved using machine-learning techniques. Classical methods of allocation included the assignment of an organ to the first patient on the waiting list without taking into account the characteristics of the donor and/or recipient. In this study, characteristics of the donor, recipient and transplant organ were used to determine graft survival. We utilised a dataset of liver transplants collected by eleven Spanish hospitals that provides data on the survival of patients three months after their operations. Methods and material: To address the problem of organ allocation, the memetic Pareto evolutionary non-dominated sorting genetic algorithm 2 (MPENSGA2 algorithm), a multi-objective evolutionary algorithm, was used to train radial basis function neural networks, where accuracy was the measure used to evaluate model performance, along with the minimum sensitivity measurement. The neural network models obtained from the Pareto fronts were used to develop a rule-based system. This system will help medical experts allocate organs. Results: The models obtained with the MPENSGA2 algorithm generally yielded competitive results for all performance metrics considered in this work, namely the correct classification rate (C), minimum sensitivity (MS), area under the receiver operating characteristic curve (AUC), root mean squared error (RMSE) and Cohen's kappa (Kappa). In general, the multi-objective evolutionary algorithm demonstrated a better performance than the mono-objective algorithm, especially with regard to the MS extreme of the Pareto front, which yielded the best values of MS (48.98) and AUC (0.5659). The rule-based system efficiently complements the current allocation system (model for end-stage liver disease, MELD) based on the principles of efficiency and equity. This complementary effect occurred in 55% of the cases used in the simulation. The proposed rule-based system minimises the prediction probability error produced by two sets of models (one of them formed by models guided by one of the objectives (entropy) and the other composed of models guided by the other objective (MS)), such that it maximises the probability of success in liver transplants, with success based on graft survival three months post-transplant. Conclusion: The proposed rule-based system is objective, because it does not involve medical experts (the expert's decision may be biased by several factors, such as his/her state of mind or familiarity with the patient). This system is a useful tool that aids medical experts in the allocation of organs; however, the final allocation decision must be made by an expert. (C) 2013 Elsevier B.V. All rights reserved.",10.1016/j.artmed.2013.02.004,Making decisions rule-based; Multi-objective evolutionary algorithm; Radial basis function neural networks; Liver transplantation; Organ allocations,,
A simplified model for structural stiffness and crashworthiness optimisation of composite fuselages,"Weiss, L",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,2019.0,"In this paper, a multi-objective optimisation for the initial design of crashworthy composite structures is proposed. The focus is on the optimisation model with its minimisation of inhomogeneous deformation capability. It promotes homogeneous contribution of all crash elements, as a representative of structural crashworthiness. For the first time, homogeneous contribution has been identified as a metric for crashworthiness of composite structures and been transferred into a mathematical expression. The structural model uses discrete elements for very efficient computation in combination with the genetic algorithm NSGA-II. Clustering as a machine learning technique is applied to the Pareto set of solutions in order to identify representative structural solutions. The approach uses positioning of elements and the shape of the spring characteristic of these elements as variables. The method enables the simultaneous consideration of static and crash loads, which is demonstrated by a case study featuring a composite aircraft fuselage substructure. So far, static and crash loads have been only considered separately, but never at the same time. The novelty of this approach is in the combination of an appropriate simplified modelling technique and a new formulation of the optimisation model. The proposed optimisation is beneficial in improving the crashworthiness of composites, as optimisation of the geometry and material behaviour enables a non-linear response to be obtained in an otherwise brittle material.",10.1007/s00158-018-2166-1,Multi-objective; Multi-disciplinary; Composite structure; Crashworthiness; System response; NSGA-II,,
MOPSO-Based CNN for Keyword Selection on Google Ads,"Liang, J; Yang, HT; Gao, JJ; Yue, CT; Ge, SL; Qu, BY",IEEE ACCESS,2019.0,"Google Ads is an advertising agency that provides ads to advertisers. Advertisers match the user's search terms and push ads by selecting keywords related to their ad content. Keywords can determine the type of users an advertiser pushes, the effectiveness of the ad promotion, and the sales of the ad product. Automatically selecting keywords that are satisfactory to advertisers from a large number of keywords provided by Google Ads is the main task of this paper. But there is not too much time for the model to judge whether keywords are selected, choosing correct keywords in the shortest time is another task of this paper. Therefore, a structure of the model that can get some useful keywords for advertisers is designed and an improved multi-objective particle swarm optimization algorithm is proposed to achieve this multiobjective task. These are also the main contributions of this paper. To accomplish this multi-objective task, many technical issues need to be overcome, such as the mixed language problem, the imbalance problem, the problem of extracting features from corpora and so on. This paper proposes a corpus selection method to solve the mixed problem of Chinese and English in keywords, word embedding method to solve the representation of keywords, re-sampling to solve data imbalance problem, improved convolutional neural network (CNN) to solve classification problem, and a multi-objective particle swarm optimization algorithm (MOPSO) to achieve neural structure search of CNN so that the effect of the classification is improved and the training time is reduced. The keyword selection problem is solved with the combination of evolutionary computing, deep learning, machine learning, and text processing techniques. Experimental results show that the proposed algorithm greatly improved the accuracy of keyword selection and shortened the time of selecting keywords. Therefore, this algorithm has a good application value.",10.1109/ACCESS.2019.2937339,Google Ads; corpus selection; word embedding; re-sampling; CNN; MOPSO; keyword selection; neural structure search,,
Automating the mixture design of lightweight foamed concrete using multi-objective firefly algorithm and support vector regression,"Zhang, JF; Huang, YM; Ma, GW; Yuan, YM; Nener, B",CEMENT & CONCRETE COMPOSITES,2021.0,"Lightweight concrete (LWC) is widely used in the construction industry due to a variety of advantages. However, compared with traditional normal-weight concrete, more influencing variables (e.g. types of lightweight aggregates) must be considered to optimize multiple properties including uniaxial compressive strength (UCS), density and cost. This makes the mixture design of LWC more difficult or sometimes impossible using laboratory experiments. To address this issue, this study proposes a multi-objective optimization (MOO) method using machine learning and metaheuristic approaches for LWC mixture design through a two-step approach. In the first step, a least squares support vector regression (LSSVR) model is constructed to predict multiple properties of LWC. The hyper-parameters of the LSSVR model are tuned using the firefly algorithm (FA). A dataset containing a large number of different mixtures of LWC is compiled from published literature. High prediction accuracy (0.97 for UCS and 0.90 for density) is achieved on the test dataset (including 30% of all the instances). In the second step, a newly developed multi-objective FA (MOFA) model is used to optimize the LWC mixture, while satisfying the constraints. The Pareto fronts of the triple objectives (UCS, cost and density) are successfully obtained. The proposed MOO method is powerful and efficient in finding optimal LWC mixtures with conflicting objectives and therefore decision making can be facilitated in early phases of construction.",10.1016/j.cemconcomp.2021.104103,Lightweight concrete; Mixture design; Multi-objective optimization; Least-square support vector regression; Firefly algorithm; Uniaxial compressive strength; Density,,
CEOF: Enhanced Clustering-based Entries Optimization scheme to prevent Flow table overflow,"Priyanka, N; Reshmi, TR; Murugan, K",WIRELESS NETWORKS,2022.0,"Software-Defined Networking is an advanced networking architecture that decouples the control and data plane for efficient and flexible network administration. The packets are forwarded based on the rules existing in the flow table that resides in the Ternary Content Addressable Memory (TCAM) and plays a key role in packet forwarding. TCAM is prominent for wire-speed processing with certain limitations such as high power consumption, expensive, and limited storage. It creates a serious challenge in terms of scalability where the limited sized flow tables are over-utilized and are easily overflowed during a high traffic rate. The flow table overflow creates blocking of new incoming flows or eviction of existing entries that are accessed by active flows. To overcome these challenges and to provide Quality of Service to the current network design, an entry reduction scheme is proposed using machine learning algorithms. It consists of two phases (1) Detection of overflow by estimating the cardinality of entries in each snapshot of the flow table which is carried out using HyperLogLog. (2) When overflow is detected, immediately the mitigation is carried out by evicting the extravagant entries using Hierarchical Agglomerative Clustering followed by entries optimization of each cluster using Pareto Optimizer. The simulation results proved that the proposed work reduces 99.99% of redundant entries and, 99.98% of increased network throughput with reduced controller overhead.",10.1007/s11276-021-02823-8,SDN; TCAM; Flow table; HLL; HAC; Pareto optimization,,
Towards Efficient Robust Optimization using Data based Optimal Segmentation of Uncertain Space,"Pantula, PD; Mitra, K",RELIABILITY ENGINEERING & SYSTEM SAFETY,2020.0,"Performing multi-objective optimization under uncertainty is a common requirement in industries and academia. Robust optimization (RO) is considered as an efficient and tractable approach provided one has access to behavioral data for the uncertain parameters. However, solutions of RO may be far from the real solution and less reliable due to inability to map the uncertain space accurately, especially when the data appears discontinuous and scattered in the uncertain domain. Amalgamating machine learning algorithms with RO, this paper proposes a data-driven methodology, where a novel fuzzy clustering mechanism is implemented along-with boundary construction, to transcript the uncertain space such that the specific regions of uncertainty are identified. Subsequently, using intelligent Sobol sampling, samples are generated in the mapped uncertain regions. Results of two test cases are presented along with a comprehensive comparison study. Considered case-studies include highly nonlinear model for continuous casting process from steelmaking industries, where a multi-objective optimization problem under uncertainty is solved to balance the conflict between productivity and energy consumption. The Pareto-optimal solutions of the resulting RO problem are obtained through Non-Dominated Sorting Genetic Algorithm - II, and similar to 23-29% improvement is observed in the uncertain objective function. Further, the spread and diversity metrics are enhanced by similar to 10-95% as compared to those obtained using other standard uncertainty sets.",10.1016/j.ress.2020.106821,Data-driven robust optimization; Fuzzy clustering; Boundary construction; Sobol sampling; Multi-objective optimization; Pareto-optimal solutions,,
Enhancing the effectiveness of Ant Colony Decision Tree algorithms by co-learning,"Boryczka, U; Kozak, J",APPLIED SOFT COMPUTING,2015.0,"Data mining and visualization techniques for high-dimensional data provide helpful information to substantially augment decision-making. Optimization techniques provide a way to efficiently search for these solutions. ACO applied to data mining tasks - a decision tree construction - is one of these methods and the focus of this paper. The Ant Colony Decision Tree (ACDT) approach generates solutions efficiently and effectively but scales poorly to large problems. This article merges the methods that have been developed for better construction of decision trees by ants. The ACDT approach is tested in the context of the bi-criteria evaluation function by focusing on two problems: the size of the decision trees and the accuracy of classification obtained during ACDT performance. This approach is tested in co-learning mechanism, it means agents-ants can interact during the construction decision trees via pheromone values. This cooperation is a chance of getting better results. The proposed methodology of analysis of ACDT is tested in a number of well-known benchmark data sets from the UCI Machine Learning Repository. The empirical results clearly show that the ACDT algorithm creates good solutions which are located in the Pareto front. The software that implements the ACDT algorithm used to generate the results of this study can be downloaded freely from http://www.acdtalgorithm.com. (C) 2015 Elsevier B.V. All rights reserved.",10.1016/j.asoc.2014.12.036,Ant Colony Optimization; Ant Colony Decision Trees; Decision trees; Pareto front; Quality of decision trees; Ant-Miner,,
Inverse Design of Broadband and Reconfigurable Meta-Optics,"Campbell, SD; Werner, DH; Werner, PL",METAMATERIALS XIII,2021.0,"By exploiting the generalized form of Snell's law, metasurfaces afford optical engineers a tremendous increase in degrees of design freedom compared to conventional optical components. These meta-optics can achieve unprecedented levels of performance through engineered wavelength-, angular-, and polarization-dependent responses which can be tailored by arranging subwavelength unit cells, or meta-atoms, in an intelligent way. Moreover, these metaatoms can be constructed from phase change materials which give the added flexibility of realizing reconfigurable metaoptics. Devices such as achromatic flat lenses and non-mechanical zoom lenses are becoming a reality through the advent of metasurface-augmented optical systems. However, achieving high-performance meta-optics relies heavily on proper meta-atom design. This challenge is best overcome through the use of advanced inverse-design tools and state-of-the-art optimization algorithms. To this end, a number of successful meta-device inverse-design approaches have been demonstrated in the literature including those based on topology optimization, deep learning, and global optimization. While each has its pros and cons, multi-objective optimization strategies have proven quite successful do their ability to optimize problems with multiple competing objectives: a common occurrence in optical design. Moreover, multiobjective algorithms produce a Pareto Set of optimal solutions that designers can analyze in order to directly study the tradeoffs between the various design goals. In our presentation, we will introduce an efficient multi-objective optimization enabled design framework for the generation of broadband and multifunctional meta-atoms. Additionally, several meta-optic design examples will be presented, and future research directions discussed.",10.1117/12.2589333,Metamaterials; Metasurfaces; Optimization; Inverse-Design; Optical Design,,
ARTIFICIAL NEURAL NETWORKS VERSUS NATURAL NEURAL NETWORKS - A CONNECTIONIST PARADIGM FOR PREFERENCE ASSESSMENT,"WANG, J",DECISION SUPPORT SYSTEMS,1994.0,Preference is an essential ingredient in all decision processes. This paper presents a new connectionist paradigm for preference assessment in a general multicriteria decision setting. A general structure of an artificial neural network for representing two specified prototypes of preference structures is discussed. An interactive preference assessment procedure and an autonomous learning algorithm based on a novel scheme of supervised learning are proposed. Operating characteristics of the proposed paradigm are also illustrated through detailed results of numerical simulations.,10.1016/0167-9236(94)90016-7,NEURAL NETWORKS; SUPERVISED LEARNING; PREFERENCE ASSESSMENT,,
PRELIMINARY-REPORT ON MACHINE LEARNING VIA MULTIOBJECTIVE OPTIMIZATION,"AHMAD, Z; GUEZ, A","APPLICATIONS OF ARTIFICIAL NEURAL NETWORKS III, PTS 1 AND 2",1992.0,,10.1117/12.140027,,,
Adjustable general line coordinates for visual knowledge discovery in n-D data,"Kovalerchuk, B; Grishin, V",INFORMATION VISUALIZATION,2019.0,"Preserving all multidimensional data in two-dimensional visualization is a long-standing problem in Visual Analytics, Machine Learning/Data Mining, and Multiobjective Pareto Optimization. While Parallel and Radial (Star) coordinates preserve all n-D data in two dimensions, they are not sufficient to address visualization challenges of all possible datasets such as occlusion. More such methods are needed. Recently, the concepts of lossless General Line Coordinates that generalize Parallel, Radial, Cartesian, and other coordinates were proposed with initial exploration and application of several subclasses of General Line Coordinates such as Collocated Paired Coordinates and Star Collocated Paired Coordinates. This article explores and enhances benefits of General Line Coordinates. It shows the ways to increase expressiveness of General Line Coordinates including decreasing occlusion and simplifying visual pattern while preserving all n-D data in two dimensions by adjusting General Line Coordinates for given n-D datasets. The adjustments include relocating, rescaling, and other transformations of General Line Coordinates. One of the major sources of benefits of General Line Coordinates relative to Parallel Coordinates is twice less number of point and lines in visual representation of each n-D points. This article demonstrates the benefits of different General Line Coordinates for real data visual analysis such as health monitoring and benchmark Iris data classification compared with results from Parallel Coordinates, Radvis, and Support Vector Machine. The experimental part of the article presents the results of the experiment with about 70 participants on efficiency of visual pattern discovery using Star Collocated Paired Coordinates, Parallel, and Radial Coordinates. It shows advantages of visual discovery of n-D patterns using General Line Coordinates subclass Star Collocated Paired Coordinates with n = 160 dimensions.",10.1177/1473871617715860,Multidimensional data visualization; knowledge discovery; visual data mining; machine learning; general line coordinates; lossless visual representation; reversible visual representation; adjustable coordinates; clutter reduction; parallel coordinates,,
An Intelligent Multicriteria Model for Diagnosing Dementia in People Infected with Human Immunodeficiency Virus,"Pinheiro, LICC; Pereira, MLD; Andrade, ECD; Nunes, LC; Abreu, WCD; Pinheiro, PGCD; Holanda, R; Pinheiro, PR",APPLIED SCIENCES-BASEL,2021.0,"Hybrid models to detect dementia based on Machine Learning can provide accurate diagnoses in individuals with neurological disorders and cognitive complications caused by Human Immunodeficiency Virus (HIV) infection. This study proposes a hybrid approach, using Machine Learning algorithms associated with the multicriteria method of Verbal Decision Analysis (VDA). Dementia, which affects many HIV-infected individuals, refers to neurodevelopmental and mental disorders. Some manuals standardize the information used in the correct detection of neurological disorders with cognitive complications. Among the most common manuals used are the DSM-5 (Diagnostic and Statistical Manual of Mental Disorders, 5th edition) of the American Psychiatric Association and the International Classification of Diseases, 10th edition (ICD-10)-both published by World Health Organization (WHO). The model is designed to explore the predictive of specific data. Furthermore, a well-defined database data set improves and optimizes the diagnostic models sought in the research.",10.3390/app112110457,cognitive dementia; HIV; machine learning; verbal decision analysis; multicriteria model; medical diagnostic optimization,,
Adaptive entropy-based learning with dynamic artificial neural network,"Pinto, T; Morais, H; Corchado, JM",NEUROCOMPUTING,2019.0,"Entropy models the added information associated to data uncertainty, proving that stochasticity is not purely random. This paper explores the potential improvement of machine learning methodologies through the incorporation of entropy analysis in the learning process. A multi-layer perceptron is applied to identify patterns in previous forecasting errors achieved by a machine learning methodology. The proposed learning approach is adaptive to the training data through a re-training process that includes only the most recent and relevant data, thus excluding misleading information from the training process. The learnt error patterns are then combined with the original forecasting results in order to improve forecasting accuracy, using the Renyi entropy to determine the amount in which the original forecasted value should be adapted considering the learnt error patterns. The proposed approach is combined with eleven different machine learning methodologies, and applied to the forecasting of electricity market prices using real data from the Iberian electricity market operator - OMIE. Results show that through the identification of patterns in the forecasting error, the proposed methodology is able to improve the learning algorithms' forecasting accuracy and reduce the variability of their forecasting errors. (C) 2019 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2018.09.092,Artificial Neural Networks; Electricity market prices; Entropy; Forecasting; Information theory; Machine learning,,
Learning multicriteria classification models from examples: Decision rules in continuous space,"Dombi, J; Zsiros, A",EUROPEAN JOURNAL OF OPERATIONAL RESEARCH,2005.0,"The classification problem statement of multicriteria decision analysis is to model the classification of the alternatives/actions according to the decision maker's preferences. These models are based on outranking relations, utility functions or (linear) discriminant functions. Model parameters can be given explicitly or learnt from a preclassified set of alternatives/actions. In this paper we propose a novel approach, the Continuous Decision (CD) method, to learn parameters of a discriminant function, and we also introduce its extension, the Continuous Decision Tree (CDT) method, which describes the classification more accurately. The proposed methods are results of integration of Machine Learning methods in Decision Analysis. From a Machine Learning point of view, the CDT method can be considered as an extension of the C4.5 decision tree building algorithm that handles only numeric criteria but applies more complex tests in the inner nodes of the tree. For the sake of easier interpretation, the decision trees are transformed to rules. (C) 2003 Elsevier B.V. All rights reserved.",10.1016/j.ejor.2003.10.006,multiple criteria analysis; classification; artificial intelligence; decision trees; fuzzy sets,,
Rapid Generation of High-Quality RISC-V Processors from Functional Instruction Set Specifications,"Liu, G; Primmer, J; Zhang, ZR",PROCEEDINGS OF THE 2019 56TH ACM/EDAC/IEEE DESIGN AUTOMATION CONFERENCE (DAC),2019.0,"The increasing popularity of compute acceleration for emerging domains such as artificial intelligence and computer vision has led to the growing need for domain-specific accelerators, often implemented as specialized processors that execute a set of domain-optimized instructions. The ability to rapidly explore (1) various possibilities of the customized instruction set, and (2) its corresponding micro-architectural features is critical to achieve the best quality-of-results (QoRs). However, this ability is frequently hindered by the manual design process at the register transfer level (RTL). Such an RTL-based methodology is often expensive and slow to react when the design specifications change at the instruction-set level and/or micro-architectural level. We address this deficiency in domain-specific processor design with ASSIST, a behavior-level synthesis framework for RISC-V processors. From an untimed functional instruction set description, ASSIST generates a spectrum of RISC-V processors implementing varying micro-architectural design choices, which enables effective tradeoffs between different QoR metrics. We demonstrate the automatic synthesis of more than 60 in-order processor implementations with varying pipeline structures from the RISC-V 32I instruction set, some of which dominate the manually optimized counterparts in the area-performance Pareto frontier. In addition, we propose an autotuning-based approach for optimizing the implementations under a given performance constraint and the technology target. We further present case studies of synthesizing various custom instruction extensions and customized instruction sets for cryptography and machine learning applications.",10.1145/3316781.3317890,,,
autoAx: An Automatic Design Space Exploration and Circuit Building Methodology utilizing Libraries of Approximate Components,"Mrazek, V; Hand, MA; Vasicek, Z; Sekanina, L; Shafique, M",PROCEEDINGS OF THE 2019 56TH ACM/EDAC/IEEE DESIGN AUTOMATION CONFERENCE (DAC),2019.0,"Approximate computing is an emerging paradigm for developing highly energy-efficient computing systems such as various accelerators. In the literature, many libraries of elementary approximate circuits have already been proposed to simplify the design process of approximate accelerators. Because these libraries contain from tens to thousands of approximate implementations for a single arithmetic operation it is intractable to find an optimal combination of approximate circuits in the library even for an application consisting of a few operations. An open problem is how to effectively combine circuits from these libraries to construct complex approximate accelerators. This paper proposes a novel methodology for searching, selecting and combining the most suitable approximate circuits from a set of available libraries to generate an approximate accelerator for a given application. To enable fast design space generation and exploration, the methodology utilizes machine learning techniques to create computational models estimating the overall quality of processing and hardware cost without performing full synthesis at the accelerator level. Using the methodology, we construct hundreds of approximate accelerators (for a Sobel edge detector) showing different but relevant tradeoffs between the quality of processing and hardware cost and identify a corresponding Pareto-frontier. Furthermore, when searching for approximate implementations of a generic Gaussian filter consisting of 17 arithmetic operations, the proposed approach allows us to identify approximately 10(3) highly relevant implementations from 10(23) possible solutions in a few hours, while the exhaustive search would take four months on a high-end processor.",10.1145/3316781.3317781,,,
A Model Falsification Approach to Learning in Non-Stationary Environments for Experimental Design,"Murari, A; Lungaroni, M; Peluso, E; Craciunescu, T; Gelfusa, M",SCIENTIFIC REPORTS,2019.0,"The application of data driven machine learning and advanced statistical tools to complex physics experiments, such as Magnetic Confinement Nuclear Fusion, can be problematic, due the varying conditions of the systems to be studied. In particular, new experiments have to be planned in unexplored regions of the operational space. As a consequence, care must be taken because the input quantities used to train and test the performance of the analysis tools are not necessarily sampled by the same probability distribution as in the final applications. The regressors and dependent variables cannot therefore be assumed to verify the i.i.d. (independent and identical distribution) hypothesis and learning has therefore to take place under non stationary conditions. In the present paper, a new data driven methodology is proposed to guide planning of experiments, to explore the operational space and to optimise performance. The approach is based on the falsification of existing models. The deployment of Symbolic Regression via Genetic Programming to the available data is used to identify a set of candidate models, using the method of the Pareto Frontier. The confidence intervals for the predictions of such models are then used to find the best region of the parameter space for their falsification, where the next set of experiments can be most profitably carried out. Extensive numerical tests and applications to the scaling laws in Tokamaks prove the viability of the proposed methodology.",10.1038/s41598-019-54145-7,,,
Change detection in synthetic aperture radar images based on evolutionary multiobjective optimization with ensemble learning,"Li, H; Ma, JJ; Gong, MG; Jiang, QZ; Jiao, LC",MEMETIC COMPUTING,2015.0,"This paper presents an unsupervised change detection approach for synthetic aperture radar (SAR) images based on a multiobjective clustering algorithm and selective ensemble strategy. A multiobjective clustering method based on the nondominated neighbor immune algorithm is proposed for classifying changed and unchanged regions in the difference image, which aims at reducing the effect of speckle noise and enhancing the cluster performance. The proposed multiobjective clustering method generates a set of mutually intermediate clustering solutions, which correspond to different trade-offs between the two objectives: restraining noise and preserving detail. Then the selective ensemble strategy is introduced to integrated theses intermediate change detection results. Experiments on real SAR images show that the proposed change detection method based on multiobjective clustering reduces the effect of speckle noise and enhancing the cluster performance. In general, the proposed method makes a balance between noise-immunity and the preservation of image detail. The final change detection results obtained by the selective ensemble strategy exhibit lower errors than other existing methods.",10.1007/s12293-015-0165-y,Change detection; Synthetic aperture radar (SAR); Multiobjective optimization; Evolutionary algorithm; Ensemble learning,,
Discretization Techniques and Genetic Algorithm for Learning the Classification Method PROAFTN,"Al-Obeidat, F; Belacel, N; Mahanti, P; Carretero, JA","EIGHTH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS, PROCEEDINGS",2009.0,"This paper introduces new techniques for learning the classification method PROAFTN from data. PROAFTN is a multi-criteria classification method and belongs to the class of supervised learning algorithms. To use PROAFTN for classification, some parameters must be obtained for this purpose. Therefore, an automatic method to extract these parameters from data with minimum classification errors is required. Here, discretization techniques and genetic algorithms are proposed for establishing these parameters and then building the classification model. Based on the obtained results, the newly proposed approach outperforms widely used classification methods.",10.1109/ICMLA.2009.37,,,
A game-based framework for crowdsourced data labeling,"Yang, JR; Fan, J; Wei, ZW; Li, GL; Liu, TY; Du, XY",VLDB JOURNAL,2020.0,"Data labeling, which assigns data with multiple classes, is indispensable for many applications, such as machine learning and data integration. However, existing labeling solutions either incur expensive cost for large datasets or produce noisy results. This paper introduces a cost-effective labeling approach and focuses on the labeling rule generation problem that aims to generate high-quality rules to largely reduce the labeling cost while preserving quality. To address the problem, we first generate candidate rules and then devise a game-based crowdsourcing approach CrowdGame to select high-quality rules by considering coverage and accuracy. CrowdGame employs two groups of crowd workers: One group answers rule validation tasks (whether a rule is valid) to play a role of rule generator, while the other group answers tuple checking tasks (whether the label of a data tuple is correct) to play a role of rule refuter. We let the two groups play a two-player game: Rule generator identifies high-quality rules with large coverage, while rule refuter tries to refute its opponent rule generator by checking some tuples that provide enough evidence to reject rules with low accuracy. This paper studies the challenges in CrowdGame. The first is to balance the trade-off between coverage and accuracy. We define the loss of a rule by considering the two factors. The second is rule accuracy estimation. We utilize Bayesian estimation to combine both rule validation and tuple checking tasks. The third is to select crowdsourcing tasks to fulfill the game-based framework for minimizing the loss. We introduce a minimax strategy and develop efficient task selection algorithms. We also develop a hybrid crowd-machine method for effective label assignment under budget-constrained crowdsourcing settings. We conduct experiments on entity matching and relation extraction, and the results show that our method outperforms state-of-the-art solutions.",10.1007/s00778-020-00613-w,Crowdsourcing; Data labeling; Labeling rules,,
A novel approach for forecasting of ground vibrations resulting from blasting: modified particle swarm optimization coupled extreme learning machine,"Armaghani, DJ; Kumar, D; Samui, P; Hasanipanah, M; Roy, B",ENGINEERING WITH COMPUTERS,2021.0,"Ground vibration is one of the most important undesirable effects induced by blasting operations in the mining or tunneling projects. Hence, developing a precise model for prediction of ground vibration would be much beneficial to control environmental issues of blasting. The present study proposes a new hybrid machine learning (ML) technique, i.e., autonomous groups particles swarm optimization (AGPSO)-extreme learning machine (ELM) to predict ground vibration resulting from blasting. In fact, AGPSO-ELM model is a modified version of PSO-ELM that can solve problems in a way with higher prediction performance. For comparison purposes, PSO-ELM, minimax probability machine regression, least square-support vector machine and Gaussian process regression models were also proposed to estimate ground vibration. The said ML models were trained and tested based on a database comprising of 102 datasets collected from a quarry site in Malaysia. In the modeling of ML techniques, six input parameters were considered: burden to spacing ratio, maximum charge per delay, stemming, distance from the blasting-face, powder factor and hole depth. The results of ML techniques were evaluated in both stages of training and testing based on five fitness parameters criteria. Considering results of both training and testing datasets, AGPSO-ELM model was able to provide higher prediction performance for PPV prediction. Root-mean-square error values of (0.08 and 0.08) and coefficient of determination values of (0.92 and 0.90) were obtained, respectively, for training and testing datasets of AGPSO-ELM model which revealed that the new hybrid model is capable enough to forecast ground vibration induced by blasting. The newly proposed model can be used in other fields of science and engineering in order to get high accuracy level of prediction.",10.1007/s00366-020-00997-x,Blasting; Ground vibration; Extreme learning machine; Autonomous groups particles swarm optimization; Hybrid model,,
Cost-Effective Data Annotation using Game-Based Crowdsourcing,"Yang, JR; Fan, J; Wei, ZW; Li, GL; Liu, TY; Du, XY",PROCEEDINGS OF THE VLDB ENDOWMENT,2018.0,"Large-scale data annotation is indispensable for many applications, such as machine learning and data integration. However, existing annotation solutions either incur expensive cost for large datasets or produce noisy results. This paper introduces a cost-effective annotation approach, and focuses on the labeling rule generation problem that aims to generate high-quality rules to largely reduce the labeling cost while preserving quality. To address the problem, we first generate candidate rules, and then devise a game-based crowdsourcing approach CROWDGAME to select high-quality rules by considering coverage and precision. CROWDGAME employs two groups of crowd workers: one group answers rule validation tasks (whether a rule is valid) to play a role of rule generator, while the other group answers tuple checking tasks (whether the annotated label of a data tuple is correct) to play a role of rule refuter. We let the two groups play a two-player game: rule generator identifies high-quality rules with large coverage and precision, while rule refuter tries to refute its opponent rule generator by checking some tuples that provide enough evidence to reject rules covering the tuples. This paper studies the challenges in CROWDGAME. The first is to balance the trade-off between coverage and precision. We define the loss of a rule by considering the two factors. The second is rule precision estimation. We utilize Bayesian estimation to combine both rule validation and tuple checking tasks. The third is to select crowdsourcing tasks to fulfill the game-based framework for minimizing the loss. We introduce a minimax strategy and develop efficient task selection algorithms. We conduct experiments on entity matching and relation extraction, and the results show that our method outperforms state-of-the-art solutions.",10.14778/3275536.3275541,,,
Developing a futuristic multi-objective optimization of the fuel management problems for the nuclear research reactors,"Hedayat, A",KERNTECHNIK,2020.0,"In this paper; at the same time, two separate objectives and two safety and operational constraints are chosen to optimize fuel reloading pattern of a Material Testing Reactor (MTR), independently and coherently. This is one of the most difficult type of engineering problems as a constrained non-continuous, combinatorial, and fully multi-objective optimization problem. Decision space is a non-continuous midtimodal space restricted by both of the combinatorial and safety constraints. A smart software application and a robust hybrid algorithm have been developed to get Pareto optimal set with respect to both of the economy of irradiating utilizations and nuclear safety based on the heuristic soft computing. The hybrid algorithm is composed of a fast and elitist Multi-Objective Genetic Algorithm (MOGA) and a fast fitness function evaluating system based on the semi-deep learning cascade feed forward Artificial Neural Networks (ANNs). The smart software is used to produce database automatically required for the ANN training and test data. It can be also used to revise data accurately, impose further irradiating benefits or Operating Limits and Conditions (OL(s), and to advise the reactor supervisor on the most desire pattern based on the smart searches and filtering. The results are highly promising. For more details, optimization results dominate conventional operating core parameters, significantly. Also chosen OLCs are protected. Furthermore, this is very good practice to reach a fully developed practical application of the complex soft computing for the nuclear fuel management problems.",10.3139/124.190094,,,
Modeling Spatial Flood using Novel Ensemble Artificial Intelligence Approaches in Northern Iran,"Arabameri, A; Saha, S; Mukherjee, K; Blaschke, T; Chen, W; Ngo, PTT; Band, SS",REMOTE SENSING,2020.0,"The uncertainty of flash flood makes them highly difficult to predict through conventional models. The physical hydrologic models of flash flood prediction of any large area is very difficult to compute as it requires lot of data and time. Therefore remote sensing data based models (from statistical to machine learning) have become highly popular due to open data access and lesser prediction times. There is a continuous effort to improve the prediction accuracy of these models through introducing new methods. This study is focused on flash flood modeling through novel hybrid machine learning models, which can improve the prediction accuracy. The hybrid machine learning ensemble approaches that combine the three meta-classifiers (Real AdaBoost, Random Subspace, and MultiBoosting) with J48 (a tree-based algorithm that can be used to evaluate the behavior of the attribute vector for any defined number of instances) were used in the Gorganroud River Basin of Iran to assess flood susceptibility (FS). A total of 426 flood positions as dependent variables and a total of 14 flood conditioning factors (FCFs) as independent variables were used to model the FS. Several threshold-dependent and independent statistical tests were applied to verify the performance and predictive capability of these machine learning models, such as the receiver operating characteristic (ROC) curve of the success rate curve (SRC) and prediction rate curve (PRC), efficiency (E), root-mean square-error (RMSE), and true skill statistics (TSS). The valuation of the FCFs was done using AdaBoost, frequency ratio (FR), and Boosted Regression Tree (BRT) models. In the flooding of the study area, altitude, land use/land cover (LU/LC), distance to stream, normalized differential vegetation index (NDVI), and rainfall played important roles. The Random Subspace J48 (RSJ48) ensemble method with an area under the curve (AUC) of 0.931 (SRC), 0.951 (PRC), E of 0.89, sensitivity of 0.87, and TSS of 0.78, has become the most effective ensemble in predicting the FS. The FR technique also showed good performance and reliability for all models. Map removal sensitivity analysis (MRSA) revealed that the FS maps have the highest sensitivity to elevation. Based on the findings of the validation methods, the FS maps prepared using the machine learning ensemble techniques have high robustness and can be used to advise flood management initiatives in flood-prone areas.",10.3390/rs12203423,ensemble machine learning; flood hazard susceptibility; Gorganroud River Basin; validation,,
Modular Design Optimization using Machine Learning-based Flexibility Analysis,"Bhosekar, A; Ierapetritou, M",JOURNAL OF PROCESS CONTROL,2020.0,"Recent studies on modular and distributed manufacturing have introduced a new angle to the traditional economies of scale that claim that large plants exhibit better efficiencies and lower costs. A modular design has several advantages, including higher flexibility of decisions, lower investment costs, shorter time-to-market, and adaptability to market conditions. While design flexibility is a widely studied concept in the process design, modular design provides an interesting new opportunity to the design optimization problem under demand variability. In this work, a framework for modular design under demand variability is proposed. The framework consists of two steps. First, the feasible region for each module is represented analytically with the help of the historical data or the data from a simulation using a classification technique. In the second step, the optimal design choice is obtained by integrating the classifier models built in the first step as constraints in the design optimization problem. The design optimization problem is first solved considering a single objective, i.e., minimizing the total cost or maximizing the flexibility. These two objectives are then addressed simultaneously using a multiobjective optimization framework that considers the tradeoff between maximizing the flexibility of design and minimizing the cost. Computational studies conducted using a case study of an air separation plant, demonstrate the efficacy of the proposed framework. Several advantages of using a modular design, as well as data-driven methods in the decision-making process in the design step, are discussed. (C) 2020 Elsevier Ltd. All rights reserved.",10.1016/j.jprocont.2020.03.014,Modular design; Design optimization; Feasibility analysis; Flexibility analysis; Design under uncertainty; Machine learning; Operability,,
An integrated computational intelligence technique based operating parameters optimization scheme for quality improvement oriented process-manufacturing system,"Yin, XH; Niu, ZW; He, Z; Li, ZJ; Lee, D",COMPUTERS & INDUSTRIAL ENGINEERING,2020.0,"The analysis and improvement of product quality for process industry is an increasing concern for academia and industry. As the outputs of a manufacturing system mainly depend on corresponding input conditions, so it is of high significance to develop an optimization scheme to actively and accurately determine operating parameters to obtain desired quality. However, the widely employed single-model modeling mode for whole production process neglects the natural characteristics within process manufacturing system such as multistage manufacturing and hysteresis. Additionally, the popular data-driven modeling techniques in current works, especially black-box machine learning models have been restricted to satisfying the requirements regarding excellent approximation capability and explicit mathematical expression simultaneously. To fill up above research gap, it is meaningful to develop a new data-driven optimization scheme in this work to effectively and accurately determine the optimum operating parameters considering the abovementioned characteristics and requirements. Firstly, two different connecting strategies are discussed to determine the more accurate and feasible quality propagation mode between adjacent stages. Then, two computational intelligence (CI) techniques, i.e., Multi-Gene Genetic Programming (MGGP) and Multi-objective Particle Swarm Optimization (MOPSO) algorithm are exploited to construct correlation model with explicit mathematical expression and derive the optimal operating parameters, respectively. Afterwards, the fuzzy Multi-criteria Decision Making (FMCDM) method is further proposed to select the optimal solution from the obtained Pareto solutions sets. The application of the proposed scheme in a coal preparation process indicates that the proposed scheme is promising and competitive on prediction accuracy and optimization efficiency over baseline methods, and can significantly improve the final product quality comparing with initial parameters setting. Moreover, the feasible quality specification for intermediate product can also be obtained by our proposed scheme which is beneficial for early detection of quality abnormality and timely parameters adjustment.",10.1016/j.cie.2020.106284,Quality improvement; Operating parameters optimization; Process industry; Multistage manufacturing; Computational intelligence; Multi-gene genetic programming (MGGP),,
Autocalibration experiments using machine learning and high performance computing,"Sloboda, M; Swayne, DA",ENVIRONMENTAL MODELLING & SOFTWARE,2013.0,"Using as example the Soil and Water Assessment Tool (SWAT) model and a Southern Ontario Canada watershed, we conduct a set of experiments on calibration using a manual approach, a parallelized version of the shuffled complex evolution (SCE), Generalized Likelihood Uncertainty Estimation (GLUE), Sequential Uncertainty Fitting (SUFI-2) and compare to a simple parallel search on a finite set of gridded input parameter values invoking the probably approximately correct (PAC) learning hypothesis. We derive an estimation of the error in fitting and a prior estimate of the probability of success, based on the PAC hypothesis. We conclude that from the equivalent effort expended on initial setup for the other named algorithms we can already find directly a good parameter set for calibration. We further note that, in this algorithm, simultaneous co-calibration of flow and chemistry (total nitrogen and total phosphorous) is more likely to produce acceptable results, as compared to flow first, even with a simple weighted multiobjective approach. This approach is especially suited to a parallel, distributed or cloud computational environment. (C) 2012 Elsevier Ltd. All rights reserved.",10.1016/j.envsoft.2012.10.007,Automatic calibration; Multiobjective calibration; High performance computing; Machine learning; SWAT; Probably approximately correct learning,,
Data-driven multiobjective decision-making in cash management,"Salas-Molina, F; Rodriguez-Aguilar, JA",EURO JOURNAL ON DECISION PROCESSES,2018.0,"The volume and availability of business and finance data may continue to increase in the near future. However, the utility of such data is by no means straightforward due to a lack of integration between data-driven techniques and usual decision-making processes. This paper aims to integrate data with multiobjective decision-making in cash management by means of machine learning. To this end, we first consider cash flow forecasting as a data-driven procedure to be used as a key input to multiobjective cash management problem in which both cost and risk are goals to minimize. Next, we compute the forecasting premium, namely, how much value can be achieved in exchange of predictive accuracy. Finally, we provide cash managers with a general methodology to improve decision-making in cash management through the use of data and machine learning techniques. This methodology is based on a novel closed-loop procedure in which the estimated forecasting premium (if any) is used as a critical feedback information to find better forecasting models and, ultimately, better cost-risk results in cash management.",10.1007/s40070-017-0075-y,Machine learning; Multiobjective decision-making; Integration; Cash management,,
Interpretable EEG seizure prediction using a multiobjective evolutionary algorithm,"Pinto, M; Coelho, T; Leal, A; Lopes, F; Dourado, A; Martins, P; Teixeira, C",SCIENTIFIC REPORTS,2022.0,"Seizure prediction might be the solution to tackle the apparent unpredictability of seizures in patients with drug-resistant epilepsy, which comprise about a third of all patients with epilepsy. Designing seizure prediction models involves defining the pre-ictal period, a transition stage between interictal brain activity and the seizure discharge. This period is typically a fixed interval, with some recent studies reporting the evaluation of different patient-specific pre-ictal intervals. Recently, researchers have aimed to determine the pre-ictal period, a transition stage between regular brain activity and a seizure. Authors have been using deep learning models given the ability of such models to automatically perform pre-processing, feature extraction, classification, and handling temporal and spatial dependencies. As these approaches create black-box models, clinicians may not have sufficient trust to use them in high-stake decisions. By considering these problems, we developed an evolutionary seizure prediction model that identifies the best set of features while automatically searching for the pre-ictal period and accounting for patient comfort. This methodology provides patient-specific interpretable insights, which might contribute to a better understanding of seizure generation processes and explain the algorithm's decisions. We tested our methodology on 238 seizures and 3687 h of continuous data, recorded on scalp recordings from 93 patients with several types of focal and generalised epilepsies. We compared the results with a seizure surrogate predictor and obtained a performance above chance for 32% patients. We also compared our results with a control method based on the standard machine learning pipeline (pre-processing, feature extraction, classifier training, and post-processing), where the control marginally outperformed our approach by validating 35% of the patients. In total, 54 patients performed above chance for at least one method: our methodology or the control one. Of these 54 patients, 21 (similar to 38%) were solely validated by our methodology, while 24 (similar to 44%) were only validated by the control method. These findings may evidence the need for different methodologies concerning different patients.",10.1038/s41598-022-08322-w,,,
A guiding teaching and dual adversarial learning framework for a single image dehazing,"Fang, ZY; Zhao, M; Yu, ZT; Li, MY; Yang, Y",VISUAL COMPUTER,,"In most existing deep learning-based image dehazing methods, the haze-free source images are only used as the ground truth for the design of the loss function, whereas the guiding role that the source image should play on different feature levels has been ignored. This will result in a sub-optimal dehazing output. To address this issue, inspired by the knowledge distillation, a guiding teaching framework is designed for single image dehazing in an end-to-end manner, where the features of the haze-free source image at different levels are completely used to promoting the restoration of the hazy image. Specifically, the framework consists of a two-stream convolutional neural network termed teacher stream (TS) and student stream (SS), respectively. The input of the former is a haze-free image while the output is the desired image after reconstruction. The input of the latter is the hazy image, and the output is the restored image. Moreover, a dual adversarial strategy is designed to further improve the ability of SS to imitate teacher stream. In this process, the output results of SS are divided into two categories according to their hazy intensity levels. Then a thick light discriminator is introduced and made against the SS pit, such that the images with better dehazing effects can be used to deal with the ones poorly dehazed. A second discriminator termed light clear discriminator (LCD) is further introduced and a minimax game between the LCD and the SS is defined to drive the final result produced by SS closer to the reconstruction result of the TS. Experimental results show that the proposed method outperforms several latest methods applied to both artificial hazy images and the hazy images from the real scene.",10.1007/s00371-021-02184-5,Image dehazing; Knowledge distillation; Adversarial learning; Teacher stream; Student stream,,
The Social Lives of Generative Adversarial Networks,"Castelle, M","FAT* '20: PROCEEDINGS OF THE 2020 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY",2020.0,"Generative adversarial networks (GANs) are a genre of deep learning model of significant practical and theoretical interest for their facility in producing photorealistic 'fake' images which are plausibly similar, but not identical, to a corpus of training data. But from the perspective of a sociologist, the distinctive architecture of GANs is highly suggestive. First, a convolutional neural network for classification, on its own, is (at present) popularly considered to be an 'AI'; and a generative neural network is a kind of inversion of such a classification network (i.e. a layered transformation from a vector of numbers to an image, as opposed to a transformation from an image to a vector of numbers). If, then, in the training of GANs, these two 'AIs' interact with each other in a dyadic fashion, shouldn't we consider that form of learning... social? This observation can lead to some surprising associations as we compare and contrast GANs with the theories of the sociologist Pierre Bourdieu, whose concept of the so-called habitus is one which is simultaneously cognitive and social: a productive perception in which classification practices and practical action cannot be fully disentangled. Bourdieu had long been concerned with the reproduction of social stratification: his early works studied formal public schooling in France not as an egalitarian system but instead as one which unintentionally maintained the persistence of class distinctions. It was, he argued, through the cultural inculcation of an embodied and partially unconscious habitus-a durably installed generative principle of regulated improvisations-that, he argued, students from the upper classes are given an advantage which is only further reinforced throughout their educational trajectories. For Bourdieu, institutions of schooling instill deeply interiorized master patterns of behavior and thought (and classification) which in turn direct the acquisition of subsequent patterns, whose character is determined not simply by this cognitive layering but by their actual use in lived practice, especially early in childhood development. In this work I develop a productive analogy between the GAN architecture and Bourdieu's habitus, in three ways. First, I call attention to the fact that connectionist approaches and Bourdieu's theories were both conceived as revolts against rule-bound paradigms. In the 1980s, Rumelhart and McClelland used a multilayer neural network to learn the phonology of English past-tense verbs because sometimes we don't follow the rules... language is full of exceptions to the rules; and in the case of Bourdieu, the habitus was an answer to a long-standing question: how can behaviour be regulated without being the product of obedience to rules? Bourdieu strove to transgress what was then seen in the social sciences as a conceptual opposition between structure-based theories of social life and those which emphasized an embodied agency. Second, I suggest that concerns about bias and discrimination in machine learning in recent years can in part be attributed due to the increased use of ML models not just for static classification but for practical action. Similarly, the habitus for Bourdieu is simultaneously durable and transposable: its judgments may be relatively stable, but are capable of being deployed dynamically in novel and varying social situations-or what ML practitioners might call generalizability. We can thus theorize generative models (including GANs) as biased not just in their stereotyped classifications, but through their potential for actively generating new biased data. These generated actions then recursively become part of the social arena Bourdieu called the field, into which new agents are 'born' and for which they may know few alternatives. Finally, it is intriguing that GAN researchers and Bourdieu both extensively use metaphors from game theory. Goodfellow described the GAN architecture as a two-player minimax game with value function V(G,D), meaning that there is a single abstract function whose output value the discriminator is trying to maximize and which the generator is trying to minimize; but the dynamic nature of the GAN training process means that convergence to Nash equilibrium is nontrivial. But for Bourdieu, such a utility-based approach to artistic creation could not be more crude when compared to the social reality of art worlds: utilitarianism is, for him, the degree zero of sociology, by which he means an isolated, inert, and amodal-and therefore not particularly sociological-starting point. Moreover, 19th-century bohemian culture was characterized primarily by its inversion of financial incentives, in which failure is a kind of success, and selling out (i.e. maximizing profit) worst of all; and thus the relentless optimization of neural networks may be fundamentally at odds with the value functions of many human artists. I conclude that deep learning, while primarily understood as a scientific and technical achievement, may also intentionally or unintentionally constitute a nascent, independent reinvention of social theory.",10.1145/3351095.3373156,generative adversarial networks; sociological theory; habitus; bias; game theory,,
A data-driven approach towards finding closer estimates of optimal solutions under uncertainty for an energy efficient steel casting process,"Pantula, PD; Mitra, K",ENERGY,2019.0,"The process of steel casting involves several energy-intensive tasks such as heat transfer, solidification process, etc. Though the evolution of continuous casting of steel from the conventional ingot casting enabled a high amount of energy savings, operational parameter optimization considering various avenues of uncertainty is the key for next level of improvement in energy efficiency and process sustainability. To achieve this, a multi-objective optimization formulation under uncertainty has been proposed that can lead to simultaneous maximization of productivity and minimization of energy consumption. Among various uncertainty handling techniques, Chance Constrained Programming (CCP) is considered as an efficient approach. However, the requirement of uncertain parameters to follow some well-behaved probability distribution for having a closed form analytical solution in CCP is a bottleneck for most of the practical situations due to the unknown nature of uncertain data. This paper proposes a novel methodology called DDCCP (Data-Driven CCP), to amalgamate machine learning algorithms with CCP, thereby making the approach data-driven. A novel fuzzy clustering mechanism is implemented to transcript uncertain space such that the specific regions of uncertainty are identified accurately based on given uncertain data for more realistic sampling and thereby impacting the optimal solution accuracy. Implementing DDCCP on the casting model, similar to 20-70% improvement in the objectives of energy calculations and 50-100% improvement in the metrics of Pareto optimal solutions are observed as compared to the existing box sampling approach showing efficacy of the proposed methodology. (C) 2019 Elsevier Ltd. All rights reserved.",10.1016/j.energy.2019.116253,Continuous casting; Energy conservation; Uncertainty; Data-driven chance constrained programming; Fuzzy clustering; Multi-objective optimization,,
Adversarial Sample Crafting for Time Series Classification with Elastic Similarity Measures,"Oregi, I; Del Ser, J; Perez, A; Lozano, JA",INTELLIGENT DISTRIBUTED COMPUTING XII,2018.0,"Adversarial Machine Learning (AML) refers to the study of the robustness of classification models when processing data samples that have been intelligently manipulated to confuse them. Procedures aimed at furnishing such confusing samples exploit concrete vulnerabilities of the learning algorithm of the model at hand, by which perturbations can make a given data instance to be misclassified. In this context, the literature has so far gravitated on different AML strategies to modify data instances for diverse learning algorithms, in most cases for image classification. This work builds upon this background literature to address AML for distance based time series classifiers (e.g., nearest neighbors), in which attacks (i.e. modifications of the samples to be classified by the model) must be intelligently devised by taking into account the measure of similarity used to compare time series. In particular, we propose different attack strategies relying on guided perturbations of the input time series based on gradient information provided by a smoothed version of the distance based model to be attacked. Furthermore, we formulate the AML sample crafting process as an optimization problem driven by the Pareto trade-off between (1) a measure of distortion of the input sample with respect to its original version; and (2) the probability of the crafted sample to confuse the model. In this case, this formulated problem is efficiently tackled by using multi-objective heuristic solvers. Several experiments are discussed so as to assess whether the crafted adversarial time series succeed when confusing the distance based model under target.",10.1007/978-3-319-99626-4_3,Adversarial Machine Learning; Time series classification; Elastic similarity measures,,
Data-driven economic dispatch for islanded micro-grid considering uncertainty and demand response,"Hou, H; Wang, Q; Xiao, ZF; Xue, MY; Wu, YF; Deng, XT; Xie, CJ",INTERNATIONAL JOURNAL OF ELECTRICAL POWER & ENERGY SYSTEMS,2022.0,"The variability and intermittency of renewable energy and power load bring great pressure to the dispatch of Micro-Grid, especially intra-day dispatch. In order to reduce the intra-day dispatch pressure, this paper proposes a data-driven two-stage day-ahead dispatch model for islanded Micro-Grid. The first stage dispatch model considers multiple demand responses, and applies phase space reconstruction and machine learning to predict renewable energy output and power load. Multi-objective particle swarm optimization algorithm is used to solve the first stage dispatch model. For the obtained Pareto Front, weight multiple objectives by entropy weight method to get the optimal solution. Since the deviation between the predicted value and the actual value can lead to renewable energy curtailment or load loss, the role of the second stage is to predict the renewable energy curtailment and load loss after the first stage dispatch. Firstly, extreme gradient boosting is used to predict when renewable energy curtailment and load loss occur. Secondly, extreme learning machine is used to predict the amount of renewable energy curtailment and load loss at the corresponding time points. Finally, linear programming and mixed integer linear programming are used to solve the second stage dispatch model. By comparative cases analysis, simulation results show that the enhancement of demand response to system efficiency is at the cost of increasing dispatch cost and reducing reliability. In contrast, the proposed two-stage dispatch method using regulation reserve capacity not only reduces dispatch cost, but also improves system efficiency and reliability.",10.1016/j.ijepes.2021.107623,Multi-objective economic dispatch; Demand response; Uncertainty of source and load; Data-driven,,
"Hybrid multicriteria fuzzy classification of network traffic patterns, anomalies, and protocols","Al-Obeidat, F; El-Alfy, ESM",PERSONAL AND UBIQUITOUS COMPUTING,2019.0,"Traffic classification in computer networks has very significant roles in network operation, management, and security. Examples include controlling the flow of information, allocating resources effectively, provisioning quality of service, detecting intrusions, and blocking malicious and unauthorized access. This problem has attracted a growing attention over years and a number of techniques have been proposed ranging from traditional port-based and payload inspection of TCP/IP packets to supervised, unsupervised, and semi-supervised machine learning paradigms. With the increasing complexity of network environments and support for emerging mobility services and applications, more robust and accurate techniques need to be investigated. In this paper, we propose a new supervised hybrid machine-learning approach for ubiquitous traffic classification based on multicriteria fuzzy decision trees with attribute selection. Moreover, our approach can handle well the imbalanced datasets and zero-day applications (i.e., those without previously known traffic patterns). Evaluating the proposed methodology on several benchmark real-world traffic datasets of different nature demonstrated its capability to effectively discriminate a variety of traffic patterns, anomalies, and protocols for unencrypted and encrypted traffic flows. Comparing with other methods, the performance of the proposed methodology showed remarkably better classification accuracy.",10.1007/s00779-017-1096-z,Decision trees; Multicriterion fuzzy decision making; Network traffic classification; Encrypted traffic; Intrusion detection; Network management and security,,
Artificial Intelligence-Based Digital Image Steganalysis,"Iskanderani, AI; Mehedi, IM; Aljohani, AJ; Shorfuzzaman, M; Akther, F; Palaniswamy, T; Latif, SA; Latif, A",SECURITY AND COMMUNICATION NETWORKS,2021.0,"Recently, deep learning-based models are being extensively utilized for steganalysis. However, deep learning models suffer from overfitting and hyperparameter tuning issues. Therefore, in this paper, an efficient theta-nondominated sorting genetic algorithm- (theta NSGA-) III based densely connected convolutional neural network (DCNN) model is proposed for image steganalysis. theta NSGA-III is utilized to tune the initial parameters of DCNN model. It can control the accuracy and f-measure of the DCNN model by utilizing them as the multiobjective fitness function. Extensive experiments are drawn on STEGRT1 dataset. Comparison of the proposed model is also drawn with the competitive steganalysis model. Performance analyses reveal that the proposed model outperforms the existing steganalysis models in terms of various performance metrics.",10.1155/2021/9923389,,,
Learning Optimal Time Series Combination and Pre-Processing by Smart Joins,"Gil, A; Quartulli, M; Olaizola, IG; Sierra, B",APPLIED SCIENCES-BASEL,2020.0,"In industrial applications of data science and machine learning, most of the steps of a typical pipeline focus on optimizing measures of model fitness to the available data. Data preprocessing, instead, is often ad-hoc, and not based on the optimization of quantitative measures. This paper proposes the use of optimization in the preprocessing step, specifically studying a time series joining methodology, and introduces an error function to measure the adequateness of the joining. Experiments show how the method allows monitoring preprocessing errors for different time slices, indicating when a retraining of the preprocessing may be needed. Thus, this contribution helps quantifying the implications of data preprocessing on the result of data analysis and machine learning methods. The methodology is applied to two case studies: synthetic simulation data with controlled distortions, and a real scenario of an industrial process.",10.3390/app10186346,optimization; machine learning; preprocessing,,
Machine learning-based multi-objective optimisation of an aerogel glazing system using NSGA-II-study of modelling and application in the subtropical climate Hong Kong,"Zhou, YK; Zheng, SQ",JOURNAL OF CLEANER PRODUCTION,2020.0,"Application of super-insulating materials in building glazing system shows promising prospects for low-energy buildings. In this research, the heat transfer, solar radiation transmission and indoor illuminance of an aerogel glazing system were characterized through an experimentally validated numerical model. Contribution ratios of multi-variables to multi-objectives were thereafter quantified, following the Taguchi standard orthogonal array. In respect to the application of aerogel glazing system in subtropical climates, an energy-related contradiction between indoor illuminance from solar and indoor heat gain, has been presented, discussed, together with effective solutions. In order to minimise the total heat gain and maximise the indoor illuminance transmitted through the aerogel glazing system, a generic multi-objective optimisation methodology, with high computational efficiency and accuracy, has been developed, to identify the optimal design. The results indicate that through the Pareto front from the multi-objective optimisation results, a significant reduction of total heat gain and an obvious increase of the indoor illuminance can be noticed. Compared to the optimal case in the standard orthogonal array, with the application of the proposed multi-objective optimisation methodology, the annual total heat gain could be reduced from 489305.5 to 333396.4 Wh by 31.9% and the annual indoor illuminance could be increased from 56786.6 to 172973.5 lux by 67.2%. The year-round performance indicates that, compared to the bi-objective optimisation (annual transmitted heat gain and annual indoor illuminance) with the annual total heat gain at 333.4 kWh/m(2) and annual indoor illuminance at 162.3 klux, the bi-objective optimisation (annual total heat gain and annual indoor illuminance) shows a lower annual total heat gain at 322.4 kWh/m(2) by 3.4%) and a higher annual indoor illuminance at 173 klux by 6.6%. This study proposes an overall framework and technical guidance of a new multi-objective optimisation methodology, which can automatically learn mechanisms of heat transfer and solar radiation transmittance through nanoporous aerogel granules, and identify the optimal multi-variables setting for the robust system design and operation. (C) 2020 Elsevier Ltd. All rights reserved.",10.1016/j.jclepro.2020.119964,Aerogel glazing system; Indoor heat gain; Indoor illuminance; Machine learning; Multi-objective optimisation; Multi-criteria decision making,,
LRP-Based path relevances for global explanation of deep architectures,"Guerrero-Gomez-Olmedo, R; Salmeron, JL; Kuchkovsky, C",NEUROCOMPUTING,2020.0,"Understanding what Machine Learning models are doing is not always trivial. This is especially true for complex models such as Deep Neural Networks (DNN), which are the best-suited algorithms for modeling very complex and nonlinear relationships. But this need to understand has become a must since privacy regulations are hardening the industrial use of these models. There are different techniques to address the interpretability issues that Machine Learning models arises. This paper is focused on opening the so-called Deep Neural architectures black-box. This research extends the technique called Layer-wise Relevant Propagation (LRP) enhancing its properties to compute the most critical paths in different deep neural architectures using multicriteria analysis. We call this technique Ranked-LRP and it was tested on four different datasets and tasks, including classification and regression. The results show the worth of our proposal. (C) 2020 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2019.11.059,Explainable AI; Deep learning; Interpretable machine learning; Layer-wise relevant propagation,,
Improving science yield for NASA Swift with automated planning technologies,"Tohuvavohu, A",18TH INTERNATIONAL WORKSHOP ON ADVANCED COMPUTING AND ANALYSIS TECHNIQUES IN PHYSICS RESEARCH (ACAT2017),2018.0,"The Neil Gehrels Swift Observatory is a uniquely capable mission, with three on-board instruments and rapid slewing capabilities. It serves as a fast-response satellite observatory for everything from gravitational-wave counterpart searches to cometary science. Swift averages 125 different observations per day, and is consistently over-subscribed, responding to about one-hundred Target of Oportunity ( ToO) requests per month from the general astrophysics community, as well as co-pointing and follow-up agreements with many other observatories. Since launch in 2004, the demands put on the spacecraft have grown consistently in terms of number and type of targets as well as schedule complexity. To facilitate this growth, various scheduling tools and helper technologies have been built by the Swift team to continue improving the scientific yield of the Swift mission. However, these tools have been used only to assist humans in exploring the local pareto surface and for fixing constraint violations. Because of the computational complexity of the scheduling task, no automation tool has been able to produce a plan of equal or higher quality than that produced by a well-trained human, given the necessary time constraints. In this proceeding we formalize the Swift Scheduling Problem as a dynamic fuzzy Constraint Satisfaction Problem ( DF-CSP) and explore the global solution space. We detail here several approaches towards achieving the goal of surpassing human quality schedules using classical optimization and algorithmic techniques, as well as machine learning and recurrent neural network ( RNN) methods. We then briefly discuss the increased scientific yield and benefit to the wider astrophysics community that would result from the further development and adoption of these technologies.",10.1088/1742-6596/1085/3/032010,,,
Surrogate-assisted Multiobjective Optimization based on Decomposition: A Comprehensive Comparative Analysis,"Berveglieri, N; Derbel, B; Liefooghe, A; Aguirre, H; Tanaka, K",PROCEEDINGS OF THE 2019 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE (GECCO'19),2019.0,"A number of surrogate-assisted evolutionary algorithms are being developed for tackling expensive multiobjective optimization problems. On the one hand, a relatively broad range of techniques from both machine learning and multiobjective optimization can be combined for this purpose. Different taxonomies exist in order to better delimit the design choices, advantages and drawbacks of existing approaches. On the other hand, assessing the relative performance of a given approach is a difficult task, since it depends on the characteristics of the problem at hand. In this paper, we focus on surrogate-assisted approaches using objective space decomposition as a core component. We propose a reined and fine-grained classification, ranging from EGO-like approaches to filtering or pre-screening. More importantly, we provide a comprehensive comparative study of a representative selection of state-of-the-art methods, together with simple baseline algorithms. We rely on selected benchmark functions taken from the bbob-biobj benchmarking test suite, that provides a variable range of objective function difficulties. Our empirical analysis highlights the effect of the available budget on the relative performance of each approach, and the impact of the training set and of the machine learning model construction on both solution quality and runtime efficiency.",10.1145/3321707.3321836,Multiobjective optimization; surrogates; benchmarking,,
ME-MEOA/Dcc : Multiobjective constrained clustering through decomposition-based memetic elitism,"Gonzalez-Almagro, G; Rosales-Perez, A; Luengo, J; Cano, JR; Garcia, S",SWARM AND EVOLUTIONARY COMPUTATION,2021.0,"A B S T R A C T Traditionally unsupervised, clustering techniques have received renewed attention recently, as they have been shown to produce better results when provided with incomplete information about the dataset in the form of constraints. Combining classic clustering and constraints leads to constrained clustering, a semi-supervised learning problem still unexplored in many aspects. Based on the exploration-exploitation requirements of constrained clustering, a memetic elitist multiobjective evolutionary algorithm based on decomposition is proposed, which combines classic multiobjective optimization strategies with single-objective optimization procedures. The application scheme of our proposal for the constrained clustering problem is scrutinized and compared to several state-of-the-art methods for 20 datasets with incremental levels of constraint-based information. Experimental results, supported by Bayesian statistical testing, show a consistent improvement in clustering and multiobjective optimization related measures in favor of our proposal over the state-of-the-art.",10.1016/j.swevo.2021.100939,Multiobjective optimization; Memetic elitism; Constrained clustering; Instance-level constraints; Bayesian statistical testing,,
A survey on automating configuration and parameterization in evolutionary design exploration,"Eichhoff, JR; Roller, D",AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING,2015.0,"Configuration and parameterization of optimization frameworks for the computational support of design exploration can become an exclusive barrier for the adoption of such systems by engineers. This work addresses the problem of defining the elements that constitute a multiple-objective design optimization problem, that is, design variables, constants, objective functions, and constraint functions. In light of this, contributions are reviewed from the field of evolutionary design optimization with respect to their concrete implementation for design exploration. Machine learning and natural language processing are supposed to facilitate feasible approaches to the support of configuration and parameterization. Hence, the authors further review promising machine learning and natural language processing methods for automatic knowledge elicitation and formalization with respect to their implementation for evolutionary design optimization. These methods come from the fields of product attribute extraction, clustering of design solutions, relationship discovery, computation of objective functions, metamodeling, and design pattern extraction.",10.1017/S0890060415000372,Conceptual Design; Design Automation; Design Optimization; Evolutionary Computing; Knowledge Engineering; Machine Learning,,
Shape optimization of segmental porous baffles for enhanced thermo-hydraulic performance of shell-and-tube heat exchanger,"Abbasi, HR; Sedeh, ES; Pourrahmani, H; Mohammadi, MH",APPLIED THERMAL ENGINEERING,2020.0,"This study presents the development and evaluation of a novel shell-and-tube heat exchanger (STHX) design with segmental porous baffles. Computational fluid dynamics (CFD) in combination with machine learning tools are utilized to investigate the thermo-hydraulic impacts of segmental porous baffles on shell side flow of a STHX. Three geometric parameters (number of baffles, baffle angle, and baffle thickness) of these baffles, which are placed inside the STHX, are selected to perform the parametric study and multi-objective optimization. Higher number of baffles are beneficial to increase the rate of heat exchange; however, it would escalate the pressure drop considerably. Results also show that baffles angle plays a critical role on the performance of a STHX. An artificial neural network (ANN) is trained to predict the system's performance. As lowering the pressure drop and increasing the heat transfer are the two main objectives in STHX, a multi-objective optimization study is conducted. Different decision-making algorithms are also applied to find the best alternative among the Pareto frontier points. Results of optimization show that a STHX with 10 porous baffles, baffle angle of 111.9, and baffle thickness of 16.69 mm would be the best geometrical configuration which results in a heat transfer rate of 523.81 kW while the pressure drop of the shell side flow would be 48.87 kPa. With this novel design, it is also possible to improve both pressure drop and heat transfer rate of STHX, simultaneously. A particular configuration of the introduced STHX could reduce the pressure drop by 61.3% while heat transfer enhances by 11.15% simultaneously.",10.1016/j.applthermaleng.2020.115835,Shell and tube heat exchanger (STHX); Heat transfer enhancement; Porous baffles; Genetic algorithm (GA); Artificial neural networks (ANN),,
Construction and Refinement of Preference Ordered Decision Classes,"Dau, HN; Chakhar, S; Ouelhadj, D; Abubahia, AM",ADVANCES IN COMPUTATIONAL INTELLIGENCE SYSTEMS (UKCI 2019),2020.0,"Preference learning methods are commonly used in multicriteria analysis. The working principle of these methods is similar to classical machine learning techniques. A common issue to both machine learning and preference learning methods is the difficulty of the definition of decision classes and the assignment of objects to these classes, especially for large datasets. This paper proposes two procedures permitting to automatize the construction of decision classes. It also proposes two simple refinement procedures, that rely on the 80-20 principle, permitting to map the output of the construction procedures into a manageable set of decision classes. The proposed construction procedures rely on the most elementary preference relation, namely dominance relation, which avoids the need for additional information or distance/(di)similarity functions, as with most of existing clustering methods. Furthermore, the simplicity of the 80-20 principle on which the refinement procedures are based, make them very adequate to large datasets. Proposed procedures are illustrated and validated using real-world datasets.",10.1007/978-3-030-29933-0_21,Clustering; Preference learning; Classification; Classes construction,,
Real data-driven occupant-behavior optimization for reduced energy consumption and improved comfort,"Amasyali, K; El-Gohary, NM",APPLIED ENERGY,2021.0,"A significant amount of energy can be saved through improving occupant behavior. However, implementing energy-saving behavioral changes requires careful consideration based on real-life data to avoid sacrificing comfort. Towards addressing this need, this paper proposes a real data-driven method to assess the potential of occupant-behavior improvements in simultaneously reducing energy consumption and enhancing comfort. The proposed method consists of two main components: (1) machine learning-based occupant-behavior-sensitive models for real data-driven prediction of cooling and lighting energy consumption and thermal and visual occupant comfort; and (2) a genetic algorithm-based optimization model, which uses the machine-learning models to compute the energy consumption and occupant comfort and accordingly optimizes occupant behavior for reduced energy consumption and improved comfort. The proposed method was tested on real data collected from an office building. The experimental results showed potential behavioral energy savings in the range of 11-22%, with a significant improvement in occupant comfort.",10.1016/j.apenergy.2021.117276,Building energy consumption prediction; Occupant behavior; Machine learning; Occupant comfort; Multi-objective optimization,,
PROAFTN Classifier for Feature Selection with Application to Alzheimer Metabolomics Data Analysis,"Belacel, N; Cuperlovic-Culf, M",INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE,2019.0,"Early and accurate Alzheimer's disease (AD) diagnosis remains a challenge. Recently, increasing efforts have been focused towards utilization of metabolomics data for the discovery of biomarkers for screening and diagnosis of AD. Several machine learning approaches were explored for classifying the blood metabolomics profiles of cognitively healthy and AD patients. Differentiation between AD, mild cognitive impairment (MCI) and cognitively healthy subjects remains difficult. In this paper, we propose a new machine learning approach for the selection of a subset of features that provide an improvement in classification rates between these three levels of cognitive disorders. Our experimental results demonstrate that utilization of these selected metabolic markers improves the performance of several classifiers in comparison to the classification accuracy obtained for the complete metabolomics dataset. The obtained results indicate that our algorithms are effective in discovering a panel of biomarkers of AD and MCI from metabolomics data suggesting the possibility to develop a noninvasive blood diagnostic technique of AD and MCI.",10.1142/S0218001419400135,Machine learning; PROAFTN; feature selection; metabolomics; Alzheimer's disease,,
GAN-GL: Generative Adversarial Networks for Glacial Lake Mapping,"Zhao, H; Zhang, MM; Chen, F",REMOTE SENSING,2021.0,"Remote sensing is a powerful tool that provides flexibility and scalability for monitoring and investigating glacial lakes in High Mountain Asia (HMA). However, existing methods for mapping glacial lakes are designed based on a combination of several spectral features and ancillary data (such as the digital elevation model, DEM) to highlight the lake extent and suppress background information. These methods, however, suffer from either the inevitable requirement of post-processing work or the high costs of additional data acquisition. Signifying a key advancement in the deep learning models, a generative adversarial network (GAN) can capture multi-level features and learn the mapping rules in source and target domains using a minimax game between a generator and discriminator. This provides a new and feasible way to conduct large-scale glacial lake mapping. In this work, a complete glacial lake dataset was first created, containing approximately 4600 patches of Landsat-8 OLI images edited in three ways-random cropping, density cropping, and uniform cropping. Then, a GAN model for glacial lake mapping (GAN-GL) was constructed. The GAN-GL consists of two parts-a generator that incorporates a water attention module and an image segmentation module to produce the glacial lake masks, and a discriminator which employs the ResNet-152 backbone to ascertain whether a given pixel belonged to a glacial lake. The model was evaluated using the created glacial lake dataset, delivering a good performance, with an F1 score of 92.17% and IoU of 86.34%. Moreover, compared to the mapping results derived from the global-local iterative segmentation algorithm and random forest for the entire Eastern Himalayas, our proposed model was superior regarding the segmentation of glacial lakes under complex and diverse environmental conditions, in terms of accuracy (precision = 93.19%) and segmentation efficiency. Our model was also very good at detecting small glacial lakes without assistance from ancillary data or human intervention.",10.3390/rs13224728,generative adversarial networks; attention mechanism; glacial lake mapping; Landsat-8 OLI,,
A Novel Approach to Lighten the Onboard Hyperspectral Anomaly Detector,"Ma, N; Peng, Y; Wang, SJ; Dong, JY","WIRELESS AND SATELLITE SYSTEMS, PT II",2019.0,"Hyperspectral image (HSI) anomaly targets detection is always applied for timeliness and onboard mission. For high detection accuracy, deep learning based HSI anomaly detectors (ADs) are widely employed in recent researches. However, their huge network scale for high-level representation ability leads to great computation burden for the onboard computation system. To decrease the computation complexity of the detector, a lightweight network is expected for the HSI AD. In this paper, by creating a multiobjective optimization with nondominated sorting genetic algorithm II (NSGA-II), an automatic evolution based deep learning network HSI AD (Auto-EDL-AD) is proposed to explore a lightweight network. The experimental results on an HSI dataset show that the proposed Auto-EDL-AD can generate an optimal network for the HSI anomaly detection which reaches up to 170% speedup without any detection accuracy loss.",10.1007/978-3-030-19156-6_41,Hyperspectral image; Deep learning; Real-time processing; Multiobjective optimization,,
Application of optimization techniques in metal cutting operations: A bibliometric analysis,"Jamwal, A; Agrawal, R; Sharma, M; Dangayach, GS; Gupta, S",MATERIALS TODAY-PROCEEDINGS,2021.0,"Bibliometric analysis focuses on the statistical analysis of publications published in a particular area. This method is used to classify the information with variables i.e. journals, institutions, authors and countries. This paper present the general overview of research that has been reported in the optimization techniques in various metal cutting operations. Optimization is becoming popular concept in the present time with its most common goal of optimizing the system by smarter use of both products and services. Optimization techniques are very popular in manufacturing industries as it is leads to time-cost savings, waste reduction and increased the quality level with higher customer satisfaction. These days optimization with the help of traditional approaches and machine learning approaches have become popular to achieve the sustainability in the manufacturing practices. The aim of present research work is to investigate the systematic literature review on optimization techniques applications in the cutting processes within the sustainable manufacturing context. This study reports the 20 years of bibliometric analysis of optimization techniques used in the metal cutting operations. The bibliometric analysis is done by using Scopus database with from the time period of 2000-2020. Keyword co-occurrence is found out with the help of network analysis. Top authors, institutes, countries and publication trends in cutting processes are investigated. It is found that majority of machine learning techniques have been applied in milling and turning applications. Optimization with machine learning techniques has enhanced the research area of metal cutting in last five years. Emerging economies like India and China are more focused towards the adoption of new optimization techniques in the machining area. (C) 2020 Elsevier Ltd. All rights reserved.",10.1016/j.matpr.2020.07.425,Cutting processes; Machine learning; Optimization; Bibliometric analysis; Sustainable manufacturing,,
Applying Reinforcement Learning to Plan Manufacturing Material Handling Part 2: Experimentation and Results,"Govindaiah, S; Petty, MD",PROCEEDINGS OF THE 2019 ANNUAL ACM SOUTHEAST CONFERENCE (ACMSE 2019),2019.0,"Applying machine learning to improve the efficiency of complex manufacturing processes, particularly logistics and material handling, can be a challenging problem. The interconnectedness of the multiple components that compose such processes and the typically large number of variables required to specify procedures and plans within those processes combine to make it very difficult to map the details of real-world manufacturing processes to an abstract mathematical representation suitable for machine learning methods. In this paper, we report on the application of machine learning methods, in particular reinforcement learning, to generate increasingly efficient plans for material handling to satisfy temporally varying product demands in a representative manufacturing facility. The essential steps in the research included defining a formal representation of a realistically complex material handling plan, defining a set of suitable two-stage plan change operators as reinforcement learning actions, implementing a simulation-based multi-objective reward function that considers multiple components of material handling costs, and abstracting the many possible material handling plans into a state set small enough to enable reinforcement learning. Extensive experimentation with multiple starting plans showed that the reinforcement learning process could consistently reduce the material handling plans' costs over time. This work may be one of the first applications of reinforcement learning with a multiobjective reward function to a realistically complex material handling process. This paper first provides an explanation of how the material handling plans and rewards were abstracted into a manageable state set. It then details the various initial plans and experimental trials used to test the plans. Finally, it reports the results of those experimental trials, including the plan change policies learned and the reductions in material handling costs achieved.",10.1145/3299815.3314427,Material handling; machine learning; reinforcement learning; planning; multi-objective learning,,
A MBCRF Algorithm Based on Ensemble Learning for Building Demand Response Considering the Thermal Comfort,"Li, YC; Han, YH; Wang, JK; Zhao, Q",ENERGIES,2018.0,"Demand response (DR) has become an effective and critical method for obtaining better savings on energy consumption and cost. Buildings are the potential demand response resource since they contribute nearly 50% of the electricity usage. Currently, more DR applications for buildings were rule-based or utilized a simplified physical model. These methods may not fully embody the interaction among various features in the building. Based on the tree model, this paper presents a novel model based control with a random forest (MBCRF) learning algorithm for the demand response of commercial buildings. The baseline load of demand response and optimal control strategies are solved to respond to the DR request signals during peak load periods. Energy cost saving of the building is achieved and occupant's thermal comfort is guaranteed simultaneously. A linguistic if-then rules-based optimal feature selection framework is also utilized to redefine the training and test set. Numerical testing results of the Pennsylvania-Jersey-Maryland (PJM) electricity market and Research and Support Facility (RSF) building show that the load forecasting error is as low as 1.28%. The peak load reduction is up to 40 kW, which achieves a 15% curtailment and outperforms rule-based DR by 5.6%.",10.3390/en11123495,demand response; load curtailment; ensemble learning; tree-based model method,,
A robust multiobjective Harris' Hawks Optimization algorithm for the binary classification problem,"Dokeroglu, T; Deniz, A; Kiziloz, HE",KNOWLEDGE-BASED SYSTEMS,2021.0,"The Harris' Hawks Optimization (HHO) is a recent metaheuristic inspired by the cooperative behavior of the hawks. These avians apply many intelligent techniques like surprise pounce (seven kills) while they are catching their prey according to the escaping patterns of the target. The HHO simulates these hunting patterns of the hawks to obtain the best/optimal solutions to the problems. In this study, we propose a new multiobjective HHO algorithm for the solution of the well-known binary classification problem. In this multiobjective problem, we reduce the number of selected features and try to keep the accuracy prediction as maximum as possible at the same time. We propose new discrete exploration (perching) and exploitation (besiege) operators for the hunting patterns of the hawks. We calculate the prediction accuracy of the selected features with four machine learning techniques, namely, Logistic Regression, Support Vector Machines, Extreme Learning Machines, and Decision Trees. To verify the performance of the proposed algorithm, we conduct comprehensive experiments on many benchmark datasets retrieved from the University of California, Irvine (UCI) Machine Learning Repository. Moreover, we apply it to a recent real-world dataset, i.e., a Coronavirus disease (COVID-19) dataset. Significant improvements are observed during the comparisons with state-of-the-art metaheuristic algorithms. (C) 2021 Elsevier B.V. All rights reserved.",10.1016/j.knosys.2021.107219,Binary classification; Multiobjective optimization; Feature selection; Harris' Hawks optimization,,
Modeling groundwater potential using novel GIS-based machine-learning ensemble techniques,"Arabameri, A; Pal, SC; Rezaie, F; Nalivan, OA; Chowdhuri, I; Saha, A; Lee, S; Moayedi, H",JOURNAL OF HYDROLOGY-REGIONAL STUDIES,2021.0,"Study region: The present study has been carried out in the Tabriz River basin (5397 km(2)) in northwestern Iran. Elevations vary from 1274 to 3678 m above sea level, and slope angles range from 0 to 150.9 %. The average annual minimum and maximum temperatures are 2 degrees C and 12 degrees C, respectively. The average annual rainfall ranges from 243 to 641 mm, and the northern and southern parts of the basin receive the highest amounts. Study focus: In this study, we mapped the groundwater potential (GWP) with a new hybrid model combining random subspace (RS) with the multilayer perception (MLP), naive Bayes tree (NBTree), and classification and regression tree (CART) algorithms. A total of 205 spring locations were collected by integrating field surveys with data from Iran Water Resources Management, and divided into 70:30 for training and validation. Fourteen groundwater conditioning factors (GWCFs) were used as independent model inputs. Statistics such as receiver operating characteristic (ROC) and five others were used to evaluate the performance of the models. New hydrological insights for the region: The results show that all models performed well for GWP mapping (AUC > 0.8). The hybrid MLP-RS model achieved high validation scores (AUC = 0.935). The relative importance of GWCFs was revealed that slope, elevation, TRI and HAND are the most important predictors of groundwater presence. This study demonstrates that hybrid ensemble models can support sustainable management of groundwater resources.",10.1016/j.ejrh.2021.100848,Groundwater; Machine learning; Random subspace; Ensemble models; RS-GIS; Iran,,
Ordinal classification based on the sequential covering strategy,"Gamez, JC; Garcia, D; Gonzalez, A; Perez, R",INTERNATIONAL JOURNAL OF APPROXIMATE REASONING,2016.0,"Ordinal classification is a supervised learning problem. The distinctive feature of ordinal classification is that there is an order relationship among the categories to learn. In this paper, we present a fuzzy rule learning algorithm based on the sequential covering strategy applied to ordinal classification. This proposal modifies a nominal classification algorithm, called NSLV, to adapt it to this kind of problems. To take into account the order relationship among the categories, a new fitness function and a new concept of negative examples for a rule are proposed. Moreover, we introduce a new rule evaluation model for ordinal classification problems. Experimental results show that the proposed algorithm offers a better performance compared to other ordinal algorithms. (C) 2016 Elsevier Inc. All rights reserved.",10.1016/j.ijar.2016.05.002,Ordinal classification; Sequential covering strategy; Genetic algorithms; Fuzzy rules; NSLV; Supervised learning,,
Deep Learning-Based High-Frequency Ultrasound Skin Image Classification with Multicriteria Model Evaluation,"Czajkowska, J; Badura, P; Korzekwa, S; Platkowska-Szczerek, A; Slowinska, M",SENSORS,2021.0,"This study presents the first application of convolutional neural networks to high-frequency ultrasound skin image classification. This type of imaging opens up new opportunities in dermatology, showing inflammatory diseases such as atopic dermatitis, psoriasis, or skin lesions. We collected a database of 631 images with healthy skin and different skin pathologies to train and assess all stages of the methodology. The proposed framework starts with the segmentation of the epidermal layer using a DeepLab v3+ model with a pre-trained Xception backbone. We employ transfer learning to train the segmentation model for two purposes: to extract the region of interest for classification and to prepare the skin layer map for classification confidence estimation. For classification, we train five models in different input data modes and data augmentation setups. We also introduce a classification confidence level to evaluate the deep model's reliability. The measure combines our skin layer map with the heatmap produced by the Grad-CAM technique designed to indicate image regions used by the deep model to make a classification decision. Moreover, we propose a multicriteria model evaluation measure to select the optimal model in terms of classification accuracy, confidence, and test dataset size. The experiments described in the paper show that the DenseNet-201 model fed with the extracted region of interest produces the most reliable and accurate results.",10.3390/s21175846,high-frequency ultrasound; inflammatory skin diseases; skin lesions; image classification; deep learning; transfer learning; Grad-CAM,,
The influence of scaling metabolomics data on model classification accuracy,"Gromski, PS; Xu, Y; Hollywood, KA; Turner, ML; Goodacre, R",METABOLOMICS,2015.0,"Correctly measured classification accuracy is an important aspect not only to classify pre-designated classes such as disease versus control properly, but also to ensure that the biological question can be answered competently. We recognised that there has been minimal investigation of pre-treatment methods and its influence on classification accuracy within the metabolomics literature. The standard approach to pre-treatment prior to classification modelling often incorporates the use of methods such as autoscaling, which positions all variables on a comparable scale thus allowing one to achieve separation of two or more groups (target classes). This is often undertaken without any prior investigation into the influence of the pre-treatment method on the data and supervised learning techniques employed. Whilst this is useful for deriving essential information such as predictive ability or visual interpretation in many cases, as shown in this study the standard approach is not always the most suitable option available. Here, a study has been conducted to investigate the influence of six pre-treatment methods-autoscaling, range, level, Pareto and vast scaling, as well as no scaling-on four classification models, including: principal components-discriminant function analysis (PC-DFA), support vector machines (SVM), random forests (RF) and k-nearest neighbours (kNN)-using three publically available metabolomics data sets. We have demonstrated that undertaking different pre-treatment methods can greatly affect the interpretation of the statistical modelling outputs. The results have shown that data pre-treatment is context dependent and that there was no single superior method for all the data sets used. Whilst we did find that vast scaling produced the most robust models in terms of classification rate for PC-DFA of both NMR spectroscopy data sets, in general we conclude that both vast scaling and autoscaling produced similar and superior results in comparison to the other four pre-treatment methods on both NMR and GC-MS data sets. It is therefore our recommendation that vast scaling is the primary pre-treatment method to use as this method appears to be more stable and robust across all the different classifiers that were conducted in this study.",10.1007/s11306-014-0738-7,Pre-treatment; Metabolomics; Classification accuracy; Scaling; Chemometrics,,
NASCaps: A Framework for Neural Architecture Search to Optimize the Accuracy and Hardware Efficiency of Convolutional Capsule Networks,"Marchisio, A; Massa, A; Mrazek, V; Bussolino, B; Martina, M; Shafique, M",2020 IEEE/ACM INTERNATIONAL CONFERENCE ON COMPUTER AIDED-DESIGN (ICCAD),2020.0,"Deep Neural Networks (DNNs) have made significant improvements to reach the desired accuracy to be employed in a wide variety of Machine Learning (ML) applications. Recently the Google Brain's team demonstrated the ability of Capsule Networks (CapsNets) to encode and learn spatial correlations between different input features, thereby obtaining superior learning capabilities compared to traditional (i.e., non-capsule based) DNNs. However, designing CapsNets using conventional methods is a tedious job and incurs significant training effort. Recent studies have shown that powerful methods to automatically select the best/optimal DNN model configuration for a given set of applications and a training dataset are based on the Neural Architecture Search (NAS) algorithms. Moreover, due to their extreme computational and memory requirements, DNNs are employed using the specialized hardware accelerators in IoT-Edge/CPS devices. In this paper, we propose NASCaps, an automated framework for the hardware-aware NAS of different types of DNNs, covering both traditional convolutional DNNs and CapsNets. We study the efficacy of deploying a multi-objective Genetic Algorithm (e.g., based on the NSGA-II algorithm). The proposed framework can jointly optimize the network accuracy and the corresponding hardware efficiency, expressed in terms of energy, memory, and latency of a given hardware accelerator executing the DNN inference. Besides supporting the traditional DNN layers (such as, convolutional and fully-connected), our framework is the first to model and supports the specialized capsule layers and dynamic routing in the NAS-flow. We evaluate our framework on different datasets, generating different network configurations, and demonstrate the tradeoffs between the different output metrics. We will open-source the complete framework and configurations of the Pareto-optimal architectures at https://github.com/ehw-fit/nascaps.",10.1145/3400302.3415731,Deep Neural Networks; DNNs; Capsule Networks; Evolutionary Algorithms; Genetic Algorithms; Neural Architecture Search; Hardware Accelerators; Accuracy; Energy Efficiency; Memory; Latency; Design Space; Multi-Objective; Optimization,,
New adaptive intelligent grey wolf optimizer based multi-objective quantitative classification rules mining approaches,"Yildirim, G; Alatas, B",JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING,2021.0,"The classification rule mining problem is one of the most important tasks of data mining. A constructed classification model is needed to have high accuracy, comprehensiveness, interestingness, etc. Furthermore, when the data composed of quantitative, numerical, or mixed data types, automatically discovering the appropriate intervals at the time of the mining process is a hard problem. The standard classification algorithms in the literature do not find the intervals of the quantitative attributes in the rules and this is performed a priori that causes the modification of datasets. Automatically constructing a successful classification model consisting of an explainable rule set without changing or modifying the data in artificial intelligence and machine learning is a very hot topic. Due to the philosophy of constantly researching to discover more accurate, surprising, and comprehensible rule sets and the absence of the most effective algorithm for all kinds of data sets, new methods or new versions of existing methods are proposed. In this study, automatic mining of high-quality rule set is handled as a multi-objective optimization problem due to the nature of the necessities and novel adaptive multi-objective intelligent search and optimization algorithms based on Grey Wolf Optimizer are proposed for this task. The datasets are considered as search spaces and the proposed adaptive Pareto based multi-objective Grey Wolf Optimizer algorithms are designed and applied as search methods for automatically discovering the high-quality rules. The proposed intelligent methods and successful classification algorithms such as naive Bayes, k-NN, support vector machines, decision trees, and RIPPER are tested in five real-world data sets with different characteristics. The obtained results show the efficiency of the proposed intelligent search and optimization methods.",10.1007/s12652-020-02701-9,Rule mining; Multi-objective optimization; Grey wolf optimizer; Classification,,
An automatic clustering method using multi-objective genetic algorithm with gene rearrangement and cluster merging,"Qu, HC; Yin, L; Tang, XM",APPLIED SOFT COMPUTING,2021.0,"As an unsupervised approach of machine learning, clustering is an important method to understand and learn structural information from data. However, current adaptive clustering approach based on multi-objective genetic algorithm have two apparent limitations. The first is that prior knowledge, i.e., sample information is needed to get the correct cluster number. The second is that no effective method can be found to select the best clustering solution from the Pareto Optimal Front (POF) generated by a multi-objective optimization. These problems become severer in applications applied on non-category datasets. Therefore, the primary goal of this research is to establish a genetic optimization based multi-objective clustering framework, in which multiple clustering validity indexes (CVIs) can be tested simultaneously to automatically obtain the optimal cluster number without knowing any sample label information in advance. In this effort, we will not only be able to consider clustering measurements such as cluster cohesion and separation, but also take other aspects, such as compactness, connectivity, variation among data elements, into consideration as well. Then, we aim to design a procedure to recommend three best solutions from the POF by using appropriate combination of CVIs without increasing computational cost. This procedure is expected to control the cluster number in a reasonable range and consequently decrease the difficulty in best solution recommendation. Finally, since we have the knowledge that using gene rearrangement in the genetic optimization does not affect partition, we take this advantage to merge clusters effectively and significantly speed the convergence of the algorithm. Our approach can outperform the state-of-the-art counterparts across diverse benchmark datasets in terms of partitioning accuracy and performance, as demonstrated in three experiments conducted on both artificial and typical real-world datasets. (C) 2020 Elsevier B.V. All rights reserved.",10.1016/j.asoc.2020.106929,Automatic clustering; Genetic algorithm; Multi-objective optimization; Gene rearrangement,,
Alternative Approach for Learning and Improving the MCDA Method PROAFTN,"Al-Obeidat, F; Belacel, N",INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS,2011.0,"The objectives of this paper are (1) to propose new techniques to learn and improve the multicriteria decision analysis (MCDA) method PROAFTN based on machine learning approaches and (2) to compare the performance of the developed methods with other well-known machine learning classification algorithms. The proposed learning methods consist of two stages: The first stage involves using the discretization techniques to obtain the required parameters for the PROAFTN method, and the second stage is the development of a new inductive approach to construct PROAFTN prototypes for classification. The comparative study is based on the generated classification accuracy of the algorithms on the data sets. For further robust analysis of the experiments, we used the Friedman statistical measure with the corresponding post hoc tests. The proposed approaches significantly improved the performance of the classification method PROAFTN. Based on the generated results on the same data sets, PROAFTN outperforms widely used classification algorithms. Furthermore, the method is simple, no preprocessing is required, and no loss of information during learning. (C) 2011 Wiley Periodicals, Inc.",10.1002/int.20476,,,
Multicriteria Evaluation of Deep Neural Networks for Semantic Segmentation of Mammographies,"Rubio, Y; Montiel, O",AXIOMS,2021.0,"Breast segmentation plays a vital role in the automatic analysis of mammograms. Accurate segmentation of the breast region increments the probability of a correct diagnostic and minimizes computational cost. Traditionally, model-based approaches dominated the landscape for breast segmentation, but recent studies seem to benefit from using robust deep learning models for this task. In this work, we present an extensive evaluation of deep learning architectures for semantic segmentation of mammograms, including segmentation metrics, memory requirements, and average inference time. We used several combinations of two-stage segmentation architectures composed of a feature extraction net (VGG16 and ResNet50) and a segmentation net (FCN-8, U-Net, and PSPNet). The training examples were taken from the mini Mammographic Image Analysis Society (MIAS) database. Experimental results using the mini-MIAS database show that the best net scored a Dice similarity coefficient of 99.37% for breast boundary segmentation and 95.45% for pectoral muscle segmentation.",10.3390/axioms10030180,breast segmentation; mammogram; deep learning; semantic segmentation,,
A siamese pedestrian alignment network for person re-identification,"Zheng, Y; Zhou, Y; Zhao, JQ; Jian, M; Yao, R; Liu, B; Chen, Y",MULTIMEDIA TOOLS AND APPLICATIONS,2021.0,"Deep learning methods show strong ability in extracting high-level features for images in the field of person re-identification. The produced features help inherently distinguish pedestrian identities in images. However, on deep learning models over-fitting and discriminative ability of the learnt features are still challenges for person re-identification. To alleviate model over-fitting and further enhance the discriminative ability of the learnt features, we propose siamese pedestrian alignment networks (SPAN) for person re-identification. SPAN employs two streams of PAN (pedestrian alignment networks) to increase the size of network inputs over limited training samples and effectively alleviate network over-fitting in learning. In addition, a verification loss is constructed between the two PANs to adjust the relative distance of two input pedestrians of the same or different identities in the learned feature space. Experimental verification is conducted on six large person re-identification data sets and the experimental results demonstrate the effectiveness of the proposed SPAN for person re-identification.",10.1007/s11042-021-11302-3,Person re-identification; Deep learning; Neural network; Verification loss; Feature learning,,
Soft Sensor Development for Nonlinear Industrial Processes Based on Ensemble Just-in-Time Extreme Learning Machine through Triple-Modal Perturbation and Evolutionary Multiobjective Optimization,"Pan, B; Jin, HP; Yang, B; Qian, B; Zhao, ZG",INDUSTRIAL & ENGINEERING CHEMISTRY RESEARCH,2019.0,"Just-in-time (JIT) learning has been widely used for data-driven soft sensor modeling. However, traditional JIT soft sensors do not always function well when applied to complex industrial processes because they are only equipped with a single learning configuration. Therefore, a novel ensemble JIT (EJIT) learning-based soft sensor, referred to as triple-modal perturbation (TP)-based EJIT extreme learning machine (TP-EJITELM), is proposed. In the method, a set of diverse and accurate base JITELM models are generated by using heterogeneous similarity measures and optimizing the model structure and input variables through an evolutionary multiobjective optimization approach. Then, a selective ensemble learning strategy is used to integrate the base models. Compared with traditional JIT soft sensors, TP-EJITELM can significantly improve prediction performance because of the complementary advantages of heterogeneous similarity measures and a good tradeoff between model complexity and accuracy. The effectiveness of the proposed method is demonstrated through two industrial applications.",10.1021/acs.iecr.9b03702,,,
Effects of hidden layer sizing on CNN fine-tuning,"Marrone, S; Papa, C; Sansone, C",FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE,2021.0,"Some applications have the property of being resilient, meaning that they are robust to noise (e.g. due to error) in the data. This characteristic is very useful in situations where an approximate computation allows to perform the task in less time or to deploy the algorithm on embedded hardware. Deep learning is one of the fields that can benefit from approximate computing to reduce the high number of involved parameters thanks to its impressive generalization ability. A common approach is to prune some neurons and perform an iterative re-training with the aim of both reducing the required memory and to speed-up the inference stage. In this work we propose to face CNN size reduction from a different perspective: instead of reducing the network weights or look for an approximated network very close to the Pareto frontier, we investigate whether it is possible to remove some neurons only from the fully connected layers before the network training without substantially affecting the network performance. As a case study, we will focus on fine-tuning'', a branch of transfer learning that has shown its effectiveness especially in domains lacking effective expert-designed features. To further compact the network, we apply weight quantization to the convolutional kernels. Results show that it is possible to tailor some layers to reduce the network size, both in terms of the number of parameters to learn and required memory, without statistically affecting the performance and without the need for any additional training. Finally, we investigate to what extent the sizing operation affects the network robustness against adversarial perturbations, a set of approaches aimed at misleading deep neural networks. (C) 2020 Elsevier B.V. All rights reserved.",10.1016/j.future.2020.12.020,CNN; Adversarial perturbation; Fine-tuning; Approximate computing,,
Machine learning and optimization models for supplier selection and order allocation planning,"Islam, S; Amin, SH; Wardley, LJ",INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,2021.0,"Supplier selection and order allocation have significant roles in supply chain management. These processes become major challenges when the demand is uncertain. This research presents a new two-stage solution approach for supplier selection and order allocation planning where a forecasting procedure is integrated with an optimization model. In the first stage, the demand is forecasted to handle the demand vagueness. A novel Relational Regressor Chain method is introduced to determine the future demand, which is compared with the Holt's Linear Trend and the Auto-Regressive Integrated Moving Average methods to ensure the forecasting accuracy. The forecasted demand is then fed to the second stage where a multi-objective programming model is developed to identify suitable suppliers and order quantities from each supplier. Weighted-sum and epsilon-constraint methods are utilized to obtain the efficient solutions. To our knowledge, this paper is the first study that has integrated demand forecasting with the supplier selection and order allocation planning. A real dataset from a Canadian food supply network is used to examine the results of the forecasting methods and to determine the orders allocated to each supplier. The results of the forecasting methods show that the proposed Relational Regressor Chain method can forecast demand with a higher precision than the other forecasting methods considered in this paper. It is also evident from the results that the selection of the forecasting methods may have impact on both the selection of suppliers and the orders allocated to them.",10.1016/j.ijpe.2021.108315,Supplier selection; Order allocation; Optimization; Multi-objective; Machine learning,,
Discovering high-performance broadband and broad angle antireflection surfaces by machine learning,"Haghanifar, S; McCourt, M; Cheng, BL; Wuenschell, J; Ohodnicki, P; Leu, PW",OPTICA,2020.0,"Eliminating light reflection from the top glass sheet in optoelectronic applications is often desirable across a broad range of wavelengths and large variety of angles. In this paper, we report on a combined simulation and experimental study of single-layer films, nanowire arrays, and nanocone arrays to meet these antireflection (AR) needs. We demonstrate the application of Bayesian learning to the multiobjective optimization of these structures for broadband and broad angle AR and show the superior performance of Bayesian learning to genetic algorithms for optimization. Our simulations indicate that nanocone structures have the best AR performance of these three structures, and we additionally provide physical insight into the AR performance of different structures. Simulations suggest nanocone arrays are able to achieve a solar integrated normal and 65 degrees incidence angle reflection of 0.15% and 1.25%, respectively. A simple and scalable maskless reactive ion etching process is used to create nanocone structures, and etched samples demonstrate a solar integrated normal and 65 degrees reflection of 0.4% and 4.9%, respectively, at the front interface. (C) 2020 Optical Society of America under the terms of the OSA Open Access Publishing Agreement",10.1364/OPTICA.387938,,,
SEVUCAS: A Novel GIS-Based Machine Learning Software for Seismic Vulnerability Assessment,"Lee, S; Panahi, M; Pourghasemi, HR; Shahabi, H; Alizadeh, M; Shirzadi, A; Khosravi, K; Melesse, AM; Yekrangnia, M; Rezaie, F; Moeini, H; Pham, BT; Bin Ahmad, B",APPLIED SCIENCES-BASEL,2019.0,"Since it is not possible to determine the exact time of a natural disaster's occurrence and the amount of physical and financial damage on humans or the environment resulting from their event, decision-makers need to identify areas with potential vulnerability in order to reduce future losses. In this paper, a GIS-based open source software entitled Seismic-Related Vulnerability Calculation Software (SEVUCAS), based on the Step-wise Weight Assessment Ratio Analysis (SWARA) method and geographic information system, has been developed to assess seismic vulnerability by considering four groups of criteria (i.e., geotechnical, structural, socio-economic, and physical distance to needed facilities and away from dangerous facilities). The software was developed in C# language using ArcGIS Engine functions, which provide enhanced visualization as well as user-friendly and automatic software for the seismic vulnerability assessment of buildings. Weighting of the criteria (indicators) and alternatives (sub-indicators) was done using SWARA. Also, two interpolation methods based on a radial basis function (RBF) and teaching-learning-based optimization (TLBO) were used to optimize the weights of the criteria and the classes of each alternative as well. After weighing the criteria and alternatives, the weighted overlay analysis was used to determine the final vulnerability map in the form of contours and statistical data. The difference between this software and similar ones is that people with a low level of knowledge in the area of earthquake crisis management can use it to determine and estimate the seismic vulnerabilities of their houses. This visualized operational forecasting software provides an applicable tool for both government and people to make quick and correct decisions to determine higher priority structures for seismic retrofitting implementation.",10.3390/app9173495,GIS; SEVUCAS; seismic vulnerability assessment; SWARA; RBF; TLBO; Tehran; seismic retrofitting,,
A machine learning-based surrogate model to approximate optimal building retrofit solutions,"Thrampoulidis, E; Mavromatidis, G; Lucchi, A; Orehounig, K",APPLIED ENERGY,2021.0,"The building sector has the highest share of operational energy consumption and greenhouse gas emissions among all sectors. Environmental targets set by many countries impose the need to improve the environmental footprint of the existing building stock. Building retrofit is considered one of the most promising solutions to-wards this direction. In this paper, a surrogate model for evaluating the necessary building envelope and energy system measures for building retrofit is presented. Artificial neural networks are exploited to build up this model in order to provide a good balance between accuracy and computational cost. The proposed model is trained and tested for the case study of the city of Zurich, in Switzerland, and is compared with one of the most advanced models for building retrofit that uses building simulation and optimization tools. The surrogate model operates on a smaller input set and the time required to derive retrofit solutions is reduced from 3.5 min to 16.4 mu sec. Results show that the proposed model can provide significantly reduced computational cost without compromising accuracy for most of the retrofit dimensions. For instance, the retrofit costs and the energy system selections are approximated with an average accuracy of R-2 = 0.9408 and f 1 score = 0.9450, respectively. Finally, yet importantly, such surrogate retrofit models may effectively be used for bottom-up retrofit analyses for wide areas and can contribute towards accelerating the adoption of retrofit measures.",10.1016/j.apenergy.2020.116024,Building retrofit; Energy efficiency; Surrogate model; Machine learning; Multi-objective optimization; Pareto-optimal,,
Using Machine Learning to Predict Retrofit Effects for a Commercial Building Portfolio,"Xu, YJ; Loftness, V; Severnini, E",ENERGIES,2021.0,"Buildings account for 40% of the energy consumption and 31% of the CO2 emissions in the United States. Energy retrofits of existing buildings provide an effective means to reduce building consumption and carbon footprints. A key step in retrofit planning is to predict the effect of various potential retrofits on energy consumption. Decision-makers currently look to simulation-based tools for detailed assessments of a large range of retrofit options. However, simulations often require detailed building characteristic inputs, high expertise, and extensive computational power, presenting challenges for considering portfolios of buildings or evaluating large-scale policy proposals. Data-driven methods offer an alternative approach to retrofit analysis that could be more easily applied to portfolio-wide retrofit plans. However, current applications focus heavily on evaluating past retrofits, providing little decision support for future retrofits. This paper uses data from a portfolio of 550 federal buildings and demonstrates a data-driven approach to generalizing the heterogeneous treatment effect of past retrofits to predict future savings potential for assisting retrofit planning. The main findings include the following: (1) There is high variation in the predicted savings across retrofitted buildings, (2) GSALink, a dashboard tool and fault detection system, commissioning, and HVAC investments had the highest average savings among the six actions analyzed; and (3) by targeting high savers, there is a 110-300 billion Btu improvement potential for the portfolio in site energy savings (the equivalent of 12-32% of the portfolio-total site energy consumption).",10.3390/en14144334,building energy retrofits; energy savings evaluation; data-driven energy analysis; causal forest; heterogeneous treatment effect,,
Optimization of Microjet Location Using Surrogate Model Coupled with Particle Swarm Optimization Algorithm,"Qidwai, MO; Badruddin, IA; Khan, NZ; Khan, MA; Alshahrani, S",MATHEMATICS,2021.0,"This study aimed to present the design methodology of microjet heat sinks with unequal jet spacing, using a machine learning technique which alleviates hot spots in heat sinks with non-uniform heat flux conditions. Latin hypercube sampling was used to obtain 30 design sample points on which three-dimensional Computational Fluid Dynamics (CFD) solutions were calculated, which were used to train the machine learning model. Radial Basis Neural Network (RBNN) was used as a surrogate model coupled with Particle Swarm Optimization (PSO) to obtain the optimized location of jets. The RBNN provides continuous space for searching the optimum values. At the predicted optimum values from the coupled model, the CFD solution was calculated for comparison. The percentage error for the target function was 0.56%, whereas for the accompanied function it was 1.3%. The coupled algorithm has variable inputs at user discretion, including gaussian spread, number of search particles, and number of iterations. The sensitivity of each variable was obtained. Analysis of Variance (ANOVA) was performed to investigate the effect of the input variable on thermal resistance. ANOVA results revealed that gaussian spread is the dominant variable affecting the thermal resistance.",10.3390/math9172167,optimization; surrogate model; Radial Basis Neural Network; Particle Swarm Optimization; thermal resistance; Computational Fluid Dynamics,,
Player profiling and quality assessment of dynamic car racing tracks using entertainment quantifier technique,"Javed, S; Zafar, K",COMPUTATIONAL INTELLIGENCE,2018.0,"Interactive games have been an interesting area of research and have many challenges. With the advancement in technology, games have been revolutionizing at each step per the emerging and variant interests of players. Recently, machine learning techniques are used for the generation of game content based on player's experience. The dynamic content generation in computer games based on player's experience and feedback is still a challenging task. This requires measurement of entertainment factor achieved by a player during a game. In order to measure entertainment factor, we need to incorporate human-computer interaction by evolution of game content with respect to player's response. Optimization techniques can be used for the measurement of entertainment factor and for the generation of dynamic game content. The use of computational intelligence techniques in game development can lead to a new domain called computational intelligence in games. This research is focused on car racing game genre, and the paradigm selected for dynamicity is track generation of car racing game. It requires player profiling and classification of players. The optimization of track generation has been performed by using single and multiobjective genetic algorithm and particle swarm optimization. Initially, classification of player's rank based on data and theory-driven approaches has been performed. Moreover, 3 different techniques of defining ranges or boundaries of race parameters for player's rank classification are studied. The techniques are based on crisp values, neural network, and fuzzy inference process. Then, an entertainment quantifier technique is proposed for a player after playing a certain number of games based on dynamic content generation using multiobjective genetic algorithm using standard Pareto optimal front and an epsilon (epsilon) front. In conclusion, the method proposed for quantifying entertainment can be used to analyze and classify the trend in interests of a player according to which the game itself can dynamically generate. This will keep the interest of player intact and provides maximum entertainment experience per the interest of an individual. The proposed solution can easily be used in generation of any game content and can effectively be used in accurate measurement of entertaining factor of any game.",10.1111/coin.12161,computational intelligence; data-driven approach; genetic algorithm; measuring entertainment; Pareto optimal front; particle swarm optimization,,
Grey-Based Taguchi Multiobjective Optimization and Artificial Intelligence-Based Prediction of Dissimilar Gas Metal Arc Welding Process Performance,"Devaraj, J; Ziout, A; Abu Qudeiri, JE",METALS,2021.0,"The quality of a welded joint is determined by key attributes such as dilution and the weld bead geometry. Achieving optimal values associated with the above-mentioned attributes of welding is a challenging task. Selecting an appropriate method to derive the parameter optimality is the key focus of this paper. This study analyzes several versatile parametric optimization and prediction models as well as uses statistical and machine learning models for further processing. Statistical methods like grey-based Taguchi optimization is used to optimize the input parameters such as welding current, wire feed rate, welding speed, and contact tip to work distance (CTWD). Advanced features of artificial neural network (ANN) and adaptive neuro-fuzzy interface system (ANFIS) models are used to predict the values of dilution and the bead geometry obtained during the welding process. The results corresponding to the initial design of the welding process are used as training and testing data for ANN and ANFIS models. The proposed methodology is validated with various experimental results outside as well as inside the initial design. From the observations, the prediction results produced by machine learning models delivered significantly high relevance with the experimental data over the regression analysis.",10.3390/met11111858,dissimilar metal welding; gas metal arc welding; grey-based Taguchi optimization; artificial neural network (ANN); adaptive neuro-fuzzy inference system (ANFIS),,
An innovative ensemble learning air pollution early-warning system for China based on incremental extreme learning machine,"Du, ZJ; Heng, JN; Niu, MF; Sun, SL",ATMOSPHERIC POLLUTION RESEARCH,2021.0,"Air pollution has lots of adverse effects on industrial production and public life. Thus, it is an urgent task to construct an efficient air quality early-warning system to guide public life and production. This paper proposes an innovative air pollution early-warning system, including four main modules: clustering, preprocessing, forecasting and evaluation. In the clustering module, with the aim of building an efficient air pollution warning system, the air pollution situation of 31 provincial capitals is clustered and the study areas of the current study are selected based on the clustering result. A new data preprocessing algorithm is conducted to excavate the potential characteristics of the raw time series in the first place in the preprocessing module. Then, the lengthchangeable incremental extreme learning machine is used to forecast each component. In the evaluation module, the air quality is qualitatively analyzed by the fuzzy evaluation method. Moreover, the DM test and the SPA test are employed to test the accuracy of the forecasting model. The experimental results of eighteen data sets from three cities show that the hybrid air quality early-warning system establish in the study not only has higher accuracy and generalization ability than other benchmark models, but can provide sufficient air quality information, which is essential to control air pollution.",10.1016/j.apr.2021.101153,Air quality early-warning system; Length-changeable incremental extreme; learning machine; Hybrid ensemble model; Fuzzy evaluation,,
Integration Multi-Model to Evaluate the Impact of Surface Water Quality on City Sustainability: A Case from Maanshan City in China,"Chen, ZB; Zhang, H; Liao, MX",PROCESSES,2019.0,"Water pollution is a worldwide problem that needs to be solved urgently and has a significant impact on the efficiency of sustainable cities. The evaluation of water pollution is a Multiple Criteria Decision-Making (MCDM) problem and using a MCDM model can help control water pollution and protect human health. However, different evaluation methods may obtain different results. How to effectively coordinate them to obtain a consensus result is the main aim of this work. The purpose of this article is to develop an ensemble learning evaluation method based on the concept of water quality to help policy-makers better evaluate surface water quality. A valid application is conducted to illustrate the use of the model for the surface water quality evaluation problem, thus demonstrating the effectiveness and feasibility of the proposed model.",10.3390/pr7010025,water pollution; multi-model evaluation; MCDM; ensemble learning,,
A methodology for energy multivariate time series forecasting in smart buildings based on feature selection,"Gonzalez-Vidal, A; Jimenez, F; Gomez-Skarmeta, AF",ENERGY AND BUILDINGS,2019.0,"The massive collection of data via emerging technologies like the Internet of Things (IoT) requires finding optimal ways to reduce the created features that have a potential impact on the information that can be extracted through the machine learning process. The mining of knowledge related to a concept is done on the basis of the features of data. The process of finding the best combination of features is called feature selection. In this paper we deal with multivariate time-dependent series of data points for energy forecasting in smart buildings. We propose a methodology to transform the time-dependent database into a structure that standard machine learning algorithms can process, and then, apply different types of feature selection methods for regression tasks. We used Weka for the tasks of database transformation, feature selection, regression, statistical test and forecasting. The proposed methodology improves MAE by 59.97% and RMSE by 40.75%, evaluated on training data, and it improves MAE by 42.28% and RMSE by 36.62%, evaluated on test data, on average for 1-step-ahead, 2-step-ahead and 3-step-ahead when compared to not applying any feature selection methodology. (C) 2019 Elsevier B.V. All rights reserved.",10.1016/j.enbuild.2019.05.021,Feature selection; Energy efficiency; Time series; Smart buildings; Smart cities,,
Surrogate-based Optimization for Reduction of Contagion Susceptibility in Financial Systems,"Michalak, K",PROCEEDINGS OF THE 2019 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE (GECCO'19),2019.0,"This paper studies a bi-objective optimization problem in which the goal is to optimize connections between entities in the market in order to make the entire system resilient to shocks of varying magnitudes. As observed in the literature concerning contagion on the inter-bank market, no system structure maximizes resilience to shocks of all sizes. For larger shocks more connections between entities make the crisis worse, and for smaller shocks more connected systems turn out to be more resilient than less connected ones. In order to benefit from the information about the system connectivity, a machine learning model is used to estimate the values of the objectives attained by a given solution. The paper presents a comparison of a multiobjective optimization algorithm using simulations for evaluating solutions with a surrogate-based algorithm using a machine learning model. In the experiments the surrogate-based method outperformed the simulation-based one. This observation along with the analysis of the dependence between system connectivity and the resilience to shocks of different magnitudes presented in the paper allow to conclude that the information on system connectivity can be used for improving the working of optimization methods aimed at making the system less susceptible to financial contagion.",10.1145/3321707.3321857,knowledge-based optimization; financial contagion; REDS graphs,,
Evolutionary inversion of class distribution in overlapping areas for multi-class imbalanced learning,"Fernandes, ERQ; de Carvalho, ACPLF",INFORMATION SCIENCES,2019.0,"Inductive learning from multi-class and imbalanced datasets is one of the main challenges for machine learning. Most machine learning algorithms have their predictive performance negatively affected by imbalanced data. Although several techniques have been proposed to deal with this difficulty, they are usually restricted to binary classification datasets. Thus, one of the research challenges in this area is how to deal with imbalanced multiclass classification datasets. This challenge become more difficult when classes containing fewer instances are located in overlapping regions of the data attribute space. In fact, several studies have indicated that the degree of class overlapping has a higher effect on predictive performance than the global class imbalance ratio. This paper proposes a novel evolutionary ensemble-based method for multi-class imbalanced learning called the evolutionary inversion of class distribution in overlapping areas for multi-class imbalanced learning (EVINCI). EVINCI uses a multiobjective evolutionary algorithm (MOEA) to evolve a set of samples taken from an imbalanced dataset. It selectively reduces the concentration of less representative instances of the majority classes in the overlapping areas while selecting samples that produce more accurate models. In experiments performed to evaluate its predictive accuracy, EVINCI was superior to state-of-the-art ensemble-based methods for imbalanced learning. (C) 2019 Published by Elsevier Inc.",10.1016/j.ins.2019.04.052,Multi-Class imbalanced learning; Ensemble of classifiers; Evolutionary algorithms,,
Regularized logistic regression and multiobjective variable selection for classifying MEG data,"Santana, R; Bielza, C; Larranaga, P",BIOLOGICAL CYBERNETICS,2012.0,"This paper addresses the question of maximizing classifier accuracy for classifying task-related mental activity from Magnetoencelophalography (MEG) data. We propose the use of different sources of information and introduce an automatic channel selection procedure. To determine an informative set of channels, our approach combines a variety of machine learning algorithms: feature subset selection methods, classifiers based on regularized logistic regression, information fusion, and multiobjective optimization based on probabilistic modeling of the search space. The experimental results show that our proposal is able to improve classification accuracy compared to approaches whose classifiers use only one type of MEG information or for which the set of channels is fixed a priori.",10.1007/s00422-012-0506-6,,,
Machine learning model and optimization of a PSA unit for methane-nitrogen separation,"Anna, HRS; Barreto, AG; Tavares, FW; de Souza, MB",COMPUTERS & CHEMICAL ENGINEERING,2017.0,"In this work we study the separation of N-2/CH4 in a bed packed with silicalite. Pressure swing adsorption (PSA) is a competitive technology for this task. Predicting PSA performance is a time consuming computational intensive problem. Direct optimization of the system of differential algebraic equations (DAE) describing the phenomena takes an impractical amount of time. We then analyze the suitability of using artificial neural networks (ANN) as a surrogate model to predict and optimize the PSA performance. Using the ANN surrogate model, optimization time decreased from 15.7 h to 50 s. We demonstrate that the PSA cycle proposed can achieve an optimized 99.5% nitrogen purity stream from an 85% inlet stream and a 50% purity stream from a 10% inlet stream. We also show that nitrogen recovery can be at most 90%. We further carry out a multi-objective optimization to demonstrate the tradeoff curve between nitrogen purity and recovery. (C) 2017 Elsevier Ltd. All rights reserved.",10.1016/j.compchemeng.2017.05.006,Pressure swing adsorption; Neural networks; Surrogate model; Optimization,,
Optimization of Deep Architectures for EEG Signal Classification: An AutoML Approach Using Evolutionary Algorithms,"Aquino-Britez, D; Ortiz, A; Ortega, J; Leon, J; Formoso, M; Gan, JQ; Escobar, JJ",SENSORS,2021.0,"Electroencephalography (EEG) signal classification is a challenging task due to the low signal-to-noise ratio and the usual presence of artifacts from different sources. Different classification techniques, which are usually based on a predefined set of features extracted from the EEG band power distribution profile, have been previously proposed. However, the classification of EEG still remains a challenge, depending on the experimental conditions and the responses to be captured. In this context, the use of deep neural networks offers new opportunities to improve the classification performance without the use of a predefined set of features. Nevertheless, Deep Learning architectures include a vast number of hyperparameters on which the performance of the model relies. In this paper, we propose a method for optimizing Deep Learning models, not only the hyperparameters, but also their structure, which is able to propose solutions that consist of different architectures due to different layer combinations. The experimental results corroborate that deep architectures optimized by our method outperform the baseline approaches and result in computationally efficient models. Moreover, we demonstrate that optimized architectures improve the energy efficiency with respect to the baseline models.",10.3390/s21062096,brain-computer interfaces (BCI); evolutionary computing; multi-objective EEG classification; deep learning,,
Ensemble strategies for a medical diagnostic decision support system: A breast cancer diagnosis application,"West, D; Mangiameli, P; Rampal, R; West, V",EUROPEAN JOURNAL OF OPERATIONAL RESEARCH,2005.0,"The model selection strategy is an important determinant of the performance and acceptance of a medical diagnostic decision support system based on supervised learning algorithms. This research investigates the potential of various selection strategies from a population of 24 classification models to form ensembles in order to increase the accuracy of decision support systems for the early detection and diagnosis of breast cancer. Our results suggest that ensembles formed from a diverse collection of models are generally more accurate than either pure-bagging ensembles (formed from a single model) or the selection of a single best model. We find that effective ensembles are formed from a small and selective subset of the population of available models with potential candidates identified by a multicriteria process that considers the properties of model generalization error, model instability, and the independence of model decisions relative to other ensemble members. (C) 2003 Elsevier B.V. All rights reserved.",10.1016/j.ejor.2003.10.013,decision support systems; medical informatics; neural networks; bootstrap aggregate models; ensemble strategies,,
A recommender system for tourism industry using cluster ensemble and prediction machine learning techniques,"Nilashi, M; Bagherifard, K; Rahmani, M; Rafe, V",COMPUTERS & INDUSTRIAL ENGINEERING,2017.0,"Recommender systems have emerged in the e-commerce domain and are developed to actively recommend the right items to online users. Traditional Collaborative Filtering (CF) recommender systems recommend the items to users based on their single-rating feedback which are used to match similar users. In multi-criteria CF recommender systems, however, multi-criteria ratings are used instead of single rating feedback which can significantly improve the accuracy of traditional CF algorithms. These systems have been successfully implemented in Tourism domain. In this paper, we propose a new recommendation method based on multi-criteria CF to enhance the predictive accuracy of recommender systems in tourism domain using clustering, dimensionality reduction and prediction methods. We use Adaptive Neuro-Fuzzy Inference Systems (ANFIS) and Support Vector Regression (SVR) as prediction techniques, Principal Component Analysis (PCA) as a dimensionality reduction technique and Self-Organizing Map (SOM) and Expectation Maximization (EM) as two well-known clustering techniques. To improve the recommendation accuracy of proposed multi-criteria CF, a cluster ensembles approach, Hypergraph Partitioning Algorithm (HGPA), is applied on SOM and EM clustering results. We evaluate the accuracy of recommendation method on TripAdvisior dataset. Our experiments confirm that cluster ensembles can provide better predictive accuracy for the proposed recommendation method in relation to the methods which solely rely on single clustering techniques. (C) 2017 Elsevier Ltd. All rights reserved.",10.1016/j.cie.2017.05.016,Tourism; Recommender systems; TripAdvisior; Cluster ensembles; Multi-criteria CF,,
Generalized approach for multi-response machining process optimization using machine learning and evolutionary algorithms,"Ghosh, T; Martinsen, K",ENGINEERING SCIENCE AND TECHNOLOGY-AN INTERNATIONAL JOURNAL-JESTECH,2020.0,"Contemporary manufacturing processes are substantially complex due to the involvement of a sizable number of correlated process variables. Uncovering the correlations among these variables would be the most demanding task in this scenario, which require exclusive tools and techniques. Data-driven surrogate-assisted optimization is an ideal modeling approach, which eliminates the necessity of resource driven mathematical or simulation paradigms for the manufacturing process optimization. In this paper, a data-driven evolutionary algorithm is introduced, which is based on the improved Non-dominated Sorting Genetic Algorithm (NSGA-III). For objective approximation, the Gaussian Kernel Regression is selected. The multi-response manufacturing process data are employed to train this model. The proposed data-driven approach is generic, which could be evaluated for any type of manufacturing process. In order to verify the proposed methodology, a comprehensive number of cases are considered from the past literature. The proposed data-driven NSGA-III is compared with the Multi-Objective Evolutionary Algorithm based on Decomposition (MOEA/D) and shown to attain improved solutions within the imposed boundary conditions. Both the algorithms are shown to perform well using statistical analysis. The obtained results could be utilized to improve the machining conditions and performances. The novelty of this research is twofold, first, the surrogate-assisted NSGA III is implemented and second, the proposed approach is adopted for the multi-response manufacturing process optimization. (C) 2019 Karabuk University. Publishing services by Elsevier B.V.",10.1016/j.jestch.2019.09.003,Machining process optimization; Data-driven surrogate model; NSGA-III; Many-response parametric design; MOEA/D,,
An improved machine learning technique based on downsized KPCA for Alzheimer's disease classification,"Neffati, S; Ben Abdellafou, K; Jaffel, I; Taouali, O; Bouzrara, K",INTERNATIONAL JOURNAL OF IMAGING SYSTEMS AND TECHNOLOGY,2019.0,"Alzheimer's disease (AD), a neurodegenerative disorder, is a very serious illness that cannot be cured, but the early diagnosis allows precautionary measures to be taken. The current used methods to detect Alzheimer's disease are based on tests of cognitive impairment, which does not provide an exact diagnosis before the patient passes a moderate stage of AD. In this article, a novel classifier of brain magnetic resonance images (MRI) based on the new downsized kernel principal component analysis (DKPCA) and multiclass support vector machine (SVM) is proposed. The suggested scheme classifies AD MRIs. First, a multiobjective optimization technique is used to determine the optimal parameter of the kernel function in order to ensure good classification results and to minimize the number of retained principle components simultaneously. The optimal parameter is used to build the optimized DKPCA model. Second, DKPCA is applied to normalized features. Downsized features are then fed to the classifier to output the prediction. To validate the effectiveness of the proposed method, DKPCA was tested using synthetic data to demonstrate its efficiency on dimensionality reduction, then the DKPCA based technique was tested on the OASIS MRI database and the results were satisfactory compared to conventional approaches.",10.1002/ima.22304,Alzheimer's disease; downsized kernel principal component analysis; medical image diagnosis; mksvm; multiobjective optimization,,
Consistency and inconsistency radii for solving systems of linear equations and inequalities,"Murav'eva, OV",COMPUTATIONAL MATHEMATICS AND MATHEMATICAL PHYSICS,2015.0,"Problems that reduce to consistency or inconsistency of systems of linear equations or inequalities arise in many divisions of theoretical informatics. The examples are problems in linear programming, machine learning, multicriteria optimization, etc. There exist different stability measures for the property of consistency or inconsistency, and different information constituents are possible (all the input parameters, the coefficient matrix, the vector of constraints). In this paper, variations of all parameters are examined in combination with an additional constraint important in applications, namely, the nonnegativity of feasible points.",10.1134/S0965542515030112,matrix correction; inconsistent systems of linear equations and inequalities; stability of systems of linear equations and inequalities,,
Learning-based ship design optimization approach,"Cui, H; Turan, O; Sayer, P",COMPUTER-AIDED DESIGN,2012.0,"With the development of computer applications in ship design, optimization, as a powerful approach, has been widely used in the design and analysis process. However, the running time, which often varies from several weeks to months in the current computing environment, has been a bottleneck problem for optimization applications, particularly in the structural design of ships. To speed up the optimization process and adjust the complex design environment, ship designers usually rely on their personal experience to assist the design work. However, traditional experience, which largely depends on the designer's personal skills, often makes the design quality very sensitive to the experience and decreases the robustness of the final design. This paper proposes a new machine-learning-based ship design optimization approach, which uses machine learning as an effective tool to give direction to optimization and improves the adaptability of optimization to the dynamic design environment. The natural human learning process is introduced into the optimization procedure to improve the efficiency of the algorithm. Q-learning, as an approach of reinforcement learning, is utilized to realize the learning function in the optimization process. The multi-objective particle swarm optimization method, multi-agent system, and CAE software are used to build an integrated optimization system. A bulk carrier structural design optimization was performed as a case study to evaluate the suitability of this method for real-world application. (C) 2011 Elsevier Ltd. All rights reserved.",10.1016/j.cad.2011.06.011,Machine learning; Ship design; Structure optimization; Structure analysis; Multi-objective optimization,,
Structural failure classification for reinforced concrete buildings using trained neural network based multi-objective genetic algorithm,"Chatterjee, S; Sarkar, S; Hore, S; Dey, N; Ashour, AS; Shi, FQ; Le, DN",STRUCTURAL ENGINEERING AND MECHANICS,2017.0,"Structural design has an imperative role in deciding the failure possibility of a Reinforced Concrete (RC) structure. Recent research works achieved the goal of predicting the structural failure of the RC structure with the assistance of machine learning techniques. Previously, the Artificial Neural Network (ANN) has been trained supported by Particle Swarm Optimization (PSO) to classify RC structures with reasonable accuracy. Though, keeping in mind the sensitivity in predicting the structural failure, more accurate models are still absent in the context of Machine Learning. Since the efficiency of multiobjective optimization over single objective optimization techniques is well established. Thus, the motivation of the current work is to employ a Multi-objective Genetic Algorithm (MOGA) to train the Neural Network (NN) based model. In the present work, the NN has been trained with MOGA to minimize the Root Mean Squared Error (RMSE) and Maximum Error (ME) toward optimizing the weight vector of the NN. The model has been tested by using a dataset consisting of 150 RC structure buildings. The proposed NN-MOGA based model has been compared with Multi-layer perceptron-feed-forward network (MLP-FFN) and NN-PSO based models in terms of several performance metrics. Experimental results suggested that the NN-MOGA has outperformed other existing well known classifiers with a reasonable improvement over them. Meanwhile, the proposed NN-MOGA achieved the superior accuracy of 93.33% and F-measure of 94.44%, which is superior to the other classifiers in the present study.",10.12989/sem.2017.63.4.429,genetic algorithm; classification; neural network; reinforced concrete,,
Learning multicriteria fuzzy classification method PROAFTN from data,"Belacel, N; Raval, HB; Punnen, AP",COMPUTERS & OPERATIONS RESEARCH,2007.0,"In this paper, we present a new methodology for learning parameters of multiple criteria classification method PROAFTN from data. There are numerous representations and techniques available for data mining, for example decision trees, rule bases, artificial neural networks, density estimation, regression and clustering. The PROAFTN method constitutes another approach for data mining. It belongs to the class of supervised learning algorithms and assigns membership degree of the alternatives to the classes. The PROAFTN method requires the elicitation of its parameters for the purpose of classification. Therefore, we need an automatic method that helps us to establish these parameters from the given data with minimum classification errors. Here, we propose variable neighborhood search metaheuristic for getting these parameters. The performances of the newly proposed method were evaluated using 10 cross validation technique. The results are compared with those obtained by other classification methods previously reported on the same data. It appears that the solutions of substantially better quality are obtained with proposed method than with these former ones. Crown Copyright (c) 2005 Published by Elsevier Ltd. All rights reserved.",10.1016/j.cor.2005.07.019,data mining; multiple criteria classification; PROAFTN procedure; variable neighborhood search,,
Balancing Exploration and Exploitation in Multiobjective Batch Bayesian Optimization,"Wang, HY; Xu, H; Yuan, Y; Sun, XM; Deng, JH",PROCEEDINGS OF THE 2019 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE COMPANION (GECCCO'19 COMPANION),2019.0,"Many applications such as hyper-parameter tunning in Machine Learning can be casted to multiobjective black-box problems and it is challenging to optimize them. Bayesian Optimization (130) is an effective method to deal with black-box functions. This paper mainly focuses on balancing exploration and exploitation in multiobjective black-box optimization problems by multiple samplings in BBO. In each iteration, multiple reconunendations are generated via two different trade-off strategies respectively, the expected improvement (El) and a multiobjective framework with the mean and variance function of the GP posterior forming two conflict objectives. We compare our algorithm with ParEGO by running on 12 test functions. Hypervoltune (HV, also known as S-metric) results show that our algoritlun works well in exploration-exploitation trade-off for multiobjective black-box optimization problems.",10.1145/3319619.3321962,expensive multiobjective optimization; batch Bayesian optimization; Gaussian Process; exploration and exploitation; ParEGO,,
MCRiceRepGP: a framework for the identification of genes associated with sexual reproduction in rice,"Golicz, AA; Bhalla, PL; Singh, MB",PLANT JOURNAL,2018.0,"Rice is an important cereal crop, being a staple food for over half of the world's population, and sexual reproduction resulting in grain formation underpins global food security. However, despite considerable research efforts, many of the genes, especially long intergenic non-coding RNA (lincRNA) genes, involved in sexual reproduction in rice remain uncharacterized. With an increasing number of public resources becoming available, information from different sources can be combined to perform gene functional annotation. We report the development of MCRiceRepGP, a machine learning framework which integrates heterogeneous evidence and employs multicriteria decision analysis and machine learning to predict coding and lincRNA genes involved in sexual reproduction in rice. The rice genome was reannotated using deep-sequencing transcriptomic data from reproduction-associated tissue/cell types identifying previously unannotated putative protein-coding genes and lincRNAs. MCRiceRepGP was used for genome-wide discovery of sexual reproduction associated coding and lincRNA genes. The protein-coding and lincRNA genes identified have distinct expression profiles, with a large proportion of lincRNAs reaching maximum expression levels in the sperm cells. Some of the genes are potentially linked to male- and female-specific fertility and heat stress tolerance during the reproductive stage. MCRiceRepGP can be used in combination with other genome-wide studies, such as genome-wide association studies, giving greater confidence that the genes identified are associated with the biological process of interest. As more data, especially about mutant plant phenotypes, become available, the power of MCRiceRepGP will grow, providing researchers with a tool to identify candidate genes for future experiments.",10.1111/tpj.14019,function prediction; machine learning; Oryza sativa; reannotation; sexual reproduction; lincRNA,,
Quick energy prediction and comparison of options at the early design stage,"Singh, MM; Singaravel, S; Klein, R; Geyer, P",ADVANCED ENGINEERING INFORMATICS,2020.0,"The energy-efficient building design requires building performance simulation (BPS) to compare multiple design options for their energy performance. However, at the early stage, BPS is often ignored, due to uncertainty, lack of details, and computational time. This article studies probabilistic and deterministic approaches to treat uncertainty; detailed and simplified zoning for creating zones; and dynamic simulation and machine learning for making energy predictions. A state-of-the-art approach, such as dynamic simulation, provide a reliable estimate of energy demand, but computationally expensive. Reducing computational time requires the use of an alternative approach, such as a machine learning (ML) model. However, an alternative approach will cause a prediction gap, and its effect on comparing options needs to be investigated. A plugin for Building information modelling (BIM) modelling tool has been developed to perform BPS using various approaches. These approaches have been tested for an office building with five design options. A method using the probabilistic approach to treat uncertainty, detailed zoning to create zones, and EnergyPlus to predict energy is treated as the reference method. The deterministic or ML approach has a small prediction gap, and the comparison results are similar to the reference method. The simplified model approach has a large prediction gap and only makes only 40% comparison results are similar to the reference method. These findings are useful to develop a BIM integrated tool to compare options at the early design stage and ascertain which approach should be adopted in a time-constraint situation.",10.1016/j.aei.2020.101185,Probabilistic; Zoning; Energy simulation; Machine learning; Prediction gap; Building Information Modelling (BIM),,
Image-based textile decoding,"Chen, SQ; Toyoura, M; Terada, T; Mao, XY; Xu, G",INTEGRATED COMPUTER-AIDED ENGINEERING,2021.0,"A textile fabric consists of countless parallel vertical yarns (warps) and horizontal yarns (wefts). While common looms can weave repetitive patterns, Jacquard looms can weave the patterns without repetition restrictions. A pattern in which the warps and wefts cross on a grid is defined in a binary matrix. The binary matrix can define which warp and weft is on top at each grid point of the Jacquard fabric. The process can be regarded as encoding from pattern to textile. In this work, we propose a decoding method that generates a binary pattern from a textile fabric that has been already woven. We could not use a deep neural network to learn the process based solely on the training set of patterns and observed fabric images. The crossing points in the observed image were not completely located on the grid points, so it was difficult to take a direct correspondence between the fabric images and the pattern represented by the matrix in the framework of deep learning. Therefore, we propose a method that can apply the framework of deep learning viau the intermediate representation of patterns and images. We show how to convert a pattern into an intermediate representation and how to reconvert the output into a pattern and confirm its effectiveness. In this experiment, we confirmed that 93% of correct pattern was obtained by decoding the pattern from the actual fabric images and weaving them again.",10.3233/ICA-200647,Textile; fabrication; intermediate representation; pattern decoding; Jacquard fabric,,
Robust path-following control design of heavy vehicles based on multiobjective evolutionary optimization,"de Morais, GAP; Marcos, LB; Barbosa, FM; Barbosa, BHG; Terra, MH; Grassi, V",EXPERT SYSTEMS WITH APPLICATIONS,2022.0,"The ability to deal with systems parametric uncertainties is an essential issue for heavy self-driving vehicles in unconfined environments. In this sense, robust controllers prove to be efficient for autonomous navigation. However, uncertainty matrices for this class of systems are usually defined by algebraic methods which demand prior knowledge of the system dynamics. In this case, the control system designer depends on the quality of the uncertain model to obtain an optimal control performance. This work proposes a robust recursive controller designed via multiobjective optimization to overcome these shortcomings. Furthermore, a local search approach for multiobjective optimization problems is presented. The proposed method applies to any multiobjective evolutionary algorithm already established in the literature. The results presented show that this combination of model-based controller and machine learning improves the effectiveness of the system in terms of robustness, stability and smoothness.",10.1016/j.eswa.2021.116304,Autonomous vehicles; Path-following; Robust control; Multiobjective optimization; Evolutionary algorithms,,
A generic optimising feature extraction method using multiobjective genetic programming,"Zhang, Y; Rockett, PI",APPLIED SOFT COMPUTING,2011.0,"In this paper, we present a generic, optimising feature extraction method using multiobjective genetic programming. We re-examine the feature extraction problem and show that effective feature extraction can significantly enhance the performance of pattern recognition systems with simple classifiers. A framework is presented to evolve optimised feature extractors that transform an input pattern space into a decision space in which maximal class separability is obtained. We have applied this method to real world datasets from the UCI Machine Learning and StatLog databases to verify our approach and compare our proposed method with other reported results. We conclude that our algorithm is able to produce classifiers of superior (or equivalent) performance to the conventional classifiers examined, suggesting removal of the need to exhaustively evaluate a large family of conventional classifiers on any new problem. (C) 2010 Elsevier B.V. All rights reserved.",10.1016/j.asoc.2010.02.008,Feature extraction; Multiobjective optimisation; Genetic programming; Pattern recognition,,
Multiobjective evolutionary algorithm assisted stacked autoencoder for PolSAR image classification,"Liu, GY; Li, YY; Jiao, LC; Chen, YQ; Shang, RH",SWARM AND EVOLUTIONARY COMPUTATION,2021.0,"Polarimetric synthetic aperture radar (PolSAR) image classification is a vital application in remote sensing image processing. In recent years, deep learning models like stacked autoencoder and its variants have been utilized to handle this problem and perform well. But their performances highly depend on proper hyper-parameter configuration. In this paper, we propose a multiobjective evolutionary algorithm assisted stacked autoencoder (SAE_MOEA/D) for PolSAR image classification, which could adaptively optimize its parameters and hyperparameters such as weights, activation functions and the balance factor in the loss function of stacked autoencoder, and decide how many layers of the network should be used according to datasets. Its performance has been tested on five PolSAR images. Compared with commonly used methods, our method obtains competitive results and could save manpower.",10.1016/j.swevo.2020.100794,Multiobjective evolutionary algorithm (MOEA); Stacked autoencoder (SAE); Polarimetric synthetic aperture radar (PolSAR); Image classification,,
Crowdtuning: systematizing auto-tuning using predictive modeling and crowdsourcing,"Memon, A; Fursin, G",PARALLEL COMPUTING: ACCELERATING COMPUTATIONAL SCIENCE AND ENGINEERING (CSE),2014.0,"Software and hardware co-design and optimization of HPC systems has become intolerably complex, ad-hoc, time consuming and error prone due to enormous number of available design and optimization choices, complex interactions between all software and hardware components, and multiple strict requirements placed on performance, power consumption, size, reliability and cost. We present our novel long-term holistic and practical solution to this problem based on customizable, plugin-based, schema-free, heterogeneous, open-source Collective Mind repository and infrastructure with unified web interfaces and online advise system. This collaborative framework distributes analysis and multiobjective off-line and on-line auto-tuning of computer systems among many participants while utilizing any available smart phone, tablet, laptop, cluster or data center, and continuously observing, classifying and modeling their realistic behavior. Any unexpected behavior is analyzed using shared data mining and predictive modeling plugins or exposed to the community at cTuning.org for collaborative explanation, top-down complexity reduction, incremental problem decomposition and detection of correlating program, architecture or run-time properties (features). Gradually increasing optimization knowledge helps to continuously improve optimization heuristics of any compiler, predict optimizations for new programs or suggest efficient run-time (online) tuning and adaptation strategies depending on end-user requirements. We decided to share all our past research artifacts including hundreds of codelets, numerical applications, data sets, models, universal experimental analysis and auto-tuning pipelines, self-tuning machine learning based meta compiler, and unified statistical analysis and machine learning plugins in a public repository to initiate systematic, reproducible and collaborative R&D with a new publication model where experiments and techniques are validated, ranked and improved by the community.",10.3233/978-1-61499-381-0-656,Collective Mind; crowdtuning; crowdsourcing auto-tuning and co-design; software and hardware co-design and co-optimization; compiler-agnostic tuning; on-line tuning and learning; systematic behavior modeling; predictive modeling; data mining; machine learning; on-line advice system; metadata; top-down optimization; incremental problem decomposition; decremental (differential) analysis; complexity reduction; tuning dimension reduction; customizable plugin-based infrastructure; public repository of knowledge; big data processing and compaction; agile research and development; cTuning.org; c-mind.org; systematic and reproducible research and experimentation; validation by community,,
Feature selection and semi-supervised clustering using multiobjective optimization,"Saha, S; Ekbal, A; Alok, AK; Spandana, R",SPRINGERPLUS,2014.0,"In this paper we have coupled feature selection problem with semi-supervised clustering. Semi-supervised clustering utilizes the information of unsupervised and supervised learning in order to overcome the problems related to them. But in general all the features present in the data set may not be important for clustering purpose. Thus appropriate selection of features from the set of all features is very much relevant from clustering point of view. In this paper we have solved the problem of automatic feature selection and semi-supervised clustering using multiobjective optimization. A recently created simulated annealing based multiobjective optimization technique titled archived multiobjective simulated annealing (AMOSA) is used as the underlying optimization technique. Here features and cluster centers are encoded in the form of a string. We assume that for each data set for 10% data points class level information are known to us. Two internal cluster validity indices reflecting different data properties, an external cluster validity index measuring the similarity between the obtained partitioning and the true labelling for 10% data points and a measure counting the number of features present in a particular string are optimized using the search capability of AMOSA. AMOSA is utilized to detect the appropriate subset of features, appropriate number of clusters as well as the appropriate partitioning from any given data set. The effectiveness of the proposed semi-supervised feature selection technique as compared to the existing techniques is shown for seven real-life data sets of varying complexities.",10.1186/2193-1801-3-465,Clustering; Multiobjective optimization (MOO); Symmetry; Cluster validity indices; Semi-supervised clustering; Feature selection; Multi-center; Automatic determination of number of clusters,,
A Method for Planning the Routes of Harvesting Equipment using Unmanned Aerial Vehicles,"Mezhuyev, V; Gunchenko, Y; Shvorov, S; Chyrchenko, D",INTELLIGENT AUTOMATION AND SOFT COMPUTING,2020.0,"The widespread distribution of precision farming systems necessitates improvements in the methods for the control of unmanned harvesting equipment (UHE). While unmanned aerial vehicles (UAVs) provide an effective solution to this problem, there are many challenges in the implementation of technology. This paper considers the problem of identifying optimal routes of UHE movement as a multicriteria evaluation problem, which can be solved by a nonlinear scheme of compromises. The proposed method uses machine learning algorithms and statistical processing of the spectral characteristics obtained from UAV digital images. Developed method minimizes the resources needed for a harvesting campaign and reduces the costs of fuel consumption.",10.31209/2019.100000133,Artificial Neural Networks; Nonlinear Scheme of Compromises; Precision Farming Systems; Unmanned Aerial Vehicles; Unmanned Harvesting Equipment,,
An experimental evaluation of some classification methods,"Doumpos, M; Chatzi, E; Zopounidis, C",JOURNAL OF GLOBAL OPTIMIZATION,2006.0,"The classification problem is of major importance to a plethora of research fields. The outgrowth in the development of classification methods has led to the development of several techniques. The objective of this research is to provide some insight on the relative performance of some well-known classification methods, through an experimental analysis covering data sets with different characteristics. The methods used in the analysis include statistical techniques, machine learning methods and multicriteria decision aid. The results of the study can be used to support the design of classification systems and the identification of the proper methods that could be used given the data characteristics.",10.1007/s10898-005-6152-y,classification; machine learning; Monte Carlo simulation; multicriteria decision aid; Statistical techniques,,
Automatic design of quantum feature maps,"Altares-Lopez, S; Ribeiro, A; Garcia-Ripoll, JJ",QUANTUM SCIENCE AND TECHNOLOGY,2021.0,"We propose a new technique for the automatic generation of optimal ad-hoc ansatze for classification by using quantum support vector machine. This efficient method is based on non-sorted genetic algorithm II multiobjective genetic algorithms which allow both maximize the accuracy and minimize the ansatz size. It is demonstrated the validity of the technique by a practical example with a non-linear dataset, interpreting the resulting circuit and its outputs. We also show other application fields of the technique that reinforce the validity of the method, and a comparison with classical classifiers in order to understand the advantages of using quantum machine learning.",10.1088/2058-9565/ac1ab1,quantum machine learning; genetic algorithms; artificial intelligence; automatic quantum classifier generation; optimization; quantum computing,,
Novel Ultrabright and Air-Stable Photocathodes Discovered from Machine Learning and Density Functional Theory Driven Screening,"Antoniuk, ER; Schindler, P; Schroeder, WA; Dunham, B; Pianetta, P; Vecchione, T; Reed, EJ",ADVANCED MATERIALS,2021.0,"The high brightness, low emittance electron beams achieved in modern X-ray free-electron lasers (XFELs) have enabled powerful X-ray imaging tools, allowing molecular systems to be imaged at picosecond time scales and sub-nanometer length scales. One of the most promising directions for increasing the brightness of XFELs is through the development of novel photocathode materials. Whereas past efforts aimed at discovering photocathode materials have typically employed trial-and-error-based iterative approaches, this work represents the first data-driven screening for high brightness photocathode materials. Through screening over 74 000 semiconducting materials, a vast photocathode dataset is generated, resulting in statistically meaningful insights into the nature of high brightness photocathode materials. This screening results in a diverse list of photocathode materials that exhibit intrinsic emittances that are up to 4x lower than currently used photocathodes. In a second effort, multiobjective screening is employed to identify the family of M2O (M = Na, K, Rb) that exhibits photoemission properties that are comparable to the current state-of-the-art photocathode materials, but with superior air stability. This family represents perhaps the first intrinsically bright, visible light photocathode materials that are resistant to reactions with oxygen, allowing for their transport and storage in dry air environments.",10.1002/adma.202104081,free-electron lasers; materials discovery; photocathodes; photonics; work functions,,
Accelerated discovery of 3D printing materials using data-driven multiobjective optimization,"Erps, T; Foshey, M; Lukovic, MK; Shou, W; Goetzke, HH; Dietsch, H; Stoll, K; von Vacano, B; Matusik, W",SCIENCE ADVANCES,2021.0,"Additive manufacturing has become one of the forefront technologies in fabrication, enabling products impossible to manufacture before. Although many materials exist for additive manufacturing, most suffer from performance trade-offs. Current materials are designed with inefficient human-driven intuition-based methods, leaving them short of optimal solutions. We propose a machine learning approach to accelerating the discovery of additive manufacturing materials with optimal trade-offs in mechanical performance. A multiobjective optimization algorithm automatically guides the experimental design by proposing how to mix primary formulations to create better performing materials. The algorithm is coupled with a semiautonomous fabrication platform to substantially reduce the number of performed experiments and overall time to solution. Without prior knowledge of the primary formulations, the proposed methodology autonomously uncovers 12 optimal formulations and enlarges the discovered performance space 288 times after only 30 experimental iterations. This methodology could be easily generalized to other material design systems and enable automated discovery.",10.1126/sciadv.abf7435,,,
An evolutionary parallel multiobjective feature selection framework,"Kiziloz, HE; Deniz, A",COMPUTERS & INDUSTRIAL ENGINEERING,2021.0,"Feature selection has become an indispensable preprocessing step in data mining problems as high amount of data become prevalent with the advances in technology. The objective of feature selection is twofold: reducing data amount and improving learning performance. In this study, we leverage the multi-core nature of a regular PC to build a robust framework for feature selection. This framework executes the feature selection algorithm on four processors, in parallel. As per the No Free Lunch Theorem, we facilitate 40 different execution settings for the processors by employing two multiobjective selection algorithms, four initial population generation methods, and five machine learning techniques. Besides, we introduce six setting selection schemes to decide the most fruitful setting for each processor. We carry out extensive experiments on 11 UCI benchmark datasets and analyze the results with statistical tests. Finally, we compare our proposed method with state-of-the-art studies and record remarkable improvement in terms of maximum accuracy.",10.1016/j.cie.2021.107481,Feature selection; Multiobjective optimization; Parallel processing; Evolutionary computation,,
Defect prediction as a multiobjective optimization problem,"Canfora, G; De Lucia, A; Di Penta, M; Oliveto, R; Panichella, A; Panichella, S",SOFTWARE TESTING VERIFICATION & RELIABILITY,2015.0,"In this paper, we formalize the defect-prediction problem as a multiobjective optimization problem. Specifically, we propose an approach, coined as multiobjective defect predictor (MODEP), based on multiobjective forms of machine learning techniqueslogistic regression and decision trees specificallytrained using a genetic algorithm. The multiobjective approach allows software engineers to choose predictors achieving a specific compromise between the number of likely defect-prone classes or the number of defects that the analysis would likely discover (effectiveness), and lines of code to be analysed/tested (which can be considered as a proxy of the cost of code inspection). Results of an empirical evaluation on 10 datasets from the PROMISE repository indicate the quantitative superiority of MODEP with respect to single-objective predictors, and with respect to trivial baseline ranking classes by size in ascending or descending order. Also, MODEP outperforms an alternative approach for cross-project prediction, based on local prediction upon clusters of similar classes. Copyright (c) 2015John Wiley & Sons, Ltd.",10.1002/stvr.1570,defect prediction; multiobjective optimization; cost-effectiveness; cross-project defect prediction,,
Explainable Deep Relational Networks for Predicting Compound- Protein Affinities and Contacts,"Karimi, M; Wu, D; Wang, ZY; Shen, Y",JOURNAL OF CHEMICAL INFORMATION AND MODELING,2021.0,"Predicting compound-protein affinity is beneficial for accelerating drug discovery. Doing so without the often-unavailable structure data is gaining interest. However, recent progress in structure-free affinity prediction, made by machine learning, focuses on accuracy but leaves much to be desired for interpretability. Defining intermolecular contacts underlying affinities as a vehicle for interpretability; our large-scale interpretability assessment finds previously used attention mechanisms inadequate. We thus formulate a hierarchical multiobjective learning problem, where predicted contacts form the basis for predicted affinities. We solve the problem by embedding protein sequences (by hierarchical recurrent neural networks) and compound graphs (by graph neural networks) with joint attentions between protein residues and compound atoms. We further introduce three methodological advances to enhance interpretability: (1) structure-aware regularization of attentions using protein sequence-predicted solvent exposure and residue-residue contact maps; (2) supervision of attentions using known intermolecular contacts in training data; and (3) an intrinsically explainable architecture where atomic-level contacts or relations lead to molecular-level affinity prediction. The first two and all three advances result in DeepAffinity+ and DeepRelations, respectively. Our methods show generalizability in affinity prediction for molecules that are new and dissimilar to training examples. Moreover, they show superior interpretability compared to state-of-the-art interpretable methods: with similar or better affinity prediction, they boost the AUPRC of contact prediction by around 33-, 35-, 10-, and 9-fold for the default test, new-compound, new-protein, and both-new sets, respectively. We further demonstrate their potential utilities in contact-assisted docking, structure-free binding site prediction, and structure-activity relationship studies without docking. Our study represents the first model development and systematic model assessment dedicated to interpretable machine learning for structure-free compound-protein affinity prediction.",10.1021/acs.jcim.0c00866,,,
Flash flood susceptibility modelling using functional tree and hybrid ensemble techniques,"Arabameri, A; Saha, S; Chen, W; Roy, J; Pradhan, B; Bui, DT",JOURNAL OF HYDROLOGY,2020.0,"The present research aims to assess and judge the capability of flash flood susceptibility (FFS) models considering hybrid machine learning ensemble techniques for the FFS assessment in the Gorgan Basin in Iran. Three novel intelligence approaches, namely, bagging-functional tree (BFT), dagging-functional tree, and rotational forest-functional tree are used for modelling, with consideration to 15 flood conditioning factors (FCFs) as independent variables and 426 flood locations as dependent variables. Three threshold-dependent and -independent approaches are used to evaluate the goodness-of-fit and prediction capability of the ensemble models with a single functional tree (FT). These approaches include the area under the receiver operating characteristic curve of the success rate curve (SRC) and prediction rate curve (PRC), efficiency (E) and true skill statistics (TSS). The random forest model is used to determine the relative importance of FCFs. Elevation, stream distance and normalized difference vegetation index (NDVI) have crucial roles in the study area during flash flood occurrences. According to the results of all threshold-dependent and -independent approaches (AUC of SRC = 0.933, AUC of PRC = 0.959, E = 0.76 and TSS = 0.72), the BFT ensemble model has the greatest accuracy in terms of modelling FFS. Results also show that the performance of the FT model is enhanced by three meta-classifiers. The seed cell area index technique is also used to check model classification accuracy and reliability. Results of this technique show that all the models demonstrate good performance and reliability. However, the FFS maps prepared by machine learning ensemble techniques have excellent accuracy and reliability, as per the results of validation methods. Thus, these FFS maps can be used as a convenient tool to reduce the effect of flood in flash flood-prone areas.",10.1016/j.jhydrol.2020.125007,Flash flood susceptibility; Functional tree (FT); Bagging-FT (BFT); Dagging-FT (DFT); Rotational tree-FT (RFT); GIS,,
Bankruptcy prediction using ELECTRE-based single-layer perceptron,"Hu, YC",NEUROCOMPUTING,2009.0,"For the outranking relation theory, the ELECTRE methods are one of the most extensively used outranking methods. To measure the degree of agreement and the degree of disagreement of the proposition one alternative outranks another alternative, the concordance and discordance relations are usually associated with the outranking relation. Instead of the traditional single-layer perceptron (SLP) developed according to the multiple-attribute utility theory, this paper contributes to develop a novel ELECTRE-based SLP for multicriteria classification problems based on the ELECTRE methods involving pairwise comparisons among patterns. A genetic-algorithm-based method is then designed to determine connection weights. A real-world data set involving bankruptcy analysis obtained from Moody's Industrial Manuals is employed to examine the classification performance of the proposed ELECTRE-based model. The results demonstrate that the proposed model performs well compared to an arsenal of well-known classification methods involving quantitative disciplines of statistics and machine learning. (C) 2009 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2009.03.002,ELECTRE; Multicriteria decision aid; Bankruptcy prediction; Genetic algorithm; Single-layer perceptron,,
Closed-Loop Multitarget Optimization for Discovery of New Emulsion Polymerization Recipes,"Houben, C; Peremezhney, N; Zubov, A; Kosek, J; Lapkin, AA",ORGANIC PROCESS RESEARCH & DEVELOPMENT,2015.0,"Self-optimization of chemical reactions enables faster optimization of reaction conditions or discovery of molecules with required target properties. The technology of self-optimization has been expanded to discovery of new process recipes for manufacture of complex functional products. A new machine-learning algorithm, specifically designed for multiobjective target optimization with an explicit aim to minimize the number of expensive experiments, guides the discovery process. This black-box approach assumes no a priori knowledge of chemical system and hence particularly suited to rapid development of processes to manufacture specialist low-volume, high-value products. The approach was demonstrated in discovery of process recipes for a semibatch emulsion copolymerization, targeting a specific particle size and full conversion.",10.1021/acs.oprd.5b00210,,,
"Big Data-Based Approach to Detect, Locate, and Enhance the Stability of an Unplanned Microgrid Islanding","Jiang, HG; Li, Y; Zhang, YC; Zhang, JJ; Gao, DW; Muljadi, E; Gu, Y",JOURNAL OF ENERGY ENGINEERING,2017.0,"In this paper, a big data-based approach is proposed for the security improvement of an unplanned microgrid islanding (UMI). The proposed approach contains two major steps: the first step is big data analysis of wide-area monitoring to detect a UMI and locate it; the second step is particle swarm optimization (PSO)-based stability enhancement for the UMI. First, an optimal synchrophasor measurement device selection (OSMDS) and matching pursuit decomposition (MPD)-based spatial-temporal analysis approach is proposed to significantly reduce the volume of data while keeping appropriate information from the synchrophasor measurements. Second, a random forest-based ensemble learning approach is trained to detect the UMI. When combined with grid topology, the UMI can be located. Then the stability problem of the UMI is formulated as an optimization problem and the PSO is used to find the optimal operational parameters of the UMI. An eigenvalue-based multiobjective function is proposed, which aims to improve the damping and dynamic characteristics of the UMI. Finally, the simulation results demonstrate the effectiveness and robustness of the proposed approach. (c) 2017 American Society of Civil Engineers.",10.1061/(ASCE)EY.1943-7897.0000473,Smart grid; Big data; Unplanned microgrid islanding; Synchrophasor measurement device; Ensemble learning; Random forest; Particle swarm optimization,,
Metaheuristics for data mining: Survey and opportunities for big data,"Dhaenens, C; Jourdan, L",4OR-A QUARTERLY JOURNAL OF OPERATIONS RESEARCH,2019.0,"In the context of big data, many scientific communities aim to provide efficient approaches to accommodate large-scale datasets. This is the case of the machine-learning community, and more generally, the artificial intelligence community. The aim of this article is to explain how data mining problems can be considered as combinatorial optimization problems, and how metaheuristics can be used to address them. Four primary data mining tasks are presented: clustering, association rules, classification, and feature selection. This article follows the publication of a book in 2016 concerning this subject (Dhaenens and Jourdan, Metaheuristics for big data, Wiley, New York, 2016); additionally, updated references and an analysis of the current trends are presented.",10.1007/s10288-019-00402-4,Metaheuristics; Clustering; Association rules; Classification; Feature selection; Big data,,
Multiobjective and categorical global optimization of photonic structures based on ResNet generative neural networks,"Jiang, JQ; Fan, JA",NANOPHOTONICS,2021.0,"We show that deep generative neural networks, based on global optimization networks (GLOnets), can be configured to perform the multiobjective and categorical global optimization of photonic devices. A residual network scheme enables GLOnets to evolve from a deep architecture, which is required to properly search the full design space early in the optimization process, to a shallow network that generates a narrow distribution of globally optimal devices. As a proof-of-concept demonstration, we adapt our method to design thin-film stacks consisting of multiple material types. Benchmarks with known globally optimized antireflection structures indicate that GLOnets can find the global optimum with orders of magnitude faster speeds compared to conventional algorithms. We also demonstrate the utility of our method in complex design tasks with its application to incandescent light filters. These results indicate that advanced concepts in deep learning can push the capabilities of inverse design algorithms for photonics.",10.1515/nanoph-2020-0407,categorical optimization; global optimization; multiobjective optimization; neural networks; thin-film stack,,
Flood susceptibility mapping using an improved analytic network process with statistical models,"Yariyan, P; Avand, M; Abbaspour, RA; Haghighi, AT; Costache, R; Ghorbanzadeh, O; Janizadeh, S; Blaschke, T",GEOMATICS NATURAL HAZARDS & RISK,2020.0,"Flooding is a natural disaster that causes considerable damage to different sectors and severely affects economic and social activities. The city of Saqqez in Iran is susceptible to flooding due to its specific environmental characteristics. Therefore, susceptibility and vulnerability mapping are essential for comprehensive management to reduce the harmful effects of flooding. The primary purpose of this study is to combine the Analytic Network Process (ANP) decision-making method and the statistical models of Frequency Ratio (FR), Evidential Belief Function (EBF), and Ordered Weight Average (OWA) for flood susceptibility mapping in Saqqez City in Kurdistan Province, Iran. The frequency ratio method was used instead of expert opinions to weight the criteria in the ANP. The ten factors influencing flood susceptibility in the study area are slope, rainfall, slope length, topographic wetness index, slope aspect, altitude, curvature, distance from river, geology, and land use/land cover. We identified 42 flood points in the area, 70% of which was used for modelling, and the remaining 30% was used to validate the models. The Receiver Operating Characteristic (ROC) curve was used to evaluate the results. The area under the curve obtained from the ROC curve indicates a superior performance of the ANP and EBF hybrid model (ANP-EBF) with 95.1% efficiency compared to the combination of ANP and FR (ANP-FR) with 91% and ANP and OWA (ANP-OWA) with 89.6% efficiency.",10.1080/19475705.2020.1836036,Flood mapping; analytic network process (ANP); statistical models; Saqqez City,,
A CFD-ML augmented alternative to residence time for clarification basin scaling and design,"Li, HC; Sansalone, J",WATER RESEARCH,2022.0,"Particulate matter (PM), while not an emerging contaminant, remains the primary labile substrate for parti-tioning and transport of emerging and known chemicals and pathogens. As a common unit operation and also green infrastructure, clarification basins are widely implemented to sequester PM as well as PM-partitioned chemicals and pathogens. Despite ubiquitous application for urban drainage, stormwater clarification basin design and optimization lacks robust and efficient design guidance and tools. Current basin design and regulation primarily adopt residence time (RT) as presumptive guidance. This study examines the accuracy and general-izability of RT and nondimensional groups of basin geometric and dynamic similarity (Hazen, Reynolds, Schmidt numbers) to scale clarification basin performance (measured as PM separation and total PM separation). Pub-lished data and 160,000 computational fluid dynamics (CFD) simulations of basin PM separation over a wide range of basin configurations, loading conditions, and PM granulometry (particle size distribution [PSD], den-sity) are examined. Based on the CFD database, a novel implementation of machine learning (ML) models: de-cision tree (DT), random forest (RF), artificial neural networks (ANN), and symbolic regression (SR) are developed and trained as surrogate models for basin PM separation predictions. Study results indicate that: (1) Models based solely on RT are not accurate or generalizable for basin PM separation, with significant differences between CFD and RT models primarily for RT < 200 hr, (2) RT models are agnostic to basin configurations and PM granulometrics and therefore do not reproduce total PM separation, (3) Trained ML models provide high predictive capability, with (R-2) above 0.99 and prediction for total PM separation within +/- 15%. In particular, the SR model distilled from CFD simulations is entirely defined by only two compact algebraic equations (allowing use in a spreadsheet tool). The SR model has a physical basis and indicates PM separation is primarily a function of the Hazen number and basin horizontal and vertical aspect ratios, (4) With common presumptive guidance of 80% for PM separation, a Pareto frontier analysis indicates that the CFD-ML augmented SR model generates significant economic benefit for basin planning/design, and (5) CFD-ML models show that enlarging basin dimensions (increasing RT) to address impaired behavior can result in exponential cost increases, irre-spective of land/infrastructure adjacency conflicts. CFD-ML applications can extend to intra-basin retrofits (permeable baffles) to upgrade impaired basins.",10.1016/j.watres.2021.117965,Water treatment; Computer aided design; CFD; Machine learning; Green water infrastructure; Retention,,
Designing parallelism in Surrogate-assisted multiobjective optimization based on decomposition,"Berveglieri, N; Derbel, B; Liefooghe, A; Aguirre, H; Zhang, QF; Tanaka, K",GECCO'20: PROCEEDINGS OF THE 2020 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE,2020.0,"On the one hand, surrogate-assisted evolutionary algorithms are established as a method of choice for expensive black-box optimization problems. On the other hand, the growth in computing facilities has seen a massive increase in potential computational power, granted the users accommodate their approaches with the offered parallelism. While a number of studies acknowledge the impact of parallelism for single-objective expensive optimization assisted by surrogates, extending such techniques to the multi-objective setting has not yet been properly investigated, especially within the state-of-the-art decomposition framework. We first highlight the different degrees of parallelism in existing surrogate-assisted multi-objective evolutionary algorithms based on decomposition (S-MOEA/D). We then provide a comprehensive analysis of the key steps towards a successful parallel S-MOEA/D approach. Through an extensive benchmarking effort relying on the well-established bbob-biobj test functions, we analyze the performance of the different algorithm designs with respect to the problem dimensionality and difficulty, the amount of parallel cores available, and the supervised learning models considered. In particular, we show the difference in algorithm scalability based on the selected surrogate-assisted approaches, the performance impact of distributing the model training task and the efficacy of the designed parallel-surrogate methods.",10.1145/3377930.3390202,Multiobjective optimization; surrogates; parallelism benchmarking,,
MoDeSuS: A Machine Learning Tool for Selection of Molecular Descriptors in QSAR Studies Applied to Molecular Informatics,"Martinez, MJ; Razuc, M; Ponzoni, I",BIOMED RESEARCH INTERNATIONAL,2019.0,"The selection of the most relevant molecular descriptors to describe a target variable in the context of QSAR (Quantitative Structure-Activity Relationship) modelling is a challenging combinatorial optimization problem. In this paper, a novel software tool for addressing this task in the context of regression and classification modelling is presented. The methodology that implements the tool is organized into two phases. The first phase uses a multiobjective evolutionary technique to perform the selection of subsets of descriptors. The second phase performs an external validation of the chosen descriptors subsets in order to improve reliability. The tool functionalities have been illustrated through a case study for the estimation of the ready biodegradation property as an example of classification QSAR modelling. The results obtained show the usefulness and potential of this novel software tool that aims to reduce the time and costs of development in the drug discovery process.",10.1155/2019/2905203,,,
Coherent averaging estimation autoencoders applied to evoked potentials processing,"Gareis, IE; Vignolo, LD; Spies, RD; Rufiner, HL",NEUROCOMPUTING,2017.0,"The success of machine learning algorithms strongly depends on the feature extraction and data representation stages. Classification and estimation of small repetitive signals masked by relatively large noise usually requires recording and processing several different realizations of the signal of interest. This is one of the main signal processing problems to solve when estimating or classifying P300 evoked potentials in brain-computer interfaces. To cope with this issue we propose a novel autoencoder variation, called Coherent Averaging Estimation Autoencoder with a new multiobjective cost function. We illustrate its use and analyze its performance in the problem of event related potentials processing. Experimental results showing the advantages of the proposed approach are finally presented. (C) 2017 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2017.02.050,Coherent Averaging; Artificial Neural Networks; Event Related Potentials; Brain Computer Interfaces; Autoencoders,,
Advances in Meta-Heuristic Optimization Algorithms in Big Data Text Clustering,"Abualigah, L; Gandomi, AH; Elaziz, MA; Al Hamad, H; Omari, M; Alshinwan, M; Khasawneh, AM",ELECTRONICS,2021.0,"This paper presents a comprehensive survey of the meta-heuristic optimization algorithms on the text clustering applications and highlights its main procedures. These Artificial Intelligence (AI) algorithms are recognized as promising swarm intelligence methods due to their successful ability to solve machine learning problems, especially text clustering problems. This paper reviews all of the relevant literature on meta-heuristic-based text clustering applications, including many variants, such as basic, modified, hybridized, and multi-objective methods. As well, the main procedures of text clustering and critical discussions are given. Hence, this review reports its advantages and disadvantages and recommends potential future research paths. The main keywords that have been considered in this paper are text, clustering, meta-heuristic, optimization, and algorithm.",10.3390/electronics10020101,meta-heuristic; optimization algorithms; machine learning; optimization problems; big data; text clustering applications,,
Single-Valued Neutrosophic Clustering Algorithms Based on Similarity Measures,"Ye, J",JOURNAL OF CLASSIFICATION,2017.0,"Clustering plays an important role in data mining, pattern recognition, and machine learning. Then, single-valued neutrosophic sets (SVNSs) can describe and handle indeterminate and inconsistent information, while fuzzy sets and intuitionistic fuzzy sets cannot describe and deal with it. To cluster the information represented by single-valued neutrosophic data, this paper proposes single-valued neutrosophic clustering algorithms based on similarity measures of SVNSs. Firstly, we introduce a similarity measure between SVNSs based on the min and max operators and propose another new similarity measure between SVNSs. Then, we present clustering algorithms based on the similarity measures of SVNSs for the clustering analysis of single-valued neutrosophic data. Finally, an illustrative example is given to demonstrate the application and effectiveness of the single-valued neutrosophic clustering algorithms.",10.1007/s00357-017-9225-y,Single-valued neutrososophic set; Clustering algorithm; Similarity measure; Similarity matrix,,
On the efficient use of uncertainty when performing expensive ROC optimisation,"Fieldsend, JE; Everson, RM","2008 IEEE CONGRESS ON EVOLUTIONARY COMPUTATION, VOLS 1-8",2008.0,"When optimising receiver operating characteristic (ROC) curves there is an inherent degree of uncertainty associated with the operating point evaluation of a model parameterisation x. This is due to the finite amount of training data used to evaluate the true and false positive rates of x. The uncertainty associated with any particular x can be reduced, but only at the computation cost of evaluating more data. Here we explicitly represent this uncertainty through the use of probabilistically non-dominated archives, and show how expensive ROC optimisation problems may be tackled by only evaluating a small subset of the available data at each generation of an optimisation algorithm. Illustrative results are given on data sets from the well known UCI machine learning repository.",10.1109/CEC.2008.4631340,,,
Interpretable neural networks based on continuous-valued logic and multicriteria decision operators,"Csiszar, O; Csiszar, G; Dombi, J",KNOWLEDGE-BASED SYSTEMS,2020.0,"Combining neural networks with continuous logic and multicriteria decision-making tools can reduce the black-box nature of neural models. In this study, we show that nilpotent logical systems offer an appropriate mathematical framework for hybridization of continuous nilpotent logic and neural models, helping to improve the interpretability and safety of machine learning. In our concept, perceptrons model soft inequalities; namely membership functions and continuous logical operators. We design the network architecture before training, using continuous logical operators and multicriteria decision tools with given weights working in the hidden layers. Designing the structure appropriately leads to a drastic reduction in the number of parameters to be learned. The theoretical basis offers a straightforward choice of activation functions (the cutting function or its differentiable approximation, the squashing function), and also suggests an explanation to the great success of the rectified linear unit (ReLU). In this study, we focus on the architecture of a hybrid model and introduce the building blocks for future applications in deep neural networks. (C) 2020 Elsevier B.V. All rights reserved.",10.1016/j.knosys.2020.105972,Explainable artificial intelligence; Continuous logic; Nilpotent logic; Neural network; Adversarial problems,,
The influence of mutation on population dynamics in multiobjective genetic programming,"Badran, K; Rockett, PI",GENETIC PROGRAMMING AND EVOLVABLE MACHINES,2010.0,"Using multiobjective genetic programming with a complexity objective to overcome tree bloat is usually very successful but can sometimes lead to undesirable collapse of the population to all single-node trees. In this paper we report a detailed examination of why and when collapse occurs. We have used different types of crossover and mutation operators (depth-fair and sub-tree), different evolutionary approaches (generational and steady-state), and different datasets (6-parity Boolean and a range of benchmark machine learning problems) to strengthen our conclusion. We conclude that mutation has a vital role in preventing population collapse by counterbalancing parsimony pressure and preserving population diversity. Also, mutation controls the size of the generated individuals which tends to dominate the time needed for fitness evaluation and therefore the whole evolutionary process. Further, the average size of the individuals in a GP population depends on the evolutionary approach employed. We also demonstrate that mutation has a wider role than merely culling single-node individuals from the population; even within a diversity-preserving algorithm such as SPEA2 mutation has a role in preserving diversity.",10.1007/s10710-009-9084-3,Multiobjective genetic programming; Population collapse; Mutation; Population dynamics,,
Energy modeling of a solar dish/Stirling by artificial intelligence approach,"Khosravi, A; Syri, S; Pabon, JJG; Sandoval, OR; Caetano, BC; Barrientos, MH",ENERGY CONVERSION AND MANAGEMENT,2019.0,"Solar energy is a renewable energy resources that is available across the world. A solar dish/Stirling system means a parabolic dish concentrator and a Stirling engine combined to generate mechanical and/or electrical output power. In this system, the input energy of Stirling engine is provided by sunlight as a source of heat. This study presents the effect of different variables on the power generation and efficiency of the system. In addition, artificial intelligence approach is employed to model a solar dish/Stirling system. For this target, a huge dataset was provided by considering a wide range of input variables. The intelligent methods are group method of data handling (GMDH) type neural network, adaptive neuro-fuzzy inference system (ANFIS) and multilayer perception (MLP) neural network (ANN). The MLP and ANFIS are optimized with particle swarm optimization (PSO) and genetic algorithm (GA). The intelligent methods are trained with inputs and targets. The considered input parameters are the ratio of focal point to dish diameter, hour of day, solar radiation, geometric concentration factor and working gas specific constant. The power generation, global efficiency, heat used to run the Stirling cycle, hot Stirling chamber temperature and engine speed are selected to be the targets. The results depict that the intelligent methods operate successfully for energy modeling of the solar dish/Stirling system and the statistical indicators illustrate that the ANFIS-PSO method performs better than the other developed methods.",10.1016/j.enconman.2019.112021,Solar dish/Stirling system; Particle swarm optimization; Genetic algorithm; Adaptive neuro-fuzzy inference system; Artificial neural network; Energy modeling,,
Prognosis and Multiobjective Optimization of the Sampling Plan for Cylindricity Evaluation,"Mian, SH; Umer, U; Abdulhameed, O; Alkhalefah, H",MATHEMATICAL PROBLEMS IN ENGINEERING,2020.0,"The actualization of the befitting sampling strategy and the application of an appropriate evaluation algorithm have been elementary issues in the coordinate metrology. The decisions regarding their choice for a given geometrical feature customarily rely upon the user's instinct or experience. As a consequence, the measurement results have to be accommodated between the accuracy and the inspection time. Certainly, a reliable and efficient sampling plan is imperative to accomplish a dependable inspection in minimal time, effort, and cost. This paper deals with the determination of an optimal sampling plan that minimizes the inspection cost, while still promising a measurement quality. A cylindrical-shaped component has been utilized in this work to achieve the desired objective. The inspection quality of the cylinder using a coordinate measuring machine (CMM) can be enhanced by controlling the three main parameters, which are used as input variables in the data file, namely, point distribution schemes, total number of points, and form evaluation algorithms. These factors affect the inspection output, in terms of cylindricity and measurement time, which are considered as target variables. The dataset, which comprises input and intended parameters, has been acquired through experimentation on the CMM machine. This work has utilized state-of-the-art machine learning algorithms to establish predictive models, which can predict the inspection output. The different algorithms have been examined and compared to seek for the most relevant machine learning regression method. The best performance has been observed using the support vector regression for cylindricity, with a mean absolute error of 0.000508 mm and a root-mean-squared error of 0.000885 mm. Likewise, the best prediction performance for measuring time has been demonstrated by the decision trees. Finally, the optimal parameters are estimated by employing the grey relational analysis (GRA) and the fuzzy technique for order performance by similarity to ideal solution (FTOPSIS). It has been approved that the values obtained from GRA are comparable with those of the FTOPSIS. Moreover, the quality of the optimal results is bettered by incorporating the measurement uncertainty in the outcome.",10.1155/2020/8201312,,,
Analysis of Supercritical CO2 Cycle Using Zigzag Channel Pre-Cooler: A Design Optimization Study Based on Deep Neural Network,"Saeed, M; Berrouk, AS; Singh, MP; Alawadhi, K; Siddiqui, MS",ENERGIES,2021.0,"The role of a pre-cooler is critical to the sCO(2)-BC as it not only acts as a sink but also controls the conditions at the main compressor's inlet that are vital to the cycle's overall performance. Despite their prime importance, studies on the pre-cooler's design are hard to find in the literature. This is partly due to the unavailability of data around the complex thermohydraulic characteristics linked with their operation close to the critical point. Henceforth, the current work deals with designing and optimizing pre-cooler by utilizing machine learning (ML), an in-house recuperator and pre-cooler design, an analysis code (RPDAC), and a cycle design point code (CDPC). Initially, data computed using 3D Reynolds averaged Navier-Stokes (RANS) equation is used to train the machine learning (ML) model based on the deep neural network (DNN) to predict Nusselt number (Nu) and friction factor (f). The trained ML model is then used in the pre-cooler design and optimization code (RPDAC) to generate various designs of the pre-cooler. Later, RPDAC was linked with the cycle design point code (CDPC) to understand the impact of various designs of the pre-cooler on the cycle's performance. Finally, a multi-objective genetic algorithm was used to optimize the pre-cooler geometry in the environment of the power cycle. Results suggest that the trained ML model can approximate 99% of the data with 90% certainty in the pre-cooler's operating regime. Cycle simulation results suggest that the cycle's performance calculation can be misleading without considering the pre-cooler's pumping power. Moreover, the optimization study indicates that the compressor's inlet temperature ranging from 307.5 to 308.5 and pre-cooler channel's Reynolds number ranging from 28,000 to 30,000 would be a good compromise between the cycle's efficiency and the pre-cooler's size.",10.3390/en14196227,pre-cooler design 1; PCHEs 2; sCO(2)-BC; deep learning neural network; machine learning; optimization; multiobjective genetic algorithm,,
Design of ultra-thin shell structures in the stochastic post-buckling range using Bayesian machine learning and optimization,"Bessa, MA; Pellegrino, S",INTERNATIONAL JOURNAL OF SOLIDS AND STRUCTURES,2018.0,"A data-driven computational framework combining Bayesian regression for imperfection-sensitive quantities of interest, uncertainty quantification and multi-objective optimization is developed for the design of complex structures. The framework is used to design ultra-thin carbon fiber deployable shells subjected to two bending conditions. Significant increases in the ultimate buckling loads are shown to be possible, with potential gains on the order of 100% as compared to a previously proposed design. The key to this result is the existence of a large load reserve capability after the initial bifurcation point and well into the post-buckling range that can be effectively explored by the data-driven approach. The computational strategy here presented is general and can be applied to different problems in structural and materials design, with the potential of finding relevant designs within high-dimensional spaces. (C) 2018 Elsevier Ltd. All rights reserved.",10.1016/j.ijsolstr.2018.01.035,Ultra-thin composites; Buckling; Post-buckling; Design charts; Data mining; Heteroscedastic Gaussian process; Evolutionary optimization,,
Characterizing the performance of an image-based recognizer for planar mechanical linkages in textbook graphics and hand-drawn sketches,"Eicholtz, M; Kara, LB",COMPUTERS & GRAPHICS-UK,2015.0,"In this work, we present a computational framework for automatically generating kinematic models of planar mechanical linkages from raw images. The hallmark of our approach is a novel combination of supervised learning methods for detecting mechanical parts (e.g. joints, rigid bodies) with the optimizing power of a multiobjective evolutionary algorithm, which concurrently maximizes image consistency and mechanical feasibility. A rigorous set of experiments was conducted to systematically evaluate the performance of each phase in our framework, comparing various combinations of joint and body detection schemes and feasibility constraints. Precision-recall curves are used to assess object detection performance. For the optimization, in addition to standard accuracy measures such as top-N accuracy, we introduce a new performance metric called user effort ratio that quantifies the amount of user interaction required to correct an inaccurate optimization solution. Current state-of-the-art performance is achieved with (i) one (or a cascade of) support vector machines for joint detection, (ii) foreground extraction to reduce false positives, (iii) supervised body detection using normalized geodesic time, distance, and detected joint confidence, and (iv) feasibility constraints derived from graph theory. The proposed framework generalizes moderately well from textbook graphics to hand-drawn sketches, and user effort ratio results demonstrate the potential power of an interactive system in which simple user interactions complement computer recognition for fast kinematic modeling. (C) 2015 Elsevier Ltd. All rights reserved.",10.1016/j.cag.2015.06.002,Computer vision; Evolutionary multiobjective optimization; Image processing; Kinematic modeling; Object recognition; Sketch recognition,,
Voice Pathology Detection Using Artificial Neural Networks and Support Vector Machines Powered by a Multicriteria Optimization Algorithm,"Areiza-Laverde, HJ; Castro-Ospina, AE; Peluffo-Ordonez, DH","APPLIED COMPUTER SCIENCES IN ENGINEERING, WEA 2018, PT I",2018.0,"Computer-aided diagnosis (CAD) systems have allowed to enhance the performance of conventional, medical diagnosis procedures in different scenarios. Particularly, in the context of voice pathology detection, the use of machine learning algorithms has proved to be a promising and suitable alternative. This work proposes the implementation of two well known classification algorithms, namely artificial neural networks (ANN) and support vector machines (SVM), optimized by particle swarm optimization (PSO) algorithm, aimed at classifying voice signals between healthy and pathologic ones. Three different configurations of the Saarbrucken voice database (SVD) are used. The effect of using balanced and unbalanced versions of this dataset is proved as well as the usefulness of the considered optimization algorithm to improve the final performance outcomes. Also, proposed approach is comparable with state-of-the-art methods.",10.1007/978-3-030-00350-0_13,Voice pathology; Computer-aided diagnosis; Optimization; Classification,,
Integrating estimation of distribution algorithms versus Q-learning into Meta-RaPS for solving the 0-1 multidimensional knapsack problem,"Arin, A; Rabadi, G",COMPUTERS & INDUSTRIAL ENGINEERING,2017.0,"Finding near-optimal solutions in an acceptable amount of time is a challenge when developing sophisticated approximate approaches. A powerful answer to this challenge might be reached by incorporating intelligence into metaheuristics. We propose integrating two methods into Meta-RaPS (Metaheuristic for Randomized Priority Search), which is currently classified as a memoryless metaheuristic. The first method is the Estimation of Distribution Algorithms (EDA), and the second is utilizing a machine learning algorithm known as Q-Learning. To evaluate their performance, the proposed algorithms are tested on the 0-1 Multidimensional Knapsack Problem (MKP). Meta-RaPS EDA appears to perform better than Meta-RaPS Q-Learning. However, both showed promising results compared to other approaches presented in the literature for the 0-1 MKP. (C) 2016 Elsevier Ltd. All rights reserved.",10.1016/j.cie.2016.10.022,Machine learning; Estimation of distribution algorithms; Q-learning; Meta-RaPS; 0-1 multidimensional knapsack problem,,
Multimodal Emotion Recognition Model Based on a Deep Neural Network with Multiobjective Optimization,"Li, MY; Qiu, X; Peng, S; Tang, LR; Li, QQ; Yang, WH; Ma, Y",WIRELESS COMMUNICATIONS & MOBILE COMPUTING,2021.0,"With the rapid development of deep learning and wireless communication technology, emotion recognition has received more and more attention from researchers. Computers can only be truly intelligent when they have human emotions, and emotion recognition is its primary consideration. This paper proposes a multimodal emotion recognition model based on a multiobjective optimization algorithm. The model combines voice information and facial information and can optimize the accuracy and uniformity of recognition at the same time. The speech modal is based on an improved deep convolutional neural network (DCNN); the video image modal is based on an improved deep separation convolution network (DSCNN). After single mode recognition, a multiobjective optimization algorithm is used to fuse the two modalities at the decision level. The experimental results show that the proposed model has a large improvement in each evaluation index, and the accuracy of emotion recognition is 2.88% higher than that of the ISMS_ALA model. The results show that the multiobjective optimization algorithm can effectively improve the performance of the multimodal emotion recognition model.",10.1155/2021/6971100,,,
A Novel Regression Modeling Method for PMSLM Structural Design Optimization Using a Distance-Weighted KNN Algorithm,"Song, JC; Zhao, JW; Dong, F; Zhao, J; Qian, Z; Zhang, Q",IEEE TRANSACTIONS ON INDUSTRY APPLICATIONS,2018.0,"This paper investigates the modeling methods for the structural design optimization of permanent magnet synchronous linear motors (PMSLMs), which are applied to linear motion machines. First, an initial PMSLM model is established by finite element analysis (FEA), and the data space is obtained for subsequent rapid regression modeling. Second, a powerful regression machine learning method, that is, distance-weighted K-nearest neighbor algorithm (DW-KNNA), is introduced to establish a calculation model by grasping the nonlinear relationships between input structural parameters and output motor performances. Model comparison experiments among analytical method, response surface method, K-nearest neighbor algorithm, and DW-KNNA demonstrate the superiority of the DW-KNNA. Finally, the differential evolution algorithm is used to determine the best combination of structural parameters and performances. The FEA and prototype motor experiments proved the validity and advantages of the proposed method.",10.1109/TIA.2018.2836953,Differential evolution algorithm (DEA); distance-weighted K-nearest neighbor algorithm (DW-KNNA); finite element analysis (FEA); permanent magnet synchronous linear motors (PMSLMs); rapid regression modeling,,
PTIME: Personalized Assistance for Calendaring,"Berry, PM; Gervasio, M; Peintner, B; Yorke-Smith, N",ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY,2011.0,"In a world of electronic calendars, the prospect of intelligent, personalized time management assistance seems a plausible and desirable application of AI. PTIME (Personalized Time Management) is a learning cognitive assistant agent that helps users handle email meeting requests, reserve venues, and schedule events. PTIME is designed to unobtrusively learn scheduling preferences, adapting to its user over time. The agent allows its user to flexibly express requirements for new meetings, as they would to an assistant. It interfaces with commercial enterprise calendaring platforms, and it operates seamlessly with users who do not have PTIME. This article overviews the system design and describes the models and technical advances required to satisfy the competing needs of preference modeling and elicitation, constraint reasoning, and machine learning. We further report on a multifaceted evaluation of the perceived usefulness of the system.",10.1145/1989734.1989744,Design; Experimentation; Human Factors; Personal assistant agents; preference modeling; machine learning; calendaring,,
Neural Network Inversion-Based Model for Predicting an Optimal Hardware Configuration: Solving Computationally Intensive Problems,"Al-Qutt, MM; Khaled, H; El Gohary, R",INTERNATIONAL JOURNAL OF GRID AND HIGH PERFORMANCE COMPUTING,2021.0,Deciding the number of processors that can efficiently speed-up solving a computationally intensive problem while perceiving efficient power consumption constitutes a major challenge to researcher in the HPC high performance computing realm. This paper exploits machine learning techniques to propose and implement a recommender system that recommends the optimal HPC architecture given the problem size. An approach for multi-objective function optimization based on neural network (neural network inversion) is employed. The neural network inversion approach is used for forward problem optimization. The objective functions in concern are maximizing the speedup and minimizing the power consumption. The recommendations of the proposed prediction systems achieved more than 89% accuracy for both validation and testing set. The experiments were conducted on 2500 CUDA core on Tesla K20 Kepler GPU Accelerator and Intel(R) Xeon(R) CPU E5-2695 v2.,10.4018/IJGHPC.2021040106,CUDA; GPU; High Performance Computing; Machine Learning; Motif Finding (MF); Neural Network Inversion; Power Consumption,,
EVOLUTIONARY SHALLOW NATURAL LANGUAGE PARSING,"Atkinson, J; Matamala, J",COMPUTATIONAL INTELLIGENCE,2012.0,"Identifying syntactical information from natural-language texts requires the use of sophisticated parsing techniques mainly based on statistical and machine-learning methods. However, due to complexity and efficiency issues many intensive natural-language processing applications using full syntactic analysis methods may not be effective when processing large amounts of natural-language texts. These tasks can adequately be performed by identifying partial syntactical information through shallow parsing (or chunking) techniques. In this work, a new approach to natural-language chunking using an evolutionary model is proposed. It uses previously captured training information to guide the evolution of the model. In addition, a multiobjective optimization strategy is used to produce unique quality values for objective functions involving the internal and the external quality of chunking. Experiments and the main results obtained using the model and state-of-the-art approaches are discussed.",10.1111/j.1467-8640.2012.00412.x,chunking; Genetic Algorithms; natural-language processing; parsing; statistical language models,,
CEG - A CASE-BASED DECISION MODELING ARCHITECTURE,"MOUSTAKIS, VS",EUROPEAN JOURNAL OF OPERATIONAL RESEARCH,1995.0,"In this paper we present a case evidence generation architecture, CEG, that supports case based decision and knowledge modeling. CEG is a blackboard based system that supports case evidence generation and acquisition. It can be used to define case decision classes, to model case knowledge elements, to represent prototypical case features and cases, to index cases according to decision classes or prototypical case realizations and to model elements of bias or error in case representation. We place CEG in the context of multicriteria decision making and draw architectural components from Machine Learning. We give an overview of CEG and CEG knowledge components. We demonstrate CEG by means of two 'real world' decision making exemplars drawn from retail sales evaluation and medical decision making. Finally, we explore architectural properties and discuss the potential for further research on the subject.",10.1016/0377-2217(94)00325-7,CASE BASED DECISION MAKING; MULTIPLE CRITERIA; MACHINE LEARNING,,
Flood susceptibility mapping and assessment using a novel deep learning model combining multilayer perceptron and autoencoder neural networks,"Ahmadlou, M; Al-Fugara, A; Al-Shabeeb, AR; Arora, A; Al-Adamat, R; Pham, QB; Al-Ansari, N; Linh, NTT; Sajedi, H",JOURNAL OF FLOOD RISK MANAGEMENT,2021.0,"Floods are one of the most destructive natural disasters causing financial damages and casualties every year worldwide. Recently, the combination of data-driven techniques with remote sensing (RS) and geographical information systems (GIS) has been widely used by researchers for flood susceptibility mapping. This study presents a novel hybrid model combining the multilayer perceptron (MLP) and autoencoder models to produce the susceptibility maps for two study areas located in Iran and India. For two cases, nine, and twelve factors were considered as the predictor variables for flood susceptibility mapping, respectively. The prediction capability of the proposed hybrid model was compared with that of the traditional MLP model through the area under the receiver operating characteristic (AUROC) criterion. The AUROC curve for the MLP and autoencoder-MLP models were, respectively, 75 and 90, 74 and 93% in the training phase and 60 and 91, 81 and 97% in the testing phase, for Iran and India cases, respectively. The results suggested that the hybrid autoencoder-MLP model outperformed the MLP model and, therefore, can be used as a powerful model in other studies for flood susceptibility mapping.",10.1111/jfr3.12683,deep learning; flood susceptibility; GIS; mapping; multilayer perceptron,,
MultiMiTar: A Novel Multi Objective Optimization based miRNA-Target Prediction Method,"Mitra, R; Bandyopadhyay, S",PLOS ONE,2011.0,"Background: Machine learning based miRNA-target prediction algorithms often fail to obtain a balanced prediction accuracy in terms of both sensitivity and specificity due to lack of the gold standard of negative examples, miRNA-targeting site context specific relevant features and efficient feature selection process. Moreover, all the sequence, structure and machine learning based algorithms are unable to distribute the true positive predictions preferentially at the top of the ranked list; hence the algorithms become unreliable to the biologists. In addition, these algorithms fail to obtain considerable combination of precision and recall for the target transcripts that are translationally repressed at protein level. Methodology/Principal Finding: In the proposed article, we introduce an efficient miRNA-target prediction system MultiMiTar, a Support Vector Machine (SVM) based classifier integrated with a multiobjective metaheuristic based feature selection technique. The robust performance of the proposed method is mainly the result of using high quality negative examples and selection of biologically relevant miRNA-targeting site context specific features. The features are selected by using a novel feature selection technique AMOSA-SVM, that integrates the multi objective optimization technique Archived Multi-Objective Simulated Annealing (AMOSA) and SVM. Conclusions/Significance: MultiMiTar is found to achieve much higher Matthew's correlation coefficient (MCC) of 0.583 and average class-wise accuracy (ACA) of 0.8 compared to the others target prediction methods for a completely independent test data set. The obtained MCC and ACA values of these algorithms range from -0.269 to 0.155 and 0.321 to 0.582, respectively. Moreover, it shows a more balanced result in terms of precision and sensitivity (recall) for the translationally repressed data set as compared to all the other existing methods. An important aspect is that the true positive predictions are distributed preferentially at the top of the ranked list that makes MultiMiTar reliable for the biologists. MultiMiTar is now available as an online tool at www.isical.ac.in/similar to bioinfo similar to miu/multimitarhtm. MultiMiTar software can be downloaded from www.isical.ac.in/similar to bioinfo_miu/multimitar-download.htm..",10.1371/journal.pone.0024583,,,
Approach and assessment of automated stereotactic radiotherapy planning for early stage non-small-cell lung cancer,"Bai, X; Shan, GP; Chen, M; Wang, BB",BIOMEDICAL ENGINEERING ONLINE,2019.0,"Background Intensity-modulated radiotherapy (IMRT) and volumetric-modulated arc therapy (VMAT) are standard physical technologies of stereotactic body radiotherapy (SBRT) that are used for patients with non-small-cell lung cancer (NSCLC). The treatment plan quality depends on the experience of the planner and is limited by planning time. An automated planning process can save time and ensure a high-quality plan. This study aimed to introduce and demonstrate an automated planning procedure for SBRT for patients with NSCLC based on machine-learning algorithms. The automated planning was conducted in two steps: (1) determining patient-specific optimized beam orientations; (2) calculating the organs at risk (OAR) dose achievable for a given patient and setting these dosimetric parameters as optimization objectives. A model was developed using data of historical expertise plans based on support vector regression. The study cohort comprised patients with NSCLC who were treated using SBRT. A training cohort (N = 125) was used to calculate the beam orientations and dosimetric parameters for the lung as functions of the geometrical feature of each case. These plan-geometry relationships were used in a validation cohort (N = 30) to automatically establish the SBRT plan. The automatically generated plans were compared with clinical plans established by an experienced planner. Results All 30 automated plans (100%) fulfilled the dose criteria for OARs and planning target volume (PTV) coverage, and were deemed acceptable according to evaluation by experienced radiation oncologists. An automated plan increased the mean maximum dose for ribs (31.6 +/- 19.9 Gy vs. 36.6 +/- 18.1 Gy, P < 0.05). The minimum, maximum, and mean dose; homogeneity index; conformation index to PTV; doses to other organs; and the total monitor units showed no significant differences between manual plans established by experts and automated plans (P > 0.05). The hands-on planning time was reduced from 40-60 min to 10-15 min. Conclusion An automated planning method using machine learning was proposed for NSCLC SBRT. Validation results showed that the proposed method decreased planning time without compromising plan quality. Plans generated by this method were acceptable for clinical use.",10.1186/s12938-019-0721-7,Machine learning; Non-small-cell lung cancer radiotherapy planning; Stereotactic body radiotherapy; Machine learning,,
A Survey on Evolutionary Computation Approaches to Feature Selection,"Xue, B; Zhang, MJ; Browne, WN; Yao, X",IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION,2016.0,"Feature selection is an important task in data mining and machine learning to reduce the dimensionality of the data and increase the performance of an algorithm, such as a classification algorithm. However, feature selection is a challenging task due mainly to the large search space. A variety of methods have been applied to solve feature selection problems, where evolutionary computation (EC) techniques have recently gained much attention and shown some success. However, there are no comprehensive guidelines on the strengths and weaknesses of alternative approaches. This leads to a disjointed and fragmented field with ultimately lost opportunities for improving performance and successful applications. This paper presents a comprehensive survey of the state-of-the-art work on EC for feature selection, which identifies the contributions of these different algorithms. In addition, current issues and challenges are also discussed to identify promising areas for future research.",10.1109/TEVC.2015.2504420,Classification; data mining; evolutionary computation; feature selection; machine learning,,
Joint model for feature selection and parameter optimization coupled with classifier ensemble in chemical mention recognition,"Ekbal, A; Saha, S",KNOWLEDGE-BASED SYSTEMS,2015.0,"Mention recognition in chemical texts plays an important role in a wide-spread range of application areas. Feature selection and parameter optimization are the two important issues in machine learning. While the former improves the quality of a classifier by removing the redundant and irrelevant features, the later concerns finding the most suitable parameter values, which have significant impact on the overall classification performance. In this paper we formulate a joint model that performs feature selection and parameter optimization simultaneously, and propose two approaches based on the concepts of single and multiobjective optimization techniques. Classifier ensemble techniques are also employed to improve the performance further. We identify and implement variety of features that are mostly domain-independent. Experiments are performed with various configurations on the benchmark patent and Medline datasets. Evaluation shows encouraging performance in all the settings. (C) 2015 Elsevier B.V. All rights reserved.",10.1016/j.knosys.2015.04.015,Feature selection; Parameter optimization; Multiobjective optimization (MOO); Single objective optimization (SOO); Conditional random field (CRF); Support vector machine (SVM),,
Landslide Susceptibility Prediction Based on Positive Unlabeled Learning Coupled With Adaptive Sampling,"Fang, ZC; Wang, Y; Niu, RQ; Peng, L",IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2021.0,"Many studies consider landslide susceptibility prediction as a binary classification problem when using machine learning methods, which requires both landslide and nonlandslide samples for modeling. Nevertheless, there are only landslide and unlabeled areas in the real world, and directly considering unlabeled areas as nonlandslide areas may cause bias and incorrect label assignment. In this article, we present a positive unlabeled learning method coupled with adaptive sampling and random forest (AdaPU-RF) to predict landslide susceptibility in the Three Gorges Reservoir area, China. This method can make full use of the landslide and nonlandslide information contained in unlabeled areas. Experimental results show that the AdaPU-RF method achieves desirable predication outcomes in terms of accuracy analysis, sensitivity analysis, and uncertainty analysis. Overall, the application of AdaPU-RF provides a new perspective for landslide susceptibility prediction, and can be recommended for other areas with similar geo-environmental conditions.",10.1109/JSTARS.2021.3125741,Terrain factors; Rivers; Reservoirs; Geology; Indexes; Learning systems; Vegetation mapping; Adaptive sampling; landslide susceptibility prediction (LSP); positive unlabeled (PU) learning; sensitivity analysis; uncertainty analysis,,
Decision Support System to Determine Intention to Use Mobile Payment Systems on Social Networks: A Methodological Analysis,"Guillen, A; Herrera, LJ; Pomares, H; Rojas, I; Liebana-Cabanillas, F",INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS,2016.0,"The rapid growth of social networks has led many companies to use mobile payment systems as business sales tools. As these platforms have an increasing acceptance among the consumers, the main goal of this research is to analyze the individuals' use intention of these systems in a social network environment. The data used were collected after performing an experiment using the Zong Payment tool. The paper includes a comparative analysis of several machine learning methods applied to the variable selection problem considering strategic, sociodemographic, and behavioral variables. The methods were applied isolatedly and, in a novel approach, using multiobjective optimization criteria. The results obtained by these algorithms were validated by an expert committee, who ranked them on the basis of their quality. The proposed methodology obtained the best results in terms of optimization as well as the highest scores from the experts. (C) 2015 Wiley Periodicals, Inc.",10.1002/int.21749,,,
A Multicriteria Decision Making Approach for Estimating the Number of Clusters in a Data Set,"Peng, Y; Zhang, Y; Kou, G; Shi, Y",PLOS ONE,2012.0,"Determining the number of clusters in a data set is an essential yet difficult step in cluster analysis. Since this task involves more than one criterion, it can be modeled as a multiple criteria decision making (MCDM) problem. This paper proposes a multiple criteria decision making (MCDM)-based approach to estimate the number of clusters for a given data set. In this approach, MCDM methods consider different numbers of clusters as alternatives and the outputs of any clustering algorithm on validity measures as criteria. The proposed method is examined by an experimental study using three MCDM methods, the well-known clustering algorithm-k-means, ten relative measures, and fifteen public-domain UCI machine learning data sets. The results show that MCDM methods work fairly well in estimating the number of clusters in the data and outperform the ten relative measures considered in the study.",10.1371/journal.pone.0041713,,,
Prediction and Optimization of Surface Roughness in a Turning Process Using the ANFIS-QPSO Method,"Alajmi, MS; Almeshal, AM",MATERIALS,2020.0,"This study presents a prediction method of surface roughness values for dry and cryogenic turning of AISI 304 stainless steel using the ANFIS-QPSO machine learning approach. ANFIS-QPSO combines the strengths of artificial neural networks, fuzzy systems and evolutionary optimization in terms of accuracy, robustness and fast convergence towards global optima. Simulations revealed that ANFIS-QPSO results in accurate prediction of surface roughness with RMSE = 4.86%, MAPE = 4.95% and R-2= 0.984 for the dry turning process. Similarly, for the cryogenic turning process, ANFIS-QPSO resulted in surface roughness predictions with RMSE = 5.08%, MAPE = 5.15% and R-2= 0.988 that are of high agreement with the measured values. Performance comparisons between ANFIS-QPSO, ANFIS, ANFIS-GA and ANFIS-PSO suggest that ANFIS-QPSO is an effective method that can ensure a high prediction accuracy of surface roughness values for dry and cryogenic turning processes.",10.3390/ma13132986,adaptive neuro-fuzzy inference system; turning process; surface roughness; machine learning; quantum particle swarm optimization; ANFIS-QPSO; ANN,,
Graphics Processing Unit-Accelerated Semiempirical Born Oppenheimer Molecular Dynamics Using PyTorch,"Zhou, GQ; Nebgen, B; Lubbers, N; Malone, W; Niklasson, AMN; Tretiak, S",JOURNAL OF CHEMICAL THEORY AND COMPUTATION,2020.0,"A new open-source high-performance implementation of Born Oppenheimer molecular dynamics based on semiempirical quantum mechanics models using PyTorch called PYSEQM is presented. PYSEQM was designed to provide researchers in computational chemistry with an open-source, efficient, scalable, and stable quantum-based molecular dynamics engine. In particular, PYSEQM enables computation on modern graphics processing unit hardware and, through the use of automatic differentiation, supplies interfaces for model parameterization with machine learning techniques to perform multiobjective training and prediction. The implemented semiempirical quantum mechanical methods (MNDO, AM1, and PM3) are described. Additional algorithms include a recursive Fermi-operator expansion scheme (SP2) and extended Lagrangian Born Oppenheimer molecular dynamics allowing for rapid simulations. Finally, benchmark testing on the nanostar dendrimer and a series of polyethylene molecules provides a baseline of code efficiency, time cost, and scaling and stability of energy conservation, verifying that PYSEQM provides fast and accurate computations.",10.1021/acs.jctc.0c00243,,,
"Flood hazards susceptibility mapping using statistical, fuzzy logic, and MCDM methods","Akay, H",SOFT COMPUTING,2021.0,"In this study, the flood hazards susceptibility map of an area in Turkey which is frequently exposed to flooding was predicted by training 70% of inventory data. For this, statistical, and hybrid methods such as frequency ratio (FR), evidential belief function (EBF), weight of evidence (WoE), index of entropy (IoE), fuzzy logic (FL), principal component analysis (PCA), analytical hierarchy process (AHP), technique for order preference by similarity to an ideal solution (TOPSIS), and VlseKriterijumska optimizacija I Kompromisno Resenje (VIKOR) were adapted. Values at both 70% and 30% of inventory data from the generated maps were extracted to validate the training and testing processes by receiver operating characteristics (ROC) analysis and seed cell area index (SCAI). Sensitivity, specificity, accuracy, and kappa index were calculated from ROC analysis, and SCAI was computed from the classification of map by natural break method and flood pixels in that classification. Since the predicted results of the methods applied did not point out the same model for each criterion, a simple method was selected to determine the most preferable method. Analysis showed that, IoE model was found to be the best model considering the ROC parameters, while PCA and AHP methods gave more reliable results considering SCAI. This study may be considered as a comprehensive contribution to the hybridization methods in predicting accurate flood hazards susceptibility maps.",10.1007/s00500-021-05903-1,Bivariate statistical models; Flood hazards susceptibility; Fuzzy logic model; Hybrid methods; Multicriteria decision making methods,,
Dynamic multiobjective optimization driven by inverse reinforcement learning,"Zou, F; Yen, GG; Zhao, C",INFORMATION SCIENCES,2021.0,"Due to the widespread interest in dynamic multiobjective optimization in real-world applications, more and more approaches exploiting machine learning are deployed to tackle this type of problems. Unfortunately, recent works do not make full use of the data obtained during the optimization process, which could be benefit for model training thereby mining the dynamic characteristics of the underlying problem. To address this issue, this paper proposes a dynamic multiobjective evolutionary algorithm driven by inverse reinforcement learning to solve the dynamic multiobjective optimization problems. IRL is widely used to recover the unknown reward function, making it possible to perform at an expert level. The notable features of the proposed algorithm mainly consist of data-driven evolutionary technique, which uses inverse reinforcement learning as a surrogate-assisted model for model training. This design makes full use of the surrogate management strategy based on inverse reinforcement learning to optimize the reward function within a reinforcement learning framework. At the same time, the algorithm can generate a promising policy based on limited training data during the optimization process to achieve better algorithm evolution and guide the search. The experimental results on the benchmark problems validate that the proposed algorithm is effective in dealing with dynamic multi objective optimization. (c) 2021 Elsevier Inc. All rights reserved.",10.1016/j.ins.2021.06.054,Dynamic multiobjective optimization; Machine learning; Inverse reinforcement learning; Evolutionary algorithm; Surrogate,,
Flash-flood propagation susceptibility estimation using weights of evidence and their novel ensembles with multicriteria decision making and machine learning,"Costache, R; Pham, QB; Arabameri, A; Diaconu, DC; Costache, I; Craciun, A; Ciobotaru, N; Pandey, M; Arora, A; Ali, SA; Pham, BT; Nguyen, H; Tuan, HA; Avand, M",GEOCARTO INTERNATIONAL,,"The present study aims to enrich the specialized literature by proposing and calculating a new flash-flood propagation susceptibility index (FFPSI). Thus, firstly the Flash-Flood Potential Index (FFPI) using the ensembles of the next models was calculated: Weights of Evidence (WOE), Analytical Hierarchy Process (AHP), Logistic Regression (LR), Classification and Regression Trees (CART), and Radial Basis Function Neural Network-Weights of Evidence (RBFN-WOE). A number of 255 flash-flood locations, split into training (70%) and validating (30%) samples, along with 10 predictors were used as input in the five models. The Receiver Operating Characteristics (ROC) Curve and several statistical metrics were used to evaluate the Flash-Flood Potential Index results. LR-WOE and AHP-WOE were the most performant models. Nevertheless, all the applied models performed very well (AUC > 0.85). Further, the FFPSI was determined by integrating the FFPI results into a Flow Accumulation procedure. Over 55% of the valleys identified are characterized by high and very high values of FFPSI.",10.1080/10106049.2021.2001580,Flash-floods propagation susceptibility; bivariate statistics; multicriteria decision-making; machine learning; Romania,,
Optimisation of wireless sensor networks using supervision information,"Khasteh, SH; Rokhsati, H",INTERNATIONAL JOURNAL OF SENSOR NETWORKS,2021.0,"Energy saving in wireless sensor networks (WSNs) is a critical problem for diversity of applications. In many scenarios using WSNs, we have incomplete and imperfect prior knowledge about the problem, if this knowledge can be incorporated into the problem-solving process, our performance can be improved. The main goal of this paper is to demonstrate the positive effect of supervision information, i.e., information such as our prior knowledge about the problem domain, on the performance of machine learning in WSNs. To achieve this goal, the routing problem in WSNs is solved with and without supervision information. First, the problem is solved using a very simple routing method, 'Gossiping'. Next, a reinforcement learning-based technique is used to find the most energy-efficient routes. Our methods are analysed theoretically and tested using a simulation. The results are highly promising and show that the utilisation of supervision information can reduce energy consumption by nearly 60%.",10.1504/IJSNET.2021.117963,WSNs; wireless sensor networks; machine learning; reinforcement learning; supervision information; energy conservation; routing,,
K-modes and Entropy Cluster Centers Initialization Methods,"Ali, DS; Ghoneim, A; Saleh, M",PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON OPERATIONS RESEARCH AND ENTERPRISE SYSTEMS (ICORES),2017.0,"Data clustering is an important unsupervised technique in data mining which aims to extract the natural partitions in a dataset without a priori class information. Unfortunately, every clustering model is very sensitive to the set of randomly initialized centers, since such initial clusters directly influence the formation of final clusters. Thus, determining the initial cluster centers is an important issue in clustering models. Previous work has shown that using multiple clustering validity indices in a multiobjective clustering model (e.g., MODEK-Modes model) yields more accurate results than using a single validity index. In this study, we enhance the performance of MODEK-Modes model by introducing two new initialization methods. The two proposed methods are the K-Modes initialization method and the entropy initialization method. The two proposed methods are tested using ten benchmark real life datasets obtained from the UCI Machine Learning Repository. Experimental results show that the two initialization methods achieve significant improvement in the clustering performance compared to other existing initialization methods.",10.5220/0006245504470454,Multiobjective Data Clustering; Categorical Datasets; K-modes Clustering Algorithm; Entropy,,
Accelerating manufacturing for biomass conversion via integrated process and bench digitalization: a perspective,"Batchu, SP; Hernandez, B; Malhotra, A; Fang, H; Ierapetritou, M; Vlachos, DG",REACTION CHEMISTRY & ENGINEERING,,"We present a perspective for accelerating biomass manufacturing via digitalization. We summarize the challenges for manufacturing and identify areas where digitalization can help. A profound potential in using lignocellulosic biomass and renewable feedstocks, in general, is to produce new molecules and products with unmatched properties that have no analog in traditional refineries. Discovering such performance-advantaged molecules and the paths and processes to make them rapidly and systematically can transform manufacturing practices. We discuss retrosynthetic approaches, text mining, natural language processing, and modern machine learning methods to enable digitalization. Laboratory and multiscale computation automation via active learning are crucial to complement existing literature and expedite discovery and valuable data collection without a human in the loop. Such data can help process simulation and optimization select the most promising processes and molecules according to economic, environmental, and societal metrics. We propose the close integration between bench and process scale models and data to exploit the low dimensionality of the data and transform the manufacturing for renewable feedstocks.",10.1039/d1re00560j,,,
From Cancer to Pain Target by Automated Selectivity Inversion of a Clinical Candidate,"Turk, S; Merget, B; Eid, S; Fulle, S",JOURNAL OF MEDICINAL CHEMISTRY,2018.0,"Elimination of inadvertent binding is crucial for inhibitor design targeting conserved protein classes like kinases. Compounds in clinical trials provide a rich source for initiating drug design efforts by exploiting such secondary binding events. Considering both aspects, we shifted the selectivity of tozasertib, originally developed against AurA as cancer target, toward the pain target TrkA. First, selectivity-determining features in binding pockets were identified by fusing interaction grids of several key and off-target conformations. A focused library was subsequently created and prioritized using a multiobjective selection scheme that filters for selective and highly active compounds based on orthogonal methods grounded in computational chemistry and machine learning. Eighteen high-ranking compounds were synthesized and experimentally tested. The top-ranked compound has 10000-fold improved selectivity versus AurA, nanomolar cellular activity, and is highly selective in a kinase panel. This was achieved in a single round of automated in silico optimization, highlighting the power of recent advances in computer-aided drug design to automate design and selection processes.",10.1021/acs.jmedchem.8b00140,,,
"Visible, near- and mid-infrared spectroscopy coupled with an innovative chemometric strategy to control apple puree quality","Lan, WJ; Bureau, S; Chen, SC; Leca, A; Renard, CMGC; Jaillais, B",FOOD CONTROL,2021.0,"Vis-NIRS, MIRS, and a combination of both coupled with PLS and machine learning were applied to i) trace the composed proportions of different apple varieties in formulated purees and ii) predict the quality characteristics of formulated purees from spectral information of initial puree cultivars. The PLS models could estimate proportions of each apple cultivar in puree mixtures using MIR spectra (RMSEP<8.1%, RPD> 3.6), especially for Granny Smith (RMSEP = 2.7%, RPD = 11.4). The concentration profiles from multivariate curve resolution-alternative least squares (MCR-ALS) made possible to reconstruct spectra of formulated purees. MIRS technique was evidenced to predict the final puree quality, such as viscosity (RPD>4.0), contents of soluble solids (RPD = 4.1), malic acid (RPD = 4.7) and glucose (RPD = 4.3), based only on the spectral data of composed puree cultivars. Infrared technique should be a powerful tool for puree traceability, even for multicriteria optimization of final products from the characteristics of composed puree cultivars before formulation.",10.1016/j.foodcont.2020.107546,Malus domestica borkh.; Vis-NIR; MIR; Machine learning; MCR-ALS,,
Robust ordinal regression in preference learning and ranking,"Corrente, S; Greco, S; Kadzinski, M; Slowinski, R",MACHINE LEARNING,2013.0,"Multiple Criteria Decision Aiding (MCDA) offers a diversity of approaches designed for providing the decision maker (DM) with a recommendation concerning a set of alternatives (items, actions) evaluated from multiple points of view, called criteria. This paper aims at drawing attention of the Machine Learning (ML) community upon recent advances in a representative MCDA methodology, called Robust Ordinal Regression (ROR). ROR learns by examples in order to rank a set of alternatives, thus considering a similar problem as Preference Learning (ML-PL) does. However, ROR implements the interactive preference construction paradigm, which should be perceived as a mutual learning of the model and the DM. The paper clarifies the specific interpretation of the concept of preference learning adopted in ROR and MCDA, comparing it to the usual concept of preference learning considered within ML. This comparison concerns a structure of the considered problem, types of admitted preference information, a character of the employed preference models, ways of exploiting them, and techniques to arrive at a final ranking.",10.1007/s10994-013-5365-4,Robust ordinal regression; Ranking; Preference learning; Multiple criteria decision aiding; Comparison; Preference modeling; Preference construction,,
A Survey of Advances in Landscape Analysis for Optimisation,"Malan, KM",ALGORITHMS,2021.0,"Fitness landscapes were proposed in 1932 as an abstract notion for understanding biological evolution and were later used to explain evolutionary algorithm behaviour. The last ten years has seen the field of fitness landscape analysis develop from a largely theoretical idea in evolutionary computation to a practical tool applied in optimisation in general and more recently in machine learning. With this widened scope, new types of landscapes have emerged such as multiobjective landscapes, violation landscapes, dynamic and coupled landscapes and error landscapes. This survey is a follow-up from a 2013 survey on fitness landscapes and includes an additional 11 landscape analysis techniques. The paper also includes a survey on the applications of landscape analysis for understanding complex problems and explaining algorithm behaviour, as well as algorithm performance prediction and automated algorithm configuration and selection. The extensive use of landscape analysis in a broad range of areas highlights the wide applicability of the techniques and the paper discusses some opportunities for further research in this growing field.",10.3390/a14020040,fitness landscape; landscape analysis; violation landscape; error landscape; automated algorithm selection,,
Neural networks and reinforcement learning in control of water systems,"Bhattacharya, B; Lobbrecht, AH; Solomatine, DP",JOURNAL OF WATER RESOURCES PLANNING AND MANAGEMENT,2003.0,"In dynamic real-time control (RTC) of regional water systems, a multicriteria optimization problem has to be solved to determine the optimal control strategy. Nonlinear and/or dynamic programming based on simulation models can be used to find the solution, an approach being used in the Aquarius decision support system (DSS) developed in The Netherlands. However, the computation time required for complex models is often prohibitively long, and therefore such a model cannot be applied in RTC of water systems. In this study, Aquarius DSS is chosen as a reference model for building a controller using machine learning techniques such as artificial neural networks (ANN) and reinforcement learning (RL), where RL is used to decrease the error of the ANN-based component. The model was tested with complex water systems in The Netherlands, and very good results were obtained. The general conclusion is that a controller, which has learned to replicate the optimal control strategy, can be used in RTC operations.",10.1061/(ASCE)0733-9496(2003)129:6(458),water supply systems; neural networks; learning; Netherlands; dynamic programs,,
A review of the role of heuristics in stochastic optimisation: from metaheuristics to learnheuristics,"Juan, AA; Keenan, P; Marti, R; McGarraghy, S; Panadero, J; Carroll, P; Oliva, D",ANNALS OF OPERATIONS RESEARCH,,"In the context of simulation-based optimisation, this paper reviews recent work related to the role of metaheuristics, matheuristics (combinations of exact optimisation methods with metaheuristics), simheuristics (hybridisation of simulation with metaheuristics), biased-randomised heuristics for 'agile' optimisation via parallel computing, and learnheuristics (combination of statistical/machine learning with metaheuristics) to deal with NP-hard and large-scale optimisation problems in areas such as transport and logistics, manufacturing and production, smart cities, telecommunication networks, finance and insurance, sustainable energy consumption, health care, military and defence, e-marketing, or bioinformatics. The manuscript provides the main related concepts and updated references that illustrate the applications of these hybrid optimisation-simulation-learning approaches in solving rich and real-life challenges under dynamic and uncertainty scenarios. A numerical analysis is also included to illustrate the benefits that these approaches can offer across different application fields. Finally, this work concludes by highlighting open research lines on the combination of these methodologies to extend the concept of simulation-based optimisation.",10.1007/s10479-021-04142-9,Metaheuristics; Simheuristics; Learnheuristics; Biased-randomised heuristics; Stochastic optimisation; Dynamic optimisation,,
Pseudo-Labeling Optimization Based Ensemble Semi-Supervised Soft Sensor in the Process Industry,"Li, YW; Jin, HP; Dong, SL; Yang, B; Chen, XG",SENSORS,2021.0,"Nowadays, soft sensor techniques have become promising solutions for enabling real-time estimation of difficult-to-measure quality variables in industrial processes. However, labeled data are often scarce in many real-world applications, which poses a significant challenge when building accurate soft sensor models. Therefore, this paper proposes a novel semi-supervised soft sensor method, referred to as ensemble semi-supervised negative correlation learning extreme learning machine (EnSSNCLELM), for industrial processes with limited labeled data. First, an improved supervised regression algorithm called NCLELM is developed, by integrating the philosophy of negative correlation learning into extreme learning machine (ELM). Then, with NCLELM as the base learning technique, a multi-learner pseudo-labeling optimization approach is proposed, by converting the estimation of pseudo labels as an explicit optimization problem, in order to obtain high-confidence pseudo-labeled data. Furthermore, a set of diverse semi-supervised NCLELM models (SSNCLELM) are developed from different enlarged labeled sets, which are obtained by combining the labeled and pseudo-labeled training data. Finally, those SSNCLELM models whose prediction accuracies were not worse than their supervised counterparts were combined using a stacking strategy. The proposed method can not only exploit both labeled and unlabeled data, but also combine the merits of semi-supervised and ensemble learning paradigms, thereby providing superior predictions over traditional supervised and semi-supervised soft sensor methods. The effectiveness and superiority of the proposed method were demonstrated through two chemical applications.",10.3390/s21248471,soft sensor; unlabeled data; label scarcity; semi-supervised learning; ensemble learning; pseudo labeling; evolutionary optimization; negative correlation learning; extreme learning machine,,
Surrogate modeling of waveform response using singular value decomposition and Bayesian optimization,"Shintani, K; Fujimoto, T; Okamoto, M; Abe, A; Yamamoto, Y",JOURNAL OF ADVANCED MECHANICAL DESIGN SYSTEMS AND MANUFACTURING,2021.0,"In the early stage of vehicle development, it is required to implement a target cascading study by solving inverse problems. However, simulation costs of vehicle dynamics to predict transient responses and frequency responses make the target cascading study difficult. The purpose of this paper is to propose a method to construct a surrogate model which can predict waveform responses and a solution of Bayesian optimization using posterior distribution of trained waveform responses. Replacement of the expensive simulation by the more economical surrogate model can enhance the target cascading study. In this paper, we construct a vectorized training data matrix from the waveform responses which can be evaluated from CAE simulations based on the Design of Experiments. In this proposed method, supervised and unsupervised learning are introduced. The singular value decomposition is used as a feature extraction method (Unsupervised learning) and applied to the training data. Obtained singular vectors are used as feature modes to represent the training data. Gaussian Process model is introduced as a regression model (Supervised learning) and applied to each weight of feature modes which can be obtained by projection of training data to feature modes. The waveform response can be predicted by the superposition of prediction feature values and feature modes. By using the posterior distribution of trained Gaussian Process, Expected Improvement function is evaluated and used in Bayesian optimization to minimize a cost function which is evaluated from the posterior mean of a predicted waveform. The feasibility of the proposed method is illustrated by an application for the suspension design problem of impact harshness phenomenon.",10.1299/jamdsm.2021jamdsm0018,Gaussian process; Design of experiments; Singular value decomposition; Bayesian optimization; Inverse problem; Vehicle dynamics; Multibody dynamics,,
"Performance evaluation of the GIS-based data mining techniques of best-first decision tree, random forest, and naive Bayes tree for landslide susceptibility modeling","Chen, W; Zhang, S; Li, RW; Shahabi, H",SCIENCE OF THE TOTAL ENVIRONMENT,2018.0,"The main aim of the present study is to explore and compare three state-of-the art data mining techniques, best-first decision tree, randomforest, and naive Bayes tree, for landslide susceptibility assessment in the Longhai area of China. First, a landslide inventory map with 93 landslide locations was randomly divided, with 70% of the area used for training landslide models and 30% used for the validation process. A spatial database of 14 conditioning factors was constructed under a geographic information system environment. Subsequently, the ReliefF method was employed to assess the prediction capability of the conditioning factors in landslide models. Multicollinearity of these factors was verified using the variance inflation factor, tolerance, and Pearson's correlation coefficient. Finally, the three resulting models were evaluated and compared using the area under the receiver operating characteristic (AUROC) curve, standard error, 95% confidence interval, accuracy, precision, recall, and F-measure. The random forest model showed the AUROC values (0.869), smallest standard error (0.025), narrowest 95% confidence interval (0.819-0.918), highest accuracy value (0.774), highest precision (0.662), and highest F-measure (0.662) for the training dataset. Thus, the random forest model is a promising technique that could be used for landslide susceptibility mapping. (c) 2018 Elsevier B.V. All rights reserved.",10.1016/j.scitotenv.2018.06.389,Landslide; Best-first decision tree; Random forest; naive Bayes tree; China,,
Modeling spatial distribution of flow depth in fluvial systems using a hybrid two-dimensional hydraulic-multigene genetic programming approach,"Yan, XH; Mohammadian, A; Khelifa, A",JOURNAL OF HYDROLOGY,2021.0,"Modeling spatial distribution of flow depth in fluvial systems is crucial for flow mitigation, river rehabilitation, and design of water resources infrastructure. Flow depth in fluvial systems can be typically estimated using hydrological or physics-based hydraulic models. However, hydrological models may not be able to provide satisfactory predictions for catchments with limited data because they normally ignored the strict conservation of momentum. Traditional fully physics-based hydraulic models are often very computationally expensive, limiting their wide usage in practical applications. In this study, a novel method, based on a hybrid two-dimensional (2D) hydraulic-multigene genetic programming (MGGP) approach, is proposed and employed to model the spatial distribution of flow depth in fluvial systems. A 2D hydraulic model was constructed using the TELEMAC-2D software and validated against field measurements. The validated model was then assumed to reflect the real physical processes and utilized to carry out additional computations to obtain spatial distribution of flow depth under different discharge scenarios, which provided a sufficient synthetic dataset for training machine learning models based on the MGGP technique. The study area (a segment of the Ottawa River near the island named Ile Kettle) was divided into 34 sub-regions to further reduce the computational costs of the training processes and the complexity of the evolved models. The numerical data were distributed to the corresponding sub-regions, and an MGGP-based model was trained for each sub-region. These models are compact explicit arithmetic equations that can be readily transferable and can immediately output the flow depth at any point in the corresponding sub-region as functions of the flow rate, longitudinal, and transversal coordinates. The best MGGP model for each sub-region amongst all the generated models was identified using the Pareto optimization approach. The results showed that the best MGGP models satisfactorily reproduced the training data and predicted the testing data (the root mean square errors were 0.303 m and 0.306 m, respectively), demonstrating the predictive capability of the approach. A comparison between MGGP and single-gene genetic programming (SGGP) approaches and confidence analysis were also reported, which demonstrated the good performance of the proposed approach. Furthermore, it took about 53 min for the hydraulic model to complete each simulation, but it took only about 0.56 s using the final model; the total size of the hydraulic output files for 12 different sizes was 432, 948 KB, but the total size of the script file for the final model was only about 46 KB. Therefore, the present study found that the hybrid 2D hydraulic-MGGP approach was satisfactorily accurate, fast to run, and easy to use, and thus, it is a promising tool for modeling spatial distribution of flow depth in fluvial systems.",10.1016/j.jhydrol.2021.126517,Spatial distribution; Flow depth; 2D hydraulic; Multigene genetic programming; Ottawa River,,
A GIS-based novel approach for suitable sanitary landfill site selection using integrated fuzzy analytic hierarchy process and machine learning algorithms,"Mohsin, M; Ali, SA; Shamim, SK; Ahmad, A",ENVIRONMENTAL SCIENCE AND POLLUTION RESEARCH,,"Disposal of waste without treatment is the least preferable way of sustainable solid waste management (SWM). But most cities in developing nations still use open dumps, causing negative impacts on the environment and human health. This study offered a novel approach for selecting landfill sites and sustainable SWM in Aligarh city, India. This was done through data collection, selecting models for criterion weighting, and validation. In order to prepare a landfill site suitability map, a geographic information system (GIS)-based ensemble fuzzy analytic hierarchy process-support vector machine (FAHP-SVM) and fuzzy analytic hierarchy process-random forest (FAHP-RF) models were implemented. Considering the previous studies and the study area characteristics, eighteen thematic layers were selected. The result revealed that land value; distance from residential roads, hospitals and clinics, and waste bins; and normalized difference built-up index (NDBI) have a fuzzy weight greater than 0.10, indicating significant factors. In contrast, land elevation, land slope, surface temperature, soil moisture index, normalized difference vegetation index (NDVI), and urban classification have a zero fuzzy weight, indicating these criteria have no importance. The result further revealed that FAHP-RF with an area under curve (AUC) value of 0.91 is the more accurate model than FAHP-SVM. According to the final weight-based overlay result, seven potential landfill sites were identified, out of which three were determined as most suitable by considering current land cover, public opinions, and environmental and economic concerns. This research proposed a zonal division model based on landfill sites location for sustainable SWM in Aligarh city. However, the findings may provide a guideline to the decision-makers and planners for optimal landfill site selection in other cities of developing countries.",10.1007/s11356-021-17961-x,Sustainable solid waste management; Multi-criteria decision-making; Support vector machine; Random forest; Aligarh city,,
Multiobjective grammar-based genetic programming applied to the study of asthma and allergy epidemiology,"Veiga, RV; Barbosa, HJC; Bernardino, HS; Freitas, JM; Feitosa, CA; Matos, SMA; Alcantara-Neves, NM; Barreto, ML",BMC BIOINFORMATICS,2018.0,"Background: Asthma and allergies prevalence increased in recent decades, being a serious global health problem. They are complex diseases with strong contextual influence, so that the use of advanced machine learning tools such as genetic programming could be important for the understanding the causal mechanisms explaining those conditions. Here, we applied a multiobjective grammar-based genetic programming (MGGP) to a dataset composed by 1047 subjects. The dataset contains information on the environmental, psychosocial, socioeconomics, nutritional and infectious factors collected from participating children. The objective of this work is to generate models that explain the occurrence of asthma, and two markers of allergy: presence of IgE antibody against common allergens, and skin prick test positivity for common allergens (SPT). Results: The average of the accuracies of the models for asthma higher in MGGP than C4.5. IgE were higher in MGGP than in both, logistic regression and C4.5. MGGP had levels of accuracy similar to RF, but unlike RF, MGGP was able to generate models that were easy to interpret. Conclusions: MGGP has shown that infections, psychosocial, nutritional, hygiene, and socioeconomic factors may be related in such an intricate way, that could be hardly detected using traditional regression based epidemiological techniques. The algorithm MGGP was implemented in c ++ and is available on repository: http://bitbucket.org/cimluflf/ciml-lib.",10.1186/s12859-018-2233-z,Genetic programming; Asthma; Allergy; Classifier; Multiobjective,,
MOEA/D with opposition-based learning for multiobjective optimization problem,"Ma, XL; Liu, F; Qi, YT; Gong, MG; Yin, ML; Li, LL; Jiao, LC; Wu, JS",NEUROCOMPUTING,2014.0,"Multiobjective evolutionary algorithm based on decomposition (MOEA/D) has attracted a great deal of attention and has obtained enormous success in the field of evolutionary multiobjective optimization. It converts a multiobjective optimization problem (MOP) into a set of scalar optimization subproblems and then uses the evolutionary algorithm (EA) to optimize these subproblems simultaneously. However, there is a great deal of randomness in MOEA/D. Researchers in the field of evolutionary algorithm, reinforcement learning and neural network have reported that the simultaneous consideration of randomness and opposition has an advantage over pure randomness. A new scheme, called opposition-based learning (OBL), has been proposed in the machine learning field. In this paper, OBL has been integrated into the framework of MOEA/D to accelerate its convergence speed. Hence, our proposed approach is called opposition-based learning MOEA/D (MOEA/D-OBL). Compared with MOEA/D, MOEA/D-OBL uses an opposition-based initial population and opposition-based learning strategy to generate offspring during the evolutionary process. It is compared with its parent algorithm MOEA/D on four representative kinds of MOPs and many-objective optimization problems. Experimental results indicate that MOEA/D-OBL outperforms or performs similar to MOEA/D. Moreover, the parameter sensitivity of generalization opposite point and the probable to use OBL is experimentally investigated. (C) 2014 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2014.04.068,Multi-objective optimization; Evolutionary algorithm; Decomposition; Opposition-based learning,,
An Ensemble Prediction System Based on Artificial Neural Networks and Deep Learning Methods for Deterministic and Probabilistic Carbon Price Forecasting,"Yang, Y; Guo, HG; Jin, Y; Song, AY",FRONTIERS IN ENVIRONMENTAL SCIENCE,2021.0,"Carbon price prediction is important for decreasing greenhouse gas emissions and coping with climate change. At present, a variety of models are widely used to predict irregular, nonlinear, and nonstationary carbon price series. However, these models ignore the importance of feature extraction and the inherent defects of using a single model; thus, accurate and stable prediction of carbon prices by relevant industry practitioners and the government is still a huge challenge. This research proposes an ensemble prediction system (EPS) that includes improved data feature extraction technology, three prediction submodels (GBiLSTM, CNN, and ELM), and a multiobjective optimization algorithm weighting strategy. At the same time, based on the best fitting distribution of the prediction error of the EPS, the carbon price prediction interval is constructed as a way to explore its uncertainty. More specifically, EPS integrates the advantages of various submodels and provides more accurate point prediction results; the distribution function based on point prediction error is used to establish the prediction interval of carbon prices and to mine and analyze the volatility characteristics of carbon prices. Numerical simulation of the historical data available for three carbon price markets is also conducted. The experimental results show that the ensemble prediction system can provide more effective and stable carbon price forecasting information and that it can provide valuable suggestions that enterprise managers and governments can use to improve the carbon price market.",10.3389/fenvs.2021.740093,carbon price forecasting; ensemble prediction system; deep learning methods; error distribution function; multiobjective optimization algorithm,,
GuacaMol: Benchmarking Models for de Novo Molecular Design,"Brown, N; Fiscato, M; Segler, MHS; Vaucher, AC",JOURNAL OF CHEMICAL INFORMATION AND MODELING,2019.0,"De novo design seeks to generate molecules with required property profiles by virtual design-make-test cycles. With the emergence of deep learning and neural generative models in many application areas, models for molecular design based on neural networks appeared recently and show promising results. However, the new models have not been profiled on consistent tasks, and comparative studies to well established algorithms have only seldom been performed. To standardize the assessment of both classical and neural models for de novo molecular design, we propose an evaluation framework, GuacaMol, based on a suite of standardized benchmarks. The benchmark tasks encompass measuring the fidelity of the models to reproduce the property distribution of the training sets, the ability to generate novel molecules, the exploration and exploitation of chemical space, and a variety of single and multiobjective optimization tasks. The benchmarking open-source Python code and a leaderboard can be found on https://benevolent.ai/guacamol.",10.1021/acs.jcim.8b00839,,,
Integrated vehicle dynamics management for distributed-drive electric vehicles with active front steering using adaptive neural approaches against unknown nonlinearity,"Huang, W; Wong, PK",INTERNATIONAL JOURNAL OF ROBUST AND NONLINEAR CONTROL,2019.0,"This paper proposes a new integrated vehicle dynamics management for enhancing the yaw stability and wheel slip regulation of the distributed-drive electric vehicle with active front steering. To cope with the unknown nonlinear tire dynamics with uncertain disturbances in integrated control problem of vehicle dynamics, a neuro-adaptive predictive control is therefore proposed for multiobjective coordination of constrained systems with unknown nonlinearity. Unknown nonlinearity with unmodeled dynamics is modeled using a random projection neural network via adaptive machine learning, where a new adaptation law is designed in premise of Lyapunov stability. Given the computational efficiency, a neurodynamic method is extended to solve the constrained programming problem with unknown nonlinearity. To test the performance of the proposed control method, simulations were conducted using a validated vehicle model. Simulation results show that the proposed neuro-adaptive predictive controller outperforms the classical model predictive controller in tracking nominal wheel slip ratio, desired vehicle yaw rate and sideslip angle, showing its significance in vehicle yaw stability enhancement and wheels slip regulation.",10.1002/rnc.4657,adaptive neural network; distributed-drive electric vehicles; integrated vehicle dynamics management; nonlinear predictive control,,
The Future of Sensitivity Analysis: An essential discipline for systems modeling and policy support,"Razavi, S; Jakeman, A; Saltelli, A; Prieur, C; Iooss, B; Borgonovo, E; Plischke, E; Lo Piano, S; Iwanaga, T; Becker, W; Tarantola, S; Guillaume, JHA; Jakeman, J; Gupta, H; Melillo, N; Rabitti, G; Chabridon, V; Duan, QY; Sun, XF; Smith, S; Sheikholeslami, R; Hosseini, N; Asadzadeh, M; Puy, A; Kucherenko, S; Maier, HR",ENVIRONMENTAL MODELLING & SOFTWARE,2021.0,"Sensitivity analysis (SA) is en route to becoming an integral part of mathematical modeling. The tremendous potential benefits of SA are, however, yet to be fully realized, both for advancing mechanistic and data-driven modeling of human and natural systems, and in support of decision making. In this perspective paper, a multidisciplinary group of researchers and practitioners revisit the current status of SA, and outline research challenges in regard to both theoretical frameworks and their applications to solve real-world problems. Six areas are discussed that warrant further attention, including (1) structuring and standardizing SA as a discipline, (2) realizing the untapped potential of SA for systems modeling, (3) addressing the computational burden of SA, (4) progressing SA in the context of machine learning, (5) clarifying the relationship and role of SA to uncertainty quantification, and (6) evolving the use of SA in support of decision making. An outlook for the future of SA is provided that underlines how SA must underpin a wide variety of activities to better serve science and society.",10.1016/j.envsoft.2020.104954,Sensitivity analysis; Mathematical modeling; Machine learning; Uncertainty quantification; Decision making; Model validation and verification; Model robustness; Policy support,,
Rapid Process Modeling of the Aerosol Jet Printing Based on Gaussian Process Regression with Latin Hypercube Sampling,"Zhang, HN; Moon, SK; Ngo, TH; Tou, JJ; Yusoff, MAB",INTERNATIONAL JOURNAL OF PRECISION ENGINEERING AND MANUFACTURING,2020.0,"Aerosol jet printing (AJP) technology is a relatively new 3D printing technology for producing customized microelectronic components due to its high design flexibility and fine feature deposition. However, complex interactions between machine, process parameters and materials will influence line morphology and remain a challenge on modeling effectively. And the system drift which induced by many changing and uncertain factors will affect the printing process significantly. Hence, it is necessary to develop a small data set based machine learning approach to model relationship between the process parameters and the line morphology. In this paper, we propose a rapid process modeling method for AJP process and consider sheath gas flow rate, carrier gas flow rate, stage speed as AJP process parameters, and line width and line roughness as the line morphology. Latin hypercube sampling is adopted to generate experimental points. And, Gaussian process regression (GPR) is used for modeling the AJP process because GPR has the capability of providing the prediction uncertainty in terms of variance. The experimental result shows that the proposed GPR model has competitive modeling accuracy comparing to the other regression models.",10.1007/s12541-019-00237-3,3D printing; Aerosol jet process; Gaussian process regression; Machine learning; Process modeling,,
Risk Averseness and Emotional Stability in e-Commerce,"Popchev, I; Ketipov, R; Angelova, V",CYBERNETICS AND INFORMATION TECHNOLOGIES,2021.0,"The study aims to examine the issue of the relationship between Emotional stability, one of the fundamental personality determinants, and users' Risk Averseness, on the one hand, and user behavior in the field of e-Commerce, on the other hand. In the beginning, a brief overview of today's primary benchmark for the measurement of human personality - the Big Five Model is proposed. A study with 226 participants has been conducted for the aim of the research, based on the TIPI test. The TIPI test is a validated and abridged version of the Five-Factor model. The result of the conducted survey confirms the existence of significant relationships between personality determinant Emotional stability and consumer's Risk awareness, on one side, and some of the observed main functionalities of the online stores, on the other side. Two regression models of the field of Machine Learning (Linear Regression and Random Forest) have been implemented to make a reliable forecast about the user's preferences in the process of online shopping. The conclusions made rely on the obtained results and analysis.",10.2478/cait-2020-0030,Personality; emotional stability; risk averseness; TIPI; machine learning; e-Commerce; consumer behavior,,
How can statistical and artificial intelligence approaches predict piping erosion susceptibility?,"Hosseinalizadeh, M; Kariminejad, N; Rahmati, O; Keesstra, S; Alinejad, M; Behbahani, AM",SCIENCE OF THE TOTAL ENVIRONMENT,2019.0,"It is of fundamental importance to model the relationship between geo-environmental factors and piping erosion because of the environmental degradation attributed to soil loss. Methods that identify areas prone to piping erosion at the regional scale are limited. The main objective of this research is to develop a novel modeling approach by using three machine learning algorithms-mixture discriminant analysis (MDA), flexible discriminant analysis (FDA), and support vector machine (SVM) in addition to an unmanned aerial vehicle (UAV) images to map susceptibility to piping erosion in the loess-covered hilly region of Golestan Province, Northeast Iran. In this research, we have used 22 geo-environmental indices/factors and 345 identified pipes as predictors and dependent variables. The piping susceptibility maps were assessed by the area under the ROC curve (AUC). Validation of the results showed that the AUC for the three mentioned algorithms varied from 90.32% to 92.45%. We concluded that the proposed approach could efficiently produce a piping susceptibility map. (c) 2018 Elsevier B.V. All rights reserved.",10.1016/j.scitotenv.2018.07.396,Piping collapse; Unmanned aerial vehicle (UAV); Susceptibility map; Machine learning algorithms; Loess plateau,,
Learning Multimodal Parameters: A Bare-Bones Niching Differential Evolution Approach,"Gong, YJ; Zhang, J; Zhou, YC",IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS,2018.0,"Most learning methods contain optimization as a substep, where the nondifferentiability and multimodality of objectives push forward the interplay of evolutionary optimization algorithms and machine learning models. The recently emerged evolutionary multimodal optimization (MMOP) technique enables the learning of diverse sets of effective parameters for the models simultaneously, providing new opportunities to the applications requiring both accuracy and diversity, such as ensemble, interactive, and interpretive learning. Targeting at locating multiple optima simultaneously in the multimodal landscape, this paper develops an efficient neighborhood-based niching algorithm. Bare-bones differential evolution is used as the baseline. Further, using Gaussian mutation with local mean and standard deviations, the neighborhoods capture niches that match well with the contours of peaks in the landscape. To increase diversity and enhance global exploration, the proposed algorithm embeds a diversity preserving operator to reinitialize converged or overlapped neighborhoods. The experimental results verify that the proposed algorithm has superior and consistent performance for a wide range of MMOP problems. Further, the algorithm has been successfully applied to train neural network ensembles, which validates its effectiveness and benefits of learning multimodal parameters.",10.1109/TNNLS.2017.2708712,Fitness landscape; Gaussian model; multimodal optimization (MMOP); neighborhood strategy; neural network ensemble (NNE); niching,,
EMONAS: Efficient Multiobjective Neural Architecture Search Framework for 3D Medical Image Segmentation,"Calisto, MGB; Lai-Yuen, SK",MEDICAL IMAGING 2021: IMAGE PROCESSING,2021.0,"Deep learning plays a critical role in medical image segmentation. Nevertheless, manually designing a neural network for a specific segmentation problem is a very difficult and time-consuming task due to the massive hyperparameter search space, long training time and large volumetric data. Therefore, most designed networks are highly complex, task specific and over-parametrized. Recently, multiobjective neural architecture search (NAS) methods have been proposed to automate the design of accurate and efficient segmentation architectures. However, they only search for either the macro- or micro-structure of the architecture, and do not use the information produced during the optimization process to increase the efficiency of the search. In this work, we propose EMONAS, an Efficient MultiObjective Neural Architecture Search framework for 3D medical image segmentation. EMONAS is composed of a search space that considers both the macro- and micro-structure of the architecture, and a surrogate-assisted multiobjective evolutionary based algorithm that efficiently searches for the best hyperparameters using a Random Forest surrogate and guiding selection probabilities. EMONAS is evaluated on the task of cardiac segmentation from the ACDC MICCAI challenge. The architecture found is ranked within the top 10 submissions in all evaluation metrics, performing better or comparable to other approaches while reducing the search time by more than 50% and having considerably fewer number of parameters.",10.1117/12.2577088,Medical Image Segmentation; Deep Learning; Neural Architecture Search; Hyperparameter Optimization; Multiobjective Optimization,,
Use of optimised MLP neural networks for spatiotemporal estimation of indoor environmental conditions of existing buildings,"Martinez-Comesana, M; Ogando-Martinez, A; Troncoso-Pastoriza, F; Lopez-Gomez, J; Febrero-Garrido, L; Granada-Alvarez, E",BUILDING AND ENVIRONMENT,2021.0,"Controlling the indoor environmental quality in real time is essential for the health, well-being and productivity of occupants of a building. In recent years, research has focused on improving monitoring devices and strategies and developing techniques for estimating indoor conditions. The use of machine learning algorithms in this context has increased considerably. However, monitoring data in real time from large multizone working areas is challenging. The aim of this work is to provide an interpolation methodology based on the use of optimised multilayered perceptron neural networks to estimate the indoor environmental conditions of a building in real time. These estimations are obtained without the need for neither monitoring in the occupied working area nor human intervention and considering low-cost sensors. The neural network is optimised by implementing the multiobjective genetic algorithm NSGA-II to find the best architecture in terms of error and complexity. This method was applied to the building of a research centre in north-western Spain, where interpolated values for indoor air temperature, relative humidity and CO2 concentration were obtained. The results of this case study yielded relative errors close to 6% for temperature, 5% for relative humidity, and 12% for CO2 concentration. These values validate the methodology developed for the estimation of indoor environmental conditions and the contribution of this research to the improvement of the monitoring and control of the indoor environmental quality of a building.",10.1016/j.buildenv.2021.108243,Air quality; IEQ; Interpolation; MLP; NSGA-II; Thermal comfort,,
COKO III: The Cooper-Koz Chess Program,"Kozdrowicki E.W., Cooper D.W.",Communications of the ACM,1973.0,"OKO III is a chess player written entirely in Fortran. On the IBM 360-65, COKO III plays a minimal chess game at the rate of.2 sec cpu time per move, with a level close to lower chess club play. A selective tree searching procedure controlled by tactical chess logistics allows a deployment of multiple minimal game calculations to achieve some optimal move selection. The tree searching algorithms are the heart of COKO's effectiveness, yet they are conceptually simple. In addition, an interesting phenomenon called a tree searching catastrophe has plagued COKO's entire development just as it troubles a human player. Standard exponential growth is curbed to a large extent by the definition and trimming of the Fisher set. A clear distinction between tree pruning and selective tree searching is also made. Representation of the chess environment is described along with a strategical preanalysis procedure that maps the Lasker regions. Specific chess algorithms are described which could be used as a command structure by anyone desiring to do some chess program experimentation. A comparison is made of some mysterious actions of human players and COKO III. © 1973, ACM. All rights reserved.",10.1145/362280.362288,alpha beta; artificial intelligence; auxiliary minimal game; chess algorithms; command structure; computer chess tournament; concept formation; Fischer set; game playing; heuristic programming; Lasker regions; machine learning; minimal chess game; minimax; selective searching; strategical; tactical; tactical control mode; tree searching; tree searching catastrophe,4.0,
Gram-Schmidt orthogonalization neural nets for O.C.R,"Szu Harold, Scheff Kim",,1989.0,"A description is given of a three-layer neural network for pattern classification/character recognition. The first layer is a heteroassociative feedforward network with bipolar output (±1) and zero threshold neurons. The second layer is an autoassociative memory whose input-output characteristics are the same as those in the first layer. The third layer is used to recognize the pattern and control whether the new orthogonal feature vector should be installed by the outer product formula to increase the memory capacity to M′ = M + 1. With this network, conventional pattern recognition of the minimax type is used to determine the initial interconnection matrix. The samples are classified by means of supervised learning. Only a single physical layer need be built, since the same layer can repeatedly be used three times in series. The performance of the network is studied.",10.1109/ijcnn.1989.118632,,1.0,
Learning and exploitation do not conflict under minimax optimality,Szepesvári C.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1997.0,We show that adaptive real time dynamic programming extended with the action selection strategy which chooses the best action according to the latest estimate of the cost function yields asymptotically optimal policies within finite time under the minimax optimality criterion. From this it follows that learning and exploitation do not conflict under this special optimality criterion. We relate this result to learning optimal strategies in repeated two-player zero-sum deterministic games. © Springer-Verlag Berlin Heidelberg 1997.,10.1007/3-540-62858-4_89,Dynamic games; Reinforcement learning; Self-optimizing systems,6.0,
Seeding genetic programming populations,"Langdon W.B., Nordin J.P.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2000.0,"We show genetic programming (GP) populations can evolve under the influence of a Pareto multi-objective fitness and program size selection scheme, from “perfect.” programs which match the training material to general solutions. The technique is demonstrated with programmatic image compression, two machine learning benchmark problems (Pima Diabetes and Wisconsin Breast Cancer) and an insurance customer profiling task (Benelearn99 data mining). © Springer-Verlag Berlin Heidelberg 2000.",10.1007/978-3-540-46239-2_23,,34.0,
A new distributed reinforcement learning algorithm for multiple objective optimization problems,"Mariano C., Morales E.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2000.0,"This paper describes a new algorithm, called MDQL, for the solution of multiple objective optimization problems. MDQL is based on a new distributed Q-learning algorithm, called DQL, which is also introduced in this paper. In DQL a family of independent agents, exploring different options, finds a common policy in a common environment. Information about action goodness is transmitted using traces over state-action pairs. MDQL extends this idea to multiple objectives, assigning a family of agents for each objective involved. A non-dominant criterion is used to construct Pareto fronts and by delaying adjustments on the rewards MDQL achieves better distributions of solutions. Furthermore, an extension for applying reinforcement learning to continuous functions is also given. Successful results of MDQL on several test-bed problems suggested in the literature are described. © Springer-Verlag 2000.",10.1007/3-540-44399-1_30,,14.0,
Neural minimax classifiers,"Alaiz-Rodríguez R., Cid-Sueiro J.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2002.0,"Many supervised learning algorithms are based on the assumption that the training data set reflects the underlying statistical model of the real data. However, this stationarity assumption may be partially violated in practice: for instance, if the cost of collecting data is class dependent, the class priors of the training data set may be different from that of the test set. A robust solution to this problem is selecting the classifier that minimize the error probability under the worst case conditions. This is known as the minimax strategy. In this paper we propose a mechanism to train a neural network in order to estimate the minimax classifier that is robust to changes in the class priors. This procedure is illustrated on a softmax-based neural network, although it can be applied to other structures. Several experimental results show the advantages of the proposed methods with respect to other approaches. © Springer-Verlag Berlin Heidelberg 2002.",10.1007/3-540-46084-5_66,,,
Minimax fuzzy Q-learning in cooperative multi-agent systems,"Kilic A., Arslan A.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2002.0,"Recently, delayed reinforcement learning (RL) has been proposed as a strong method for learning in multi-agent systems (MASs). In this method, agents are concerned with the problem of discovering an optimal policy, a function mapping states to actions. The most popular RL technique, Q-learning, has been proven to produce an optimal policy under certain conditions. In this paper, we consider a multi-agent cooperation problem, and propose a multiagent reinforcement learning method based on the other agents’ actions. In our learning method, the agent under consideration observes other agents’ action, and uses the minimax Q-learning using fuzzy state and fuzzy goal representation for updating fuzzy Q values. © Springer-Verlag Berlin Heidelberg 2002.",10.1007/3-540-36077-8_27,,3.0,
Evolutionary model selection in unsupervised learning,"Kim Y., Street W.N., Menczer F.",Intelligent Data Analysis,2002.0,"Feature subset selection is important not only for the insight gained from determining relevant modeling variables but also for the improved understandability, scalability, and possibly, accuracy of the resulting models. Feature selection has traditionally been studied in supervised learning situations, with some estimate of accuracy used to evaluate candidate subsets. However, we often cannot apply supervised learning for lack of a training signal. For these cases, we propose a new feature selection approach based on clustering. A number of heuristic criteria can be used to estimate the quality of clusters built from a given feature subset. Rather than combining such criteria, we use ELSA, an evolutionary local selection algorithm that maintains a diverse population of solutions that approximate the Pareto front in a multi-dimensional objective space. Each evolved solution represents a feature subset and a number of clusters; two representative clustering algorithms, K-means and EM, are applied to form the given number of clusters based on the selected features. Experimental results on both real and synthetic data show that the method can consistently find approximate Pareto-optimal solutions through which we can identify the significant features and an appropriate number of clusters. This results in models with better and clearer semantic relevance. © 2002-IOS Press. All rights reserved.",10.3233/ida-2002-6605,,76.0,
Pareto neuro-ensembles,Abbass H.A.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2003.0,"The formation of a neural network ensemble has attracted much attention in the machine learning literature. A set of trained neural networks are combined using a post-gate to form a single super-network. One main challenge in the literature is to decide on which network to include in, or exclude from the ensemble. Another challenge is how to define an optimum size for the ensemble. Some researchers also claim that for an ensemble to be effective, the networks need to be different. However, there is not a consistent definition of what “different” means. Some take it to mean weakly correlated networks, networks with different bias-variance trade-off, and/or networks which are specialized on different parts of the input space. In this paper, we present a theoretically sound approach for the formation of neural network ensembles. The approach is based on the dominance concept that determines which network to include/exclude, identifies a suitable size for the ensemble, and provides a mechanism for quantifying differences between networks. The approach was tested on a number of standard dataset and showed competitive results. © Springer-Verlag Berlin Heidelberg 2003.",10.1007/978-3-540-24581-0_47,,28.0,
Model-based reinforcement learning for alternating markov games,Mellor D.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2003.0,"Online training is a promising technique for training reinforcement learning agents to play strategy board games over the internet against human opponents. But the limited training experience that can be generated by playing against real humans online means that learning must be data-efficient. Data-efficiency has been achieved in other domains by augmenting reinforcement learning with a model: model-based reinforcement learning. In this paper the Minimax-MBTD algorithm is presented, which extends model-based reinforcement learning to deterministic alternating Markov games, a generalisation of two-player zerosum strategy board games like chess and Go. By using a minimax measure of optimality the strategy learnt generalises to arbitrary opponents, unlike approaches that explicitly model specific opponents. Minimax- MBTD is applied to Tic-Tac-Toe and found to converge faster than direct reinforcement learning, but focussing planning on successors to the current state resulted in slower convergence than unfocussed random planning. © Springer-Verlag Berlin Heidelberg 2003.",10.1007/978-3-540-24581-0_44,Game playing; Machine learning; Planning; Reinforcement learning; Search,,
Prediction of tumor outcome based on gene expression data,"Liu J., Iba H.",Wuhan University Journal of Natural Sciences,2004.0,"Gene expression microarray data can be used to classify tumor types. We proposed a new procedure to classify human tumor samples based on microarray gene expressions by using a hybrid supervised learning method called MOEA-WV (Multi-Objective Evolutionary Algorithm + Weighted Voting). MOEA is used to search for a relatively few subsets ill informative genes from the high-dimensional gene space, mid WV is used as a classification toot. This new method has been applied to predicate the subtypes of lymphoma and outlines of medulloblastoma. The results are relatively accurate and meaningful compared to those from other methods.",10.1007/bf02830598,Bioinformatics; MOEA; Pareto optimization; Tumor classification,1.0,
Pareto-optimal patterns in logical analysis of data,"Hammer P.L., Kogan A., Simeone B., Szedmák S.",Discrete Applied Mathematics,2004.0,"Patterns are the key building blocks in the logical analysis of data (LAD). It has been observed in empirical studies and practical applications that some patterns are more ""suitable"" than others for use in LAD. In this paper, we model various such suitability criteria as partial preorders defined on the set of patterns. We introduce three such preferences, and describe patterns which are Pareto-optimal with respect to any one of them, or to certain combinations of them. We develop polynomial time algorithms for recognizing Pareto-optimal patterns, as well as for transforming an arbitrary pattern to a better Pareto-optimal one with respect to any one of the considered criteria, or their combinations. We obtain analytical representations characterizing some of the sets of Pareto-optimal patterns, and investigate the computational complexity of generating all Pareto-optimal patterns. The empirical evaluation of the relative merits of various types of Pareto-optimality is carried out by comparing the classification accuracy of Pareto-optimal theories on several real life data sets. This evaluation indicates the advantages of ""strong patterns"", i.e. those patterns which are Pareto-optimal with respect to the ""evidential preference"" introduced in this paper. © 2004 Elsevier B.V. All rights reserved.",10.1016/j.dam.2003.08.013,"Boolean functions; Classification accuracy; Extremal patterns, Data mining; Machine learning",60.0,
Semi-supervised feature selection via multiobjective optimization,"Handl J., Knowles J.",IEEE International Conference on Neural Networks - Conference Proceedings,2006.0,"In previous work, we have shown that both unsupervised feature selection and the semi-supervised clustering problem can be usefully formulated as multiobjective optimization problems. In this paper, we discuss the logical extension of this prior work to cover the problem of semi-supervised feature selection. Our extensive experimental results provide evidence for the advantages of semi-supervised feature selection when both labelled and unlabelled data are available. Moreover, the particular effectiveness of a Pareto-based optimization approach can also be seen. © 2006 IEEE.",10.1109/ijcnn.2006.247330,,17.0,
Pattern classification by evolutionary RBF networks ensemble based on multi-objective optimization,"Kondo N., Hatanaka T., Uosaki K.",IEEE International Conference on Neural Networks - Conference Proceedings,2006.0,"In this paper, evolutionary multi-objective selection method of RBF networks structure and its application to the ensemble learning is considered. The candidates of RBF network structure are encoded into the chromosomes in GAs. Then, they evolve toward Pareto-optimal front defined by several objective functions concerning with model accuracy, model complexity and model smoothness. RBF network ensemble is constructed of the obtained Pareto-optimal models since such models are diverse. This method is applied to the pattern classification problem. Experiments on the benchmark problem demonstrate that the proposed method has comparable generalization ability to conventional ensemble methods. © 2006 IEEE.",10.1109/ijcnn.2006.247224,,8.0,
Imbalanced learning in relevance feedback with biased minimax probability machine for image retrieval tasks,"Peng X., King I.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2006.0,"In recent years, Minimax Probability Machine (MPM) have demonstrated excellent performance in a variety of pattern recognition problems. At the same time various machine learning methods have been used on relevance feedback tasks in Content-based Image Retrieval (CBIR). One of the problems in typical techniques for relevance feedback is that they treat the relevant feedback and irrelevant feedback equally. In other words, the negative instances largely outnumber the positive instances. Hence, the assumption that they are balanced is incorrect. In this paper we study how MPM can be applied to image retrieval, more precisely, Biased MPM during the relevance feedback iterations. We formulate the relevance feedback based on a modified MPM called Biased Minimax Probability Machine (BMPM). Different from previous methods, this model directly controls the accuracy of classification of the future data to build up biased classifiers. Hence, it provides a rigorous treatment on imbalanced data. Mathematical formulation and explanations are provided for showing the advantages. Experiments are conducted to evaluate the performance of our proposed framework, in which encouraging and promising experimental results are obtained. © Springer-Verlag Berlin Heidelberg 2006.",10.1007/11893028_39,,2.0,
Pareto-coevolutionary genetic programming classifier,"Lemczyk V., Heywood M.",GECCO 2006 - Genetic and Evolutionary Computation Conference,2006.0,"The conversion and extension of the Incremental Pareto-Coevolution Archive algorithm (IPCA) into the domain of Genetic Programming classifier evolution is presented. In order to accomplish efficiency in regards to classifier evaluation on training data, the coevolutionary aspect of the IPCA algorithm is utilized to simultaneously evolve a subset of the training data that provides distinctions between candidate classifiers. The algorithm is compared in terms of classification ""score"" (equal weight to detection rate, and 1 - false positive rate), and run-time against a traditional GP classifier using the entinety of the training data for evaluation, and a GP classifier which performs Dynamic Subset Selection. The results indicate that the presented algorithm outperforms the subset, selection algorithm in terms of classification score, and outperforms the traditional classifier while requiring roughly 1/430 of the wall-clock time.",10.1145/1143997.1144162,Co-evolution; Evolutionary Computation; Genetic Programming; Subset Selection; Supervised Learning,4.0,
Information preserving multi-objective feature selection for unsupervised learning,"Mierswa I., Wurst M.",GECCO 2006 - Genetic and Evolutionary Computation Conference,2006.0,"In this work we propose a novel, sound framework for evolutionary feature selection in unsupervised machine learning problems. We show that unsupervised feature selection is inhemulti-objectiverently multi-objective and behaves differently from supervised feature selection in that the number of features must be maximized instead of being minimized. Although this might sound surprising from a supervised learning point of view, we exemplify this relationship on the problem of data clustering and show that existing approaches do not pose the optimization problem in an appropriate way. Another important consequence of this paradigm change is a method which segments the Pareto sets produced by our approach. Inspecting only prototypical points from these segments drastically reduces the amount of work for selecting a final solution. We compare our methods against existing approaches on eight data sets. Copyright 2006 ACM.",10.1145/1143997.1144248,Multi-objective feature selection; Pareto front segmentation; Unsupervised learning,34.0,
Learning long-term chess strategies from databases,"Sadikov A., Bratko I.",Machine Learning,2006.0,"We propose an approach to the learning of long-term plans for playing chess endgames. We assume that a computer-generated database for an endgame is available, such as the king and rook vs. king, or king and queen vs. king and rook endgame. For each position in the endgame, the database gives the ""value"" of the position in terms of the minimum number of moves needed by the stronger side to win given that both sides play optimally. We propose a method for automatically dividing the endgame into stages characterised by different objectives of play. For each stage of such a game plan, a stage-specific evaluation function is induced, to be used by minimax search when playing the endgame. We aim at learning playing strategies that give good insight into the principles of playing specific endgames. Games played by these strategies should resemble human expert's play in achieving goals and subgoals reliably, but not necessarily as quickly as possible.",10.1007/s10994-006-6747-7,Chess da-ta-ba-ses; Chess endgames; Computer chess; Long-term Strategy; Machine learning,6.0,
A DC-programming algorithm for kernel selection,"Argyriou A., Hauser R., Micchelli C.A., Pontil M.",ACM International Conference Proceeding Series,2006.0,"We address the problem of learning a kernel for a given supervised learning task. Our approach consists in searching within the convex hull of a prescribed set of basic kernels for one which minimizes a convex regularization functional. A unique feature of this approach compared to others in the literature is that the number of basic kernels can be infinite. We only require that they are continuously parameterized. For example, the basic kernels could be isotropic Gaussians with variance in a prescribed interval or even Gaussians parameterized by multiple continuous parameters. Our work builds upon a formulation involving a minimax optimization problem and a recently proposed greedy algorithm for learning the kernel. Although this optimization problem is not convex, it belongs to the larger class of DC (difference of convex functions) programs. Therefore, we apply recent results from DC optimization theory to create a new algorithm for learning the kernel. Our experimental results on benchmark data sets show that this algorithm outperforms a previously proposed method.",10.1145/1143844.1143850,,31.0,
Incremental refinement of solutions for dynamic multi objective optimization problems,"Mariano-Romero C.E., Morales M E.F.","Proceedings - 2007 6th Mexican International Conference on Artificial Intelligence, Special Session, MICAI 2007",2007.0,"MDQL is an algorithm, based on reinforcement learning, for solving multiple objective optimization problems, that has been tested on several applications with promising results [6, 7]. MDQL discretizes the decision variables into a set of states, each associated with actions to move agents to contiguous states. A group of agents explore this state space and are able to find Pareto sets applying a distributed reinforcement learning algorithm. The precision of the Pareto solutions depends on the chosen granularity of the states. A finer granularity on the states creates more precise solutions but at the expense of a larger search space, and consequently the need for more computational resources. In this paper, a very important improvement is introduced into the original MDQL algorithm to incrementally refined the Pareto solutions. The new algorithm, called IMDQL, starts with a coarse granularity to find an initial Pareto set. A vicinity for each of the Pareto solutions in refined and a new Pareto set is founded in this refined state space. This process continues until there is no more improvement within a small threshold value. It is shown that IMDQL not only improves the solutions found by MDQL, but also converges faster. MDQL has also been tested on the solutions of dynamic optimization problems. In this paper, it is also shown that the adaptation capabilities observed in MDQL can be improved with IMDQL. IMDQL was tested on the benchmark problems proposed by Jin [4]. Performance evaluation was made using the Collective Mean Fitness metric proposed by Morrison [10]. IMDQL was compared against an standard evolution strategy with the covariance matrix adaptation (CMA-ES) with very promising results. © 2008 IEEE.",10.1109/MICAI.2007.47,,1.0,
An agent-based approach to the multiple-objective selection of reference vectors,"Czarnowski I., Jȩdrzejowicz P.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2007.0,"The paper proposes an agent-based approach to the multipleobjective selection of reference vectors from original datasets. Effective and dependable selection procedures are of vital importance to machine learning and data mining. The suggested approach is based on the multiple agent paradigm. The authors propose using JABAT middleware as a tool and the original instance reduction procedure as a method for selecting reference vectors under multiple objectives. The paper contains a brief introduction to the multiple objective optimization, followed by the formulation of the multiple-objective, agent-based, reference vectors selection optimization problem. Further sections of the paper provide details on the proposed algorithm generating a non-dominated (or Pareto-optimal) set of reference vector sets. To validate the approach the computational experiment has been planned and carried out. Presentation and discussion of experiment results conclude the paper. © Springer-Verlag Berlin Heidelberg 2007.",10.1007/978-3-540-73499-4_10,,7.0,
Supervised tensor learning,"Tao D., Li X., Wu X., Hu W., Maybank S.J.",Knowledge and Information Systems,2007.0,"Tensor representation is helpful to reduce the small sample size problem in discriminative subspace selection. As pointed by this paper, this is mainly because the structure information of objects in computer vision research is a reasonable constraint to reduce the number of unknown parameters used to represent a learning model. Therefore, we apply this information to the vector-based learning and generalize the vector-based learning to the tensor-based learning as the supervised tensor learning (STL) framework, which accepts tensors as input. To obtain the solution of STL, the alternating projection optimization procedure is developed. The STL framework is a combination of the convex optimization and the operations in multilinear algebra. The tensor representation helps reduce the overfitting problem in vector-based learning. Based on STL and its alternating projection optimization procedure, we generalize support vector machines, minimax probability machine, Fisher discriminant analysis, and distance metric learning, to support tensor machines, tensor minimax probability machine, tensor Fisher discriminant analysis, and the multiple distance metrics learning, respectively. We also study the iterative procedure for feature extraction within STL. To examine the effectiveness of STL, we implement the tensor minimax probability machine for image classification. By comparing with minimax probability machine, the tensor version reduces the overfitting problem. © Springer-Verlag London Limited 2006.",10.1007/s10115-006-0050-6,Alternating projection; Convex optimization; Supervised learning; Tensor,286.0,
Neighbor search with global geometry: A minimax message passing algorithm,"Kim K.-H., Choi S.",ACM International Conference Proceeding Series,2007.0,"Neighbor search is a fundamental task in machine learning, especially in classification and retrieval. Efficient nearest neighbor search methods have been widely studied, with their emphasis on data structures but most of them did not consider the underlying global geometry of a data set. Recent graph-based semi-supervised learning methods capture the global geometry, but suffer from scalability and parameter tuning problems. In this paper we present a (nearest) neighbor search method where the underlying global geometry is incorporated and the parameter tuning is not required. To this end, we introduce deterministic walks as a deterministic counterpart of Markov random walks, leading us to use the minimax distance as a global dissimilarity measure. Then we develop a message passing algorithm for efficient minimax distance calculation, which scales linearly in both time and space. Empirical study reveals the useful behavior of the method in image retrieval and semi-supervised learning.",10.1145/1273496.1273547,,18.0,
Controlling overfitting with multi-objective support vector machines,Mierswa I.,Proceedings of GECCO 2007: Genetic and Evolutionary Computation Conference,2007.0,"Recently, evolutionary computation has been successfully integrated into statistical learning methods. A Support Vector Machine (SVM) using evolution strategies for its optimization problem frequently deliver better results with respect to the optimization criterion and the prediction accuracy. Moreover, evolutionary computation allows for the efficient large margin optimization of a huge family of new kernel functions, namely non-positive semi definite kernels as the Epanechnikov kernel. For these kernel functions, evolutionary SVM even outperform other learning methods like the Relevance Vector Machine. In this paper, we will discuss another major advantage of evolutionary SVM compared to traditional SVM solutions: we can explicitly optimize the inherent trade-off between training error and model complexity by embedding multi-objective optimization into the evolutionary SVM. This leads to three advantages: first, it is no longer necessary to tune the SVM parameter C which weighs both conflicting criteria. This is a very time-consuming task for traditional SVM. Second, the shape and size of the Pareto front give interesting insights about the complexity of the learning task at hand. Finally, the user can actually see the point where overfitting occurs and can easily select a solution from the Pareto front best suiting his or her needs. Copyright 2007 ACM.",10.1145/1276958.1277323,Evolution strategies; Kernel methods; Machine learning; Support vector machines,20.0,
Pareto-coevolutionary genetic programming for problem decomposition in multi-class classification,"Lichodzijewski P., Heywood M.I.",Proceedings of GECCO 2007: Genetic and Evolutionary Computation Conference,2007.0,"A bid-based approach for coevolving Genetic Programming classifiers is presented. The approach coevolves a population of learners thatdecompose the instance space by way of their aggregate bidding behaviour. To reduce computation overhead, a small, relevant, subsetof training exemplars is (competitively) coevolved alongside the learners. The approach solves multi-class problems using a single population and is evaluated on three large datasets. It is found tobe competitive, especially compared to classifier systems, whilesignificantly reducing the computation overhead associated withtraining. Copyright 2007 ACM.",10.1145/1276958.1277058,Classification; Coevolution; Genetic programming; Problem decomposition; Subset selection; Supervised learning; Training efficiency,17.0,
Incremental refinement of solutions for multiple objective optimization problems,"Mariano C.E., Alcocer V.H., Morales E.F.",Proceedings of GECCO 2007: Genetic and Evolutionary Computation Conference,2007.0,"MDQL is an algorithm, based on reinforcement learning, for solving multiple objective optimization problems, that has been tested on several applications with promising results [1]. MDQL discretizes the decision variables into a set of states, each associated with actions to move agents to contiguous states. A group of agents explore this state space and are able to find Pareto sets applying a distributed reinforcement learning algorithm. The precision of the Pareto solutions depends on the chosen granularity of the states. A finer granularity on the states creates more precise solutions but at the expense of a larger search space, and consequently the need for more computational resources. An important improvement is presented. The new algorithm, called IMDQL, starts with a coarse granularity to find an initial Pareto set. A vicinity for each of the Pareto solutions in refined and a new Pareto set is founded in this refined state space. It is shown that IMDQL not only improves the solutions found by MDQL, but also converges faster and is capable to approximate dynamic Pareto fronts. The main consideration in the application of IMDQL to dynamic environments is that the agents in the algorithm start from the Pareto solutions obtained. Agents start with a deterministic environment constructed with fixed values for the value functions for the first dynamic parameters; when convergence is reached and a Pareto set is obtained, a new cycle is started, changing to the next value for the dynamic parameters. Agents start searching (adapting solutions) from the existing environments which correspond to the Pareto solutions obtained for the previous value for the dynamic parameters. Searching for new solutions, from the last Pareto set, given the new values for the dynamic parameters, significantly reduces the convergence time. IMDQL is tested on a real water distribution network design involving water-reusing treatment plants and different contaminants concentrations [1]. In this problem, the concentration of contaminants can change over time so the search for optimal solutions becomes a continuous process. It is shown that IMDQL improves on the solutions found by MDQL with fixed concentrations and, that due to its incremental nature, it is able to adequately adjust its Pareto set solutions with dynamic changes in the contaminants con-centrations as long as they are within the vicinity of the previous Pareto set. This is, to our knowledge, the first multi-objective optimization algorithm that is able to dynamically adjust the Pareto set with changing conditions and that can adjust the accuracy of its solutions. The solutions were also compared against MDQL that was compared against a reduced gradient method using a weighted combination of the two objective functions [1], see Figures 1 and 2 for comparisons of Pareto fronts obtained with MDQL, ×, and IMDQL.",10.1145/1276958.1277140,Machine learning; Multiobjective optimization,,
"Acquisition of optimal operational strategies for water distribution systems using evolutionary algorithms, machine learning and preference oredering routine","Carrijo I.B., Reis L.F.R.",8th Annual Water Distribution Systems Analysis Symposium 2006,2007.0,"The efficient operation of a system is a fundamental tool to postpone the system's service life as much as possible, thus ensuring a good service to the consumer while keeping electrical energy and maintenance costs at acceptable levels. Efficient operation requires knowledge of the system, for this knowledge, supported by tools such as models for hydraulic simulation, optimization, and definition of rules, provides the operator with proper conditions for the rational operating of the system's units without depending exclusively on personal experience while maintaining the system's reliability. The purpose of this work is to present a methodology to achieve the optimal operation of water distribution systems, essentially macro systems (skeleton), concerning to the costs of operation and hydraulic benefits. It represents an attempt to provide adequate operation rules in order to minimize cost and maximize hydraulic benefits. Based on the knowledge of the system supported by technical and commercial geo-referenced records, the intention is hydraulically simulate the system using EPANET 2, optimize its operation though multi-objective genetic algorithms (MOGAs) and produce operational rules through machine learning (ML) and the preference ordering routine (POR). In this study an elitist multi-objective genetic algorithm, called strength Pareto evolutionary algorithm (SPEA) is used to obtain a Pareto front. The optimal operation analyses were conducted on the macro water system of the city of Goiania in Brazil. This was in order to minimize the cost of the electrical energy in the pump stations and to maximize the hydraulic benefits in terms of the required pressure at the demand nodes and of adequate reservoir levels. The results show that solutions for satisfactory operation can be quickly produced as a substitute to the personal judgment of the operator. Copyright ASCE 2006.",10.1061/40941(247)80,Genetic algorithms; Machine learning; Multi-objective optimization; Optimal operation; Preference ordering,,
Prescreening of candidate rules using association rule mining and pareto-optimality in genetic rule selection,"Ishibuchi H., Kuwajima I., Nojima Y.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2007.0,"Genetic rule selection is an approach to the design of classifiers with high accuracy and high interpretability. It searches for a small number of simple classification rules from a large number of candidate rules. The effectiveness of genetic rule selection strongly depends on the choice of candidate rules. If we have hundreds of thousands of candidate rules, it is very difficult to efficiently search for their good subsets. On the other hand, if we have only a few candidate rules, rule selection does not make sense. In this paper, we examine the use of Pareto-optimal and near Pareto-optimal rules with respect to support and confidence as candidate rules in genetic rule selection. © Springer-Verlag Berlin Heidelberg 2007.",10.1007/978-3-540-74827-4_64,Classifier design; Data mining; Evolutionary multiobjective optimization; Genetic rule selection; Multiobjective machine learning,13.0,
Managing team-based problem solving with symbiotic bid-based genetic programming,"Lichodzijewski P., Heywood M.I.",GECCO'08: Proceedings of the 10th Annual Conference on Genetic and Evolutionary Computation 2008,2008.0,"Bid-based Genetic Programming (GP) provides an elegant mechanism for facilitating cooperative problem decomposition without an a priori specification of the number of team members. This is in contrast to existing teaming approaches where individuals learn a direct input-output map (e.g., from exemplars to class labels), allowing the approach to scale to problems with multiple outcomes (classes), while at the same time providing a mechanism for choosing an outcome from those suggested by team members. This paper proposes a symbiotic relationship that continues to support the cooperative bid-based process for problem decomposition while making the credit assignment process much clearer. Specifically, team membership is defined by a team population indexing combinations of GP individuals in a separate team member population. A Pareto-based competitive coevolutionary component enables the approach to scale to large problems by evolving informative test points in a third population. The ensuing Symbiotic Bid-Based (SBB) model is evaluated on three large classification problems and compared to the XCS learning classifier system (LCS) formulation and to the support vector machine (SVM) implementation LIBSVM. On two of the three problems investigated the overall accuracy of the SBB classifiers was found to be competitive with the XCS and SVM results. At the same time, on all problems, the SBB classifiers were able to detect instances of all classes whereas the XCS and SVM models often ignored exemplars of minor classes. Moreover, this was achieved with a level of model complexity significantly lower than that identified by the SVM and XCS solutions. Copyright 2008 ACM.",10.1145/1389095.1389162,Active learning; Classification; Coevolution; Efficiency; Genetic programming; Problem decomposition; Supervised learning; Teaming,44.0,
Pareto analysis for the selection of classifier ensembles,"Dos Santos E.M., Sabourin R., Maupin P.",GECCO'08: Proceedings of the 10th Annual Conference on Genetic and Evolutionary Computation 2008,2008.0,"The overproduce-and-choose strategy involves the generation of an initial large pool of candidate classifiers and it is intended to test different candidate ensembles in order to select the best performing solution. The ensemble's error rate, ensemble size and diversity measures are the most frequent search criteria employed to guide this selection. By applying the error rate, we may accomplish the main objective in Pattern Recognition and Machine Learning, which is to find high-performance predictors. In terms of ensemble size, the hope is to increase the recognition rate while minimizing the number of classifiers in order to meet both the performance and low ensemble size requirements. Finally, ensembles can be more accurate than individual classifiers only when classifier members present diversity among themselves. In this paper we apply two Pareto front spread quality measures to analyze the relationship between the three main search criteria used in the overproduce-and-choose strategy. Experimental results conducted demonstrate that the combination of ensemble size and diversity does not produce conflicting multi-objective optimization problems. Moreover, we cannot decrease the generalization error rate by combining this pair of search criteria. However, when the error rate is combined with diversity or the ensemble size, we found that these measures are conflicting objective functions and that the performances of the solutions are much higher. Copyright 2008 ACM.",10.1145/1389095.1389229,Classifier ensembles; Diversity measures; Ensemble selection; Pareto analysis,10.0,
MODENAR: Multi-objective differential evolution algorithm for mining numeric association rules,"Alatas B., Akin E., Karci A.",Applied Soft Computing Journal,2008.0,"In this paper, a Pareto-based multi-objective differential evolution (DE) algorithm is proposed as a search strategy for mining accurate and comprehensible numeric association rules (ARs) which are optimal in the wider sense that no other rules are superior to them when all objectives are simultaneously considered. The proposed DE guided the search of ARs toward the global Pareto-optimal set while maintaining adequate population diversity to capture as many high-quality ARs as possible. ARs mining problem is formulated as a four-objective optimization problem. Support, confidence value and the comprehensibility of the rule are maximization objectives while the amplitude of the intervals which conforms the itemset and rule is minimization objective. It has been designed to simultaneously search for intervals of numeric attributes and the discovery of ARs which these intervals conform in only single run of DE. Contrary to the methods used as usual, ARs are directly mined without generating frequent itemsets. The proposed DE performs a database-independent approach which does not rely upon the minimum support and the minimum confidence thresholds which are hard to determine for each database. The efficiency of the proposed DE is validated upon synthetic and real databases. © 2007 Elsevier B.V. All rights reserved.",10.1016/j.asoc.2007.05.003,Data mining; Differential evolution; Evolutionary computation; Machine learning; Multi-objective optimization,170.0,
Robust BMPM training based on second-order cone programming and its application in medical diagnosis,"Peng X., King I.",Neural Networks,2008.0,"The Biased Minimax Probability Machine (BMPM) constructs a classifier which deals with the imbalanced learning tasks. It provides a worst-case bound on the probability of misclassification of future data points based on reliable estimates of means and covariance matrices of the classes from the training data samples, and achieves promising performance. In this paper, we develop a novel yet critical extension training algorithm for BMPM that is based on Second-Order Cone Programming (SOCP). Moreover, we apply the biased classification model to medical diagnosis problems to demonstrate its usefulness. By removing some crucial assumptions in the original solution to this model, we make the new method more accurate and robust. We outline the theoretical derivatives of the biased classification model, and reformulate it into an SOCP problem which could be efficiently solved with global optima guarantee. We evaluate our proposed SOCP-based BMPM (BMPMSOCP) scheme in comparison with traditional solutions on medical diagnosis tasks where the objectives are to focus on improving the sensitivity (the accuracy of the more important class, say ""ill"" samples) instead of the overall accuracy of the classification. Empirical results have shown that our method is more effective and robust to handle imbalanced classification problems than traditional classification approaches, and the original Fractional Programming-based BMPM (BMPMFP). © 2008 Elsevier Ltd. All rights reserved.",10.1016/j.neunet.2007.12.051,Biased minimax probability machine; Medical diagnosis; Second-order cone programming,23.0,
Evolutionary multi-objective rule selection for classification rule mining,"Ishibuchi H., Kuwajima I., Nojima Y.",Studies in Computational Intelligence,2008.0,"This chapter discusses the application of evolutionary multi-objective optimization (EMO) to classification rule mining. In the field of classification rule mining, classifiers are designed through the following two phases: rule discovery and rule selection. In the rule discovery phase, a large number of classification rules are extracted from training data. This phase is based on two rule evaluation criteria: support and confidence. An association rule mining technique such as Apriori is usually used to extract classification rules satisfying pre-specified threshold values of the minimum support and confidence. In some studies, EMO algorithms were used to search for Pareto-optimal rules with respect to support and confidence. On the other hand, a small number of rules are selected from the extracted rules to design an accurate and compact classifier in the rule selection phase. A heuristic rule sorting criterion is usually used for rule selection. In some studies, EMO algorithms were used for multi-objective rule selection to maximize the accuracy of rule sets and minimize their complexity. In this chapter, first we explain the above-mentioned two phases in classification rule mining. Next we explain the search for Pareto-optimal rules and the search for Pareto-optimal rule sets. Then we explain evolutionary multi-objective rule selection as a post processing procedure in the second phase of classification rule mining. A number of Pareto-optimal rule sets are found from a large number of candidate rules, which are extracted from training data in the first phase. Finally we show experimental results on some data sets from the UCI machine learning repository. Through computational experiments, we demonstrate that evolutionary rule selection can drastically decrease the number of extracted rules without severely degrading their classification accuracy. We also examine the relation between Paretooptimal rules and Pareto- optimal rule sets. © 2008 Springer-Verlag Berlin Heidelberg.",10.1007/978-3-540-77467-9_3,,8.0,
Hybrid unsupervised/supervised virtual reality spaces for visualizing gastric and liver cancer databases: An evolutionary computation approach,"Barton A.J., Valdés J.J.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2008.0,"This paper expands a multi-objective optimization approach to the problem of computing virtual reality spaces for the visual representation of relational structures (e.g. databases), symbolic knowledge and others, in the context of visual data mining and knowledge discovery. Procedures based on evolutionary computation are discussed. In particular, the NSGA-II algorithm is used as a framework for an instance of this methodology; simultaneously minimizing Sammon's error for dissimilarity measures, and mean cross-validation error on a k-nn pattern classifier. The proposed approach is illustrated with two examples from cancer genomics data (e.g. gastric and liver cancer) by constructing virtual reality spaces resulting from multi-objective optimization. Selected solutions along the Pareto front approximation are used as nonlinearly transformed features for new spaces that compromise similarity structure preservation (from an unsupervised perspective) and class separability (from a supervised pattern recognition perspective), simultaneously. The possibility of spanning a range of solutions between these two important goals, is a benefit for the knowledge discovery and data understanding process. The quality of the set of discovered solutions is superior to the ones obtained separately, from the point of view of visual data mining. © 2008 Springer-Verlag Berlin Heidelberg.",10.1007/978-3-540-68123-6_28,,4.0,
A one-step network traffic prediction,"Mu X., Tang N., Gao W., Li L., Zhou Y.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2008.0,"In the information society today computer networks are an indispensable part of people's life. Network traffic prediction is important to network planning, performance evaluation and network management directly. A variety of machine learning models such as artificial neural networks (ANN) and support vector machine (SVM) have been applied in traffic prediction. In this paper, a novel network traffic one-step-ahead prediction technique is proposed based on a state-of-the-art learning model called minimax probability machine (MPM). The predictive performance is tested on traffic data of Ethernet, experimental results show that the predictions of MPM match the actual traffics accurately and the proposed methods can increases the computational efficiency. Furthermore, we compare the MPM-based prediction technique with the SVM-based techniques. The results show that the predictive performance of MPM is competitive with SVM. © 2008 Springer-Verlag Berlin Heidelberg.",10.1007/978-3-540-85984-0_74,Minimax Probability Machine; Network Traffic; Prediction; Support Vector Machine,5.0,
Optimal construction of a fast and accurate polarisable water potential based on multipole moments trained by machine learning,"Handley C.M., Hawe G.I., Kell D.B., Popelier P.L.A.",Physical Chemistry Chemical Physics,2009.0,"To model liquid water correctly and to reproduce its structural, dynamic and thermodynamic properties warrants models that account accurately for electronic polarisation. We have previously demonstrated that polarisation can be represented by fluctuating multipole moments (derived by quantum chemical topology) predicted by multilayer perceptrons (MLPs) in response to the local structure of the cluster. Here we further develop this methodology of modeling polarisation enabling control of the balance between accuracy, in terms of errors in Coulomb energy and computing time. First, the predictive ability and speed of two additional machine learning methods, radial basis function neural networks (RBFNN) and Kriging, are assessed with respect to our previous MLP based polarisable water models, for water dimer, trimer, tetramer, pentamer and hexamer clusters. Compared to MLPs, we find that RBFNNs achieve a 14-26% decrease in median Coulomb energy error, with a factor 2.5-3 slowdown in speed, whilst Kriging achieves a 40-67% decrease in median energy error with a 6.5-8.5 factor slowdown in speed. Then, these compromises between accuracy and speed are improved upon through a simple multi-objective optimisation to identify Pareto-optimal combinations. Compared to the Kriging results, combinations are found that are no less accurate (at the 90th energy error percentile), yet are 58% faster for the dimer, and 26% faster for the pentamer. © 2009 the Owner Societies.",10.1039/b905748j,,100.0,
Simultaneous feature selection and classification via minimax probability machine,"Yang L., Wang L., Sun Y., Zhang R.",International Journal of Computational Intelligence Systems,2010.0,"This paper presents a novel method for simultaneous feature selection and classification by incorporating a robust L1-norm into the objective function of Minimax Probability Machine (MPM). A fractional programming framework is derived by using a bound on the misclassification error involving the mean and covariance of the data. Furthermore, the problems are solved by the Quadratic Interpolation method. Experiments show that our methods can select fewer features to improve the generalization compared to MPM, which illustrates the effectiveness of the proposed algorithms. © 2010 Taylor & Francis Group, LLC.",10.1080/18756891.2010.9727738,Feature selection; Machine learning; Minimax probability machine; Probability of misclassification,19.0,
A model free method to generate human genetics datasets with complex gene-disease relationships,"Greene C.S., Himmelstein D.S., Moore J.H.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2010.0,"A goal of human genetics is to discover genetic factors that influence individuals' susceptibility to common diseases. Most common diseases are thought to result from the joint failure of two or more interacting components instead of single component failures. This greatly complicates both the task of selecting informative genetic variations and the task of modeling interactions between them. We and others have previously developed algorithms to detect and model the relationships between these genetic factors and disease. Previously these methods have been evaluated with datasets simulated according to pre-defined genetic models. Here we develop and evaluate a model free evolution strategy to generate datasets which display a complex relationship between individual genotype and disease susceptibility. We show that this model free approach is capable of generating a diverse array of datasets with distinct gene-disease relationships for an arbitrary interaction order and sample size. We specifically generate six-hundred pareto fronts; one for each independent run of our algorithm. In each run the predictiveness of single genetic variation and pairs of genetic variations have been minimized, while the predictiveness of third, fourth, or fifth order combinations is maximized. This method and the resulting datasets will allow the capabilities of novel methods to be tested without pre-specified genetic models. This could improve our ability to evaluate which methods will succeed on human genetics problems where the model is not known in advance. We further make freely available to the community the entire pareto-optimal front of datasets from each run so that novel methods may be rigorously evaluated. These 56,600 datasets are available from http://discovery.dartmouth.edu/model-free-data/ . © 2010 Springer-Verlag Berlin Heidelberg.",10.1007/978-3-642-12211-8_7,,2.0,
Skyline queries with constraints: Integrating skyline and traditional query operators,"Zhang M., Alhajj R.",Data and Knowledge Engineering,2010.0,"Multi-objective optimization has been extensively studied in the machine learning literature. And recently the database community adapted the concept as skyline queries focusing mainly on retrieving optimal values from the full-space. In this paper, we consider sub-space skyline queries in a more general database environment, such that the skyline operator does not stand alone in users' queries. In particular, the skyline operator may commute with the selection operator which may express users' preferences or constraints on the skylines; we call this class skyline queries with constraints. Queries in this class are different from constrained skyline queries as described in the literature. We introduce an algorithm to answer sub-space skyline queries with constraints. We investigate the conditions under which the two classes of queries are equivalent; this allows for more efficient computation of skyline queries. Unlike the previous works, we do not design a new index specifically for handling the skylines. We try to make full use of the resources available in traditional relational databases for skyline computation. Further, we consider the case when the constraints are absent. We study the relationship between the skylines of different sub-spaces and record this information in a special data structure to help in pruning the search space. © 2009 Elsevier B.V. All rights reserved.",10.1016/j.datak.2009.10.001,High dimensionality; Multi-objective optimization; Pareto optimality; Skyline,13.0,
Deriving concepts and strategies from chess tablebases,"Guid M., Možina M., Sadikov A., Bratko I.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2010.0,"Complete tablebases, indicating best moves for every position, exist for chess endgames. There is no doubt that tablebases contain a wealth of knowledge, however, mining for this knowledge, manually or automatically, proved as extremely difficult. Recently, we developed an approach that combines specialized minimax search with the argument-based machine learning (ABML) paradigm. In this paper, we put this approach to test in an attempt to elicit human-understandable knowledge from tablebases. Specifically, we semi-automatically synthesize knowledge from the KBNK tablebase for teaching the difficult king, bishop, and knight versus the lone king endgame. © 2010 Springer-Verlag Berlin Heidelberg.",10.1007/978-3-642-12993-3_18,,4.0,
Automated just-in-time compiler tuning,"Hoste K., Georges A., Eeckhout L.",Proceedings of the 2010 CGO - The 8th International Symposium on Code Generation and Optimization,2010.0,"Managed runtime systems, such as a Java virtual machine (JVM), are complex pieces of software with many interacting components. The Just-In-Time (JIT) compiler is at the core of the virtual machine, however, tuning the compiler for optimum performance is a challenging task. There are (i) many compiler optimizations and options, (ii) there may be multiple optimization levels (e.g., -O0, -O1, -O2), each with a specific optimization plan consisting of a collection of optimizations, (iii) the Adaptive Optimization System (AOS) that decides which method to optimize to which optimization level requires fine-tuning, and (iv) the effectiveness of the optimizations depends on the application as well as on the hardware platform. Current practice is to manually tune the JIT compiler which is both tedious and very time-consuming, and in addition may lead to suboptimal performance. This paper proposes automated tuning of the JIT compiler through multi-objective evolutionary search. The proposed framework (i) identifies optimization plans that are Pareto-optimal in terms of compilation time and code quality, (ii) assigns these plans to optimization levels, and (iii) fine-tunes the AOS accordingly. The key benefit of our framework is that it automates the entire exploration process, which enables tuning the JIT compiler for a given hardware platform and/or application at very low cost. By automatically tuning Jikes RVM using our framework for average performance across the DaCapo and SPECjvm98 benchmark suites, we achieve similar performance to the hand-tuned default Jikes RVM. When optimizing the JIT compiler for individual benchmarks, we achieve statistically significant speedups for most benchmarks, up to 40% for startup and up to 19% for steady-state performance. We also show that tuning the JIT compiler for a new hardware platform can yield significantly better performance compared to using a JIT compiler that was tuned for another platform. © 2010 ACM.",10.1145/1772954.1772965,compiler tuning; evolutionary search; java virtual machine (JVM); just-in-time (JIT) compiler; machine learning,16.0,
Generalization improvement of radial basis function network based on multi-objective particle swarm optimization,"Qasem S.N., Shamsuddin S.M.",Journal of Artificial Intelligence,2010.0,"The problem of unsupervised and supervised learning of RBF networks is discussed with Multi-Objective Particle Swarm Optimization (MOPSO). This study presents an evolutionary multi-objective selection method of RBF networks structure. The candidates of RBF networks structures are encoded into particles in PSO. These particles evolve toward Pareto-optimal front defined by several objective functions with model accuracy and complexity. This study suggests an approach of RBF network training through simultaneous optimization of architectures and connections with PSO-based multi-objective algorithm. Present goal is to determine whether MOPSO can train RBF networks and the performance is validated on accuracy and complexity. The experiments are conducted on two benchmark datasets obtained from the machine learning repository. The results show that; the best results are obtained for our proposed method that has obtained 100 and 80.21 % classification accuracy from the experiments made on the data taken from breast cancer and diabetes diseases database, respectively. The results also show that our approach provides an effective means to solve multi-objective RBF networks and outperforms multi-objective genetic algorithm. © 2010 Asian Network for Scientific Information.",10.3923/jai.2010.1.16,Elitist non-dominated sorting genetic algorithm; Hybrid learning; Multi-objective optimization; Multi-objective particle swarm optimization; Radial basis function network,16.0,
DBSCAN-based multi-objective niching to approximate equivalent pareto-subsets,"Kramer O., Danielsiek H.","Proceedings of the 12th Annual Genetic and Evolutionary Computation Conference, GECCO '10",2010.0,"In systems optimization and machine learning multiple alternative solutions may exist in different parts of decision space for the same parts of the Pareto-front. The detection of equivalent Pareto-subsets may be desirablethis paper we introduce a niching method that approximates Paretooptimal solutions with diversity mechanisms in objective and decision space. For diversity in objective space we use rake selection, a selection method based on the distances to reference lines in objective space. For diversity in decision space we introduce a niching approach that uses the density-based clustering method DBSCAN. The clustering process assigns the population to niches while the multi-objective optimization process concentrates on each niche independently. We introduce an indicator for the adaptive control of clustering processes, and extend rake selection by the concept of adaptive corner points. The niching method is experimentally validated on parameterized test function with the help of the S-metric. Copyright 2010 ACM.",10.1145/1830483.1830575,Hybrid evolutionary multiobjective algorithm; Hybrid metaheuristics; Local search; Memetic algorithms,16.0,
Evolutionary optimization of flavors,"Veeramachaneni K., Vladislavleva K., Burland M., Parcon J., O'Reilly U.-M.","Proceedings of the 12th Annual Genetic and Evolutionary Computation Conference, GECCO '10",2010.0,"We have acquired panelist data that provides hedonic (liking) ratings for a set of 40 flavors each composed of the same 7 ingredients at different concentration levels. Our goal is to use this data to predict other flavors, composed of the same ingredients in new combinations, which the panelist will like. We describe how we first employ Pareto-Genetic Programming (GP) to generate a surrogate for the human panelist from the 40 observations. This surrogate, in fact an ensemble of GP symbolic regression models, can predict liking scores for flavors outside the observations and provide a confidence in the prediction. We then employ a multi-objective particle swarm optimization (MOPSO) to design a well and consistently liked flavor suite for a panelist. The MOPSO identifies flavors that are well liked (high liking score), and consistently-liked (of maximum confidence). Further, we generate flavors that are well and consistently liked by a cluster of panelists, by giving the MOPSO slightly different objectives. Copyright 2010 ACM.",10.1145/1830483.1830713,Ensemble learning; Genetic programming; Particle swarm optimization; Sensory evaluation,9.0,
Discriminative topic modeling based on manifold learning,"Huh S., Fienberg S.E.",Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,2010.0,"Topic modeling has been popularly used for data analysis in various domains including text documents. Previous topic models, such as probabilistic Latent Semantic Analysis (pLSA) and Latent Dirichlet Allocation (LDA), have shown impressive success in discovering low-rank hidden structures for modeling text documents. These models, however, do not take into account the manifold structure of data, which is generally informative for the non-linear dimensionality reduction mapping. More recent models, namely Laplacian PLSI (LapPLSI) and Locally-consistent Topic Model (LTM), have incorporated the local manifold structure into topic models and have shown the resulting benefits. But these approaches fall short of the full discriminating power of manifold learning as they only enhance the proximity between the low-rank representations of neighboring pairs without any consideration for non-neighboring pairs. In this paper, we propose Discriminative Topic Model (DTM) that separates non-neighboring pairs from each other in addition to bringing neighboring pairs closer together, thereby preserving the global manifold structure as well as improving the local consistency. We also present a novel model fitting algorithm based on the generalized EM and the concept of Pareto improvement. As a result, DTM achieves higher classification performance in a semi-supervised setting by effectively exposing the manifold structure of data. We provide empirical evidence on text corpora to demonstrate the success of DTM in terms of classification accuracy and robustness to parameters compared to state-of-the-art techniques. © 2010 ACM.",10.1145/1835804.1835888,Dimensionality reduction; Document classification; Semi-supervised learning; Topic modeling,17.0,
Feature selection for multi-purpose predictive models: A many-objective task,"Reynolds A.P., Corne D.W., Chantler M.J.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2010.0,"The target of machine learning is a predictive model that performs well on unseen data. Often, such a model has multiple intended uses, related to different points in the tradeoff between (e.g.) sensitivity and specificity. Moreover, when feature selection is required, different feature subsets will suit different target performance characteristics. Given a feature selection task with such multiple distinct requirements, one is in fact faced with a very-many-objective optimization task, whose target is a Pareto surface of feature subsets, each specialized for (e.g.) a different sensitivity/specificity tradeoff profile. We argue that this view has many advantages. We motivate, develop and test such an approach. We show that it can be achieved successfully using a dominance-based multiobjective algorithm, despite an arbitrarily large number of objectives. © 2010 Springer-Verlag.",10.1007/978-3-642-15844-5_39,,7.0,
Inducing multi-objective clustering ensembles with genetic programming,"Coelho A.L.V., Fernandes E., Faceli K.",Neurocomputing,2010.0,"The recent years have witnessed a growing interest in two advanced strategies to cope with the data clustering problem, namely, clustering ensembles and multi-objective clustering. In this paper, we present a genetic programming based approach that can be considered as a hybrid of these strategies, thereby allowing that different hierarchical clustering ensembles be simultaneously evolved taking into account complementary validity indices. Results of computational experiments conducted with artificial and real datasets indicate that, in most of the cases, at least one of the Pareto optimal partitions returned by the proposed approach compares favorably or go in par with the consensual partitions yielded by two well-known clustering ensemble methods in terms of clustering quality, as gauged by the corrected Rand index. © 2010 Elsevier B.V.",10.1016/j.neucom.2010.09.014,Cluster analysis; Ensembles; Genetic programming; Multi-objective optimization,14.0,
Smart PAC-learners,"Darnstädt M., Simon H.U.",Theoretical Computer Science,2011.0,"The PAC-learning model is distribution-independent in the sense that the learner must reach a learning goal with a limited number of labeled random examples without any prior knowledge of the underlying domain distribution. In order to achieve this, one needs generalization error bounds that are valid uniformly for every domain distribution. These bounds are (almost) tight in the sense that there is a domain distribution which does not admit a generalization error being significantly smaller than the general bound. Note however that this leaves open the possibility to achieve the learning goal faster if the underlying distribution is ""simple"". Informally speaking, we say a PAC-learner L is ""smart"" if, for a ""vast majority"" of domain distributions D, L does not require significantly more examples to reach the ""learning goal"" than the best learner whose strategy is specialized to D. In this paper, focusing on sample complexity and ignoring computational issues, we show that smart learners do exist. This implies (at least from an information-theoretical perspective) that full prior knowledge of the domain distribution (or access to a huge collection of unlabeled examples) does (for a vast majority of domain distributions) not significantly reduce the number of labeled examples required to achieve the learning goal. © 2010 Elsevier B.V. All rights reserved.",10.1016/j.tcs.2010.12.053,Learning under a fixed distribution; Machine learning; Minimax theorem; PAC-learning; Semi-supervised learning; Smart PAC-learner; Value of unlabeled data,5.0,
Evolving ensembles in Multi-objective Genetic Programming for classification with unbalanced data,"Bhowan U., Johnston M., Zhang M.","Genetic and Evolutionary Computation Conference, GECCO'11",2011.0,"Machine learning algorithms can suffer a performance bias when data sets are unbalanced. This paper proposes a Multi-objective Genetic Programming approach using negative correlation learning to evolve accurate and diverse ensembles of non-dominated solutions where members vote on class membership. We also compare two popular Pareto-based fitness schemes on the classification tasks. We show that the evolved ensembles achieve high accuracy on both classes using six unbalanced binary data sets, and that this performance is usually better than many of its individual members. Copyright 2011 ACM.",10.1145/2001576.2001756,Class imbalance; Classification; Evolutionary multi-objective optimisation; Genetic programming,25.0,
A geometric approach to find nondominated policies to imprecise reward MDPs,"Freire Da Silva V., Reali Costa A.H.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2011.0,"Markov Decision Processes (MDPs) provide a mathematical framework for modelling decision-making of agents acting in stochastic environments, in which transitions probabilities model the environment dynamics and a reward function evaluates the agent's behaviour. Lately, however, special attention has been brought to the difficulty of modelling precisely the reward function, which has motivated research on MDP with imprecisely specified reward. Some of these works exploit the use of nondominated policies, which are optimal policies for some instantiation of the imprecise reward function. An algorithm that calculates nondominated policies is πWitness, and nondominated policies are used to take decision under the minimax regret evaluation. An interesting matter would be defining a small subset of nondominated policies so that the minimax regret can be calculated faster, but accurately. We modified πWitness to do so. We also present the πHull algorithm to calculate nondominated policies adopting a geometric approach. Under the assumption that reward functions are linearly defined on a set of features, we show empirically that πHull can be faster than our modified version of πWitness. © 2011 Springer-Verlag.",10.1007/978-3-642-23780-5_38,Imprecise Reward MDP; Minimax Regret; Preference Elicitation,2.0,
Ensembles and multiple classifiers: A game-theoretic view,Cesa-Bianchi N.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2011.0,"The study of multiple classifier systems is a fundamental topic in modern machine learning. However, early work on aggregation of predictors can be traced back to the Fifties, in the area of game theory. At that time, the pioneering work of James Hannan [11] and David Blackwell [2] laid down the foundations of repeated game theory. In a nutshell, a repeated game is the game-theoretic interpretation of learning. In games played once, lacking any information about the opponent, the best a player can do is to play the minimax strategy (the best strategy against the worst possible opponent). In repeated games, by examining the history of past opponent moves, the player acquires information about the opponent's behavior and can adapt to it, in order to achieve a better payoff than that guaranteed by the minimax strategy. © 2011 Springer-Verlag.",10.1007/978-3-642-21557-5_2,,,
Managing performance vs. accuracy trade-offs with loop perforation,"Sidiroglou S., Misailovic S., Hoffmann H., Rinard M.",SIGSOFT/FSE 2011 - Proceedings of the 19th ACM SIGSOFT Symposium on Foundations of Software Engineering,2011.0,"Many modern computations (such as video and audio encoders, Monte Carlo simulations, and machine learning algorithms) are designed to trade off accuracy in return for increased performance. To date, such computations typically use ad-hoc, domain-specific techniques developed specifically for the computation at hand. Loop perforation provides a general technique to trade accuracy for performance by transforming loops to execute a subset of their iterations. A criticality testing phase filters out critical loops (whose perforation produces unacceptable behavior) to identify tunable loops (whose perforation produces more efficient and still acceptably accurate computations). A perforation space exploration algorithm perforates combinations of tunable loops to find Pareto-optimal perforation policies. Our results indicate that, for a range of applications, this approach typically delivers performance increases of over a factor of two (and up to a factor of seven) while changing the result that the application produces by less than 10%. © 2011 ACM.",10.1145/2025113.2025133,Loop perforation; Profiling; Quality of service,314.0,
On the curse of dimensionality in supervised learning of smooth regression functions,"Liitiäinen E., Corona F., Lendasse A.",Neural Processing Letters,2011.0,"In this paper, the effect of dimensionality on the supervised learning of infinitely differentiable regression functions is analyzed. By invoking the Van Trees lower bound, we prove lower bounds on the generalization error with respect to the number of samples and the dimensionality of the input space both in a linear and non-linear context. It is shown that in non-linear problems without prior knowledge, the curse of dimensionality is a serious problem. At the same time, we speculate counter-intuitively that sometimes supervised learning becomes plausible in the asymptotic limit of infinite dimensionality. © 2011 Springer Science+Business Media, LLC.",10.1007/s11063-011-9188-7,Analytic function; High dimensional; Minimax; Nonparametric regression; Supervised learning; Van Trees,4.0,
Predicting best design trade-offs: A case study in processor customization,"Zuluaga M., Bonilla E., Topham N.","Proceedings -Design, Automation and Test in Europe, DATE",2012.0,"Given the high level description of a task, many different hardware modules may be generated while meeting its behavioral requirements. The characteristics of the generated hardware can be tailored to favor energy efficiency, performance, accuracy or die area. The inherent trade-offs between such metrics need to be explored in order to choose a solution that meets design and cost expectations. We address the generic problem of automatically deriving a hardware implementation from a high-level task description. In this paper we present a novel technique that exploits previously explored implementation design spaces in order to find optimal trade-offs for new high-level descriptions. This technique is generalizable to a range of high-level synthesis problems in which trade-offs can be exposed by changing the parameters of the hardware generation tool. Our strategy, based upon machine learning techniques, models the impact of the parameterization of the tool on the target objectives, given the characteristics of the input. Thus, a predictor is able to suggest a subset of parameters that are likely to lead to optimal hardware implementations. The proposed method is evaluated on a resource sharing problem which is typical in high level synthesis, where the trade-offs between area and performance need to be explored. In this case study, we show that the technique can reduce by two orders of magnitude the number of design points that need to be explored in order to find the Pareto optimal solutions. © 2012 EDAA.",10.1109/date.2012.6176647,,7.0,
Discriminative topic modeling based on manifold learning,"Huh S., Fienberg S.E.",ACM Transactions on Knowledge Discovery from Data,2012.0,"Topic modeling has become a popular method used for data analysis in various domains including text documents. Previous topic model approaches, such as probabilistic Latent Semantic Analysis (pLSA) and Latent Dirichlet Allocation (LDA), have shown impressive success in discovering low-rank hidden structures for modeling text documents. These approaches, however do not take into account the manifold structure of the data, which is generally informative for nonlinear dimensionality reduction mapping. More recent topic model approaches, Laplacian PLSI (LapPLSI) and Locally-consistent Topic Model (LTM), have incorporated the local manifold structure into topic models and have shown resulting benefits. But they fall short of achieving full discriminating power of manifold learning as they only enhance the proximity between the low-rank representations of neighboring pairs without any consideration for non-neighboring pairs. In this article, we propose a new approach, Discriminative Topic Model (DTM), which separates non-neighboring pairs from each other in addition to bringing neighboring pairs closer together, thereby preserving the global manifold structure as well as improving local consistency. We also present a novel model-fitting algorithm based on the generalized EM algorithm and the concept of Pareto improvement. We empirically demonstrate the success of DTM in terms of unsupervised clustering and semisupervised classification accuracies on text corpora and robustness to parameters compared to state-of-the-art techniques. © 2012 ACM.",10.1145/2086737.2086740,Dimensionality reduction; Document clustering and classification; Semisupervised learning; Topic modeling,23.0,
Manifold-regularized minimax probability machine,"Yoshiyama K., Sakurai A.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2012.0,"In this paper we propose Manifold-Regularized Minimax Probability Machine, called MRMPM. We show that Minimax Probability Machine can properly be extended to semi-supervised version in the manifold regularization framework and that its kernelized version is obtained for non-linear case. Our experiments show that the proposed methods achieve results competitive to existing learning methods, such as Laplacian Support Vector Machine and Laplacian Regularized Least Square for publicly available datasets from UCI machine learning repository. © 2012 Springer-Verlag.",10.1007/978-3-642-28258-4_5,,4.0,
Coevolutionary learning of neural network ensemble for complex classification tasks,"Tian J., Li M., Chen F., Kou J.",Pattern Recognition,2012.0,"Ensemble approaches to classification have attracted a great deal of interest recently. This paper presents a novel method for designing the neural network ensemble using coevolutionary algorithm. The bootstrap resampling procedure is employed to obtain different training subsets that are used to estimate different component networks of the ensemble. Then the cooperative coevolutionary algorithm is developed to optimize the ensemble model via the divide-and-cooperative mechanism. All component networks are coevolved in parallel in the scheme of interacting co-adapted subpopulations. The fitness of an individual from a particular subpopulation is assessed by associating it with the representatives from other subpopulations. In order to promote the cooperation of all component networks, the proposed method considers both the accuracy and the diversity among the component networks that are evaluated using the multi-objective Pareto optimality measure. A hybrid output-combination method is designed to determine the final ensemble output. Experimental results illustrate that the proposed method is able to obtain neural network ensemble models with better classification accuracy in comparison with currently popular ensemble algorithms. © 2011 Elsevier Ltd All rights reserved.",10.1016/j.patcog.2011.09.012,Classification; Coevolutionary algorithm; Ensemble learning; Neural network,30.0,
Function approximation with LWPR and XCSF: A comparative study,"Stalph P.O., Rubinsztajn J., Sigaud O., Butz M.V.",Evolutionary Intelligence,2012.0,"Function approximation, also called regression, is an important tool in numerical mathematics and engineering. The most challenging approximation problems arise, when the function class is unknown and the surface has to be approximated online from incoming samples. One way to achieve good approximations of complex non-linear functions is to cluster the input space into small patches, apply linear models in each niche, and recombine these models via a weighted sum. While it is rather simple to optimally fit a linear model to given data, it is fairly complex to find a reasonable structuring of the input space in order to exploit linearities in the underlying function. We compare two non-parametric regression algorithms that are able to approximate multi-dimensional, non-linear functions online. The XCSF Learning Classifier System is a modified version of XCS, which is a genetics-based machine learning algorithm. Locally Weighted Projection Regression is a statistics-based machine learning technique that is mainly used for function approximation tasks in robotics. For both algorithms the relevant, conflicting performance criteria are accuracy and population size, that is, the number of local models. We explore the trade-off between those criteria on three benchmark problems by means of intense grid search for Pareto optimal solutions. Detailed learning behavior is investigated using selected Pareto optimal parameters. The illustration of final input space clusterings sheds light on the structuring capabilities. A discussion of advantages and drawbacks completes this comparative study. © 2012 Springer-Verlag.",10.1007/s12065-012-0082-7,LWPR; Machine learning; Non-parametric regression; XCSF,7.0,
"""Smart"" design space sampling to predict pare-to-optimal solutions","Zuluaga M., Krause A., Milder P., Püschel M.","Proceedings of the ACM SIGPLAN Conference on Languages, Compilers, and Tools for Embedded Systems (LCTES)",2012.0,"Many high-level synthesis tools offer degrees of freedom in mapping high-level specifications to Register-Transfer Level descriptions. These choices do not affect the functional behavior but span a design space of different cost-performance tradeoffs. In this paper we present a novel machine learning-based approach that efficiently determines the Pareto-optimal designs while only sampling and synthesizing a fraction of the design space. The approach combines three key components: (1) A regression model based on Gaussian processes to predict area and throughput based on synthesis training data. (2) A ""smart"" sampling strategy, GP-PUCB, to iteratively refine the model by carefully selecting the next design to synthesize to maximize progress. (3) A stopping criterion based on assessing the accuracy of the model without access to complete synthesis data. We demonstrate the effectiveness of our approach using IP generators for discrete Fourier transforms and sorting networks. However, our algorithm is not specific to this application and can be applied to a wide range of Pareto front prediction problems. Copyright © 2012 ACM.",10.1145/2248418.2248436,Area and performance estimation; High-level synthesis; Machine learning; Pareto optimality,11.0,
Lazy meta-learning: Creating customized model ensembles on demand,Bonissone P.P.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2012.0,"In the not so distant future, we expect analytic models to become a commodity. We envision having access to a large number of data-driven models, obtained by a combination of crowdsourcing, crowdservicing, cloud-based evolutionary algorithms, outsourcing, in-house development, and legacy models. In this new context, the critical question will be model ensemble selection and fusion, rather than model generation. We address this issue by proposing customized model ensembles on demand, inspired by Lazy Learning. In our approach, referred to as Lazy Meta-Learning, for a given query we find the most relevant models from a DB of models, using their meta-information. After retrieving the relevant models, we select a subset of models with highly uncorrelated errors. With these models we create an ensemble and use their meta-information for dynamic bias compensation and relevance weighting. The output is a weighted interpolation or extrapolation of the outputs of the models ensemble. Furthermore, the confidence interval around the output is reduced as we increase the number of uncorrelated models in the ensemble. We have successfully tested this approach in a power plant management application. © 2012 Springer-Verlag.",10.1007/978-3-642-30687-7_1,coal-fired power plant management; computational intelligence; ensemble; entropy; fusion; lazy learning; Machine learning; meta-learning; neural networks; Pareto set,12.0,
Reinforcement learning with n-tuples on the game connect-4,"Thill M., Koch P., Konen W.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2012.0,"Learning complex game functions is still a difficult task. We apply temporal difference learning (TDL), a well-known variant of the reinforcement learning approach, in combination with n-tuple networks to the game Connect-4. Our agent is trained just by self-play. It is able, for the first time, to consistently beat the optimal-playing Minimax agent (in game situations where a win is possible). The n-tuple network induces a mighty feature space: It is not necessary to design certain features, but the agent learns to select the right ones. We believe that the n-tuple network is an important ingredient for the overall success and identify several aspects that are relevant for achieving high-quality results. The architecture is sufficiently general to be applied to similar reinforcement learning tasks as well. © 2012 Springer-Verlag.",10.1007/978-3-642-32937-1_19,board games; feature generation; Machine learning; n-tuple systems; reinforcement learning; self-play; TDL,15.0,
A computational geometry approach for pareto-optimal selection of neural networks,"Torres L.C.B., Castro C.L., Braga A.P.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2012.0,"This paper presents a Pareto-optimal selection strategy for multiobjective learning that is based on the geometry of the separation margin between classes. The Gabriel Graph, a method borrowed from Computational Geometry, is constructed in order to obtain margin patterns and class borders. From border edges, a target separator is obtained in order to obtain a large margin classifier. The selected model from the generated Pareto-set is the one that is closer to the target separator. The method presents robustness in both synthetic and real benchmark datasets. It is efficient for Pareto-Optimal selection of neural networks and no claim is made that the obtained solution is equivalent to a maximum margin separator. © 2012 Springer-Verlag.",10.1007/978-3-642-33266-1_13,classification; decision-making; gabriel graph; multiobjective machine learning,2.0,
A genetic algorithm approach for technology characterization,"Galvan E., Malak R.",Proceedings of the ASME Design Engineering Technical Conference,2012.0,"It is important for engineers to understand the capabilities and limitations of the technologies they consider for use in their systems. Several researchers have investigated approaches for modeling the capabilities of a technology with the aim of supporting the design process. In these works, the information about the physical form is typically abstracted away. However, the efficient generation of an accurate model of technical capabilities remains a challenge. Pareto frontier based methods are often used but yield results that are of limited use for subsequent decision making and analysis. Models based on parameterized Pareto frontiers-Termed Technology Characterization Models (TCMs)-Are much more reusable and composable. However, there exists no efficient technique for modeling the parameterized Pareto frontier. The contribution of this paper is a new algorithm for modeling the parameterized Pareto frontier to be used as a model of the characteristics of a technology. The proposed algorithm uses fundamental concepts from multiobjective genetic optimization and machine learning to generate a model of the technology frontier. © 2012 by ASME.",10.1115/DETC2012-70465,,6.0,
A classification model based on incomplete information on features in the form of their average values,"Utkin L.V., Zhuk Y.A., Selikhovkin I.A.",Scientific and Technical Information Processing,2012.0,"This paper presents a model of classification under incomplete information in the form of mathematical expectations of features; it is based on the minimax (minimin) strategy of decision making. The discriminant function is calculated by maximization (minimization) of the risk functional as a measure of misclassification, by a set of distributions of probabilities with bounds determined by information on features, and minimization by the set of parameters. The algorithm is reduced to solution of the parametric problem of linear programming. © 2012 Allerton Press, Inc.",10.3103/S0147688212060068,classification; linear programming; machine learning; mathematical expectation; minimax strategy; risk functional; the loss function,3.0,
Categorical feature reduction using multi objective genetic Algorithm in cluster analysis,"Dutta D., Dutta P., Sil J.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2013.0,"In the paper, real coded multi objective genetic algorithm based K-clustering method has been studied, K represents the number of clusters. In K-clustering algorithm value of K is known. The searching power of Genetic Algorithm (GA) is exploited to search for suitable clusters and centers of clusters so that intra-cluster distance (Homogeneity, H) and inter-cluster distances (Separation, S) are simultaneously optimized. It is achieved by measuring H and S using Mod distance per feature metric, suitable for categorical features (attributes). We have selected 3 benchmark data sets from UCI Machine Learning Repository containing categorical features only. The paper proposes two versions of MOGA based K-clustering algorithm. In proposed MOGA (H, S), all features are taking part in building chromosomes and calculation of H and S values. In MOGA-Feature-Selection (H, S), selected features take part to build chromosomes, relevant for clusters. Here, K-modes is hybridized with GA. We have used hybridized GA to combine global searching capabilities of GA with local searching capabilities of K-modes. Considering context sensitivity, we have used a special crossover operator called ""pairwise crossover"" and ""substitution"". The main contribution of this paper is simultaneous dimensionality reduction and optimization of objectives using MOGA. © 2013 Springer-Verlag Berlin Heidelberg.",10.1007/978-3-642-45318-2_7,Clustering; dimensionality reduction; homogeneity and separation; Pareto optimal front; real coded multi objective genetic algorithm,3.0,
A location selection policy of live virtual machine migration for power saving and load balancing,"Zhao J., Ding Y., Xu G., Hu L., Dong Y., Fu X.",The Scientific World Journal,2013.0,"Green cloud data center has become a research hotspot of virtualized cloud computing architecture. And load balancing has also been one of the most important goals in cloud data centers. Since live virtual machine (VM) migration technology is widely used and studied in cloud computing, we have focused on location selection (migration policy) of live VM migration for power saving and load balancing. We propose a novel approach MOGA-LS, which is a heuristic and self-adaptive multiobjective optimization algorithm based on the improved genetic algorithm (GA). This paper has presented the specific design and implementation of MOGA-LS such as the design of the genetic operators, fitness values, and elitism. We have introduced the Pareto dominance theory and the simulated annealing (SA) idea into MOGA-LS and have presented the specific process to get the final solution, and thus, the whole approach achieves a long-term efficient optimization for power saving and load balancing. The experimental results demonstrate that MOGA-LS evidently reduces the total incremental power consumption and better protects the performance of VM migration and achieves the balancing of system load compared with the existing research. It makes the result of live VM migration more high-effective and meaningful. © 2013 Jia Zhao et al.",10.1155/2013/492615,,19.0,
Analog circuit design based on robust POFs using an enhanced MOEA with SVM models,"Lourenço N., Martins R., Barros M., Horta N.",Lecture Notes in Electrical Engineering,2013.0,"In this chapter, a multi-objective design methodology for automatic analog integrated circuits (IC) synthesis, which enhances the robustness of the solution by varying technological and environmental parameters, is presented. The automatic analog IC sizing tool GENOM-POF was implemented and used to demonstrate the methodology, and to verify the effect of corner cases on the Pareto optimal front (POF). To enhance the efficiency of the tool, a supervised learning strategy, which is based on Support Vector Machines (SVM), is used to create feasibility models that efficiently prune the design search space during the optimization process, thus, reducing the overall number of required evaluations. The GPOF-SVM optimization kernel consists of a modified version of the multi-objective evolutionary algorithm (MOEA), NSGA-II, and uses HSPICE® as the evaluation engine. The usage of standard inputs and outputs eases the integration with other design automation tools, either at system level or at physical level, which is the case of LAYGEN, an in-house layout generation tool. Finally, the approach was validated using benchmark examples, which consist of circuits tested with similar tools, particularly, the former GENOM tool and other tools from literature. © Springer-Verlag Berlin Heidelberg 2013.",10.1007/978-3-642-36329-0_7,,8.0,
Robust novelty detection in the framework of a contamination neighbourhood,"Utkin L.V., Zhuk Y.A.",International Journal of Intelligent Information and Database Systems,2013.0,A novelty detection robust model is studied in the paper. It is based on contaminated (robust) models which produce a set of probability distributions of data points instead of the empirical distribution. The minimax and minimin strategies are used to construct optimal separating functions. An algorithm for computing the optimal parameters of the novelty detection model is reduced to a finite number of standard SVM tasks with weighted data points. Experimental results with synthetic and some real data illustrate the proposed novelty detection robust model. Copyright © 2013 Inderscience Enterprises Ltd.,10.1504/IJIIDS.2013.053830,Classification; Machine learning; Minimax strategy; Novelty detection; Quadratic programming; Support vector machine; SVM,5.0,
Using objective reduction and interactive procedure to handle many-objective optimization problems,"Sinha A., Saxena D.K., Deb K., Tiwari A.",Applied Soft Computing Journal,2013.0,"A number of practical optimization problems are posed as many-objective (more than three objectives) problems. Most of the existing evolutionary multi-objective optimization algorithms, which target the entire Pareto-front are not equipped to handle many-objective problems. Though there have been copious efforts to overcome the challenges posed by such problems, there does not exist a generic procedure to effectively handle them. This paper presents a simplify and solve framework for handling many-objective optimization problems. In that, a given problem is simplified by identification and elimination of the redundant objectives, before interactively engaging the decision maker to converge to the most preferred solution on the Pareto-optimal front. The merit of performing objective reduction before interacting with the decision maker is two fold. Firstly, the revelation that certain objectives are redundant, significantly reduces the complexity of the optimization problem, implying lower computational cost and higher search efficiency. Secondly, it is well known that human beings are not efficient in handling several factors (objectives in the current context) at a time. Hence, simplifying the problem a priori addresses the fundamental issue of cognitive overload for the decision maker, which may help avoid inconsistent preferences during the different stages of interactive engagement. The implementation of the proposed framework is first demonstrated on a three-objective problem, followed by its application on two real-world engineering problems. © 2012 Elsevier B.V. All rights reserved.",10.1016/j.asoc.2012.08.030,Evolutionary algorithms; Evolutionary multi- and many-objective optimization; Interactive optimization; Machine learning; Multi-criteria decision making,29.0,
Robust extreme learning machine,"Horata P., Chiewchanwattana S., Sunat K.",Neurocomputing,2013.0,"The output weights computing of extreme learning machine (ELM) encounters two problems, the computational and outlier robustness problems. The computational problem occurs when the hidden layer output matrix is a not full column rank matrix or an ill-conditioned matrix because of randomly generated input weights and biases. An existing solution to this problem is Singular Value Decomposition (SVD) method. However, the training speed is still affected by the large complexity of SVD when computing the Moore-Penrose (MP) pseudo inverse. The outlier robustness problem may occur when the training data set contaminated with outliers then the accuracy rate of ELM is extremely affected. This paper proposes the Extended Complete Orthogonal Decomposition (ECOD) method to solve the computational problem in ELM weights computing via ECODLS algorithm. And the paper also proposes the other three algorithms, i.e. the iteratively reweighted least squares (IRWLS-ELM), ELM based on the multivariate least-trimmed squares (MLTS-ELM), and ELM based on the one-step reweighted MLTS (RMLTS-ELM) to solve the outlier robustness problem. However, they also encounter the computational problem. Therefore, the ECOD via ECODLS algorithm is also used successfully in the three proposed algorithms. The experiments of regression problems were conducted on both toy and real-world data sets. The outlier types are one-sided and two-sided outliers. Each experiment was randomly contaminated with outliers, of one type only, with 10%, 20%, 30%, 40%, and 50% of the total training data size. Meta-metrics evaluation was used to measure the outlier robustness of the proposed algorithms compared to the existing algorithms, i.e. the minimax probability machine regression (MPMR) and the ordinary ELM. The experimental results showed that ECOD can effectively replace SVD. The ECOD is robust to the not full column rank or the ill-conditional problem. The speed of the ELM training using ECOD is also faster than the ordinary training algorithm. Moreover, the meta-metrics measure showed that the proposed algorithms are less affected by the increasing number of outliers than the existing algorithms. © 2012 Elsevier B.V.",10.1016/j.neucom.2011.12.045,Extended Complete Orthogonal Decomposition; Extreme learning machine; Iteratively reweighted least squares; Meta-metrics evaluation; Minimax probability machine regression; Moore-Penrose pseudo inverse; Multivariate least-trimmed squares; Outlier; Robustness; Singular Value Decomposition,128.0,
Comprehensive preference optimization of an irreversible thermal engine using pareto based mutable smart bee algorithm and generalized regression neural network,"Mozaffari A., Gorji-Bandpy M., Samadian P., Rastgar R., Rezania Kolaei A.",Swarm and Evolutionary Computation,2013.0,"Optimizing and controlling of complex engineering systems is a phenomenon that has attracted an incremental interest of numerous scientists. Until now, a variety of intelligent optimizing and controlling techniques such as neural networks, fuzzy logic, game theory, support vector machines and stochastic algorithms were proposed to facilitate controlling of the engineering systems. In this study, an extended version of mutable smart bee algorithm (MSBA) called Pareto based mutable smart bee (PBMSB) is inspired to cope with multi-objective problems. Besides, a set of benchmark problems and four well-known Pareto based optimizing algorithms i.e. multi-objective bee algorithm (MOBA), multi-objective particle swarm optimization (MOPSO) algorithm, non-dominated sorting genetic algorithm (NSGA-II), and strength Pareto evolutionary algorithm (SPEA 2) are utilized to confirm the acceptable performance of the proposed method. In order to find the maximum exploration potentials, these techniques are equipped with an external archive. These archives aid the methods to record all of the non-dominated solutions. Eventually, the proposed method and generalized regression neural network (GRNN) are simultaneously used to optimize the major parameters of an irreversible thermal engine. In order to direct the PBMSB to explore deliberate spaces within the solution domain, a reference point obtained from finite time thermodynamic (FTT) approach, is utilized in the optimization. The outcome results show the acceptable performance of the proposed method to optimize complex real-life engineering systems. © 2012 Elsevier B.V. All rights reserved.",10.1016/j.swevo.2012.11.004,Comprehensive preference optimization; Generalized regression neural network; Irreversible thermal engine; Machine learning; Multiobjective optimization; Mutable smart bee algorithm,19.0,
Radial basis function regularization for linear inverse problems with random noise,"Valencia C., Yuan M.",Journal of Multivariate Analysis,2013.0,"In this paper, we study the statistical properties of the method of regularization with radial basis functions in the context of linear inverse problems. Radial basis function regularization is widely used in machine learning because of its demonstrated effectiveness in numerous applications and computational advantages. From a statistical viewpoint, one of the main advantages of radial basis function regularization in general and Gaussian radial basis function regularization in particular is their ability to adapt to varying degrees of smoothness in a direct problem. We show here that similar approaches for inverse problems not only share such adaptivity to the smoothness of the signal but also can accommodate different degrees of ill-posedness. These results render further theoretical support to the superior performance observed empirically for radial basis function regularization. © 2012 Elsevier Inc.",10.1016/j.jmva.2012.09.007,Inverse problem; Minimax rate of convergence; Primary; Radial basis function; Regularization,2.0,
A dimensionally-aware genetic programming architecture for automated innovization,"Bandaru S., Deb K.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2013.0,"Automated innovization is an unsupervised machine learning technique for extracting useful design knowledge from Pareto-optimal solutions in the form of mathematical relationships of a certain structure. These relationships are known as design principles. Past studies have shown the applicability of automated innovization on a number of engineering design optimization problems using a multiplicative form for the design principles. In this paper, we generalize the structure of the obtained principles using a tree-based genetic programming framework. While the underlying innovization algorithm remains the same, evolving multiple trees, each representing a different design principle, is a challenging task. We also propose a method for introducing dimensionality information in the search process to produce design principles that are not just empirical in nature, but also meaningful to the user. The procedure is illustrated for three engineering design problems. © 2013 Springer-Verlag.",10.1007/978-3-642-37140-0_39,automated innovization; design principles; dimensional awareness; genetic programming; multi-objective optimization,10.0,
Comparing ensemble learning approaches in genetic programming for classification with unbalanced data,"Bhowan U., Johnston M., Zhang M.",GECCO 2013 - Proceedings of the 2013 Genetic and Evolutionary Computation Conference Companion,2013.0,"This paper compares three approaches to evolving ensembles in Genetic Programming (GP) for binary classification with unbalanced data. The first uses bagging with sampling, while the other two use Pareto-based multi-objective GP (MOGP) for the trade-off between the two (unequal) classes. In MOGP, two ways are compared to build the ensembles: using the evolved Pareto front alone, and using the whole evolved population of dominated and non-dominated individuals alike. Experiments on several benchmark (binary) unbalanced tasks find that smaller, more diverse ensembles chosen during ensemble selection perform best due to better generalisation, particularly when the combined knowledge of the whole evolved MOGP population forms the ensemble.",10.1145/2464576.2464643,Clasfisification; Class imbalance; Genetic programming; Multi-objective optimisation,1.0,
Label free change detection on streaming data with cooperative multi-objective genetic programming,"Rahimi S., McIntyre A.R., Heywood M.I., Zincir-Heywood N.",GECCO 2013 - Proceedings of the 2013 Genetic and Evolutionary Computation Conference Companion,2013.0,"Classification under streaming data conditions requires that the machine learning (ML) approach operate interactively with the stream content. Thus, given some initial ML clas- sification capability, it is not possible to assume that stream content will be stationary. It is therefore necessary to first detect when the stream content changes. Only after detect- ing a change, can classifier retraining be triggered. Current methods for change detection tend to assume an entropy fil- ter approach, where class labels are necessary. In practice, labeling the stream would be extremely expensive. This work proposes an approach in which the behaviour of GP individuals is used to detect change without the use of la- bels. Only after detecting a change is label information re- quested. Benchmarking under a computer network traffic analysis scenario demonstrates that the proposed approach performs at least as well as the filter method, while retaining the advantage of requiring no labels.",10.1145/2464576.2464652,Coevolution; Dynamic environments; Genetic programming; Pareto archiving; Streaming data,2.0,
One-side probability machine: Learning imbalanced classifiers locally and globally,"Zhang R., Huang K.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2013.0,"Imbalanced learning is a challenged task in machine learning, where the data associated with one class are far fewer than those associated with the other class. In this paper, we propose a novel model called One-Side Probability Machine (OSPM) able to learn from imbalanced data rigorously and accurately. In particular, OSPM can lead to a rigorous treatment on biased or imbalanced classification tasks, which is significantly different from previous approaches. Importantly, the proposed OSPM exploits the reliable global information from one side only, i.e., the majority class , while engaging the robust local learning [2] from the other side, i.e., the minority class. Such setting proves much effective than other models such as Biased Minimax Probability Machine (BMPM). To our best knowledge, OSPM presents the first model capable of learning from imbalanced data both locally and globally. Our proposed model has also established close connections with various famous models such as BMPM and Support Vector Machine. One appealing feature is that the optimization problem involved can be cast as a convex second order conic programming problem with a global optimum guaranteed. A series of experiments on three data sets demonstrate the advantages of our proposed method against four competitive approaches. © Springer-Verlag 2013.",10.1007/978-3-642-42042-9_18,,1.0,
Self-organizing maps for multi-objective Pareto Frontiers,"Chen S., Amid D., Shir O.M., Limonad L., Boaz D., Anaby-Tavor A., Schreck T.",IEEE Pacific Visualization Symposium,2013.0,"Decision makers often need to take into account multiple conflicting objectives when selecting a solution for their problem. This can result in a potentially large number of candidate solutions to be considered. Visualizing a Pareto Frontier, the optimal set of solutions to a multi-objective problem, is considered a difficult task when the problem at hand spans more than three objective functions. We introduce a novel visual-interactive approach to facilitate coping with multi-objective problems. We propose a characterization of the Pareto Frontier data and the tasks decision makers face as they reach their decisions. Following a comprehensive analysis of the design alternatives, we show how a semantically-enhanced Self-Organizing Map, can be utilized to meet the identified tasks. We argue that our newly proposed design provides both consistent orientation of the 2D mapping as well as an appropriate visual representation of individual solutions. We then demonstrate its applicability with two real-world multi-objective case studies. We conclude with a preliminary empirical evaluation and a qualitative usefulness assessment. © 2013 IEEE.",10.1109/PacificVis.2013.6596140,[Computing Methodologies]: Machine Learning - Machine Learning Approaches Neural Networks; [Human-Centered Computing]: Visualization - Visualization Design and Evaluation Methods; [Information Systems]: Information Systems Applications - Decision Support Systems,22.0,
Machine learning enhanced multi-objective evolutionary algorithm based on decomposition,"Liau Y.S., Tan K.C., Hu J., Qiu X., Gee S.B.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2013.0,"We address the problem of expensive multi-objective optimization using a machine learning assisted model of evolutionary computation. Specifically, we formulate a meta-objective function tailored to the framework of MOEA/D, which can be solved by means of supervised regression learning using the Support Vector Machine (SVM) algorithm. The learned model constitutes the knowledge which can be then utilized to guide the evolution process within MOEA/D so as to reach better regions in the search space more quickly. Simulation results on a variety of benchmark problems show that the machine-learning enhanced MOEA/D is able to obtain better estimation of Pareto fronts when the allowed computational budget, measured in terms of number of objective function evaluation, is scarce. © 2013 Springer-Verlag.",10.1007/978-3-642-41278-3_67,Evolutionary multi-objective optimization; expensive optimization; support vector machine; surrogate modelling,6.0,
The randomized greedy modularity clustering algorithm and the core groups graph clustering scheme,"Geyer-Schulz A., Ovelgönne M.","Studies in Classification, Data Analysis, and Knowledge Organization",2014.0,"The modularity measure of Newman and Girvan is a popular formal cluster criterium for graph clustering. Although the modularity maximization problem has been shown to be NP-hard, a large number of heuristic modularity maximization algorithms have been developed. In the 10th DIMACS Implementation Challenge of the Center for Discrete Mathematics & Theoretical Computer Science (DIMACS) for graph clustering our core groups graph clustering scheme combined with a randomized greedy modularity clustering algorithm won both modularity optimization challenges: the Modularity (highest modularity) and the Pareto Challenge (tradeoff between modularity and performance). The core groups graph clustering scheme is an ensemble learning clustering method which combines the local solutions of several base algorithms to form a good start solution for the final algorithm. The randomized greedy modularity algorithm is a nondeterministic agglomerative hierarchical clustering approach which finds locally optimal solutions. In this contribution we analyze the similarity of the randomized greedy modularity algorithm with incomplete solvers for the satisfiability problem and we establish an analogy between the cluster core group heuristic used in core groups graph clustering and a sampling of restart points on the Morse graph of a continuous optimization problem with the same local optima. © Springer International Publishing Switzerland 2014.",10.1007/978-3-319-01264-3_2,,6.0,
Virtual machine placement based on ant colony optimization for minimizing resource wastage,"Tawfeek M.A., El-Sisi A.B., Keshk A.E., Torkey F.A.",Communications in Computer and Information Science,2014.0,"Cloud computing is concept of computing technology in which user uses remote server for maintain their data and application. Resources in cloud computing are demand driven utilized in forms of virtual machines to facilitate the execution of complicated tasks. Virtual machine placement is the process of mapping virtual machines to physical machines. This is an active research topic and different strategies have been adopted in literature for this problem. In this paper, the problem of virtual machine placement is formulated as a multiobjective optimization problem aiming to simultaneously optimize total processing resource wastage and total memory resource wastage. After that ant colony optimization algorithm is proposed for solving the formulated problem. The main goal of the proposed algorithm is to search the solution space more efficiently and obtain a set of non-dominated solutions called the Pareto set. The proposed algorithm has been compared with the well-known algorithms for virtual machine placement problem existing in the literature. The comparison results elucidate that the proposed algorithm is more efficient and significantly outperforms the compared methods on the basis of CPU resource wastage and memory resource wastage. © Springer International Publishing Switzerland 2014.",10.1007/978-3-319-13461-1_16,Ant colony optimization; Cloud computing; Virtual machine placement,31.0,
Bayesian reinforcement learning with exploration,"Lattimore T., Hutter M.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2014.0,We consider a general reinforcement learning problem and show that carefully combining the Bayesian optimal policy and an exploring policy leads to minimax sample-complexity bounds in a very general class of (history-based) environments. We also prove lower bounds and show that the new algorithm displays adaptive behaviour when the environment is easier than worst-case. © Springer International Publishing Switzerland 2014.,10.1007/978-3-319-11662-4_13,,3.0,
Feature selection with positive region constraint for test-cost-sensitive data,"Liu J., Min F., Zhao H., Zhu W.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2014.0,"In many data mining and machine learning applications, data are not free, and there is a test cost for each data item. Due to economic, technological and legal reasons, it is neither possible nor necessary to obtain a classifier with 100 % accuracy. In this paper, we consider such a situation and propose a new constraint satisfaction problem to address it. With this in mind, one has to minimize the test cost to keep the accuracy of the classification under a budget. The constraint is expressed by the positive region, whereas the object is to minimizing the total test cost. The new problem is essentially a dual of the test cost constraint attribute reduction problem, which has been addressed recently. We propose a heuristic algorithm based on the information gain, the test cost, and a user specified parameter to deal with the new problem. The algorithm is tested on four University of California - Irvine datasets with various test cost settings. Experimental results indicate that the algorithm finds optimal feature subset in most cases, the rational setting of is different among datasets, and the algorithm is especially stable when the test cost is subject to the Pareto distribution. © 2014 Springer-Verlag.",10.1007/978-3-662-44680-5_2,Cost-sensitive learning; Feature selection; Positive region; Test cost,1.0,
Transductive minimax probability machine,"Huang G., Song S., Xu Z., Weinberger K.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2014.0,"The Minimax Probability Machine (MPM) is an elegant machine learning algorithm for inductive learning. It learns a classifier that minimizes an upper bound on its own generalization error. In this paper, we extend its celebrated inductive formulation to an equally elegant transductive learning algorithm. In the transductive setting, the label assignment of a test set is already optimized during training. This optimization problem is an intractable mixed-integer programming. Thus, we provide an efficient label-switching approach to solve it approximately. The resulting method scales naturally to large data sets and is very efficient to run. In comparison with nine competitive algorithms on eleven data sets, we show that the proposed Transductive MPM (TMPM) almost outperforms all the other algorithms in both accuracy and speed. © 2014 Springer-Verlag.",10.1007/978-3-662-44848-9_37,minimax probability machine; semi-supervised learning; transductive learning,3.0,
"LS-VisionDraughts: Improving the performance of an agent for checkers by integrating computational intelligence, reinforcement learning and a powerful search method","Neto H.C., Julia R.M.S., Caexeta G.S., Barcelos A.R.A.",Applied Intelligence,2014.0,"This paper presents LS-VisionDraughts: an efficient unsupervised evolutionary learning system for Checkers whose contribution is to automate the process of selecting an appropriate representation for the board states - by means of Evolutionary Computation - keeping a deep look-ahead (search depth) at the moment of choosing an adequate move. It corresponds to a player Multi Layer Perceptron Neural Network whose weights are updated through an evaluation function that is automatically adjusted by means of the Temporal Difference methods. A Genetic Algorithm automatically chooses a concise and efficient set of functions, which describe various scenarios associated with Checkers - called features - to represent the board states in the input layer of the Neural Network. It means that each individual of the Genetic Algorithm is a candidate set of features that is associated to a distinct Multi Layer Perceptron Neural Network. The output layer of the Neural Network is a real number (prediction) that indicates to which extent the input state is favorable to provide a better agent performance. In LS-VisionDraughts, a particular version of the search algorithm Alpha-Beta, called fail-soft Alpha-Beta, combined with Table Transposition, Iterative Deepening and ordered tree, uses this prediction value to choose the best move corresponding to the current board state. The best individual is chosen by means of numerous tournaments involving these selfsame Neural Networks. The architecture of LS-VisionDraught is inspired on the agent NeuroDraughts. However, the former system enhances the performance of the latter by automating the selection of the features through Evolutionary Computation and by replacing its Minimax search algorithm with the improved search strategy resumed above. This procedure allows for a 95 % reduction in the search runtime. Further, it remarkably increases the search tree depth. The results obtained from evaluative tournaments confirm the advances of LS-VisionDraughts compared to its opponents. It is however important to point out that LS-VisionDraughts learns practically without human supervision, contrary to the current automatic world champion Chinook, which has been built in a strongly supervised manner. © 2014 Springer Science+Business Media New York.",10.1007/s10489-014-0536-y,Artificial intelligence; Artificial neural network; Evolutionary computation; Game theory; Genetic algorithms; Machine learning; Reinforcement learning; Temporal differences,11.0,
ABACUS: A technique for automated behavioral synthesis of approximate computing circuits,"Nepal K., Li Y., Bahar R.I., Reda S.","Proceedings -Design, Automation and Test in Europe, DATE",2014.0,"Many classes of applications, especially in the domains of signal and image processing, computer graphics, computer vision, and machine learning, are inherently tolerant to inaccuracies in their underlying computations. This tolerance can be exploited to design approximate circuits that perform within acceptable accuracies but have much lower power consumption and smaller area footprints (and often better run times) than their exact counterparts. In this paper, we propose a new class of automated synthesis methods for generating approximate circuits directly from behavioral-level descriptions. In contrast to previous methods that operate at the Boolean level or use custom modifications, our automated behavioral synthesis method enables a wider range of possible approximations and can operate on arbitrary designs. Our method first creates an abstract synthesis tree (AST) from the input behavioral description, and then applies variant operators to the AST using an iterative stochastic greedy approach to identify the optimal inexact designs in an efficient way. Our method is able to identify the optimal designs that represent the Pareto frontier trade-off between accuracy and power consumption. Our methodology is developed into a tool we call ABACUS, which we integrate with a standard ASIC experimental flow based on industrial tools. We validate our methods on three realistic Verilog-based benchmarks from three different domains - signal processing, computer vision and machine learning. Our tool automatically discovers optimal designs, providing area and power savings of up to 50% while maintaining good accuracy. © 2014 EDAA.",10.7873/DATE2014.374,,127.0,
A community detection method based on multi-objective optimization method,"Wang L., Liang Y.Q., Tian Q.J., Yang J., Song C., Wu Z.",Applied Mechanics and Materials,2014.0,"Community detection in complex network has been an active research area in data mining and machine learning. This paper proposed a community detection method based on multi-objective evolutionary algorithm, named CDMOEA, which tries to find the Pareto front by maximize two objectives, community score and community fitness. Fast and Elitist Multi-objective Genetic Algorithm is used to attained a set of optimal solutions, and then use Modularity function to choose the best one from them. The locus based adjacency representation is used to realize genetic representation, which ensures the effective connections of the nodes in the network during the process of population Initialization and other genetic operator. Uniform crossover is introduced to ensure population's diversity. We compared it with some popular community detection algorithms in computer generated network and real world networks. Experiment results show that it is more efficient in community detection. © (2014) Trans Tech Publications, Switzerland.",10.4028/www.scientific.net/AMM.571-572.177,Community detection; Complex network; Modularity; Multi-objective evolutionary algorithm; Pareto front,3.0,
A framework for imprecise robust one-class classification models,Utkin L.V.,International Journal of Machine Learning and Cybernetics,2014.0,"A framework for constructing robust one-class classification models is proposed in the paper. It is based on Walley's imprecise extensions of contaminated models which produce a set of probability distributions of data points instead of a single empirical distribution. The minimax and minimin strategies are used to choose an optimal probability distribution from the set and to construct optimal separating functions. It is shown that an algorithm for computing optimal parameters is determined by extreme points of the probability set and is reduced to a finite number of standard SVM tasks with weighted data points. Important special cases of the models, including pari-mutuel, constant odd-ratio, contaminated models and Kolmogorov-Smirnov bounds are studied. Experimental results with synthetic and real data illustrate the proposed models. © 2012 Springer-Verlag Berlin Heidelberg.",10.1007/s13042-012-0140-6,Classification; Machine learning; Minimax strategy; Novelty detection; Quadratic programming; Support vector machine,25.0,
Robust boosting classification models with local sets of probability distributions,"Utkin L.V., Zhuk Y.A.",Knowledge-Based Systems,2014.0,Robust classification models based on the ensemble methodology are proposed in the paper. The main feature of the models is that the precise vector of weights assigned for examples in the training set at each iteration of boosting is replaced by a local convex set of weight vectors. The minimax strategy is used for building weak classifiers at each iteration. The local sets of weights are constructed by means of imprecise statistical models. The proposed models are called RILBoost (Robust Imprecise Local Boost). Numerical experiments with real data show that the proposed models outperform the standard AdaBoost algorithm for several well-known data sets. © 2014 Elsevier B.V. All rights reserved.,10.1016/j.knosys.2014.02.007,Boosting; Classification; Imprecise model; Machine learning; Robust,11.0,
A genetic algorithm to multi-objective cost-sensitive attribute reduction,"Xu B., Min F., Zhu W., Chen H.",Journal of Computational Information Systems,2014.0,"Cost-sensitive learning is both hot and difficult in data mining and machine learning applications. Recently, some algorithms have been designed for the minimal test cost attribute reduction problem. They deal with either money cost or time cost, therefore they are single-objective. In this paper, we propose a genetic algorithm to the multi-objective attribute reduction problem involving multiple types of test cost, and define three metrics to evaluate the performance of our algorithm. In the algorithm, the fitness function is constructed based on positive region, the selected multiple types of test cost, a given set of weighting parameters and a user-specified non-positive exponent λ. We adopt the cross generational elitist selection strategy which can ensure better individuals maintained from one generation to the next. With different parameter settings, a number of reducts are produced and worse ones are filtered out. Then the remaining reducts form a Pareto optimal solution set. We test our algorithm with three representative cost distributions on four UCI datasets. Experimental results indicate that our algorithm performs well. © 2014 Binary Information Press.",10.12733/jcis9968,Attribute reduction; Cost-sensitive learning; Genetic algorithm; Rough sets,5.0,
Imprecise prior knowledge incorporating into one-class classification,"Utkin L.V., Zhuk Y.A.",Knowledge and Information Systems,2014.0,"An extension of Campbell and Bennett’s novelty detection or one-class classification model incorporating prior knowledge is studied in the paper. The proposed extension relaxes the strong assumption of the empirical probability distribution over elements of a training set and deals with a set of probability distributions produced by prior knowledge about training data. The classification problem is solved by considering extreme points of the probability distribution set or by means of the conjugate duality technique. Special cases of prior knowledge are considered in detail, including the imprecise linear-vacuous mixture model and interval-valued moments of feature values. Numerical experiments show that the proposed models outperform Campbell and Bennett’s model for many real and synthetic data. © 2013, Springer-Verlag London.",10.1007/s10115-013-0661-7,Extreme points; Imprecise statistical model; Linear programming; Machine learning; Minimax strategy; Novelty detection; One-class classification,8.0,
Privacy aware learning,"Duchi J.C., Jordan M.I., Wainwright M.J.",Journal of the ACM,2014.0,"We study statistical risk minimization problems under a privacy model in which the data is kept confidential even from the learner. In this local privacy framework, we establish sharp upper and lower bounds on the convergence rates of statistical estimation procedures. As a consequence, we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility, as measured by convergence rate, of any statistical estimator or learning procedure. © 2014 ACM.",10.1145/2666468,Differential privacy; Lower bounds; Machine learning; Minimax convergence rates; Saddle points,58.0,
A hybrid meta-learning architecture for multi-objective optimization of SVM parameters,"Miranda P.B.C., Prudêncio R.B.C., de Carvalho A.P.L.F., Soares C.",Neurocomputing,2014.0,"Support Vector Machines (SVMs) have achieved a considerable attention due to their theoretical foundations and good empirical performance when compared to other learning algorithms in different applications. However, the SVM performance strongly depends on the adequate calibration of its parameters. In this work we proposed a hybrid multi-objective architecture which combines meta-learning (ML) with multi-objective particle swarm optimization algorithms for the SVM parameter selection problem. Given an input problem, the proposed architecture uses a ML technique to suggest an initial Pareto front of SVM configurations based on previous similar learning problems; the suggested Pareto front is then refined by a multi-objective optimization algorithm. In this combination, solutions provided by ML are possibly located in good regions in the search space. Hence, using a reduced number of successful candidates, the search process would converge faster and be less expensive. In the performed experiments, the proposed solution was compared to traditional multi-objective algorithms with random initialization, obtaining Pareto fronts with higher quality on a set of 100 classification problems. © 2014 Elsevier B.V.",10.1016/j.neucom.2014.06.026,Meta-learning; Multi-objective optimization; Parameter selection; Particles swarm optimization; Support vector machines,29.0,
A general framework for evolutionary multiobjective optimization via manifold learning,"Li K., Kwong S.",Neurocomputing,2014.0,"Under certain mild condition, the Pareto-optimal set (PS) of a continuous multiobjective optimization problem, with m objectives, is a piece-wise continuous (m-1)-dimensional manifold. This regularity property is important, yet has been unfortunately ignored in many evolutionary multiobjective optimization (EMO) studies. The first work that explicitly takes advantages of this regularity property in EMO is the regularity model-based multiobjective estimation of distribution algorithm (RM-MEDA). However, its performance largely depends on its model parameter, which is problem dependent. Manifold learning, also known as nonlinear dimensionality reduction, is able to discover the geometric property of a low-dimensional manifold embedded in the high-dimensional ambient space. This paper presents a general framework that applies advanced manifold learning techniques in EMO. At each generation, we first use a principal curve algorithm to obtain an approximation of the PS manifold. Then, the Laplacian eigenmaps algorithm is employed to find the low-dimensional representation of this PS approximation. Afterwards, we identify the neighborhood relationship in this low-dimensional representation, which is also applicable for the original high-dimensional data. Based on the neighborhood relationship, we can interpolate new candidate solutions that obey the geometric property of the PS manifold. Empirical results validate the effectiveness of our proposal. © 2014 Elsevier B.V.",10.1016/j.neucom.2014.03.070,Evolutionary computation; Laplacian eigenmaps; Manifold learning; Multiobjective optimization; Principal curve; Regularity,24.0,
Statistical tests for joint analysis of performance measures,"Benavoli A., de Campos C.P.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2015.0,"Recently there has been an increasing interest in the development of new methods using Pareto optimality to deal with multiobjective criteria (for example, accuracy and architectural complexity). Once one has learned a model based on their devised method, the problem is then how to compare it with the state of art. In machine learning, algorithms are typically evaluated by comparing their performance on different data sets by means of statistical tests. Unfortunately, the standard tests used for this purpose are not able to jointly consider performance measures. The aim of this paper is to resolve this issue by developing statistical procedures that are able to account for multiple competing measures at the same time. In particular, we develop two tests: a frequentist procedure based on the generalized likelihood-ratio test and a Bayesian procedure based on a multinomial-Dirichlet conjugate model. We further extend them by discovering conditional independences among measures to reduce the number of parameter of such models, as usually the number of studied cases is very reduced in such comparisons. Real data from a comparison among general purpose classifiers is used to show a practical application of our tests. © Springer International Publishing Switzerland 2015.",10.1007/978-3-319-28379-1_6,,4.0,
Multiobjective design optimization in the Lightweight Dataflow for DDDAS Environment (LiD4E),"Sudusinghe K., Jiao Y., Salem H.B., Van Der Schaar M., Bhattacharyya S.S.",Procedia Computer Science,2015.0,"In this paper, we introduce new methods for multiobjective, system-level optimization that have been incorporated into the Lightweight Dataflow for Dynamic Data Driven Application Systems (DDDAS) Environment (LiD4E). LiD4E is a design tool for optimized implementation of dynamic, data-driven stream mining systems using high-level dataflow models of computation. More specifically, we develop in this paper new methods for integrated modeling and optimization of real-time stream mining constraints, multidimensional stream mining performance (precision and recall), and energy efficiency. Using a design methodology centered on data-driven control of and coordination between alternative dataflow subsystems for stream mining (classification modes), we develop systematic methods for exploring complex, multidimensional design spaces associated with dynamic stream mining systems, and deriving sets of Pareto-optimal system configurations that can be switched among based on data characteristics and operating constraints. © The Authors. Published by Elsevier B.V.",10.1016/j.procs.2015.05.364,Dataflow; DDDAS; Machine learning; Model-based design; Stream mining,3.0,
A flexible cluster-oriented alternative clustering algorithm for choosing from the Pareto front of solutions,"Truong D.T., Battiti R.",Machine Learning,2015.0,"Supervised alternative clustering is the problem of finding a set of clusterings which are of high quality and different from a given negative clustering. The task is therefore a clear multi-objective optimization problem. Optimizing two conflicting objectives at the same time requires dealing with trade-offs. Most approaches in the literature optimize these objectives sequentially (one objective after another one) or indirectly (by some heuristic combination of the objectives). Solving a multi-objective optimization problem in these ways can result in solutions which are dominated, and not Pareto-optimal. We develop a direct algorithm, called COGNAC, which fully acknowledges the multiple objectives, optimizes them directly and simultaneously, and produces solutions approximating the Pareto front. COGNAC performs the recombination operator at the cluster level instead of at the object level, as in the traditional genetic algorithms. It can accept arbitrary clustering quality and dissimilarity objectives and provides solutions dominating those obtained by other state-of-the-art algorithms. Based on COGNAC, we propose another algorithm called SGAC for the sequential generation of alternative clusterings where each newly found alternative clustering is guaranteed to be different from all previous ones. The experimental results on widely used benchmarks demonstrate the advantages of our approach. © 2013, The Author(s).",10.1007/s10994-013-5350-y,Alternative clustering; Cluster-oriented recombination; Genetic algorithms; Multi-objective optimization,4.0,
Improved multi-objective particle swarm optimization with preference strategy for optimal DG integration into the distribution system,"Cheng S., Chen M.-Y., Fleming P.J.",Neurocomputing,2015.0,"Considering the different requirements for decision and state variables in engineering optimizations, an improved multi-objective particle swarm optimization with preference strategy (IMPSO-PS) is presented and applied to the optimal integration of distributed generation (DG) into the distribution system. Preference factors are introduced to quantify the degree of preference for certain attributes in the constraint-space. In addition to the application of a popular non-dominated sorting technique for identifying Pareto solutions, the performance of IMPSO-PS is strengthened via the inclusion of a dynamic selection of the global bests, a novel circular non-dominated selection of particles from one iteration to the next and a special mutation operation. The proposed algorithm has been successfully applied to benchmark functions and to the multi-objective optimal integration of DG into an IEEE 33-bus system. This real-world application aims to satisfy some special preferences and determine the optimal locations and capacities of DG units to minimize the total active power loss of the system and decrease cost caused by power generation and pollutant emissions. The results show that the proposed approach can provide a wider range of Pareto solutions of high quality, while satisfying special preference demands. © 2014 Elsevier B.V.",10.1016/j.neucom.2012.08.074,Circular non-dominated selection; Distributed generation (DG); Multi-objective particle swarm optimization; NSGA-II; Optimal allocation; Preference strategy,38.0,
Robust classifiers using imprecise probability models and importance of classes,"Utkin L.V., Zhuk Y.A.",International Journal on Artificial Intelligence Tools,2015.0,"A framework for constructing robust classification models is proposed in the paper. An assumption about importance of one of the classes in comparison with other classes is incorporated into the models. It often takes place in the real application, for example, in reliability, in medical diagnostic, etc. A main idea underlying the models is to consider a set of probability distributions on training examples produced by the imprecise probability models such as linear-vacuous mixture and constant odd-ratio contaminated models. Extreme points of the sets of probability distributions are a main tool for constructing the robust classifiers. It is shown that algorithms for computing optimal classification parameters are reduced to a finite number of weighted support vector machines with weights determined by the extreme points. Experimental results with synthetic and real data illustrate the proposed models. © 2015 World Scientific Publishing Company.",10.1142/S0218213015500086,classification; extreme points; imprecise probability model; Machine learning; minimax strategy; quadratic programming; support vector machine,,
A probabilistic graphical model-based approach for minimizing energy under performance constraints,"Mishra N., Zhang H., Lafferty J.D., Hoffmann H.",International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS,2015.0,"In many deployments, computer systems are underutilized - meaning that applications have performance requirements that demand less than full system capacity. Ideally, we would take advantage of this under-utilization by allocating system resources so that the performance requirements are met and energy is minimized. This optimization problem is complicated by the fact that the performance and power consumption of various system configurations are often application - or even input - dependent. Thus, practically, minimizing energy for a performance constraint requires fast, accurate estimations of application-dependent performance and power tradeoffs. This paper investigates machine learning techniques that enable energy savings by learning Pareto-optimal power and performance tradeoffs. Specifically, we propose LEO, a probabilistic graphical model-based learning system that provides accurate online estimates of an application's power and performance as a function of system configuration. We compare LEO to (1) offline learning, (2) online learning, (3) a heuristic approach, and (4) the true optimal solution. We find that LEO produces the most accurate estimates and near optimal energy savings. Copyright © 2015 ACM.",10.1145/2694344.2694373,Adaptation; Dynamic systems; Energy minimization; Probabilistic graphical models; Statistics,27.0,
Thin-Walled Compliant Mechanism Component Design Assisted by Machine Learning and Multiple Surrogates,"Liu K., Tovar A., Nutwell E., Detwiler D.",SAE Technical Papers,2015.0,"This work introduces a new design algorithm to optimize progressively folding thin-walled structures and in order to improve automotive crashworthiness. The proposed design algorithm is composed of three stages: conceptual thickness distribution, design parameterization, and multi-objective design optimization. The conceptual thickness distribution stage generates an innovative design using a novel one-iteration compliant mechanism approach that triggers progressive folding even on irregular structures under oblique impact. The design parameterization stage optimally segments the conceptual design into a reduced number of clusters using a machine learning K-means algorithm. Finally, the multi-objective design optimization stage finds non-dominated designs of maximum specific energy absorption and minimum peak crushing force. The proposed optimization problem is addressed by a multi-objective genetic algorithm on sequentially updated surrogate models, which are optimally selected from a set of 24 surrogates. The effectiveness of the design algorithm is demonstrated on an S-rail thin-walled structure. The best compromised Pareto design increases specific energy absorption and decreases peak crushing force in the order of 8% and 12%, respectively. Copyright © 2015 SAE International.",10.4271/2015-01-1369,,8.0,
Improved sampling of decision space for pareto estimation,"Yan Y., Giagkiozis I., Fleming P.J.",GECCO 2015 - Proceedings of the 2015 Genetic and Evolutionary Computation Conference,2015.0,"Pareto Estimation (PE) is a novel method for increasing the density of Pareto optimal solutions across the entire Pareto Front or in a specific region of interest. PE identifies the inverse mapping of Pareto optimal solutions, namely, from objective space to decision space. This identification can be performed using a number of modeling techniques, however, for the sake of simplicity in this work we use a radial basis neural network. In any modeling method, the quality of the resulting model depends heavily on the training samples used. The original version of PE uses the resulting set of Pareto optimal solutions from any multi-objective optimization algorithm and then utilizes this set to identify the aforementioned mapping. However, we argue that this selection may not always be the best possible and propose an alternative scheme to improve the resulting set of Pareto optimal solutions in order to produce higher quality samples for the identification scheme in PE. The proposed approach is integrated with MAEA-gD, and the resulting solutions are used with PE. The results show that the proposed method shows promise, in that there is measurable improvement in the quality of the estimated PE in terms of the coverage and density. © 2015 ACM.",10.1145/2739480.2754713,Local search; Machine learning; Metaheuristics; Neural networks; Surrogate model/fitness approximation,1.0,
A new robust model of one-class classification by interval-valued training data using the triangular kernel,"Utkin L.V., Chekh A.I.",Neural Networks,2015.0,A robust one-class classification model as an extension of Campbell and Bennett's (C-B) novelty detection model on the case of interval-valued training data is proposed in the paper. It is shown that the dual optimization problem to a linear program in the C-B model has a nice property allowing to represent it as a set of simple linear programs. It is proposed also to replace the Gaussian kernel in the obtained linear support vector machines by the well-known triangular kernel which can be regarded as an approximation of the Gaussian kernel. This replacement allows us to get a finite set of simple linear optimization problems for dealing with interval-valued data. Numerical experiments with synthetic and real data illustrate performance of the proposed model. © 2015 Elsevier Ltd.,10.1016/j.neunet.2015.05.004,Extreme points; Interval-valued data; Kernel; Linear programming; Minimax strategy; Novelty detection; One-class classification; Support vector machine,10.0,
MyBehavior: Automatic personalized health feedback from user behaviors and preferences using smartphones,"Rabbi M., Aung M.H., Zhang M., Choudhury T.",UbiComp 2015 - Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing,2015.0,"Mobile sensing systems have made significant advances in tracking human behavior. However, the development of personalized mobile health feedback systems is still in its infancy. This paper introduces MyBehavior, a smartphone application that takes a novel approach to generate deeply personalized health feedback. It combines state-of-The-Art behavior tracking with algorithms that are used in recommendation systems. MyBehavior automatically learns a user's physical activity and dietary behavior and strategically suggests changes to those behaviors for a healthier lifestyle. The system uses a sequential decision making algorithm, Multiarmed Bandit, to generate suggestions that maximize calorie loss and are easy for the user to adopt. In addition, the system takes into account user's preferences to encourage adoption using the pareto-frontier algorithm. In a 14-week study, results show statistically significant increases in physical activity and decreases in food calorie when using MyBehavior compared to a control condition. Copyright © 2015 ACM.",10.1145/2750858.2805840,Health Feedback; Machine learning; Mobile Health; Mobile Phone Sensing,118.0,
Determination of seismic liquefaction potential of soil based on strain energy concept,"Samui P., Kim D., Hariharan R.",Environmental Earth Sciences,2015.0,"In the present study, minimax probability machine regression (MPMR) and extreme learning machine (ELM) have been adopted for prediction of seismic liquefaction of soil based on strain energy. Initial effective mean confining pressure (σ′mean), initial relative density after consolidation (Dr), percentage of fines content (FC), coefficient of uniformity (Cu), and mean grain size (D50) have been taken as inputs of MPMR and ELM models. MPMR and ELM have been used as regression techniques. The performances of MPMR and ELM have been compared with the artificial neural network. A sensitivity analysis has been carried out to determine the effect of each input. The experimental results demonstrate that proposed methods are robust models for determination seismic liquefaction potential of soil based on strain energy. © 2015, Springer-Verlag Berlin Heidelberg.",10.1007/s12665-015-4567-4,Extreme learning machine; Liquefaction; Minimax probability machine regression; Strain energy,6.0,
Multi-objective composite panel optimization using machine learning classifiers and genetic algorithms,"Zeliff K., Bennette W., Ferguson S.",Proceedings of the ASME Design Engineering Technical Conference,2016.0,"Design spaces that consist of millions or billions of design combinations pose a challenge to current methods for identifying optimal solutions. Complex analyses can also lead to lengthy computation times that further challenge the effectiveness of an algorithm in terms of solution quality and run-time. This work explores combining the design space exploration approach of a Multi-Objective Genetic Algorithm with different instance-based, statistical, rule-based and ensemble classifiers to reduce the number of unnecessary function evaluations associated with poorly performing designs. Results indicate that introducing a classifier to identify child designs that are likely to push the Pareto frontier toward an optima reduce the number of function calculations by 75-85%, depending on the classifier implemented. © Copyright 2016 by ASME.",10.1115/DETC2016-60125,,2.0,
Visualizing portfolio tradeoffs with perceptually accurate self-organizing maps,"Kostuik K., Mu Y., El-Beltagy M., Naiem A.",Proceedings - SPE Annual Technical Conference and Exhibition,2016.0,"Multi-objective optimization and unsupervised machine learning were used to visualize relationships between portfolio composition and business performance tradeoffs. A multiobjective global optimization algorithm reduced the exponentially large space of possible portfolio outcomes to a set of Pareto optimal tradeoff frontiers for various combinations of decision and outcome constraints. The high-dimensional decision space was then reduced with an unsupervised neural network to generate Kohonen self-organizing maps (SOMs) for each tradeoff frontier. The resulting decision SOMs were rendered with a perceptually accurate color map and interactively linked with the tradeoff frontiers to enable visual data mining. The interactive visualization revealed relationships between planning decisions and business outcomes in the complex and high-dimensional decision space. The interactive SOM automatically classifies similar strategies together and shows how neighboring strategies relate to each other. The analysis demonstrated that the proximity of portfolios on the tradeoff frontier does not imply similarity of underlying portfolio decisions, and that the mapping between decisions and outcomes becomes more complex with stricter constraints. Copyright 2016, Society of Petroleum Engineers.",10.2118/181632-ms,,,
Hybrid multi-objective optimization approach for neural network classification using local search,"Mane S., Sonawani S., Sakhare S.",Advances in Intelligent Systems and Computing,2016.0,"Classification is inherently multi-objective problem. It is one of the most important tasks of data mining to extract important patterns from large volume of data. Traditionally, either only one objective is considered or the multiple objectives are accumulated to one objective function. In the last decade, Pareto-based multi-objective optimization approach have gained increasing popularity due to the use of multi-objective optimization using evolutionary algorithms and population-based search methods. Multi-objective optimization approaches are more powerful than traditional single-objective methods as it addresses various topics of data mining such as classification, clustering, feature selection, ensemble learning, etc. This paper proposes improved approach of non-dominated sorting algorithm II (NSGA II) for classification using neural network model by augmenting with local search. It tries to enhance two conflicting objectives of classifier: Accuracy and mean squared error. NSGA II is improved by augmenting backpropagation as a local search method to deal with the disadvantage of genetic algorithm, i.e. slow convergence to best solutions. By using backpropagation we are able to speed up the convergence. This approach is applied in various classification problems obtained from UCI repository. The neural network modes obtained shows high accuracy and low mean squared error. © Springer Science+Business Media Singapore 2016.",10.1007/978-981-10-0419-3_21,Classificationm; Local search; Multi-objective optimization; Neural network; NSGA II; Pareto optimality,2.0,
Automatic Synthesizer Preset Generation with PresetGen,"Tatar K., Macret M., Pasquier P.",Journal of New Music Research,2016.0,"We refer the task of finding preset(s) (i.e. set(s) of synthesizer parameters) that approximates a target sound best, as the preset generation problem. PresetGen addresses this problem regarding the real world synthesizer, OP-1. The OP-1 consists of several synthesis blocks, and it is not fully deterministic. We propose and evaluate a solution to preset generation using a multi-objective Non-dominated Sorting-Genetic-Algorithm-II. PresetGen handles the full problem complexity and returns a small set of presets that approximate the target sound best by covering the Pareto front of this multi-objective optimization problem. Moreover, we present an empirical evaluation experiment that compares the performance of three human sound designers to that of PresetGen. The results show that PresetGen is human-competitive. © 2016 Informa UK Limited, trading as Taylor & Francis Group.",10.1080/09298215.2016.1175481,audio analysis; instruments; machine learning; sound synthesis,9.0,
Adaptive Threshold Non-Pareto Elimination: Re-thinking machine learning for system level design space exploration on FPGAs,"Meng P., Althoff A., Gautier Q., Kastner R.","Proceedings of the 2016 Design, Automation and Test in Europe Conference and Exhibition, DATE 2016",2016.0,"One major bottleneck of the system level OpenCL-to-FPGA design tools is their extremely time consuming synthesis process (including place and route). The design space for a typical OpenCL application contains thousands of possible designs even when considering a small number of design space parameters. It costs months of compute time to synthesize all these possible designs into end-to-end FPGA implementations. Thus, the brute force design space exploration (DSE) is impractical for these design tools. Machine learning is one solution that identifies the valuable Pareto designs by sampling only a small portion of the entire design space. However, most of the existing machine learning frameworks focus on improving the design objective regression accuracy, which is not necessarily suitable for the FPGA DSE task. To address this issue, we propose a novel strategy - Adaptive Threshold Non-Pareto Elimination (ATNE). Instead of focusing on regression accuracy improvement, ATNE focuses on understanding and estimating the inaccuracy. ATNE provides a Pareto identification threshold that adapts to the estimated inaccuracy of the regressor. This adaptive threshold results in a more efficient DSE. For the same prediction quality, ATNE reduces the synthesis complexity by 1.6 - 2.89× (hundreds of synthesis hours) against the other state of the art frameworks for FPGA DSE. In addition, ATNE is capable of identifying the Pareto designs for certain difficult design spaces which the other existing frameworks are incapable of exploring effectively. © 2016 EDAA.",10.3850/9783981537079_0350,,32.0,
Kernel-Based Domain-Invariant Feature Selection in Hyperspectral Images for Transfer Learning,"Persello C., Bruzzone L.",IEEE Transactions on Geoscience and Remote Sensing,2016.0,"This paper presents a kernel-based feature selection method for the classification of hyperspectral images. The proposed method aims at selecting a subset of the original features that are both 1) relevant (discriminant) for the considered classification problem, i.e., preserve the functional relationship between input and output variables, and 2) invariant (stable) across different domains, i.e., minimize the data-set shift between the source and the target domains. Domains can be associated with hyperspectral images collected either on different geographical areas or on the same area at different times. We propose a novel measure of data-set shift for evaluating the domain stability, which computes the distance of the conditional distributions between the source and target domains in a reproducing kernel Hilbert space. Such a measure is defined on the basis of the kernel embeddings of the conditional distributions resulting in a nonparametric approach that does not require estimating the distribution of the classes. The adopted search strategy is based on a multiobjective optimization algorithm, which optimizes the two terms of the criterion function for the estimation of the Pareto-optimal solutions. This results in an effective approach of performing feature selection in a transfer learning setting. The experimental results obtained on two hyperspectral images show the effectiveness of the proposed method in selecting features with high generalization capabilities. © 1980-2012 IEEE.",10.1109/TGRS.2015.2503885,Domain adaptation; feature selection; hyperspectral images; image classification; kernel methods; remote sensing; support vector machines (SVMs); transfer learning,81.0,
Machine learning approach to generate Pareto front for list-scheduling algorithms,"Pham N.K., Kumar A., Aung K.M.M.","Proceedings of the 19th International Workshop on Software and Compilers for Embedded Systems, SCOPES 2016",2016.0,"List Scheduling is one of the most widely used techniques for scheduling due to its simplicity and efficiency. In traditional list-based schedulers, a cost/priority function is used to compute the priority of tasks/jobs and put them in an ordered list. The cost function has been becoming more and more complex to cover increasing number of constraints in the system design. However, most of the existing list-based schedulers implement a static priority function that usually provides only one schedule for each task graph input. Therefore, they may not be able to satisfy the desire of system designers, who want to examine the trade-off between a number of design requirements (performance, power, energy, reliability ...). To address this problem, we propose a framework to utilize the Genetic Algorithm (GA) for exploring the design space and obtaining Pareto-optimal design points. Furthermore, multiple regression techniques are used to build predictive models for the Pareto fronts to limit the execution time of GA. The models are built using training task graph datasets and applied on incoming task graphs. The Pareto fronts for incoming task graphs are produced in time 2 orders of magnitude faster than the traditional GA, with only 4% degradation in the quality. © 2016 ACM.",10.1145/2906363.2906380,Design space exploration; List-scheduling; Machine learning,3.0,
Biobjective Nonnegative Matrix Factorization: Linear Versus Kernel-Based Models,"Zhu F., Honeine P.",IEEE Transactions on Geoscience and Remote Sensing,2016.0,"Nonnegative matrix factorization (NMF) is a powerful class of feature extraction techniques that has been successfully applied in many fields, particularly in signal and image processing. Current NMF techniques have been limited to a single-objective optimization problem, in either its linear or nonlinear kernel-based formulation. In this paper, we propose to revisit the NMF as a multiobjective problem, particularly a biobjective one, where the objective functions defined in both input and feature spaces are taken into account. By taking the advantage of the sum-weighted method from the literature of multiobjective optimization, the proposed biobjective NMF determines a set of nondominated, Pareto optimal, solutions. Moreover, the corresponding Pareto front is approximated and studied. Experimental results on unmixing synthetic and real hyperspectral images confirm the efficiency of the proposed biobjective NMF compared with the state-of-the-art methods. © 2016 IEEE.",10.1109/TGRS.2016.2535298,Hyperspectral image; kernel machines; nonnegative matrix factorization (NMF); Pareto optimal; unmixing problem,27.0,
Integrated Analysis of EEG and fMRI Using Sparsity of Spatial Maps,"Samadi S., Soltanian-Zadeh H., Jutten C.",Brain Topography,2016.0,"Integration of electroencephalography (EEG) and functional magnetic resonance imaging (fMRI) is an open problem, which has motivated many researches. The most important challenge in EEG-fMRI integration is the unknown relationship between these two modalities. In this paper, we extract the same features (spatial map of neural activity) from both modality. Therefore, the proposed integration method does not need any assumption about the relationship of EEG and fMRI. We present a source localization method from scalp EEG signal using jointly fMRI analysis results as prior spatial information and source separation for providing temporal courses of sources of interest. The performance of the proposed method is evaluated quantitatively along with multiple sparse priors method and sparse Bayesian learning with the fMRI results as prior information. Localization bias and source distribution index are used to measure the performance of different localization approaches with or without a variety of fMRI-EEG mismatches on simulated realistic data. The method is also applied to experimental data of face perception of 16 subjects. Simulation results show that the proposed method is significantly stable against the noise with low localization bias. Although the existence of an extra region in the fMRI data enlarges localization bias, the proposed method outperforms the other methods. Conversely, a missed region in the fMRI data does not affect the localization bias of the common sources in the EEG-fMRI data. Results on experimental data are congruent with previous studies and produce clusters in the fusiform and occipital face areas (FFA and OFA, respectively). Moreover, it shows high stability in source localization against variations in different subjects. © 2016, Springer Science+Business Media New York.",10.1007/s10548-016-0506-2,Elastic net; Integration analysis; Pareto optimization; Referenced-based signal processing; Weighted sparse decomposition,3.0,
A multiobjective based automatic framework for classifying cancer-microRNA biomarkers,"Saha S., Mitra S., Yadav R.K.",Gene Reports,2016.0,"Short endogenous RNA aka miRNAs play significant roles in biological processes like RNA silencing and regulation of gene expressions. Several studies have revealed that there might be possible links between oncogenesis and some miRNA expression profiles since profiles of some specific miRNAs are expressed differently in case of normal and tumor tissues. In this paper, a technique based on multiobjective optimization for automatic selection of classifier, its parameters and feature combinations is used for classifying the miRNAs. The proposed approach is divided into two stages. In the first stage, a multiobjective framework in combination with four different classifiers namely Random Tree (RT), Random Forest (RF), Sequential Minimal Optimization (SMO), and Logistic Regression is used. The multiobjective based framework is capable of automatically determining the appropriate classifier, its different parameter combinations and feature combinations for any classification problem. The proposed approach is very generic and can be solved using any multiobjective evolutionary approach. But in the current study the search capability of popular NSGA-II is used. A new encoding strategy is proposed in the current paper to represent all the relevant information (classifier type, parameter combination, feature combination) in the form of a chromosome. Several different mutation operators are developed to accelerate the search process. In the second stage, the best solutions obtained from the first stage are combined using two different approaches: frequency-based approach and a simple ensemble-based approach. The performance of the proposed method has been demonstrated on several real life miRNA and mRNA datasets. Biological relevance of the obtained biomarkers has been reported. Results are compared with several state-of-the-art approaches. © 2016 Published by Elsevier Inc.",10.1016/j.genrep.2016.04.001,MicroRNA markers; Multiobjective optimization; Pareto optimal front; Supervised classification,4.0,
Multi-objective reinforcement learning through continuous pareto manifold approximation,"Parisi S., Pirotta M., Restelli M.",Journal of Artificial Intelligence Research,2016.0,"Many real-world control applications, from economics to robotics, are characterized by the presence of multiple conicting objectives. In these problems, the standard concept of optimality is replaced by Pareto{optimality and the goal is to find the Pareto frontier, a set of solutions representing different compromises among the objectives. Despite recent advances in multi{objective optimization, achieving an accurate representation of the Pareto frontier is still an important challenge. In this paper, we propose a reinforcement learning policy gradient approach to learn a continuous approximation of the Pareto frontier in multi{objective Markov Decision Problems (MOMDPs). Differently from previous policy gradient algorithms, where n optimization routines are executed to have n solutions, our approach performs a single gradient ascent run, generating at each step an improved continuous approximation of the Pareto frontier. The idea is to optimize the parameters of a function defining a manifold in the policy parameters space, so that the corresponding image in the objectives space gets as close as possible to the true Pareto frontier. Besides deriving how to compute and estimate such gradient, we will also discuss the non{trivial issue of defining a metric to assess the quality of the candidate Pareto frontiers. Finally, the properties of the proposed approach are empirically evaluated on two problems, a linear-quadratic Gaussian regulator and a water reservoir control task. © 2016 AI Access Foundation. All rights reserved.",10.1613/jair.4961,,15.0,
Downscaling near-surface atmospheric fields with multi-objective Genetic Programming,"Zerenner T., Venema V., Friederichs P., Simmer C.",Environmental Modelling and Software,2016.0,"We present a new Genetic Programming based method to derive downscaling rules (i.e., functions or short programs) generating realistic high-resolution fields of atmospheric state variables near the surface given coarser-scale atmospheric information and high-resolution information on land surface properties. Such downscaling rules can be applied in coupled subsurface-land surface-atmosphere simulations or to generate high-resolution atmospheric input data for offline applications of land surface and subsurface models. Multiple features of the high-resolution fields, such as the spatial distribution of subgrid-scale variance, serve as objectives. The downscaling rules take an interpretable form and contain on average about 5 mathematical operations. The method is applied to downscale 10 m-temperature fields from 2.8 km to 400 m grid resolution. A large part of the spatial variability is reproduced, also in stable nighttime situations, which generate very heterogeneous near-surface temperature fields in regions with distinct topography. © 2016 The Authors",10.1016/j.envsoft.2016.06.009,Coupled modeling; Disaggregation; Evolutionary computation; Machine learning; Pareto optimality; Statistical downscaling,11.0,
Adversarial data mining: Big data meets cyber security,"Kantarcioglu M., Xi B.",Proceedings of the ACM Conference on Computer and Communications Security,2016.0,"As more and more cyber security incident data ranging from systems logs to vulnerability scan results are collected, manually analyzing these collected data to detect important cyber security events become impossible. Hence, data mining techniques are becoming an essential tool for real-world cyber security applications. For example, a report from Gartner [4] claims that ""Information security is becoming a big data analytics problem, where massive amounts of data will be correlated, analyzed and mined for meaningful patterns"". Of course, data mining/analytics is a means to an end where the ultimate goal is to provide cyber security analysts with prioritized actionable insights derived from big data. This raises the question, can we directly apply existing techniques to cyber security applications? One of the most important differences between data mining for cyber security and many other data mining applications is the existence of malicious adversaries that continuously adapt their behavior to hide their actions and to make the data mining models ineffective. Unfortunately, traditional data mining techniques are insufficient to handle such adversarial problems directly. The adversaries adapt to the data miner's reactions, and data mining algorithms constructed based on a training dataset degrades quickly. To address these concerns, over the last couple of years new and novel data mining techniques which is more resilient to such adversarial behavior are being developed in machine learning and data mining community. We believe that lessons learned as a part of this research direction would be beneficial for cyber security researchers who are increasingly applying machine learning and data mining techniques in practice. To give an overview of recent developments in adversarial data mining, in this three hour long tutorial, we introduce the foundations, the techniques, and the applications of adversarial data mining to cyber security applications. We first introduce various approaches proposed in the past to defend against active adversaries, such as a minimax approach to minimize the worst case error through a zero-sum game. We then discuss a game theoretic framework to model the sequential actions of the adversary and the data miner, while both parties try to maximize their utilities. We also introduce a modified support vector machine method and a relevance vector machine method to defend against active adversaries. Intrusion detection and malware detection are two important application areas for adversarial data mining models that will be discussed in details during the tutorial. Finally, we discuss some practical guidelines on how to use adversarial data mining ideas in generic cyber security applications and how to leverage existing big data management tools for building data mining algorithms for cyber security. © 2016 Copyright held by the owner/author(s).",10.1145/2976749.2976753,Adversarial data mining; Big data analytics for cyber security,4.0,
Multi-objective optimization of vehicle occupant restraint system by using evolutionary algorithm with response surface model,Horii H.,International Journal of Computational Methods and Experimental Measurements,2017.0,"This research reports a vehicle occupant restraint system design by using evolutionary multi-objective optimization with response surface model. The vehicle occupant restraint systems are composed of restraint equipment, such as an airbag, a seat belt and a knee bolster. The optimization aims to improve the safety of the system by evaluating some indexes based on some safety regulations. Estimation models of the safety indexes are introduced for accelerating the optimization. The estimation models, which are called the response surface models, are constructed by using Gaussian Process, which is a kind of machine learning method. The Gaussian Process constructs the estimation model from sampling results, which are calculated by using multi-body dynamics simulation. Some helpful information for designing the restraint systems, such as trade-off information of safety performance and contribution of design variables for the safety performance, is obtained by analysing the Pareto optimal solutions. © 2017 WIT Press.",10.2495/CMEM-V5-N2-163-170,Evolutionary algorithm; Machine learning; Multi-objective optimization; Occupant safety,3.0,
Benchmarking the peformance of a machine learning classifier enabled multiobjective genetic algorithm on six standard test functions,"Zeliff K., Bennette W., Ferguson S.",Proceedings of the ASME Design Engineering Technical Conference,2017.0,"Previous work tested a multi-objective genetic algorithm that was integrated with a machine learning classifier to reduce the number of objective function calls. Four machine learning classifiers and a baseline ""No Classifier"" option were evaluated. Using a machine learning classifier to create a hybrid multiobjective genetic algorithm reduced objective function calls by 75-85% depending on the classifier used. This work expands the analysis of algorithm performance by considering six standard benchmark problems from the literature. The problems are designed to test the ability of the algorithm to identify the Pareto frontier and maintain population diversity. Results indicate a tradeoff between the objectives of Pareto frontier identification and solution diversity. The ""No Classifier"" baseline multiobjective genetic algorithm produces the frontier with the closest proximity to the true frontier while a classifier option provides the greatest diversity when the number of generations is fixed. However, there is a significant reduction in computational expense as the number of objective function calls required is significantly reduced, highlighting the advantage of this hybrid approach. © Copyright 2017 ASME.",10.1115/DETC2017-68332,,3.0,
Intraoperative organ motion models with an ensemble of conditional generative adversarial networks,"Hu Y., Gibson E., Vercauteren T., Ahmed H.U., Emberton M., Moore C.M., Noble J.A., Barratt D.C.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017.0,"In this paper, we describe how a patient-specific, ultrasound-probe-induced prostate motion model can be directly generated from a single preoperative MR image. Our motion model allows for sampling from the conditional distribution of dense displacement fields, is encoded by a generative neural network conditioned on a medical image, and accepts random noise as additional input. The generative network is trained by a minimax optimisation with a second discriminative neural network, tasked to distinguish generated samples from training motion data. In this work, we propose that (1) jointly optimising a third conditioning neural network that pre-processes the input image, can effectively extract patient-specific features for conditioning; and (2) combining multiple generative models trained separately with heuristically pre-disjointed training data sets can adequately mitigate the problem of mode collapse. Trained with diagnostic T2-weighted MR images from 143 real patients and 73,216 3D dense displacement fields from finite element simulations of intraoperative prostate motion due to transrectal ultrasound probe pressure, the proposed models produced physically-plausible patient-specific motion of prostate glands. The ability to capture biomechanically simulated motion was evaluated using two errors representing generalisability and specificity of the model. The median values, calculated from a 10-fold cross-validation, were 2.8 ± 0.3 mm and 1.7 ± 0.1 mm, respectively. We conclude that the introduced approach demonstrates the feasibility of applying state-of-the-art machine learning algorithms to generate organ motion models from patient images, and shows significant promise for future research. © Springer International Publishing AG 2017.",10.1007/978-3-319-66185-8_42,,12.0,
Multi-objective particle swarm optimization approach for cost-based feature selection in classification,"Zhang Y., Gong D.-W., Cheng J.",IEEE/ACM Transactions on Computational Biology and Bioinformatics,2017.0,"Feature selection is an important data-preprocessing technique in classification problems such as bioinformatics and signal processing. Generally, there are some situations where a user is interested in not only maximizing the classification performance but also minimizing the cost that may be associated with features. This kind of problem is called cost-based feature selection. However, most existing feature selection approaches treat this task as a single-objective optimization problem. This paper presents the first study of multi-objective particle swarm optimization (PSO) for cost-based feature selection problems. The task of this paper is to generate a Pareto front of nondominated solutions, that is, feature subsets, to meet different requirements of decision-makers in real-world applications. In order to enhance the search capability of the proposed algorithm, a probability-based encoding technology and an effective hybrid operator, together with the ideas of the crowding distance, the external archive, and the Pareto domination relationship, are applied to PSO. The proposed PSO-based multi-objective feature selection algorithm is compared with several multi-objective feature selection algorithms on five benchmark datasets. Experimental results show that the proposed algorithm can automatically evolve a set of nondominated solutions, and it is a highly competitive feature selection method for solving cost-based feature selection problems. © 2004-2012 IEEE.",10.1109/TCBB.2015.2476796,Cost; Feature selection; Multi-objective; Particle swarm optimization,231.0,
From Extraction to Generation of Design Information -Paradigm Shift in Data Mining via Evolutionary Learning Classifier System,"Chiba K., Nakata M.",Procedia Computer Science,2017.0,"This paper aims at generating as well as extracting design strategies for a real world problem using an evolutionary learning classifier system. Data mining for a design optimization result as a virtual database specifies design information and discovers latent design knowledge. It is essential for decision making in real world problems. Although we employed several methods from classic statistics to artificial intelligence to obtain design information from optimization results, we may not cognize anything beyond a prepared database. In this study, we have applied an evolutionary learning classifier system as a data mining technique to a real world engineering problem. Consequently, not only it extracted known design information but also it successfully generated design strategies not to extract from the database. The generated design rules do not physically become innovative knowledge because the prepared dataset include Pareto solutions owing to complete exploration to the edge of the feasible region in the optimization. However, this problem is independent of the method, our evolutionary learning classifier system is a useful method for incomplete datasets. © 2017 The Authors. Published by Elsevier B.V.",10.1016/j.procs.2017.05.233,data mining; design information generation; evolutionary machine learning; knowledge discovery; learning classifier system; real-world application,,
Specializing a Planet's Computation: ASIC Clouds,"Khazraee M., Gutierrez L.V., Magaki I., Taylor M.B.",IEEE Micro,2017.0,"GPU- and FPGA-based clouds have been deployed to accelerate computationally intensive workloads. ASIC-based clouds are a natural evolution as cloud services expand across the planet. ASIC Clouds are purpose-built datacenters comprising large arrays of ASIC accelerators that optimize the total cost of ownership (TCO) of large, high-volume scale-out computations. On the surface, ASIC Clouds may seem improbable due to high nonrecurring engineering (NRE) costs and ASIC inflexibility, but large-scale ASIC Clouds have been deployed for the Bitcoin cryptocurrency system. This article distills lessons from these Bitcoin ASIC Clouds and applies them to other large-scale workloads, including YouTube-style video-transcoding and Deep Learning, showing superior TCO versus CPU and GPU. It derives Pareto-optimal ASIC Cloud servers based on accelerator properties, by jointly optimizing ASIC architecture, DRAM, motherboard, power delivery, cooling, and operating voltage. Finally, the authors examine the impact of ASIC NRE and when it makes sense to build an ASIC Cloud. © 1981-2012 IEEE.",10.1109/MM.2017.49,accelerator; ASIC Cloud; datacenter; nonrecurring engineering; NRE; planet-scale computation; TCO; total cost of ownership,6.0,
Morphological perceptrons: Geometry and training algorithms,"Charisopoulos V., Maragos P.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017.0,"Neural networks have traditionally relied on mostly linear models, such as the multiply-accumulate architecture of a linear perceptron that remains the dominant paradigm of neuronal computation. However, from a biological standpoint, neuron activity may as well involve inherently nonlinear and competitive operations. Mathematical morphology and minimax algebra provide the necessary background in the study of neural networks made up from these kinds of nonlinear units. This paper deals with such a model, called the morphological perceptron. We study some of its geometrical properties and introduce a training algorithm for binary classification. We point out the relationship between morphological classifiers and the recent field of tropical geometry, which enables us to obtain a precise bound on the number of linear regions of the maxout unit, a popular choice for deep neural networks introduced recently. Finally, we present some relevant numerical results. © Springer International Publishing AG 2017.",10.1007/978-3-319-57240-6_1,Machine learning; Mathematical morphology; Neural networks; Optimization; Tropical geometry,17.0,
Timing the decision support for real-world many-objective optimization problems,"Duro J.A., Saxena D.K.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017.0,"Lately, there is growing emphasis on improving the scalability of multi-objective evolutionary algorithms (MOEAs) so that manyobjective problems (characterized by more than three objectives) can be effectively dealt with. Alternatively, the utility of integrating decision maker’s (DM’s) preferences into the optimization process so as to target some most preferred solutions by the DM (instead of the whole Paretooptimal front), is also being increasingly recognized. The authors here, have earlier argued that despite the promises in the latter approach, its practical utility may be impaired by the lack of—objectivity, repeatability, consistency, and coherence in the DM’s preferences. To counter this, the authors have also earlier proposed a machine learning based decision support framework to reveal the preference-structure of objectives. Notably, the revealed preference-structure may be sensitive to the timing of application of this framework along an MOEA run. In this paper the authors counter this limitation, by integrating a termination criterion with an MOEA run, towards determining the appropriate timing for application of the machine learning based framework. Results based on three real-world many-objective problems considered in this paper, highlight the utility of the proposed integration towards an objective, repeatable, consistent, and coherent decision support for many-objective problems. © Springer International Publishing AG 2017.",10.1007/978-3-319-54157-0_14,,1.0,
"Surrogate-assisted multicriteria optimization: Complexities, prospective solutions, and business case","Allmendinger R., Emmerich M.T.M., Hakanen J., Jin Y., Rigoni E.",Journal of Multi-Criteria Decision Analysis,2017.0,"Complexity in solving real-world multicriteria optimization problems often stems from the fact that complex, expensive, and/or time-consuming simulation tools or physical experiments are used to evaluate solutions to a problem. In such settings, it is common to use efficient computational models, often known as surrogates or metamodels, to approximate the outcome (objective or constraint function value) of a simulation or physical experiment. The presence of multiple objective functions poses an additional layer of complexity for surrogate-assisted optimization. For example, complexities may relate to the appropriate selection of metamodels for the individual objective functions, extensive training time of surrogate models, or the optimal use of many-core computers to approximate efficiently multiple objectives simultaneously. Thinking out of the box, complexity can also be shifted from approximating the individual objective functions to approximating the entire Pareto front. This leads to further complexities, namely, how to validate statistically and apply the techniques developed to real-world problems. In this paper, we discuss emerging complexity-related topics in surrogate-assisted multicriteria optimization that may not be prevalent in nonsurrogate-assisted single-objective optimization. These complexities are motivated using several real-world problems in which the authors were involved. We then discuss several promising future research directions and prospective solutions to tackle emerging complexities in surrogate-assisted multicriteria optimization. Finally, we provide insights from an industrial point of view into how surrogate-assisted multicriteria optimization techniques can be developed and applied within a collaborative business environment to tackle real-world problems. Copyright © 2017 John Wiley & Sons, Ltd.",10.1002/mcda.1605,evolutionary multicriteria optimization; expensive optimization problems; machine learning; metamodels; multiple criteria decision making; surrogates,45.0,
Prior knowledge guided active modules identification: An integrated multi-objective approach,"Chen W., Liu J., He S.",BMC Systems Biology,2017.0,"Background: Active module, defined as an area in biological network that shows striking changes in molecular activity or phenotypic signatures, is important to reveal dynamic and process-specific information that is correlated with cellular or disease states. Methods: A prior information guided active module identification approach is proposed to detect modules that are both active and enriched by prior knowledge. We formulate the active module identification problem as a multi-objective optimisation problem, which consists two conflicting objective functions of maximising the coverage of known biological pathways and the activity of the active module simultaneously. Network is constructed from protein-protein interaction database. A beta-uniform-mixture model is used to estimate the distribution of p-values and generate scores for activity measurement from microarray data. A multi-objective evolutionary algorithm is used to search for Pareto optimal solutions. We also incorporate a novel constraints based on algebraic connectivity to ensure the connectedness of the identified active modules. Results: Application of proposed algorithm on a small yeast molecular network shows that it can identify modules with high activities and with more cross-talk nodes between related functional groups. The Pareto solutions generated by the algorithm provides solutions with different trade-off between prior knowledge and novel information from data. The approach is then applied on microarray data from diclofenac-treated yeast cells to build network and identify modules to elucidate the molecular mechanisms of diclofenac toxicity and resistance. Gene ontology analysis is applied to the identified modules for biological interpretation. Conclusions: Integrating knowledge of functional groups into the identification of active module is an effective method and provides a flexible control of balance between pure data-driven method and prior information guidance. © 2017 The Author(s).",10.1186/s12918-017-0388-2,Active module identification; Multi-objective evolutionary algorithm; Prior knowlege,9.0,
"Multi-class computational evolution: Development, benchmark evaluation and application to RNA-Seq biomarker discovery","Crabtree N.M., Moore J.H., Bowyer J.F., George N.I.",BioData Mining,2017.0,"Background: A computational evolution system (CES) is a knowledge discovery engine that can identify subtle, synergistic relationships in large datasets. Pareto optimization allows CESs to balance accuracy with model complexity when evolving classifiers. Using Pareto optimization, a CES is able to identify a very small number of features while maintaining high classification accuracy. A CES can be designed for various types of data, and the user can exploit expert knowledge about the classification problem in order to improve discrimination between classes. These characteristics give CES an advantage over other classification and feature selection algorithms, particularly when the goal is to identify a small number of highly relevant, non-redundant biomarkers. Previously, CESs have been developed only for binary class datasets. In this study, we developed a multi-class CES. Results: The multi-class CES was compared to three common feature selection and classification algorithms: support vector machine (SVM), random k-nearest neighbor (RKNN), and random forest (RF). The algorithms were evaluated on three distinct multi-class RNA sequencing datasets. The comparison criteria were run-time, classification accuracy, number of selected features, and stability of selected feature set (as measured by the Tanimoto distance). The performance of each algorithm was data-dependent. CES performed best on the dataset with the smallest sample size, indicating that CES has a unique advantage since the accuracy of most classification methods suffer when sample size is small. Conclusion: The multi-class extension of CES increases the appeal of its application to complex, multi-class datasets in order to identify important biomarkers and features. © 2017 The Author(s).",10.1186/s13040-017-0134-8,Artificial intelligence; Biomarker discovery; Classification; Data mining; Evolutionary algorithm; Feature selection; Genetic programming; Machine learning; Multi-class,6.0,
Discovery of search objectives in continuous domains,"Liskowski P., Krawiec K.",GECCO 2017 - Proceedings of the 2017 Genetic and Evolutionary Computation Conference,2017.0,"In genetic programming (GP), the outcomes of the evaluation phase can be represented as an interaction matrix, with rows corresponding to programs in a population and columns corresponding to tests that define a program synthesis task. Recent contributions on Discovery of Objectives via Clustering (DOC) and Discovery of Objectives by Factorization of interaction matrix (DOF) show that informative characterizations of programs can be automatically derived from interaction matrices in discrete domains and used as search objectives in multidimensional setting. In this paper, we propose analogous methods for continuous domains and compare them with conventional GP that uses tournament selection, Age-Fitness Pareto Optimization, and GP with epsilon-lexicase selection. Experiments show that the proposed methods are effective for symbolic regression, systematically producing better-fitting models than the two former baselines, and surpassing epsilon-lexicase selection on some problems. We also investigate the hybrids of the proposed approach with the baselines, concluding that hybridization of DOC with epsilon-lexicase leads to the best overall results. © 2017 ACM.",10.1145/3071178.3071344,Genetic Programming; Machine Learning; Multiobjective optimization; Nonnegative Matrix Factorization,5.0,
A decomposition-based binary ACO algorithm for the multiobjective UBQP,"Zangari M., Pozo A., Santana R., Mendiburu A.",Neurocomputing,2017.0,"The multiobjective unconstrained binary quadratic programming (mUBQP) is a combinatorial optimization problem which is able to represent several multiobjective optimization problems (MOPs). The problem can be characterized by the number of variables, the number of objectives and the objective correlation strength. Multiobjective evolutionary algorithms (MOEAs) are known as an efficient technique for solving MOPs. Moreover, several recent studies have shown the effectiveness of the MOEA/D framework applied to different MOPs. Previously, we have presented a preliminary study on an algorithm based on MOEA/D framework and the bio-inspired metaheuristic called binary ant colony optimization (BACO). The metaheuristic uses a positive feedback mechanism according to the best solutions found so far to update a probabilistic model which maintains the learned information. This paper presents the improved MOEA/D-BACO framework for solving the mUBQP. The components (i) mutation-like effect, and (ii) diversity preserving method are incorporated into the framework to enhance its search ability avoiding the premature convergence of the model and consequently maintaining a more diverse population of solutions. Experimental studies were conducted on a set of mUBQP instances. The results have shown that the proposed MOEA/D-BACO has outperformed MOEA/D, which uses genetic operators, in most of the test instances. Moreover, the algorithm has produced competitive results in comparison to the best approximated Pareto fronts from the literature. © 2017 Elsevier B.V.",10.1016/j.neucom.2016.09.122,Binary Ant Colony Optimization; MOEA/D; mUBQP; Multiobjective optimization problems; Probabilistic modeling,15.0,
A pareto-based ensemble with feature and instance selection for learning from multi-class imbalanced datasets,"Fernández A., Carmona C.J., José Del Jesus M., Herrera F.",International Journal of Neural Systems,2017.0,"Imbalanced classification is related to those problems that have an uneven distribution among classes. In addition to the former, when instances are located into the overlapped areas, the correct modeling of the problem becomes harder. Current solutions for both issues are often focused on the binary case study, as multi-class datasets require an additional effort to be addressed. In this research, we overcome these problems by carrying out a combination between feature and instance selections. Feature selection will allow simplifying the overlapping areas easing the generation of rules to distinguish among the classes. Selection of instances from all classes will address the imbalance itself by finding the most appropriate class distribution for the learning task, as well as possibly removing noise and difficult borderline examples. For the sake of obtaining an optimal joint set of features and instances, we embedded the searching for both parameters in a Multi-Objective Evolutionary Algorithm, using the C4.5 decision tree as baseline classifier in this wrapper approach. The multi-objective scheme allows taking a double advantage: the search space becomes broader, and we may provide a set of different solutions in order to build an ensemble of classifiers. This proposal has been contrasted versus several state-of-the-art solutions on imbalanced classification showing excellent results in both binary and multi-class problems. © 2017 World Scientific Publishing Company.",10.1142/S0129065717500289,ensembles; feature selection; Imbalanced classification; instance selection; multi-class; multi-objective evolutionary algorithms; overlapping,36.0,
A temporal difference method for multi-objective reinforcement learning,"Ruiz-Montiel M., Mandow L., Pérez-de-la-Cruz J.-L.",Neurocomputing,2017.0,"This work describes MPQ-learning, an algorithm that approximates the set of all deterministic non-dominated policies in multi-objective Markov decision problems, where rewards are vectors and each component stands for an objective to maximize. MPQ-learning generalizes directly the ideas of Q-learning to the multi-objective case. It can be applied to non-convex Pareto frontiers and finds both supported and unsupported solutions. We present the results of the application of MPQ-learning to some benchmark problems. The algorithm solves successfully these problems, so showing the feasibility of this approach. We also compare MPQ-learning to a standard linearization procedure that computes only supported solutions and show that in some cases MPQ-learning can be as effective as the scalarization method. © 2017 Elsevier B.V.",10.1016/j.neucom.2016.10.100,MOMDPs; Multi-objective optimization; Q-learning; Reinforcement learning,12.0,
Manifold-based multi-objective policy search with sample reuse,"Parisi S., Pirotta M., Peters J.",Neurocomputing,2017.0,"Many real-world applications are characterized by multiple conflicting objectives. In such problems optimality is replaced by Pareto optimality and the goal is to find the Pareto frontier, a set of solutions representing different compromises among the objectives. Despite recent advances in multi-objective optimization, achieving an accurate representation of the Pareto frontier is still an important challenge. Building on recent advances in reinforcement learning and multi-objective policy search, we present two novel manifold-based algorithms to solve multi-objective Markov decision processes. These algorithms combine episodic exploration strategies and importance sampling to efficiently learn a manifold in the policy parameter space such that its image in the objective space accurately approximates the Pareto frontier. We show that episode-based approaches and importance sampling can lead to significantly better results in the context of multi-objective reinforcement learning. Evaluated on three multi-objective problems, our algorithms outperform state-of-the-art methods both in terms of quality of the learned Pareto frontier and sample efficiency. © 2017 Elsevier B.V.",10.1016/j.neucom.2016.11.094,Black-box optimization; Importance sampling; Multi-objective; Policy search; Reinforcement learning,14.0,
Context-aware generative adversarial privacy,"Huang C., Kairouz P., Chen X., Sankar L., Rajagopal R.",Entropy,2017.0,"Preserving the utility of published datasets while simultaneously providing provable privacy guarantees is a well-known challenge. On the one hand, context-free privacy solutions, such as differential privacy, provide strong privacy guarantees, but often lead to a significant reduction in utility. On the other hand, context-aware privacy solutions, such as information theoretic privacy, achieve an improved privacy-utility tradeoff, but assume that the data holder has access to dataset statistics. We circumvent these limitations by introducing a novel context-aware privacy framework called generative adversarial privacy (GAP). GAP leverages recent advancements in generative adversarial networks (GANs) to allow the data holder to learn privatization schemes from the dataset itself. Under GAP, learning the privacy mechanism is formulated as a constrained minimax game between two players: a privatizer that sanitizes the dataset in a way that limits the risk of inference attacks on the individuals' private variables, and an adversary that tries to infer the private variables from the sanitized dataset. To evaluate GAP's performance, we investigate two simple (yet canonical) statistical dataset models: (a) the binary data model; and (b) the binary Gaussian mixture model. For both models, we derive game-theoretically optimal minimax privacy mechanisms, and show that the privacy mechanisms learned from data (in a generative adversarial fashion) match the theoretically optimal ones. This demonstrates that our framework can be easily applied in practice, even in the absence of dataset statistics. © 2017 by the authors.",10.3390/e19120656,Adversarial network; Differential privacy; Error probability games; Generative adversarial networks; Generative adversarial privacy; Information theoretic privacy; Machine learning; Mutual information privacy; Privatizer network; Statistical data privacy,50.0,
Vehicle occupant restraint system design under uncertainty by using multi-objective robust design optimization,Horii H.,International Journal of Computational Methods and Experimental Measurements,2018.0,"This research reports a vehicle occupant restraint system design that takes account of uncertainties of crash conditions and situations by using a multi-objective robust design optimization method called MORDO. The vehicle occupant restraint system is composed of restraint equipment, such as an airbag, a seatbelt and a knee bolster. The optimization aims to improve the safety performance of the system and its robustness simultaneously. The safety of the system is evaluated by some indexes based on some safety regulations, which are calculated by response surface model of an occupant at a crash. In addition, its robustness is evaluated by the mean value and the standard deviation of objective functions, which are calculated by using Monte Carlo simulation based on a certain probabilistic distribution in space of design variables around each design candidate. Some helpful information for designing the restraint systems, such as trade-off information of safety performance and its robustness, are provided by visualizing and analysing the Pareto optimal solutions. © 2018 WIT Press.",10.2495/CMEM-V6-N4-827-834,Evolutionary algorithm; Machine learning; Multi-objective optimization; Occupant safety; Robust optimization,1.0,
A comparative study on large scale kernelized support vector machines,"Horn D., Demircioğlu A., Bischl B., Glasmachers T., Weihs C.",Advances in Data Analysis and Classification,2018.0,"Kernelized support vector machines (SVMs) belong to the most widely used classification methods. However, in contrast to linear SVMs, the computation time required to train such a machine becomes a bottleneck when facing large data sets. In order to mitigate this shortcoming of kernel SVMs,many approximate training algorithms were developed. While most of these methods claim to be much faster than the state-of-the-art solver LIBSVM, a thorough comparative study is missing.We aim to fill this gap.We choose several well-known approximate SVM solvers and compare their performance on a number of large benchmark data sets. Our focus is to analyze the trade-off between prediction error and runtime for different learning and accuracy parameter settings. This includes simple subsampling of the data, the poor-man’s approach to handling large scale problems. We employ model-based multi-objective optimization, which allows us to tune the parameters of learning machine and solver over the full range of accuracy/runtime trade-offs. We analyze (differences between) solvers by studying and comparing the Pareto fronts formed by the two objectives classification error and training time. Unsurprisingly, givenmore runtimemost solvers are able to find more accurate solutions, i.e., achieve a higher prediction accuracy. It turns out that LIBSVM with subsampling of the data is a strong baseline. Some solvers systematically outperform others, which allows us to give concrete recommendations of when to use which solver. © Springer-Verlag Berlin Heidelberg 2016.",10.1007/s11634-016-0265-7,Large scale; Machine learning; Multi-objective optimization; Nonlinear SVM; Parameter tuning; Supervised learning; Support vector machine,7.0,
Multi-objective evolutionary neural network to predict graduation success at the United States Military Academy,"Lesinski G., Corns S.",Procedia Computer Science,2018.0,"This paper presents an evolutionary neural network approach to classify student graduation status based upon selected academic, demographic, and other indicators. A pareto-based, multi-objective evolutionary algorithm utilizing the Strength Pareto Evolutionary Algorithm (SPEA2) fitness evaluation scheme simultaneously evolves connection weights and identifies the neural network topology using network complexity and classification accuracy as objective functions. A combined vector-matrix representation scheme and differential evolution recombination operators are employed. The model is trained, tested, and validated using 5100 student samples with data compiled from admissions records and institutional research databases. The inputs to the evolutionary neural network model are used to classify students as: graduates, late graduates, or non-graduates. Results of the hybrid method show higher mean classification rates (88%) than the current methodology (80%) with a potential savings of $130M. Additionally, the proposed method is more efficient in that a less complex neural network topology is identified by the algorithm. © 2018 The Authors. Published by Elsevier B.V.",10.1016/j.procs.2018.10.329,Enrollment management; Evolutionary Algorithms; Multi-objective Evolutionary Algorithms; Neural network; Student retention,4.0,
Regularized semi-supervised least squares regression with dependent samples,"Tong H., Ng M.",Communications in Mathematical Sciences,2018.0,"In this paper, we study regularized semi-supervised least squares regression with dependent samples. We analyze the regularized algorithm based on reproducing kernel Hilbert spaces, and show, with the use of unlabelled data that the regularized least squares algorithm can achieve the nearly minimax optimal learning rate with a logarithmic term for dependent samples. Our new results are better than existing results in the literature. © 2018 International Press.",10.4310/CMS.2018.v16.n5.a08,Least squares regression; Non-iid sampling; Regularization; Semi-supervised learning,,
"New value metrics using unsupervised machine learning, lexical link analysis and game theory for discovering innovation from big data and crowd-sourcing","Zhao Y., Zhou C., Bellonio J.K.","IC3K 2018 - Proceedings of the 10th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management",2018.0,"We demonstrated a machine learning and artificial intelligence method, i.e., lexical link analysis (LLA) to discover innovative ideas from big data. LLA is an unsupervised machine learning paradigm that does not require manually labeled training data. New value metrics are defined based on LLA and game theory. In this paper, we show the value metrics generated from LLA in a use case of an internet game and crowd-sourcing. We show the results from LLA are validated and correlated with the ground truth. The LLA value metrics can be used to select high-value information for a wide range of applications. Copyright © 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved",10.5220/0006959403270334,Big data; Crowd-sourcing; Game theory; Lexical link analysis; Nash equilibrium; Pareto efficient; Pareto superior; Social welfare; Unsupervised learning,,
Effects of Thermal and Auxiliary Dynamics on a Fuel Cell Based Range Extender,"Oruganti P.S., Ahmed Q., Jung D.",SAE Technical Papers,2018.0,"Batteries are useful in Fuel Cell Hybrid Electric Vehicles (FCHEV) to fulfill transient demands and for regenerative braking. Efficient energy management strategies paired with optimal powertrain design further improves the efficiency. In this paper, a new methodology to simultaneously size the propulsive elements and optimize the power-split strategy of a Range Extended Battery Electric Vehicle (REBEV), using a Polymer Electron Membrane Fuel Cell (PEMFC), is proposed and preliminary studies on the effects of the driving mission profile and the auxiliary power loads on the sizing and optimal performance of the powertrain design are carried out. Dynamic Programming is used to compute the optimal energy management strategy for a given driving mission profile, providing a global optimal solution. The component sizing problem is performed using a machine learning based, guided design space exploration to find the set of Pareto-optimal solutions that give the best trade-offs between the different objectives. The powertrain model includes the dynamic behavior of the fuel cell system compressor and a battery lumped parameter thermal model along with the quasi-static semi-empirical model of the fuel cell and a zero-order battery model. Initial results indicate an increase in the Pareto-optimal sizes with the inclusion of thermal management. © 2018 SAE International. All Rights Reserved.",10.4271/2018-01-1311,,1.0,
From extraction of local structures of protein energy landscapes to improved decoy selection in template-free protein structure prediction,"Akhter N., Shehu A.",Molecules,2018.0,"Due to the essential role that the three-dimensional conformation of a protein plays in regulating interactions with molecular partners, wet and dry laboratories seek biologically-active conformations of a protein to decode its function. Computational approaches are gaining prominence due to the labor and cost demands of wet laboratory investigations. Template-free methods can now compute thousands of conformations known as decoys, but selecting native conformations from the generated decoys remains challenging. Repeatedly, research has shown that the protein energy functions whose minima are sought in the generation of decoys are unreliable indicators of nativeness. The prevalent approach ignores energy altogether and clusters decoys by conformational similarity. Complementary recent efforts design protein-specific scoring functions or train machine learning models on labeled decoys. In this paper, we show that an informative consideration of energy can be carried out under the energy landscape view. Specifically, we leverage local structures known as basins in the energy landscape probed by a template-free method. We propose and compare various strategies of basin-based decoy selection that we demonstrate are superior to clustering-based strategies. The presented results point to further directions of research for improving decoy selection, including the ability to properly consider the multiplicity of native conformations of proteins. © 2018 by the authors.",10.3390/molecules23010216,Basins; Conformational space; Decoy selection; Energy landscape; Pareto optimality; Template-free protein structure prediction,26.0,
Analysis of and modeling for emergency medical services facility location for road accidents on highway,"Bharsakade R.S., Kulkarni O.S., Afle A.S., Kulkarni M.S.",International Journal of Mechanical and Production Engineering Research and Development,2018.0,"This paper analyses the accident-prone regions along the 94.5km stretch of Mumbai-Pune express highway to determine optimal base locations for dispatching Emergency Medical Services (EMS). The study was driven by an increasing concern over the rise in accident fatalities on the expressway, and the alarming inadequacy of ambulance services. Our research aims to determine the optimal emergency medical service (EMS) facility locations to address the needs generated by accidents on expressways with heavy flowing traffic. Expressways present the unique problem of being isolated from hospitals and trauma centers located within cities, as well as practical constraints in turning vehicles around due to unidirectional heavy flow of traffic. We have used the mean shift algorithm of machine learning to determine clusters along with their centers. We further employed the minimax facility location model for rectilinear distances to determine a second base location within the primary clusters. The purpose of the secondary base locations is to supplement the primary EMS locations and distribute the load to ensure that the dispatched ambulances reach the accident spot in 8 minutes. The proposed model combines a traditional approach with a machine learning algorithm to produce faster output. © TJPRC Pvt. Ltd.",10.24247/ijmperdfeb201866,Accident-prone regions; Unidirectional heavy flow.& produce faster output,,
Multi-objective artificial immune algorithm for fuzzy clustering based on multiple kernels,"Shang R., Zhang W., Li F., Jiao L., Stolkin R.","2017 IEEE Symposium Series on Computational Intelligence, SSCI 2017 - Proceedings",2018.0,"This paper presents a multi-objective artificial immune algorithm for fUzzy clustering based on multiple kernels (MAFC). MAFC extends the classical Fuzzy C-Means (FCM) algorithm and overcomes its important limitations, such as limited adaptability, poor handling of non-linear relationships between data, and vulnerability to local optima convergence, which can lead to poor clustering quality. To compensate these limitations, MAFC unifies multi-kernel learning and multi-objective optimization in a joint clustering framework, which preserves the geometric information of the dataset. The multikernel method maps data from the feature space to kernel space by kernel functions. This approach is effective, not only for spherical clusters, but can also discover the non-linear relationships between data, and adds robustness to the particular choice of kernel functions. Additionally, the introduction of multi-objective optimization can optimize between-cluster separation and within-cluster compactness simultaneously via two different clustering validity criteria. These properties help the proposed algorithm to avoid becoming stuck at local optima. Furthermore, this paper utilizes an artificial immune algorithm to address the multi-objective clustering problem and acquire a Pareto optimal solution set. The solution set is obtained through the process of antibody population initialization, clone proliferation, non-uniform mutation and uniformity maintaining strategy, which avoids the problems of degradation and prematurity which can occur with conventional genetic algorithms. Finally, we choose the best solution, from the Pareto optimal solution set, using a semi-supervised method, to achieve the final clustering results. We compare our method against three state-of-the-art methods from the literature by performing experiments with both UCI datasets and face datasets. The results suggest that MAFC is significantly more efficient for clustering and has a wider scope of application. © 2017 IEEE.",10.1109/SSCI.2017.8285279,artificial immune algorithm; fuzzy c-means (FCM); multi-objective optimization; multiple kernel learning,3.0,
Multiobjective optimization of ethylene cracking furnace system using self-adaptive multiobjective teaching-learning-based optimization,"Yu K., While L., Reynolds M., Wang X., Liang J.J., Zhao L., Wang Z.",Energy,2018.0,"The ethylene cracking furnace system is crucial for an olefin plant. Multiple cracking furnaces are used to convert various hydrocarbon feedstocks to smaller hydrocarbon molecules, and the operational conditions of these furnaces significantly influence product yields and fuel consumption. This paper develops a multiobjective operational model for an industrial cracking furnace system that describes the operation of each furnace based on current feedstock allocations, and uses this model to optimize two important and conflicting objectives: maximization of key products yield, and minimization of the fuel consumed per unit ethylene. The model incorporates constraints related to material balance and the outlet temperature of transfer line exchanger. The self-adaptive multiobjective teaching-learning-based optimization algorithm is improved and used to solve the designed multiobjective optimization problem, obtaining a Pareto front with a diverse range of solutions. A real industrial case is investigated to illustrate the performance of the proposed model: the set of solutions returned offers a diverse range of options for possible implementation, including several solutions with both significant improvement in product yields and lower fuel consumption, compared with typical operational conditions. © 2018 Elsevier Ltd",10.1016/j.energy.2018.01.159,Ethylene cracking furnace; Fuel consumption; Multiobjective optimization; Product yield; Teaching-learning-based optimization,33.0,
Generative adversarial network based telecom fraud detection at the receiving bank,"Zheng Y.-J., Zhou X.-H., Sheng W.-G., Xue Y., Chen S.-Y.",Neural Networks,2018.0,"Recently telecom fraud has become a serious problem especially in developing countries such as China. At present, it can be very difficult to coordinate different agencies to prevent fraud completely. In this paper we study how to detect large transfers that are sent from victims deceived by fraudsters at the receiving bank. We propose a new generative adversarial network (GAN) based model to calculate for each large transfer a probability that it is fraudulent, such that the bank can take appropriate measures to prevent potential fraudsters to take the money if the probability exceeds a threshold. The inference model uses a deep denoising autoencoder to effectively learn the complex probabilistic relationship among the input features, and employs adversarial training that establishes a minimax game between a discriminator and a generator to accurately discriminate between positive samples and negative samples in the data distribution. We show that the model outperforms a set of well-known classification methods in experiments, and its applications in two commercial banks have reduced losses of about 10 million RMB in twelve weeks and significantly improved their business reputation. © 2018 Elsevier Ltd",10.1016/j.neunet.2018.02.015,Deep learning; Denoising autoencoder; Fraud detection; Generative adversarial network (GAN); Intelligent data analysis,44.0,
Multi-objective autotuning of mobile nets across the full software/hardware stack,"Lokhmotov A., Vella F., Chunosov N., Fursin G.","Proceedings of the 1st Reproducible Quality-Efficient Systems Tournament on Co-Designing Pareto-Efficient Deep Learning, ReQuEST 2018 -  Co-located with ACM ASPLOS 2018",2018.0,"We present a customizable Collective Knowledge workflow to study the execution time vs. accuracy trade-offs for the MobileNets CNN family. We use this workflow to evaluate MobileNets on Arm Cortex CPUs using TensorFlow and Arm Mali GPUs using several versions of the Arm Compute Library. Our optimizations for the Arm Bifrost GPU architecture reduce the execution time by 2-3 times, while lying on a Pareto-optimal frontier. We also highlight the challenge of maintaining the accuracy when deploying CNN models across diverse platforms. We make all the workflow components (models, programs, scripts, etc.) publicly available to encourage further exploration by the community. © 2018 Copyright held by the owner/author(s).",10.1145/3229762.3229767,Accuracy; Autotuning; Collective Knowledge; Crowdtuning; Customizable workflows; Live scoreboard; MobileNets; Performance; Reproducible experimentation; System co-design,6.0,
Highly efficient 8-bit low precision inference of convolutional neural networks with intelcaffe,"Gong J., Shen H., Zhang G., Liu X., Li S., Jin G., Maheshwari N., Fomenko E., Segal E.","Proceedings of the 1st Reproducible Quality-Efficient Systems Tournament on Co-Designing Pareto-Efficient Deep Learning, ReQuEST 2018 -  Co-located with ACM ASPLOS 2018",2018.0,"High throughput and low latency inference of deep neural networks are critical for the deployment of deep learning applications. This paper presents the efficient inference techniques of IntelCaffe, the first Intel optimized deep learning framework that supports efficient 8-bit low precision inference and model optimization techniques of convolutional neural networks on Intel Xeon Scalable Processors. The 8-bit optimized model is automatically generated with a calibration process from FP32 model without the need of fine-tuning or retraining. We show that the inference throughput and latency with ResNet-50, Inception-v3 and SSD are improved by 1.38X-2.9X and 1.35X-3X respectively with neglectable accuracy loss from IntelCaffe FP32 baseline and by 56X-75X and 26X-37X from BVLC Caffe. All these techniques have been open-sourced on IntelCaffe GitHub1, and the artifact is provided to reproduce the result on Amazon AWS Cloud. © 2018 Association for Computing Machinery.",10.1145/3229762.3229763,Convolutional Neural Network; Deep Learning; Intel Caffe; Model Optimization,16.0,
Optimizing deep learning workloads on ARM GPU with TVM,"Zheng L., Chen T.","Proceedings of the 1st Reproducible Quality-Efficient Systems Tournament on Co-Designing Pareto-Efficient Deep Learning, ReQuEST 2018 -  Co-located with ACM ASPLOS 2018",2018.0,"With the great success of deep learning, the demand for deploying deep neural networks to mobile devices is growing rapidly. However, current popular deep learning frameworks are often poorly optimized for mobile devices, especially mobile GPU. In this paper, we follow the pipeline proposed by TVM/NNVM, and optimize both kernel implementations and dataflow graph for ARM Mali GPU. Compared with vendor-provided ARM Compute Library, our kernel implementations and end-to-end pipeline are 1.7x faster on VGG16 and 2.2x faster on mobilenet. © 2018 Copyright held by the owner/author(s).",10.1145/3229762.3229764,ARM GPU; Deep Learning; GPU Kernel; TVM,4.0,
Real-time image recognition using collaborative IoT devices,"Hadidi R., Cao J., Woodward M., Ryoo M.S., Kim H.","Proceedings of the 1st Reproducible Quality-Efficient Systems Tournament on Co-Designing Pareto-Efficient Deep Learning, ReQuEST 2018 -  Co-located with ACM ASPLOS 2018",2018.0,"Internet of things (IoT) devices capture and create various forms of sensor data such as images and videos. However, such resource-constrained devices lack the capability to efficiently process data in a timely and real-time manner. Therefore, IoT systems strongly rely on a powerful server (either local or on the cloud) to extract useful information from data. In addition, during communication with servers, unprocessed, sensitive, and private data is transmitted throughout the Internet, a serious vulnerability. What if we were able to harvest the aggregated computational power of already existing IoT devices in our system to locally process this data? In this artifact, we utilize Musical Chair [3], which enables efficient, localized, and dynamic real-time recognition by harvesting the aggregated computational power of these resource-constrained IoT devices. We apply Musical chair to two well-known image recognition models, AlexNet and VGG16, and implement them on a network of Raspberry PIs (up to 11). We compare inference per second and energy per inference of our systems with Tegra TX2, an embedded low-power platform with a six-core CPU and a GPU. We demonstrate that the collaboration of IoT devices, enabled by Musical Chair, achieves similar real-time performance without the extra costs of maintaining a server. © 2018 Association for Computing Machinery.",10.1145/3229762.3229765,,9.0,
Surrogate-assisted evolutionary biobjective optimization for objectives with non-uniform latencies,"Chugh T., Allmendinger R., Ojalehto V., Miettinen K.",GECCO 2018 - Proceedings of the 2018 Genetic and Evolutionary Computation Conference,2018.0,"We consider multiobjective optimization problems where objective functions have different (or heterogeneous) evaluation times or latencies. This is of great relevance for (computationally) expensive multiobjective optimization as there is no reason to assume that all objective functions should take an equal amount of time to be evaluated (particularly when objectives are evaluated separately). To cope with such problems, we propose a variation of the Kriging-assisted reference vector guided evolutionary algorithm (K-RVEA) called heterogeneous K-RVEA (short HK-RVEA). This algorithm is a merger of two main concepts designed to account for different latencies: A single-objective evolutionary algorithm for selecting training data to train surrogates and K-RVEA's approach for updating the surrogates. HK-RVEA is validated on a set of biobjective benchmark problems varying in terms of latencies and correlations between the objectives. The results are also compared to those obtained by previously proposed strategies for such problems, which were embedded in a non-surrogate-assisted evolutionary algorithm. Our experimental study shows that, under certain conditions, such as short latencies between the two objectives, HK-RVEA can outperform the existing strategies as well as an optimizer operating in an environment without latencies. © 2018 Association for Computing Machinery.",10.1145/3205455.3205514,Bayesian optimization; Expensive optimization; Heterogeneous objectives; Machine learning; Metamodelling; Multiobjective optimization; Pareto optimality,7.0,
Ensemble Shuffled Population Algorithm for multi-objective thermal design optimization of a plate frame heat exchanger operated with Al2O3/water nanofluid,"Turgut M.S., Turgut O.E.",Applied Soft Computing Journal,2018.0,"This study proposes a brand new optimization algorithm entitled Ensemble Shuffled Population Algorithm for solving multidimensional optimization problems. The proposed algorithm adopts the perturbation equations of the Crow Search and Differential Search algorithms with useful modifications on them and aims to maintain a reasonable balance between the intensification and diversification phases of the algorithm. A batch of 22 benchmark problems consisting of unimodal and multimodal unconstrained optimization test functions are applied using this algorithm to assess its performance on multi dimensional problems. Statistical results obtained from the proposed Ensemble Shuffled Population Algorithm are compared to those found by eleven well known metaheuristic optimizers. The comparison results show that the Ensemble Shuffled Population Algorithm outperforms the compared optimizers with regards to solution accuracy and convergence speed. After that, the proposed algorithm is applied on a multi objective optimization of a plate frame heat exchanger operated with Al2O3 nanofluid. The optimization results show that utilizing nanoparticles instead of base fluid not only increases the overall heat transfer coefficient rates but also entails a huge decline in total cost values. A Pareto frontier is constructed for these two conflicting objectives to select the final optimum solution from the set of non-dominated solutions by virtue of three famous decision making methods of LINMAP, TOPSIS, and Shannon's entropy theory. Then, sensitivity analysis is performed to observe the variational effects of the design variables on the optimization objectives. © 2018 Elsevier B.V.",10.1016/j.asoc.2018.04.057,Ensemble learning; Nanofluids; Plate frame heat exchanger; Stochastic optimization,4.0,
Amulti-objective SCOR-based decision alignment for supply chain performance management,"Rezaei M., Shirazi M.A., Karimi B.",Scientia Iranica,2018.0,"A dynamic integrated solution to three main problems through integrating all metrics using SCOR is proposed in this research. This dynamic solution comprises strategic decisions in high level, operational decisions in low level, and alignment of these two decision levels. In this regard, a human intelligence-based process for high-level decisions and machine-intelligence based Decision Support Systems (DSSs) for low-level decisions are proposed using a novel approach. The presented operational model considers important supply chain features thoroughly, such as different echelons, several suppliers, several manufacturers, and several products, during multiple periods. A multi-objective mathematical programming model is then developed to yield the operational decisions with Pareto efficient performance values and solved using a well-known meta-heuristic algorithm, i.e., NSGAII, the parameters of which are tuned using Taguchi method. Afterwards, an intermediate machine-intelligence module is used to determine the best operational solution based on the strategic idea of the decision maker. The efficiency of the proposed framework is shown through numerical example and then, a sensitivity analysis is conducted for the obtained results so as to show the impact of the strategic scenario planning on the performance of the considered supply chain. © 2018 Sharif University of Technology. All rights reserved.",10.24200/sci.2017.4463,Decision alignment; Multi-objective; NSGAII; Performance management; SCOR model; Supply chain,3.0,
Hardware architecture for high-speed object detection using decision tree ensemble,"Mitsunari K., Yu J., Onoye T., Hashimoto M.","IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences",2018.0,"Visual object detection on embedded systems involves a multi-objective optimization problem in the presence of trade-offs between power consumption, processing performance, and detection accuracy. For a new Pareto solution with high processing performance and low power consumption, this paper proposes a hardware architecture for decision tree ensemble using multiple channels of features. For efficient detection, the proposed architecture utilizes the dimensionality of feature channels in addition to parallelism in image space and adopts task scheduling to attain random memory access without conflict. Evaluation results show that an FPGA implementation of the proposed architecture with an aggregated channel features pedestrian detector can process 229 million samples per second at 100MHz operation frequency while it requires a relatively small amount of resources. Consequently, the proposed architecture achieves 350 fps processing performance for 1080P Full HD images and outperforms conventional object detection hardware architectures developed for embedded systems. © 2018 The Institute of Electronics, Information and Communication Engineers.",10.1587/transfun.E101.A.1298,Decision tree ensemble; Embedded systems; Machine learning; Object detection; Task scheduling,3.0,
Multigene genetic programming for sediment transport modeling in sewers for conditions of non-deposition with a bed deposit,"Safari M.J.S., Danandeh Mehr A.",International Journal of Sediment Research,2018.0,"It is known that construction of large sewers based on consideration of flow with non-deposition without a bed deposit is not economical. Sewer design based on consideration of flow with non-deposition with a bed deposit reduces channel bed slope and construction cost in which the presence of a small depth of sediment deposition on the bed increases the sediment transport capacity of the flow. This paper suggests a new Pareto-optimal model developed by the multigene genetic programming (MGGP) technique to estimate particle Froude number (Frp) in large sewers with conditions of sediment deposition on the bed. To this end, four data sets including wide ranges of sediment size and concentration, deposit thickness, and pipe size are used. On the basis of different statistical performance indices, the efficiency of the proposed Pareto-optimal MGGP model is compared to those of the best MGGP model developed in the current study as well as the conventional regression models available in the literature. The results indicate the higher efficiency of the MGGP-based models for Frp estimation in the case of no additional deposition onto a bed with a sediment deposit. Inasmuch as the Pareto-optimal MGGP model utilizes a lower number of input parameters to yield comparatively higher performance than the conventional regression models, it can be used as a parsimonious model for self-cleansing design of large sewers in practice. © 2018 International Research and Training Centre on Erosion and Sedimentation/the World Association for Sedimentation and Erosion Research",10.1016/j.ijsrc.2018.04.007,Bed deposition; Bed load; Multigene genetic programming; Non-deposition; Sediment transport; Sewer,35.0,
Pareto-optimal plans as ground truth for validation of a commercial system for knowledge-based DVH-prediction,"Cagni E., Botti A., Wang Y., Iori M., Petit S.F., Heijmen B.J.M.",Physica Medica,2018.0,"Purpose: Treatment plans manually generated in clinical routine may suffer from variations and inconsistencies in quality. Using such plans for validating a DVH prediction algorithm might obscure its intrinsic prediction accuracy. In this study we used a recently published large database of Pareto-optimal prostate cancer plans to assess the prediction accuracy of a commercial knowledge-based DVH prediction algorithm, RapidPlan. The database plans were consistently generated with automated planning using an independent optimizer, and can be considered as aground truth of plan quality. Methods: Prediction models were generated using training sets with 20, 30, 45, 55 and 114 Pareto-optimal plans. Model-20 and Model-30 were built using 5 groups of randomly selected training patients. For 60 independent Pareto-optimal validation plans, predicted and database DVHs were compared. Results: For model-114, differences between predicted and database mean doses of more than ± 10% in rectum, anus and bladder, occurred for 23.3%, 55.0%, and 6.7% of the validation plans, respectively. For rectum V65Gy and V75Gy, differences outside the ±10% range were observed in 21.7% and 70.0% of validation plans, respectively. For 61.7% of validation plans, inaccuracies in predicted rectum DVHs resulted in a deviation in predicted NTCP for rectal bleeding outside ±10%. With smaller training sets the DVH prediction performance deteriorated, showing dependence on the selected training patients. Conclusion: Even when analysed with Pareto-optimal plans with highly consistent quality, clinically relevant deviations in DVH predictions were observed. Such deviations could potentially result in suboptimal plans for new patients. Further research on DVH prediction models is warranted. © 2018",10.1016/j.ejmp.2018.11.002,Automated planning; Knowledge-based planning; Machine learning; Pareto-optimal plan; Prostate cancer; RapidPlan,15.0,
Computational design of light and strong high entropy alloys (HEA): Obtainment of an extremely high specific solid solution hardening,"Menou E., Tancret F., Toda-Caraballo I., Ramstein G., Castany P., Bertrand E., Gautier N., Rivera Díaz-Del-Castillo P.E.J.",Scripta Materialia,2018.0,"A multi-objective optimisation genetic algorithm combining solid solution hardening (SSH) and thermodynamic modelling (CALPHAD) with data mining is used to design high entropy alloys (HEAs). The approach searches for the best compromise between single-phase stability, SSH and density. Thousands of Pareto-optimal base-centred cubic (BCC) HEAs are designed. Al35Cr35Mn8Mo5Ti17 (at.%) is chosen for experimental validation. The alloy was cast and characterised. Its microstructure consists of large grains of a single disordered solid solution displaying a Vickers hardness of 6.45 GPa (658 HV) and a density below 5.5 g/cm3; uniquely combining exceptional hardness with medium density. © 2018 Acta Materialia Inc",10.1016/j.scriptamat.2018.07.024,Machine learning; Modeling; Phase diagram; Simulation; Thermodynamics,22.0,
Games between humans and AIs,DeCanio S.J.,AI and Society,2018.0,"Various potential strategic interactions between a “strong” Artificial intelligence (AI) and humans are analyzed using simple 2 × 2 order games, drawing on the New Periodic Table of those games developed by Robinson and Goforth (The topology of the 2 × 2 games: a new periodic table. Routledge, London, 2005). Strong risk aversion on the part of the human player(s) leads to shutting down the AI research program, but alternative preference orderings by the human and the AI result in Nash equilibria with interesting properties. Some of the AI-Human games have multiple equilibria, and in other cases Pareto-improvement over the Nash equilibrium could be attained if the AI’s behavior towards humans could be guaranteed to be benign. The preferences of a superintelligent AI cannot be known in advance, but speculation is possible as to its ranking of alternative states of the world, and how it might assimilate the accumulated wisdom (and folly) of humanity. © 2017, Springer-Verlag London.",10.1007/s00146-017-0732-5,Artificial intelligence; Machine learning; Nash equilibrium; Order games,1.0,
Multi-objective Bayesian optimization of chemical reactor design using computational fluid dynamics,"Park S., Na J., Kim M., Lee J.M.",Computers and Chemical Engineering,2018.0,"This study presents a computational fluid dynamics (CFD) based optimal design tool for chemical reactors, in which multi-objective Bayesian optimization (MBO) is utilized to reduce the number of required CFD runs. Detailed methods used to automate the process by connecting CFD with MBO are also proposed. The developed optimizer was applied to minimize the power consumption and maximize the gas holdup in a gas-sparged stirred tank reactor, which has six design variables: the aspect ratio of the tank, the diameter and clearance of each of the two impellers, and the gas sparger. The saturated Pareto front is obtained after 100 iterations. The resulting Pareto front consists of many near-optimal designs with significantly enhanced performances compared to conventional reactors reported in the literature. We anticipate that this design approach can be applied to any process unit design problems that require a large number of CFD simulation runs. © 2018 Elsevier Ltd",10.1016/j.compchemeng.2018.08.005,Bayesian optimization; CFD-based optimization; Computational fluid dynamics; Machine learning; Multi-objective optimization; Reactor design,24.0,
Generative adversarial training for neural machine translation,"Yang Z., Chen W., Wang F., Xu B.",Neurocomputing,2018.0,"Neural machine translation (NMT) is typically optimized to generate sentences which cover n-grams with ground target as much as possible. However, it is widely acknowledged that n-gram precisions, the manually designed approximate loss function, may mislead the model to generate suboptimal translations. To solve this problem, we train the NMT model to generate human-like translations directly by using the generative adversarial net, which has achieved great success in computer vision. In this paper, we build a conditional sequence generative adversarial net (CSGAN-NMT) which comprises of two adversarial sub models, a generative model (generator) which translates the source sentence into the target sentence as the traditional NMT models do and a discriminative model (discriminator) which discriminates the machine-translated target sentence from the human-translated one. The two sub models play a minimax game and achieve a win-win situation when reaching a Nash Equilibrium. As a variant of the single generator-discriminator model, the multi-CSGAN-NMT which contains multiple discriminators and generators, is also proposed. In the multi-CSGAN-NMT model, each generator is viewed as an agent which can interact with others and even transfer messages. Experiments show that the proposed CSGAN-NMT model obtains substantial improvements than the strong baseline and the improvement of the multi-CSGAN-NMT model is more remarkable. © 2018 Elsevier B.V.",10.1016/j.neucom.2018.09.006,Human-like translation; Multi generative adversarial net; Neural machine translation,11.0,
Towards a Hybrid Minimax Recommender for Free-Roaming Museum Visits,Pavlidis G.,Springer Proceedings in Business and Economics,2019.0,"This paper presents a novel minimax hybrid recommender for free-roaming museum visits that is based on a new museum visit concept that was developed to capture the spatial, temporal and content-based dynamics during free-roaming museum visits. The complex hybrid recommender applies a minimax approach as it estimates an overall visitor dissatisfaction and aims its minimisation. As a result, it is able to develop optimal routes, as sequences of points of interest for each individual visitor. This hybrid recommender, still at its fine-tuning phase, has been tested in large scale simulations, using realistic data for visitors and exhibitions and has already shown to outperform the naive baseline recommender that relies on popularity. © 2019, Springer Nature Switzerland AG.",10.1007/978-3-030-12453-3_2,Artificial intelligence; Cultural heritage; Machine learning; Museum guide; Recommendation; Recommender,,
State Representation Learning for Minimax Deep Deterministic Policy Gradient,"Hu D., Jiang X., Wei X., Wang J.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019.0,"Recently, the reinforcement learning of multi-agent has been developed rapidly, especially the Minimax Deep Deterministic Policy Gradient (M3DDPG) algorithm which improves agent robustness and solves the problem that agents trained by deep reinforcement learning (DRL) are often vulnerable and sensitive to the training environment. However, agents in the real environment may not be able to perceive certain important characteristics of the environment because of their limited perceptual capabilities. So Agents often fail to achieve the desired results. In this paper, we propose a novel algorithm State Representation Learning for Minimax Deep Deterministic Policy Gradient (SRL_M3DDPG) that combines M3DDPG with the state representation learning neural network model to extract the important characteristics of raw data. And we optimize the actor and critic network by using the neural network model of state representation learning. Then the actor and critic network learn from the state representation model instead of the raw observations. Simulation experiments show that the algorithm improves the final result. © Springer Nature Switzerland AG 2019.",10.1007/978-3-030-29551-6_43,M3DDPG; SRL_M3DDPG; State representation learning,,
Optimizing deep learning RNN topologies on intel architecture,"Banerjee K., Georganas E., Kalamkar D.D., Ziv B., Segal E., Anderson C., Heinecke A.",Supercomputing Frontiers and Innovations,2019.0,"Recurrent neural network (RNN) models have been found to be well suited for processing temporal data. In this work, we present an optimized implementation of vanilla RNN cell and its two popular variants: LSTM and GRU for Intel Xeon architecture. Typical implementations of these RNN cells employ one or two large matrix multiplication (GEMM) calls and then apply the element-wise operations (sigmoid/tanh) onto the GEMM results. While this approach is easy to implement by exploiting vendor-optimized GEMM library calls, the data reuse relies on how GEMMs are parallelized and is sub-optimal for GEMM sizes stemming from small minibatch. Also, the element-wise operations are exposed as a bandwidth-bound kernel after the GEMM which is typically a compute-bound kernel. To address this discrepancy, we implemented a parallel blocked matrix GEMM in order to (a) achieve load balance, (b) maximize weight matrix reuse, (c) fuse the element-wise operations after partial GEMM blocks are computed and while they are hot in cache. Additionally, we bring the time step loop in our cell to further increase the weight reuse and amortize the overhead to transform the weights into blocked layout. The results show that our implementation is generally faster than Intel MKL-DNN library implementations, e.g. for RNN, forward pass is up to ~3x faster whereas the backward/weight update pass is up to ~5x faster. Furthermore, we investigate high-performance implementations of sigmoid and tanh activation functions that achieve various levels of accuracy. These implementations rely on minimax polynomial approximations, rational polynomials, Taylor expansions and exponential approximation techniques. Our vectorized implementations can be flexibly integrated into deep learning computations with different accuracy requirements without compromising performance; in fact, these are able to outperform vectorized and reduced accuracy vendor-optimized (Intel SVML) libraries by 1.6-2.6 x while speep up over GNU libm is close to two orders of magnitude. All our experiments are conducted on Intel's latest CascadeLake architecture. © The Authors 2019.",10.14529/js?190304,Bandwidth-bound kernel; Compute-bound kernel; Gemm; Intel xeon; Lstm,5.0,
Multi-task Learning by Pareto Optimality,"Dyankov D., Riccio S.D., Di Fatta G., Nicosia G.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019.0,"Deep Neural Networks (DNNs) are often criticized because they lack the ability to learn more than one task at a time: Multitask Learning is an emerging research area whose aim is to overcome this issue. In this work, we introduce the Pareto Multitask Learning framework as a tool that can show how effectively a DNN is learning a shared representation common to a set of tasks. We also experimentally show that it is possible to extend the optimization process so that a single DNN simultaneously learns how to master two or more Atari games: using a single weight parameter vector, our network is able to obtain sub-optimal results for up to four games. © Springer Nature Switzerland AG 2019.",10.1007/978-3-030-37599-7_50,Atari 2600 Games; Deep artificial neural networks; Deep neuroevolution; Evolution Strategy; Hypervolume; Kullback-Leibler Divergence; Multitask learning; Neural and evolutionary computing,1.0,
On Selection of Optimal Classifiers,"Rado O., Neagu D.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019.0,"The current advances of computational power and storage allow more models to be created and stored from significant data resources. This progress opens the opportunity to re-cycle and re-use such models in similar exercises. The evaluation of the machine learning algorithms and selection of an appropriate classifier from an existing collection of classifiers are still challenging tasks. In most cases, the decision of selecting the classifier is left to the user. When the selection is not performed accurately, the outcomes can have unexpected performance results. Classification algorithms aim to optimise some of the distinct objectives such as minimising misclassification error, maximising the accuracy, or maximising the model quality. The right choice for each of these objectives is critical to the quality of the classifier selected. This work aims to study the use of a multi-objective method that can be undertaken to find a set of suitable classifiers for a problem at hand. In this study, we applied seven classifiers on mental health data sets for classifier selection in terms of correctness and reliability. The experimental results suggest that this approach is useful in finding the best trade-off among the objectives of selecting a suitable classifier framework. © 2019, Springer Nature Switzerland AG.",10.1007/978-3-030-34885-4_42,Classification algorithms; Optimization; Pareto set,1.0,
Multiple Tasks Assignment for Cooperating Homogeneous Unmanned Aerial Vehicles,"Li L., Zhai X.B., Chen B., Li C.","Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST",2019.0,"Using multiple unmanned aerial vehicles (UAVs) to perform some tasks cooperatively has received growing attention in recent years. Task assignment is a difficult problem in mission planning. Multiple tasks assignment problem for cooperating homogeneous UAVs is considered as a traditional combinatorial optimization problem. This paper addresses the problem of assigning multiple tasks to cooperative homogeneous UAVs, minimizing the total cost and balancing the cost of each UAV. We propose a centralized task assignment scheme which is based on minimum spanning tree. This scheme involves two phases. In the first phase, we use the Kruskal algorithm and the breadth first search algorithm to assign all tasks to UAVs and get a proper initial task assignment solution. The second phase involves the Pareto optimization improvement in the solution generated from the first phase. For a single UAV, we use the dynamic programming algorithm to calculate the total cost of completing all assigned tasks. The performance of the proposed scheme is compared to that of heuristic simulated annealing algorithm. The simulation results show that the proposed scheme can solve the homogeneous multi-UAV cooperative task assignment problem effectively. © 2019, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.",10.1007/978-3-030-32388-2_18,Minimum spanning tree; Pareto optimization; Task assignment; Unmanned aerial vehicle,,
Individualized 3D dose distribution prediction using deep learning,"Ma J., Bai T., Nguyen D., Folkerts M., Jia X., Lu W., Zhou L., Jiang S.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019.0,"In cancer radiotherapy, inverse treatment planning is a multi-objective optimization problem. There exists a set of plans with various trade-offs on Pareto surface which are referred as Pareto optimal plans. Currently exploring such trade-offs, i.e., physician preference is a trial and error process and often time-consuming. Therefore, it is desirable to predict desired Pareto optimal plans in an efficient way before treatment planning. The predicted plans can be used as references for dosimetrists to rapidly achieve a clinically acceptable plan. Clinically the dose volume histogram (DVH) is a useful tool that can visually indicate the specific dose received by each certain volume percentage which is supposed to describe different trade-offs. Consequently, we have proposed a deep learning method based on patient’s anatomy and DVH information to predict the individualized 3D dose distribution. Qualitative measurements have showed analogous dose distributions and DVH curves compared to the true dose distribution. Quantitative measurements have demonstrated that our model can precisely predict the dose distribution with various trade-offs for different patients, with the largest mean and max dose differences between true dose and predicted dose for all critical structures no more than 1.7% of the prescription dose. © Springer Nature Switzerland AG 2019.",10.1007/978-3-030-32486-5_14,Deep learning; Dose prediction; Trade-offs; Treatment planning,4.0,
Adversarial machine learning with double oracle,Wang K.,IJCAI International Joint Conference on Artificial Intelligence,2019.0,"We aim to improve the general adversarial machine learning solution by introducing the double oracle idea from game theory, which is commonly used to solve a sequential zero-sum game, where the adversarial machine learning problem can be formulated as a zero-sum minimax problem between learner and attacker. © 2019 International Joint Conferences on Artificial Intelligence. All rights reserved.",10.24963/ijcai.2019/925,,1.0,
A Multi-objective Reinforcement Learning Algorithm for JSSP,"Méndez-Hernández B.M., Rodríguez-Bazan E.D., Martinez-Jimenez Y., Libin P., Nowé A.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019.0,"Scheduling is a decision making process that takes care of the allocation of resources to tasks over time. The Job Shop scheduling problem is one of the most complex scheduling scenarios and is commonly encountered in manufacturing industries. Most of the existing studies are based on optimizing one objective, but in real-world problems, multiple criteria often need to be optimized at once. We propose a Multi-Objective Multi-Agent Reinforcement Learning Algorithm that aims to obtain the non-dominated solutions set for Job Shop scheduling problems. The proposed algorithm is used to solve a set of benchmark problems optimizing makespan and tardiness. The performance of our algorithm is evaluated and compared to other algorithms from the literature using two measures for evaluating the Pareto front. We show that our algorithm is able to find a set of diverse and high quality non-dominated solutions, that significantly and consistently improves upon the results obtained by other state-of-the-art algorithms. © 2019, Springer Nature Switzerland AG.",10.1007/978-3-030-30487-4_44,Job Shop Scheduling Problems; Multi-agent; Multi-objective; Pareto front; Reinforcement Learning,4.0,
A comparison of quadratic regression and artificial neural networks for the estimation of quantiles at ungauged sites in regional frequency analysis,"Khan M.S.R., Hussain Z., Ahmad I.",Applied Ecology and Environmental Research,2019.0,"The study illustrates application of Regional Flood Frequency Analysis (RFFA) using Annual Maximum Peak Flows (AMPF) of eleven gauging sites of various streams of Khyber-Pakhtunkhwa, Pakistan. Assumptions associated to recorded data at various sites have been validated through various statistical tests. The discordancy measure indicates that there is no discordant site in the cluster of eleven sites. Heterogeneity measure based on l-moments confirms that the group of eleven sites is definitely homogeneous. Criterion of |Z - Dist| statistic and L-moment ratio diagram show that Generalized Pareto (GPA) distribution is the best fitted regional distribution of the study region. Regional flood quantiles for various return periods have been estimated using the quantile function of GPA distribution. Artificial Neural Networks (ANN) and Quadratic Regression (QR) model with robust estimation method have been used for the estimation of quantiles at ungauged sites. Model evaluation criteria’s (error comparison of predicted values) suggested that estimated quantiles through ANN are accurate relative to quadratic regression. Historical comparison shows that the quantiles estimated through index flood method and ANN are closely related to the highest recorded values of AMPF at each corresponding site for shorter as well as longer return periods. © 2019, ALÖKI Kft., Budapest, Hungary.",10.15666/aeer/1703_69376959,Analyzing extremes of floods; Khyber-Pakhtunkhwa; L-moments; Machine learning; Non-linear regression; Pakistan; Regional frequency analysis,4.0,
Model-Based Multi-objective Reinforcement Learning with Unknown Weights,"Yamaguchi T., Nagahama S., Ichikawa Y., Takadama K.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019.0,"This paper describes solving multi-objective reinforcement learning problems where there are multiple conflicting objectives with unknown weights. Reinforcement learning (RL) is a popular algorithm for automatically solving sequential decision problems and most of them are focused on single-objective settings to decide a single solution. In multi-objective reinforcement learning (MORL), the reward function emits a reward vector instead of a scalar reward. A scalarization function with a vector of n weights (weight vector) is a commonly used to decide a single solution. The simple scalarization function is linear scalarization such as weighted sum. The main problem of previous MORL methods is a huge learning cost required to collect all Pareto optimal policies. Hence, it is hard to learn the high dimensional Pareto optimal policies. To solve this, this paper proposes the novel model-based MORL method by reward occurrence probability (ROP) with unknown weights. There are two main features. The first feature is that the average reward of a policy is defined by inner product of the ROP vector and the weight vector. The second feature is that it learns ROP in each policy instead of Q-values. Pareto optimal deterministic policies directly form the vertices of a convex hull in the ROP vector space. Therefore, Pareto optimal policies are calculated independently with weights and just once. The experimental results show that our proposed method collected all optimal policies under four dimensional Pareto optimal policies, and it takes a small computation time though previous MORL methods learn at most two or three dimensions. © 2019, Springer Nature Switzerland AG.",10.1007/978-3-030-22649-7_25,Average reward; Model-based; Multi-objective reinforcement learning; Reward occurrence probability; Reward vector,1.0,
Multidirectional harmony search algorithm for solving integer programming and minimax problems,"Tawhid M.A., Ali A.F.",International Journal of Bio-Inspired Computation,2019.0,"Integer programming and minimax problems are essential tools in solving various problems that arise in data mining and machine learning such as multi-class data classification and feature selection problems. In this paper, we propose a new hybrid harmony search algorithm by combining the harmony search algorithm with the multidirectional search method in order to solve the integer programming and minimax problems. The proposed algorithm is called multidirectional harmony search algorithm (MDHSA). MDHSA starts the search by applying the standard harmony search for numbers of iteration then the best-obtained solution is passing to the multidirectional search method as an intensification process in order to accelerate the search and overcome the slow convergence of the standard harmony search algorithm. The proposed algorithm is balancing between the global exploration of the harmony search algorithm and the deep exploitation of the multidirectional search method. MDHSA algorithm is tested on seven integer programming problems and 15 minimax problems and compared against 12 algorithms for solving integer programming problems and 11 algorithms for solving minimax problems. The experiments results show the efficiency of the proposed algorithm and its ability to solve integer programming and minimax problems in reasonable time. Copyright © 2019 Inderscience Enterprises Ltd.",10.1504/ijbic.2019.099179,Direct search algorithm; Evolutionary computation; Global optimisation; Harmony search algorithm; Integer programming problems; Minimax problems; Multidirectional search,5.0,
Adaptive dimensionality reduction in multiobjective optimization with multiextremal criteria,"Gergel V., Grishagin V., Israfilov R.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019.0,The paper is devoted to consideration of multicriterial optimization (MCO) problems subject to multiextremality of criteria. Application of convolution techniques for finding partial Pareto-optimal solutions generates under this assumption the multiextremal problems of scalar optimization. For solving these problems it is necessary to use efficient global optimization algorithms. As such the methods the nested schemes of dimensionality reduction in combination with univariate characteristical optimization algorithms are considered. A general description of the scheme is given and its modification accelerating the search is presented. Efficiency of the proposed approach is demonstrated on the base of representative computational experiment on a test class of bi-criterial MCO problems with essentially multiextremal criteria. © Springer Nature Switzerland AG 2019.,10.1007/978-3-030-13709-0_11,Dimensionality reduction; Global search algorithms; Multicriterial optimization; Multiextremal criteria,2.0,
User preferences in bayesian multi-objective optimization: The expected weighted hypervolume improvement criterion,"Feliot P., Bect J., Vazquez E.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019.0,"In this article, we present a framework for taking into account user preferences in multi-objective Bayesian optimization in the case where the objectives are expensive-to-evaluate black-box functions. A novel expected improvement criterion to be used within Bayesian optimization algorithms is introduced. This criterion, which we call the expected weighted hypervolume improvement (EWHI) criterion, is a generalization of the popular expected hypervolume improvement to the case where the hypervolume of the dominated region is defined using a user-defined absolutely continuous measure instead of the Lebesgue measure. The EWHI criterion takes the form of an integral for which no closed form expression exists in the general case. To deal with its computation, we propose an importance sampling approximation method. A sampling density that is optimal for the computation of the EWHI for a predefined set of points is crafted and a sequential Monte-Carlo (SMC) approach is used to obtain a sample approximately distributed from this density. The ability of the criterion to produce optimization strategies oriented by user preferences is demonstrated on a simple bi-objective test problem in the cases of a preference for one objective and of a preference for certain regions of the Pareto front. © Springer Nature Switzerland AG 2019.",10.1007/978-3-030-13709-0_45,Bayesian optimization; Importance sampling; Multi-objective optimization; Sequential monte-carlo; User preferences,3.0,
On dealing with uncertainties from kriging models in offline data-driven evolutionary multiobjective optimization,"Mazumdar A., Chugh T., Miettinen K., López-Ibáñez M.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019.0,"Many works on surrogate-assisted evolutionary multiobjective optimization have been devoted to problems where function evaluations are time-consuming (e.g., based on simulations). In many real-life optimization problems, mathematical or simulation models are not always available and, instead, we only have data from experiments, measurements or sensors. In such cases, optimization is to be performed on surrogate models built on the data available. The main challenge there is to fit an accurate surrogate model and to obtain meaningful solutions. We apply Kriging as a surrogate model and utilize corresponding uncertainty information in different ways during the optimization process. We discuss experimental results obtained on benchmark multiobjective optimization problems with different sampling techniques and numbers of objectives. The results show the effect of different ways of utilizing uncertainty information on the quality of solutions. © Springer Nature Switzerland AG 2019.",10.1007/978-3-030-12598-1_37,Gaussian process; Machine learning; Metamodelling; Pareto optimality; Surrogate,8.0,
Metalearners for estimating heterogeneous treatment effects using machine learning,"Künzel S.R., Sekhon J.S., Bickel P.J., Yu B.",Proceedings of the National Academy of Sciences of the United States of America,2019.0,"There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any supervised learning or regression method in machine learning and statistics to estimate the conditional average treatment effect (CATE) function. Metaalgorithms build on base algorithms—such as random forests (RFs), Bayesian additive regression trees (BARTs), or neural networks—to estimate the CATE, a function that the base algorithms are not designed to estimate directly. We introduce a metaalgorithm, the X-learner, that is provably efficient when the number of units in one treatment group is much larger than in the other and can exploit structural properties of the CATE function. For example, if the CATE function is linear and the response functions in treatment and control are Lipschitz-continuous, the X-learner can still achieve the parametric rate under regularity conditions. We then introduce versions of the X-learner that use RF and BART as base learners. In extensive simulation studies, the X-learner performs favorably, although none of the metalearners is uniformly the best. In two persuasion field experiments from political science, we demonstrate how our X-learner can be used to target treatment regimes and to shed light on underlying mechanisms. A software package is provided that implements our methods. © 2019 National Academy of Sciences. All Rights Reserved.",10.1073/pnas.1804597116,conditional average treatment effect; heterogeneous treatment effects; minimax optimality; Observational studies; randomized controlled trials,85.0,
Multilayer Perceptron: NSGA II for a New Multi-objective Learning Method for Training and Model Complexity,"Senhaji K., Ramchoun H., Ettaouil M.",Advances in Intelligent Systems and Computing,2019.0,"The multi-layer perceptron has proved its efficiencies in several fields as pattern and voice recognition. Unfortunately, the classical training for MLP suffers from a poor generalization. In this respect, we have proposed a new multi-objective training model with constraints, satisfies two objectives. The first one is the learning objective: minimizing the perceptron error and the second is the complexity objective: optimizing number of weights and neurons. The proposed model will provide a balance between the multi-layer perceptron learning and the complexity to get a good generalization. Our model has been solved using an evolutionary approach called the Non-Dominated Sorting Genetic Algorithm (NSGA II). This approach has led to a good representation of the Pareto set for the MLP network, from which an improved generalization performance model is selected. © 2019, Springer International Publishing AG, part of Springer Nature.",10.1007/978-3-319-91337-7_15,Multi-objective training; Multilayer perceptron; Non-dominated Sorting Genetic Algorithm II (NSGA II); Non-linear optimization; Pareto front; Supervised learning,1.0,
An Integrated Generation-Compensation optimization Strategy for Enhanced Short-Term Voltage Security of Large-Scale Power Systems Using Multi-Objective Reinforcement Learning Method,"Deng Z., Liu M.","2018 International Conference on Power System Technology, POWERCON 2018 - Proceedings",2019.0,"High penetrations of industrial loads have placed significant pressures on short-term voltage security. This paper proposes an integrated generation-compensation optimization strategy, which coordinates the generators and the switchable capacitor banks to enhance short-term voltage security as a multi-objective dynamic optimization (MODO) model. This model is established containing dynamics, power flow balances, and security constraints to minimize the voltage deviation and the cost of control strategy. The differential-algebra equations are converted into algebra equations using the Radau collocation method. Furthermore, a novel multi-objective reinforcement learning (MORL) method is utilized to mitigate the computational burdens and to obtain Pareto optimal solutions by filtering the dominated solutions. Compared with conventional MORL methods, the proposed MORL method divides the full feasible region into several small independent regions to reduce and eliminate the searching for Pareto optimal solutions. Meanwhile, the state functions of MORL are redefined, and the state sensitivities are introduced to judge whether the trial and learning accumulate sufficient knowledge. Moreover, the Pareto optimal solutions are further improved by introducing several possible solutions. Finally, the tradeoff solution is obtained based on Fuzzy decision-making strategy. The effectiveness and efficiency of the MORL method are verified by numerical simulations on a provincial 748-bus power system. © 2018 IEEE.",10.1109/POWERCON.2018.8601814,Multi-objective dynamic optimization; multi-objective reinforcement learning; Pareto optimal solution; Radau collocation method; short-term voltage security,2.0,
Chemometric data analysis with autoencoder neural network,"Bilal M., Ullah M., Ullah H.",IS and T International Symposium on Electronic Imaging Science and Technology,2019.0,"We propose novel deep learning based chemometric data analysis technique. We trained L2 regularized sparse autoencoder end-to-end for reducing the size of the feature vector to handle the classic problem of the curse of dimensionality in chemometric data analysis. We introduce a novel technique of automatic selection of nodes inside the hidden layer of an autoencoder through Pareto optimization. Moreover, Gaussian process regressor is applied on the reduced size feature vector for the regression. We evaluated our technique on orange juice and wine dataset and results are compared against 3 state-of-the-art methods. Quantitative results are shown on Normalized Mean Square Error (NMSE) and the results show considerable improvement in the state-of-the-art. © 2019 Society for Imaging Science and Technology. All rights reserved.",10.2352/ISSN.2470-1173.2019.1.VDA-679,Chemometric data; Gaussian process regressor; Pareto optimization; Sparse autoencoder,3.0,
Extreme Solutions NSGA-III (E-NSGA-III) for Scientific Workflow Scheduling on Cloud,"Wangsom P., Bouvry P., Lavangnananda K.","Proceedings - 17th IEEE International Conference on Machine Learning and Applications, ICMLA 2018",2019.0,"The execution of scientific workflows on dynamic environments such as cloud computing has become multi-objective scheduling in order to satisfy user demands from several perspectives. Among these objectives, Cost and Makespan are probably the most common. This research also includes Data Movement as an additional objective as it has significant effect to network utilization and energy consumption in network equipment in cloud data center. This paper proposes a multi-objective scheduling, Extreme Nondominated Sorting Genetic Algorithm (E-NSGA-III). It is an extension of the Nondominated Sorting Genetic Algorithm (NSGA-III). E-NSGA-III utilizes extreme solutions in the population generation module in order improve quality of solutions. Five well-known scientific workflows are selected as testbeds. Hypervolume and the Pareto front are chosen as the performance metrics. E-NSGA-III is evaluated by comparing its performance against the two previous versions (NSGA-II and NSGA-III). The comparison reveals that E-NSGA-III yields the best performance among them in multi-objective scheduling of the five scientific workflows. © 2018 IEEE.",10.1109/ICMLA.2018.00184,Cloud Computing; Cost; Data Movement; Extreme Nondominated Sorting Genetic Algorithm (E-NSGA-III); Makespan; Multi-objective Scheduling; NSGA-II; NSGA-III; Scientific Workflow,5.0,
Distributed Primal-Dual Proximal Method for Regularized Empirical Risk Minimization,Badiei Khuzani M.,"Proceedings - 17th IEEE International Conference on Machine Learning and Applications, ICMLA 2018",2019.0,"Most high-dimensional estimation and classification methods propose to minimize a loss function (empirical risk) that is the sum of losses associated with each observed data point. We consider the special case of binary classification problems, where the loss is a function of the inner product of the feature vectors and a weight vector. For this special class of classification tasks, the empirical risk minimization problem can be recast as a minimax optimization which has a unique saddle point when the losses are smooth functions. We propose a distributed proximal primal-dual method to solve the minimax problem. We also analyze the convergence of the proposed primal-dual method and show its convergence to the unique saddle point. To prove the convergence results, we present a novel analysis of the consensus terms that takes into account the non-Euclidean geometry of the parameter space. We also numerically verify the convergence of the proposed algorithm for the logistic regression on the Erdös-Réyni random graphs and lattices. © 2018 IEEE.",10.1109/ICMLA.2018.00152,Distributed Optimization; Empirical Risk; Primal-Dual Method,,
Fair allocation of heterogeneous and interchangeable resources,"Sun X., Le T.N., Chowdhury M., Liu Z.",Performance Evaluation Review,2019.0,"Motivated by the proliferation of heterogeneous processors such as multi-core CPUs, GPUs, TPUs, and other accelerators for machine learning, we formulate a novel multi-interchangeable resource allocation (MIRA) problem where some resources are interchangeable. The challenge is how to allocate interchangeable resources to users in a sharing system while maintaining desirable properties such as sharing incentive, Pareto efficiency, and envy-freeness. In this paper, we first show that existing algorithms, including the Dominant Resource Fairness used in production systems, fail to provide these properties for interchangeable resources. Then we characterize the tradeo between performance and strategyproofness, and design the Budget-based (BUD) algorithm, which preserves Pareto efficiency, sharing incentive and envy-freeness while providing better performance over currently used algorithms. © 2018 Copyright held by the owner/author(s).",10.1145/3305218.3305227,,1.0,
Enforcing Signal Temporal Logic Specifications in Multi-Agent Adversarial Environments: A Deep Q-Learning Approach,"Muniraj D., Vamvoudakis K.G., Farhood M.",Proceedings of the IEEE Conference on Decision and Control,2019.0,"This work addresses the problem of learning optimal control policies for a multi-agent system in an adversarial environment. Specifically, we focus on multi-agent systems where the mission objectives are expressed as signal temporal logic (STL) specifications. The agents are classified as either defensive or adversarial. The defensive agents are maximizers, namely, they maximize an objective function that enforces the STL specification; the adversarial agents, on the other hand, are minimizers. The interaction among the agents is modeled as a finite-state team stochastic game with an unknown transition probability function. The synthesis objective is to determine optimal control policies for the defensive agents that implement the STL specification against the best responses of the adversarial agents. A multi-agent deep Q-learning algorithm, which is an extension of the minimax Q-learning algorithm, is then proposed to learn the optimal policies. The effectiveness of the proposed approach is illustrated through a simulation case study. © 2018 IEEE.",10.1109/CDC.2018.8618746,deep Q-learning; multi-agent system; signal temporal logic,13.0,
Air-Combat Strategy Using Deep Q-Learning,"Ma X., Xia L., Zhao Q.","Proceedings 2018 Chinese Automation Congress, CAC 2018",2019.0,"Unmanned aircraft systems (UAS) are essential components in the future air-combat. Due to high dynamics and randomness of the aircrafts, traditional methods are difficult to solve the optimal control strategy. The characteristics of reinforcement learning (RL) match the difficulty of this problem. In this paper, we build an air-combat game environment and train the agent with deep Q-learning (DQN). Despite of increasing probability of loses slightly, our method performs much better than other algorithms in the simulations. Compared with the searching based methods, like Minimax and MCTS, the policy trained by DQN can take specific tactics with a long-term view of the game. Result shows that a large number of simulations and carefully designed features and reward are the essential points of DON. © 2018 IEEE.",10.1109/CAC.2018.8623434,air-combat; DQN; RL; UAS,14.0,
On runtime and classification performance of the discretize-optimize (DISCO) classification approach,"Garcia J., Korhonen T.",Performance Evaluation Review,2019.0,"Using machine learning in high-speed networks for tasks such as flow classification typically requires either very resource efficient classification approaches, large amounts of computational resources, or specialized hardware. Here we provide a sketch of the discretize-optimize (DISCO) approach which can construct an extremely efficient classifier for low-dimensional problems by combining feature selection, efficient discretization, novel bin placement, and lookup. As feature selection and discretization parameters are crucial, appropriate combinatorial optimization is an important aspect of the approach. A performance evaluation is performed for a YouTube classification task using a cellular traffic data set. The initial evaluation results show that the DISCO approach can move the Pareto boundary in the classification performance versus runtime trade-off by up to an order of magnitude compared to runtime optimized random forest and decision tree classifiers. © is is held held by by author/owner(s). author/owner(s).",10.1145/3308897.3308965,Classification; Machine learning; Runtime,2.0,
Survey of Transportation Problems,"Gupta R., Gulati N.","Proceedings of the International Conference on Machine Learning, Big Data, Cloud and Parallel Computing: Trends, Prespectives and Prospects, COMITCon 2019",2019.0,"This review paper considered the operational management on the transportation and distribution problem of various things. The main objective is to maximize profit along with minimum cost, time and distance. There is a limitless application of Transportation problem in practical life in the field of operational research. In this review paper we propose an effective improvement of algorithms in the solution procedure to get a prominent initial basic feasible solution for TPs. For obtaining the best solutions various algorithms are used such as NWCR, LCM, Vogel approximation Method, SS Method, Modified Distribution Method, BCM and MM method. The best optimal initial basic solution is obtained by these methods. Specially, TP deals with the advantages of both waterways and land transportation, waterways for transporting heavy goods or goods in bulk amount over long distances, land transportation for fetching and distributing over small to medium distances. To tackle the problem, a heuristic algorithm is chosen and tested experimentally. Final results show that the algorithm is very effective on a set of standard instances, quickly achieving most favorable solutions. © 2019 IEEE.",10.1109/COMITCon.2019.8862242,Heuristic; Lexicographic order; Pareto optima solution,1.0,
Dynamic online pricing with incomplete information using multiarmed bandit experiments,"Misra K., Schwartz E.M., Abernethy J.",Marketing Science,2019.0,"Pricing managers at online retailers face a unique challenge. They must decide on real-time prices for a large number of products with incomplete demand information. The manager runs price experiments to learn about each product’s demand curve and the profitmaximizing price. In practice, balanced field price experiments can create high opportunity costs, because a large number of customers are presented with suboptimal prices. In this paper, we propose an alternative dynamic price experimentation policy. The proposed approach extends multiarmed bandit (MAB) algorithms from statistical machine learning to include microeconomic choice theory. Our automated pricing policy solves this MAB problem using a scalable distribution-free algorithm. We prove analytically that our method is asymptotically optimal for any weakly downward sloping demand curve. In a series of MonteCarlo simulations,we showthat the proposed approach performs favorably compared with balanced field experiments and standard methods in dynamic pricing from computer science. In a calibrated simulation based on an existing pricing field experiment, we find that our algorithm can increase profits by 43% during the month of testing and 4% annually. © 2019 INFORMS.",10.1287/mksc.2018.1129,A/b testing; Dynamic pricing; E-commerce; Field experiments; Machine learning; Minimax regret; Multiarmed bandits; Nonparametric econometrics; Online experiments; Partial identification,22.0,
Pricing cloud resource based on multi-agent reinforcement learning in the competing environment,"Shi B., Yuan H., Shi R.","Proceedings - 16th IEEE International Symposium on Parallel and Distributed Processing with Applications, 17th IEEE International Conference on Ubiquitous Computing and Communications, 8th IEEE International Conference on Big Data and Cloud Computing, 11th IEEE International Conference on Social Computing and Networking and 8th IEEE International Conference on Sustainable Computing and Communications, ISPA/IUCC/BDCloud/SocialCom/SustainCom 2018",2019.0,"Multiple cloud providers compete against each other in order to attract cloud users and make profits in the cloud market. In doing so, each provider needs to charge fees to users in a proper way. In this paper, we analyze how a cloud provider sets price effectively when competing against other cloud providers. The price set by the cloud provider is affected by its opponent's price, and as well as the prices set in the last round. Specifically, we model this problem as a Markov game by considering two cloud providers competing against each other. We then adopt two different solution concepts in game theory, minimax and Nash equilibrium, to solve this problem. Specifically, we use two different multi-agent reinforcement learning algorithms, minimax-Q and Nash-Q, which correspond to those two solution concepts respectively, to design the pricing policies. Furthermore, we improve the Nash-Q learning algorithm by taking into account the probability of each Nash equilibrium happening. Based on this, we run extensive experiments to analyze the effectiveness of minimax-Q and Nash-Q based pricing policies in terms of making long-term profits. We find that the pricing policy based on Nash-Q learning algorithm with selecting Nash equilibrium according to the probability can beat other Nash-Q based pricing polices with selecting Nash equilibrium according to the maximal payoff. However, in the further experimental analysis, we find that minimax-Q based pricing policies can beat all Nash-Q based pricing policies. This is because the minimax solution concept is more suitable in this competing environment. Our experimental results provide useful insights on designing practical pricing policies for competing cloud providers. © 2018 IEEE.",10.1109/BDCloud.2018.00076,Competing Cloud Providers; Minimax-Q Learning; Nash-Q Learning; Pricing Policy,,
Machine Discovery of Comprehensible Strategies for Simple Games Using Meta-interpretive Learning,"Muggleton S.H., Hocquette C.",New Generation Computing,2019.0,"Recently, world-class human players have been outperformed in a number of complex two-person games (Go, Chess, Checkers) by Deep Reinforcement Learning systems. However, the data efficiency of the learning systems is unclear given that they appear to require far more training games to achieve such performance than any human player might experience in a lifetime. In addition, the resulting learned strategies are not in a form which can be communicated to human players. This contrasts to earlier research in Behavioural Cloning in which single-agent skills were machine learned in a symbolic language, facilitating their being taught to human beings. In this paper, we consider Machine Discovery of human-comprehensible strategies for simple two-person games (Noughts-and-Crosses and Hexapawn). One advantage of considering simple games is that there is a tractable approach to calculating minimax regret. We use these games to compare Cumulative Minimax Regret for variants of both standard and deep reinforcement learning against two variants of a new Meta-interpretive Learning system called MIGO. In our experiments, tested variants of both normal and deep reinforcement learning have consistently worse performance (higher cumulative minimax regret) than both variants of MIGO on Noughts-and-Crosses and Hexapawn. In addition, MIGO’s learned rules are relatively easy to comprehend, and are demonstrated to achieve significant transfer learning in both directions between Noughts-and-Crosses and Hexapawn. © 2019, The Author(s).",10.1007/s00354-019-00054-2,Games strategies; Inductive Logic Programming; Reinforcement learning,2.0,
Application of adversarial networks for 3d structural topology optimization,"Rawat S., Shen M.H.H.",SAE Technical Papers,2019.0,"Topology optimization is a branch of structural optimization which solves an optimal material distribution problem. The resulting structural topology, for a given set of boundary conditions and constraints, has an optimal performance (e.g. minimum compliance). Conventional 3D topology optimization algorithms achieve quality optimized results; however, it is an extremely computationally intensive task which is, in general, impractical and computationally unachievable for real-world structural optimal design processes. Therefore, the current development of rapid topology optimization technology is experiencing a major drawback. To address the issues, a new approach is presented to utilize the powerful abilities of large deep learning models to replicate this design process for 3D structures. Adversarial models, primarily Wasserstein Generative Adversarial Networks (WGAN), are constructed which consist of 2 deep convolutional neural networks (CNN) namely, a discriminator and a generator. A minimax game is conducted between the generator and the discriminator as part of training where the discriminator maximizes the loss function whereas the generator tries to minimize the loss function of the model. Once trained, the generator from GAN can produce 3D structures in a computationally inexpensive process instantaneously. The corresponding input variables of the new generated structures are evaluated using a trained convolutional neural network. The dataset needed for training is generated using the traditional 3D topology optimization algorithms. Results from the GANs are validated by comparing these optimal structures against the 3D structures generated from the traditional algorithms with the same design settings. The potential issues and future extension of this work are discussed in detail in the article. As illustrated, introducing deep learning into the field of design will remarkably reduce the work time of an iterative design process. © 2019 SAE International. All Rights Reserved.",10.4271/2019-01-0829,,5.0,
Portfolio optimization of energy communities to meet reductions in costs and emissions,"Fleischhacker A., Lettner G., Schwabeneder D., Auer H.",Energy,2019.0,"Cities are expected to grow further, and energy communities are one promising approach to promote distributed energy resources and implement energy efficiency measures. To understand the motivation of those communities, this work improves two existing open source models with a Pareto Optimization and two objectives: costs and carbon emissions. Clustering algorithms support the improvement of the models' scalability and performance. The methods developed in this work gives stakeholders the tool to calculate the capabilities and restrictions of the local energy system. The models are applied to a case study using data from an Austrian city, Linz. Four scenarios help to understand aspects of the energy community, such as the lock-in effect of existing infrastructure and future developments. The results show that it is possible to reduce both objectives, but the solutions for minimum costs and minimum carbon emissions are contrary to each other. This work quantifies the highest effect of emission reduction by the electrification of the system. It may be concluded, that a steady transformation of the local energy systems is necessary to reach economically sustainable goals. © 2019 Elsevier Ltd",10.1016/j.energy.2019.02.104,Data clustering; Emission accounting; Energy community; Machine learning; Multi-energy; Open source model; Pareto optimization,21.0,
A survey on handling computationally expensive multiobjective optimization problems with evolutionary algorithms,"Chugh T., Sindhya K., Hakanen J., Miettinen K.",Soft Computing,2019.0,"Evolutionary algorithms are widely used for solving multiobjective optimization problems but are often criticized because of a large number of function evaluations needed. Approximations, especially function approximations, also referred to as surrogates or metamodels are commonly used in the literature to reduce the computation time. This paper presents a survey of 45 different recent algorithms proposed in the literature between 2008 and 2016 to handle computationally expensive multiobjective optimization problems. Several algorithms are discussed based on what kind of an approximation such as problem, function or fitness approximation they use. Most emphasis is given to function approximation-based algorithms. We also compare these algorithms based on different criteria such as metamodeling technique and evolutionary algorithm used, type and dimensions of the problem solved, handling constraints, training time and the type of evolution control. Furthermore, we identify and discuss some promising elements and major issues among algorithms in the literature related to using an approximation and numerical settings used. In addition, we discuss selecting an algorithm to solve a given computationally expensive multiobjective optimization problem based on the dimensions in both objective and decision spaces and the computation budget available. © 2017, Springer-Verlag GmbH Germany, part of Springer Nature.",10.1007/s00500-017-2965-0,Computational cost; Machine learning; Metamodel; Multicriteria optimization; Pareto optimality; Response surface approximation; Surrogate,88.0,
A targeted Bayesian network learning for classification,"Gruber A., Ben-Gal I.",Quality Technology and Quantitative Management,2019.0,"A targeted Bayesian network learning (TBNL) method is proposed to account for a classification objective during the learning stage of the network model. The TBNL approximates the expected conditional probability distribution of the class variable. It effectively manages the trade-off between the classification accuracy and the model complexity by using a discriminative approach, constrained by information theory measurements. The proposed approach also provides a mechanism for maximizing the accuracy via a Pareto frontier over a complexity–accuracy plane, in cases of missing data in the data-sets. A comparative study over a set of classification problems shows the competitiveness of the TBNL mainly with respect to other graphical classifiers. © 2017, © 2017 International Chinese Association of Quantitative Management.",10.1080/16843703.2017.1395109,AI; Bayesian classifiers; complexity–accuracy trade-off; information theory; machine learning; target-oriented learning,2.0,
Hybrid position forecasting method for mobile robot transportation in smart indoor environment,"Duan Z., Liu H., Lv X., Ren Z., Junginger S.",ACM International Conference Proceeding Series,2019.0,"Indoor mobile robot position forecasting can improve safety and robustness of the robot navigation systems in indoor environment. In this paper, a novel hybrid computing framework for indoor mobile robot position forecasting is proposed, namely EWT-MOFEPSO-MRMRMI-ORELM. The proposed model consists of three parts, decomposition, feature selection and forecasting. The EWT (Empirical Wavelet Transform) algorithm is utilized to decompose raw series into several more predictable sublayers. For each sublayer, the MRMRMI (Maximum Relevancy Minimum Redundancy Maximum Interaction) model optimized by MOFEPSO (Multi-objective Feasibility Enhanced Particle Swarm Optimization) is applied to generate Pareto set of candidate feature set. The best feature set is selected as the one with minimum forecasting error in validation data. The selected best feature set is used as input of the ORELM (Outlier Robust Extreme Learning Machine) model to generate forecasting value. The ORELM can prevent adverse effect of outlier. Two experiments are carried out to verify the effectiveness of the proposed computing framework. The results indicate that (a) the proposed hybrid computing framework has excellent forecasting performance, (b) The EWT, MOFEPSO-MRMRMI can enhance performance of the ORELM significantly; and (c) the proposed forecasting method can guarantee the safety transportation of mobile robots in smart indoor environment. © 2019 Association for Computing Machinery.",10.1145/3335484.3335508,Empirical wavelet transform; Indoor mobile robot navigation; Multi-objective optimization; Outlier robust extreme learning machine,,
Opposition-based multi-objective whale optimization algorithm with global grid ranking,"Wang W.L., Li W.K., Wang Z., Li L.",Neurocomputing,2019.0,"Nature-inspired computing has attracted a lot of research effort especially for addressing real-world multi-objective optimization problem (MOP). This paper proposes a new nature-inspired optimization algorithm which is named opposition-based multi-objective whale optimization algorithm with global grid ranking (MOWOA). The proposed approach utilizes several parts to enhance the performance in optimization. First, the efficient evolution process is inherited from the single objective whale optimization algorithm(WOA). Second, opposition-based learning(OBL) is applied into the algorithm. Meanwhile, a novel mechanism called global grid ranking(GGR) which is inspired by grid mechanism has been incorporated into the proposed algorithm. To show the significance of the proposed algorithm, MOWOA is tested on a diverse set of benchmark with a series of well-known evolutionary algorithms and the influence of each individual strategy is also verified through 14 benchmarks. Moreover, the new proposed algorithm is also applied to the simple data clustering problem and a real-world water optimization problem in China. The results demonstrate that MOWOA is not only an algorithm with well performance for bench-mark problems but also expected to have a more wide application in real-world engineering problems. © 2019 Elsevier B.V.",10.1016/j.neucom.2019.02.054,Engineering optimization; Evolutionary algorithms; Global grid ranking; Multi-objective optimization; Opposition-based learning,34.0,
An Adaptive Online Parameter Control Algorithm for Particle Swarm Optimization Based on Reinforcement Learning,"Liu Y., Lu H., Cheng S., Shi Y.","2019 IEEE Congress on Evolutionary Computation, CEC 2019 - Proceedings",2019.0,"Parameter control is critical to the performance of any evolutionary algorithm (EA). In this paper, we propose a Q-Learning-based Particle Swarm Optimization (QLPSO) algorithm, which uses the Reinforcement Learning (RL) to train the parameters in Particle Swarm Optimization (PSO) algorithm. The core of the QLPSO algorithm is a three-dimensional Q table which consists of a state plane and an action axis. The state plane includes the state of the particles in both of the decision space and the objective space. The action axis controls the exploration and exploitation of particles by setting different parameters. The Q table can help particles to select actions according to their states. Besides, the Q table should be updated by reward function which is designed according to the performance change of particles and the number of iterations. The main difference between the QLPSO algorithms for single-objective and multi-objective optimization lies in the evaluation of the solution performance. In single-objective optimization, we only compare the fitness values of solutions, while in multi-objective optimization, we need to discuss the dominant relationship between solutions with the help of Pareto front. The performance of QLPSO is tested based on 6 single-objective and 5 multi-objective benchmark functions. The experiment results reveal the competitive performance of QLPSO compared with other algorithms. © 2019 IEEE.",10.1109/CEC.2019.8790035,optimization problem; parameter control; particle swarm optimization; reinforcement learning,10.0,
Remote Sensing Image Retrieval Based on Semi-Supervised Deep Hashing Learning,"Tang X., Liu C., Zhang X., Ma J., Jiao C., Jiao L.",International Geoscience and Remote Sensing Symposium (IGARSS),2019.0,"As an useful solution of the approximate nearest neighbor (ANN) search, hashing attracts growing attention in the topic of large-scale image retrieval. In this paper, we propose a semi-supervised deep hashing method based on the adversarial autoencoder (AAE) network for remote sensing image retrieval (RSIR), and we name it SSHAAE. Here, we assume the RS images have been represented by the visual features, and the target of our SSHAAE is mapping those features into the binary codes. First, a hashing layer is adopted to replace the part of original latent layer in AAE. In addition, the classical reconstruction loss function is selected to generate the hash code. Second, two discriminators are added simultaneously to make sure the hash code is bit balanced and the generated label variable is one-hot. Third, we design the hash loss function to guarantee the obtained hash code is discriminative, similarity persevering, and low quantization error. The presented SSHAAE model can be trained by the minimax optimization. The encouraging experimental results counted on a high-resolution RS image archive demonstrate our SSHAAE model is effective to RSIR. © 2019 IEEE.",10.1109/IGARSS.2019.8898676,Hash learning; image retrieval; remote sensing; semi-supervised,,
A tight approximation for submodular maximization with mixed packing and covering constraints,"Mizrachi E., Schwartz R., Spoerhase J., Uniyal S.","Leibniz International Proceedings in Informatics, LIPIcs",2019.0,"Motivated by applications in machine learning, such as subset selection and data summarization, we consider the problem of maximizing a monotone submodular function subject to mixed packing and covering constraints. We present a tight approximation algorithm that for any constant ε &gt; 0 achieves a guarantee of 1 − 1/e − ε while violating only the covering constraints by a multiplicative factor of 1 − ε. Our algorithm is based on a novel enumeration method, which unlike previously known enumeration techniques, can handle both packing and covering constraints. We extend the above main result by additionally handling a matroid independence constraint as well as finding (approximate) pareto set optimal solutions when multiple submodular objectives are present. Finally, we propose a novel and purely combinatorial dynamic programming approach. While this approach does not give tight bounds it yields deterministic and in some special cases also considerably faster algorithms. For example, for the well-studied special case of only packing constraints (Kulik et al. [Math. Oper. Res. '13] and Chekuri et al. [FOCS '10]), we are able to present the first deterministic non-trivial approximation algorithm. We believe our new combinatorial approach might be of independent interest. © Eyal Mizrachi, Roy Schwartz, Joachim Spoerhase, and Sumedha Uniyal; licensed under Creative Commons License CC-BY",10.4230/LIPIcs.ICALP.2019.85,Approximation algorithm; Covering; Packing; Submodular function,,
Multiobjective shape design in a ventilation system with a preference-driven surrogate-assisted evolutionary algorithm,"Chugh T., Kratky T., Miettinen K., Jin Y., Makonen P.",GECCO 2019 - Proceedings of the 2019 Genetic and Evolutionary Computation Conference,2019.0,"We formulate and solve a real-world shape design optimization problem of an air intake ventilation system in a tractor cabin by using a preference-based surrogate-assisted evolutionary multiobjective optimization algorithm. We are motivated by practical applicability and focus on two main challenges faced by practitioners in industry: 1) meaningful formulation of the optimization problem reflecting the needs of a decision maker and 2) finding a desirable solution based on a decision maker's preferences when solving a problem with computationally expensive function evaluations. For the first challenge, we describe the procedure of modelling a component in the air intake ventilation system with commercial simulation tools. The problem to be solved involves time consuming computational fluid dynamics simulations. Therefore, for the second challenge, we extend a recently proposed Kriging-assisted evolutionary algorithm K-RVEA to incorporate a decision maker's preferences. Our numerical results indicate efficiency in using the computing resources available and the solutions obtained reflect the decision maker's preferences well. Actually, two of the solutions dominate the baseline design (the design provided by the decision maker before the optimization process). The decision maker was satisfied with the results and eventually selected one as the final solution. © 2019 Association for Computing Machinery.",10.1145/3321707.3321745,Computational cost; Evolutionary multiobjective optimization; Machine learning; Metamodel; Multiple criteria decision making; Optimal shape design; Pareto optimality; Preference information,2.0,
An improved fuzzy classification system for financial credit decision using multi-objective evolutionary optimization,"Dwivedi P.K., Tripathi S.P.",International Journal of Engineering and Advanced Technology,2019.0,"In fuzzy classification system, accuracy has been gained at the cost of interpretability and vice versa. This situation is known as Interpretability-Accuracy Trade-off. To handle this trade-off between accuracy and interpretability the evolutionary algorithms (EAs) are often used to optimize the performance of the fuzzy classification system. From the last two decades, several multi-objective evolutionary systems have been designed and successfully implemented in several fields for finding multiple solutions at a single run. In Financial Decision making concerning Credit Allocation, Classification is a significant component to obtain credit scores and predict bankruptcy. A fuzzy classification system for the financial credit decision has been designed and find out the Accuracy and Interpretability parameters for applying various MOEAs to get the pareto optimal solution resulting in to improvement in the performance of the proposed system. The proposed model implemented on standard benchmark financial credit allocation datasets i.e., German Credit Approval system available from the UCI repository of machine learning databases (http://archive.ics.uci.edu/ml) and using the open source tool MOEA framework (http://www.moeaframework.org). The experimental analysis highlights that the NSGA-III works efficiently for financial credit approval system and improves the performance by making a balanced trade-off between accuracy and interpretability. © BEIESP.",10.35940/ijeat.F9136.088619,"Evolutionary algorithm; Fuzzy classifier; Fuzzy rules; I-A Trade-off; MOEA, etc; Multi-objective",,
Computational fluid dynamic enabled design optimisation of miniaturised continuous oscillatory baffled reactors in chemical processing,"González Niño C., Kapur N., King M.-F., de Boer G., Blacker A.J., Bourne R., Thompson H.",International Journal of Computational Fluid Dynamics,2019.0,"The first CFD-enabled multi-objective design optimisation methodology for continuous oscillatory baffled reactors (COBRs), used for flow chemistry-based process development, is described, where performance is quantified in terms of two metrics: a mixing efficiency index and the variance of the residence time distribution. The effect of cross-validation approaches on the surrogate modelling of these performance metrics is examined in detail and the resultant surrogate models used to demonstrate the influence of key design variables. Pareto fronts of non-dominated solutions are presented to illustrate the available design compromises for COBR performance and it is shown that these can give a narrow Residence Time Distribution and good mixing within the final design. The novel feature of offset baffles within a channel, explored here for the first time, is identified as a key parameter in improving the performance of COBRs. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.",10.1080/10618562.2019.1683169,Continuous oscillatory baffled reactor; machine learning; multi-objective optimisation; surrogate modelling,3.0,
Transfer learning of deep material network for seamless structure–property predictions,"Liu Z., Wu C.T., Koishi M.",Computational Mechanics,2019.0,"Modern materials design requires reliable and consistent structure–property relationships. The paper addresses the need through transfer learning of deep material network (DMN). In the proposed learning strategy, we store the knowledge of a pre-trained network and reuse it to generate the initial structure for a new material via a naive approach. Significant improvements in the training accuracy and learning convergence are attained. Since all the databases share the same base network structure, their fitting parameters can be interpolated to seamlessly create intermediate databases. The new transferred models are shown to outperform the analytical micromechanics methods in predicting the volume fraction effects. We then apply the unified DMN databases to the design of failure properties, where the failure criteria are defined upon the distribution of microscale plastic strains. The Pareto frontier of toughness and ultimate tensile strength is extracted from a large-scale design space enabled by the efficiency of DMN extrapolation. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.",10.1007/s00466-019-01704-4,Failure analysis; Machine learning; Materials design; Micromechanics; Multiscale modeling; Nonlinear plasticity,15.0,
"Lake Water-Level fluctuations forecasting using Minimax Probability Machine Regression, Relevance Vector Machine, Gaussian Process Regression, and Extreme Learning Machine","Bonakdari H., Ebtehaj I., Samui P., Gharabaghi B.",Water Resources Management,2019.0,"Forecasting freshwater lake levels is vital information for water resource management, including water supply management, shoreline management, hydropower generation optimization, and flood management. This study presents a novel application of four advanced artificial intelligence models namely the Minimax Probability Machine Regression (MPMR), Relevance Vector Machine (RVM), Gaussian Process Regression (GPR) and Extreme Learning Machine (ELM) for forecasting lake level fluctuation in Lake Huron utilizing historical datasets. The MPMR is a probabilistic framework that employed Mercer Kernels to achieve nonlinear regression models. The GPR, which is a probabilistic technique used tractable Bayesian framework for generalization of multivariate distribution of input samples to vast dimensional space. The ELM is a capable algorithm-based model for the implementation of the single-layer feed-forward neural network. The RVM demonstrate depends on the specification of the Bayesian method on a linear model with proper preceding that results in demonstration of sparse. The recommended techniques were tested to evaluate the current lake water-level trend monthly from the historical datasets at four previous time steps. The Lake Huron levels from 1918 to 1993 was managed for the training phase, and the rest of data (from 1994 to 2013) was used for testing. Considering the monthly and annually previous time steps, six models were introduced and found that the best results are achieved for a model with (t-1, t-2, t-3, t-12) as input combinations. The results show that all models can forecast the lake levels precisely. The results of this research study exhibit that the MPMR model (R2 = 0.984; MAE = 0.035; RMSE = 0.044; ENS = 0.984; DRefined = 0.995; ELM = 0.874) found to be more precise in lake level forecasting. The MPMR can be utilized as a practical computational tool on current and future planning with sustainable management of water resource of Lake Michigan-Huron. © 2019, Springer Nature B.V.",10.1007/s11269-019-02346-0,Gaussian Process Regression (GPR); Lake level; Minimax Probability Machine Regression (MPMR); Relevance Vector Machine (RVM),18.0,
Differential evolution based on reinforcement learning with fitness ranking for solving multimodal multiobjective problems,"Li Z., Li S., Yue C., Shang Z., Qu B.",Swarm and Evolutionary Computation,2019.0,"In multimodal multiobjective optimization problems (MMOOPs), there is more than one Pareto-optimal Set (PS) in the decision space corresponding to the same Pareto Front(PF). How to dynamically adjust the evolution direction of the population adaptively is a key problem, to ensure approaching the PF in the global sense with good convergence while finding out more PSs. In this paper, a novel Differential Evolution algorithm based on Reinforcement Learning with Fitness Ranking (DE-RLFR) is proposed. The DE-RLFR is based on the Q-learning framework, and each individual in the population is considered an agent. The fitness ranking values of each agent are used to encode hierarchical state variables. Three typical DE mutation operations are employed as optional actions for the agent. Based on the analysis of the distribution characteristics of the population in objective space, decision space and fitness-ranking space, we design a reward function of the 〉state, action〈 pairs to guide the population to move to the PF asymptotically. According to its reinforcement learning experience represented by the corresponding Q table value, each agent could adaptively select a mutation strategy to generate offspring individuals. The evaluation results on eleven MMOOP test functions show that DE-RLFR could quickly and effectively find multiple PSs in the decision space, and approach PF in the global sense. © 2019 Elsevier B.V.",10.1016/j.swevo.2019.06.010,Differential evolution; Fitness ranking; Multimodal multiobjective optimization problem; Q-learning; Reinforcement learning,31.0,
On the global optima of kernelized adversarial representation learning,"Sadeghi B., Yu R., Boddeti V.",Proceedings of the IEEE International Conference on Computer Vision,2019.0,"Adversarial representation learning is a promising paradigm for obtaining data representations that are invariant to certain sensitive attributes while retaining the information necessary for predicting target attributes. Existing approaches solve this problem through iterative adversarial minimax optimization and lack theoretical guarantees. In this paper, we first study the ''linear' form of this problem i.e., the setting where all the players are linear functions. We show that the resulting optimization problem is both non-convex and non-differentiable. We obtain an exact closed-form expression for its global optima through spectral learning and provide performance guarantees in terms of analytical bounds on the achievable utility and invariance. We then extend this solution and analysis to non-linear functions through kernel representation. Numerical experiments on UCI, Extended Yale B and CIFAR-100 datasets indicate that, (a) practically, our solution is ideal for ''imparting' provable invariance to any biased pre-trained data representation, and (b) the global optima of the ''kernel' form can provide a comparable trade-off between utility and invariance in comparison to iterative minimax optimization of existing deep neural network based approaches, but with provable guarantees. © 2019 IEEE.",10.1109/ICCV.2019.00806,,10.0,
Online Anomaly Detection in Multivariate Settings,"Mozaffari M., Yilmaz Y.","IEEE International Workshop on Machine Learning for Signal Processing, MLSP",2019.0,This paper considers the real-time and nonparametric detection of anomalies in high-dimensional systems. The goal is to detect anomalies quickly and accurately such that the appropriate countermeasures could be taken before any possible harm is caused by the anomalous event. We propose a k NN-based sequential anomaly detection method in both semi-supervised and supervised settings. We prove that the proposed method is asymptotically optimum in the minimax sense under certain conditions in terms of minimizing the average detection delay for a given false alarm constraint. The proposed method is shown to be capable of multivariate anomaly detection and also scalable to high-dimensional datasets. We further propose an online learning scheme that combines the desirable properties of our semi-supervised and supervised methods. © 2019 IEEE.,10.1109/MLSP.2019.8918893,anomaly detection; nonparametric methods; online learning,8.0,
"Dynamic multi-objective optimisation using deep reinforcement learning: benchmark, algorithm and an application to identify vulnerable zones based on water quality","Hasan M.M., Lwin K., Imani M., Shabut A., Bittencourt L.F., Hossain M.A.",Engineering Applications of Artificial Intelligence,2019.0,"Dynamic multi-objective optimisation problem (DMOP) has brought a great challenge to the reinforcement learning (RL) research area due to its dynamic nature such as objective functions, constraints and problem parameters that may change over time. This study aims to identify the lacking in the existing benchmarks for multi-objective optimisation for the dynamic environment in the RL settings. Hence, a dynamic multi-objective testbed has been created which is a modified version of the conventional deep-sea treasure (DST) hunt testbed. This modified testbed fulfils the changing aspects of the dynamic environment in terms of the characteristics where the changes occur based on time. To the authors’ knowledge, this is the first dynamic multi-objective testbed for RL research, especially for deep reinforcement learning. In addition to that, a generic algorithm is proposed to solve the multi-objective optimisation problem in a dynamic constrained environment that maintains equilibrium by mapping different objectives simultaneously to provide the most compromised solution that closed to the true Pareto front (PF). As a proof of concept, the developed algorithm has been implemented to build an expert system for a real-world scenario using Markov decision process to identify the vulnerable zones based on water quality resilience in São Paulo, Brazil. The outcome of the implementation reveals that the proposed parity-Q deep Q network (PQDQN) algorithm is an efficient way to optimise the decision in a dynamic environment. Moreover, the result shows PQDQN algorithm performs better compared to the other state-of-the-art solutions both in the simulated and the real-world scenario. © 2019 Elsevier Ltd",10.1016/j.engappai.2019.08.014,Artificial intelligence; Deep Q network; Dynamic environment; Meta-policy selection; Reinforcement learning; Water quality resilience,9.0,
Multi-objective artificial immune algorithm for fuzzy clustering based on multiple kernels,"Shang R., Zhang W., Li F., Jiao L., Stolkin R.",Swarm and Evolutionary Computation,2019.0,"This paper presents a multi-objective artificial immune algorithm for fuzzy clustering based on multiple kernels (MAFC). MAFC extends the classical Fuzzy C-Means (FCM) algorithm and improves some of its important limitations, such as vulnerability to local optima convergence, which can lead to poor clustering quality. MAFC unifies multi-kernel learning and multi-objective optimization in a joint clustering framework, which preserves the geometric information of the dataset. The multi-kernel method maps data from the feature space to kernel space by using kernel functions. Additionally, the introduction of multi-objective optimization helps to optimize between-cluster separation and within-cluster compactness simultaneously via two different clustering validity criteria. These properties help the proposed algorithm to avoid becoming stuck at local optima. Furthermore, this paper utilizes an artificial immune algorithm to address the multi-objective clustering problem and acquire a Pareto optimal solution set. The solution set is obtained through the process of antibody population initialization, clone proliferation, non-uniform mutation and uniformity maintaining strategy, which avoids the problems of degradation and prematurity which can occur with conventional genetic algorithms. Finally, we choose the best solution from the Pareto optimal solution set. We use a semi-supervised method to achieve the final clustering results. We compare our method against state-of-the-art methods from the literature by performing experiments with both UCI datasets and face datasets. The results suggest that MAFC is significantly more efficient for clustering and has a wider scope of application. © 2019 Elsevier B.V.",10.1016/j.swevo.2019.01.001,Artificial immune algorithm; Fuzzy c-means (FCM); Multi-objective optimization; Multiple kernel learning,14.0,
A Gaussian process mixture model-based hard-cut iterative learning algorithm for air quality prediction,"Zhou Y., Zhao X., Lin K.-P., Wang C.-H., Li L.",Applied Soft Computing Journal,2019.0,"Air quality is closely related to concentrations of gaseous pollutants, and the prediction of gaseous pollutant concentration plays a decisive role in regulating plant and vehicle emissions. Due to the non-linear and chaotic characteristics of the gas concentration series, traditional models may not easily capture the complex time series pattern. In this study, the Gaussian Process Mixture (GPM) model, which adopts hidden variables posterior hard-cut (HC) iterative learning algorithm, is first applied to the prediction of gaseous pollutant concentration in order to improve prediction performance. This algorithm adopts iterative learning and uses the maximizing a posteriori (MAP) estimation to achieve the optimal grouping of samples which effectively improves the expectation–maximization (EM) learning in GPM. The empirical results of the GPM model reveals improved prediction accuracy in gaseous pollutant concentration prediction, as compared with the kernel regression (K-R), minimax probability machine regression (MPMR), linear regression (L-R) and Gaussian Processes (GP) models. Furthermore, GPM with various learning algorithms, namely the HC algorithm, Leave-one-out Cross Validation (LOOCV), and variational algorithms, respectively, are also examined in this study. The results also show that the GPM with HC learning achieves superior performance compared with other learning algorithms. © 2019 Elsevier B.V.",10.1016/j.asoc.2019.105789,Gaseous pollutant time series; Gaussian processes mixtures; Machine learning; Prediction,9.0,
Automatic clustering by multi-objective genetic algorithm with numeric and categorical features,"Dutta D., Sil J., Dutta P.",Expert Systems with Applications,2019.0,"Many clustering algorithms categorized as K-clustering algorithm require the user to predict the number of clusters (K) to do clustering. Due to lack of domain knowledge an accurate value of K is difficult to predict. The problem becomes critical when the dimensionality of data points is large; clusters differ widely in shape, size, and density; and when clusters are overlapping in nature. Determining the suitable K is an optimization problem. Automatic clustering algorithms can discover the optimal K. This paper presents an automatic clustering algorithm which is superior to K-clustering algorithm as it can discover an optimal value of K. Iterative hill-climbing algorithms like K-Means work on a single solution and converge to a local optimum solution. Here, Genetic Algorithms (GAs) find out near global optimum solutions, i.e. optimal K as well as the optimal cluster centroids. Single-objective clustering algorithms are adequate for efficiently grouping linearly separable clusters. For non-linearly separable clusters they are not so good. So for grouping non-linearly separable clusters, we apply Multi-Objective Genetic Algorithm (MOGA) by minimizing the intra-cluster distance and maximizing inter-cluster distance. Many existing MOGA based clustering algorithms are suitable for either numeric or categorical features. This paper pioneered employing MOGA for automatic clustering with mixed types of features. Statistical testing on experimental results on real-life benchmark data sets from the University of California at Irvine (UCI) machine learning repository proves the superiority of the proposed algorithm. © 2019 Elsevier Ltd",10.1016/j.eswa.2019.06.056,Automatic clustering; Multi-Objective Genetic Algorithm (MOGA); Pareto approach; Statistical test,19.0,
Relaxed regularization for linear inverse problems,"Luiken N., Van Leeuwen T.",SIAM Journal on Scientific Computing,2020.0,"We consider regularized least-squares problems of the form min {equation presented}. Recently, Zheng et al. [IEEE Access, 7 (2019), pp. 1404-1423], proposed an algorithm called Sparse Relaxed Regularized Regression (SR3) that employs a splitting strategy by introducing an auxiliary variable y and solves min {equation presented}. By minimizing out the variable x, we obtain an equivalent optimization problem min {equation presented}. In our work, we view the SR3 method as a way to approximately solve the regularized problem. We analyze the conditioning of the relaxed problem in general and give an expression for the SVD of Fκ as a function of κ. Furthermore, we relate the Pareto curve of the original problem to the relaxed problem, and we quantify the error incurred by relaxation in terms of κ. Finally, we propose an efficient iterative method for solving the relaxed problem with inexact inner iterations. Numerical examples illustrate the approach. Copyright © by SIAM.",10.1137/20M1348091,Inverse problems; Machine learning; Optimization; Regularization; Sparsity; Total variation,,
Dynamic Complexity Tuning for Hardware-Aware Probabilistic Circuits,"Galindez Olascoaga L.I., Meert W., Shah N., Verhelst M.",Communications in Computer and Information Science,2020.0,"Probabilistic inference is a well suited approach to address the challenges of resource constrained embedded application scenarios. In particular, probabilistic models learned generatively are robust to missing data and are capable of encoding domain knowledge seamlessly. These traits have been leveraged to propose hardware-aware probabilistic learning and inference strategies that induce Pareto optimal accuracy versus resource consumption trade-offs. This paper proposes a model-complexity tuning strategy that relies on ensembles of probabilistic classifiers to identify the difficulty of the classification task on a given instance. It then dynamically switches to a higher or lower complexity setting accordingly. The strategy is evaluated on an embedded human activity recognition scenario and demonstrates a superior performance when compared to the Pareto-optimal trade-off obtained when the ensembles are deployed statically, especially in low cost regions of the trade-off space. This makes the strategy amenable to embedded computing scenarios, where one of the main constraints towards always-on functionality are the device’s strict resource constraints. © 2020, Springer Nature Switzerland AG.",10.1007/978-3-030-66770-2_21,Hardware-aware probabilistic models; Probabilistic circuits; Resource constrained embedded applications,,
Pareto-Weighted-Sum-Tuning: Learning-to-Rank for Pareto Optimization Problems,"Wang H., Denton B.T.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020.0,"The weighted-sum method is a commonly used technique in Multi-objective optimization to represent different criteria considered in a decision-making and optimization problem. Weights are assigned to different criteria depending on the degree of importance. However, even if decision-makers have an intuitive sense of how important each criteria is, explicitly quantifying and hand-tuning these weights can be difficult. To address this problem, we propose the Pareto-Weighted-Sum-Tuning algorithm as an automated and systematic way of trading-off between different criteria in the weight-tuning process. Pareto-Weighted-Sum-Tuning is a configurable online-learning algorithm that uses sequential discrete choices by a decision-maker on sequential decisions, eliminating the need to score items or weights. We prove that utilizing our online-learning approach is computationally less expensive than batch-learning, where all the data is available in advance. Our experiments show that Pareto-Weighted-Sum-Tuning is able to achieve low relative error with different configurations. © 2020, Springer Nature Switzerland AG.",10.1007/978-3-030-64580-9_39,Information retrieval; Machine learning; Multi-objective optimization; Online learning,,
Anti-Jerk On-Ramp Merging Using Deep Reinforcement Learning,"Lin Y., McPhee J., Azad N.L.","IEEE Intelligent Vehicles Symposium, Proceedings",2020.0,"Deep Reinforcement Learning (DRL) is used here for decentralized decision-making and longitudinal control for high-speed on-ramp merging. The DRL environment state includes the states of five vehicles: the merging vehicle, along with two preceding and two following vehicles when the merging vehicle is or is projected on the main road. The control action is the acceleration of the merging vehicle. Deep Deterministic Policy Gradient (DDPG) is the DRL algorithm for training to output continuous control actions. We investigated the relationship between collision avoidance for safety and jerk minimization for passenger comfort in the multi-objective reward function by obtaining the Pareto front. We found that, with a small jerk penalty in the multi-objective reward function, the vehicle jerk could be reduced by 73% compared with no jerk penalty while the collision rate was maintained at zero. Regardless of the jerk penalty, the merging vehicle exhibited decision-making strategies such as merging ahead or behind a main-road vehicle. © 2020 IEEE.",10.1109/IV47402.2020.9304647,,3.0,
Multi-objective modelling of leading edge serrations applied to low-pressure axial fans,"Biedermann T.M., Reich M., Paschereit C.O.",Proceedings of the ASME Turbo Expo,2020.0,"A novel modelling strategy is proposed which allows high-accuracy predictions of aerodynamic and aeroacoustic target values for a low-pressure axial fan, equipped with serrated leading edges. Inspired by machine learning processes, the sampling of the experimental space is realized by use of a Latin hypercube design plus a factorial design, providing highly diverse information on the analyzed system. The effects of four influencing parameters are tested, characterizing the inflow conditions as well as the serration geometry. A total of 65 target values in the time and frequency domains are defined and can be approximated with high accuracy by individual artificial neural networks. Furthermore, the validation of the model against fully independent test points within the experimental space yields a remarkable fit, even for the spectral distribution in 1/3rd-octave bands, proving the ability of the model to generalize. A meta-heuristic multi-objective optimization approach provides two-dimensional Pareto optimal solutions for selected pairs of target values. This is particularly important for reconciling opposing trends, such as the noise reduction capability and aerodynamic performance. The chosen optimization strategy also allows for a customized design of serrated leading edges, tailored to the specific operating conditions of the axial fan. © 2020 ASME",10.1115/GT2020-14400,Aeroacoustic modelling; Artificial neural networks; Leading edge serrations; Low-pressure axial fans; Multi-objective optimization,,
Imputation of Missing Boarding Stop Information in Smart Card Data with Machine Learning Methods,"Shalit N., Fire M., Ben-Elia E.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020.0,"With the increase in population densities and environmental awareness, public transport has become an important aspect of urban life. Consequently, large quantities of transportation data are generated, and mining data from smart card use has become a standardized method to understand the travel habits of passengers. Increase in available data and computation power demands more sophisticated methods to analyze big data. Public transport datasets, however, often lack data integrity. Boarding stop information may be missing either due to imperfect acquirement processes or inadequate reporting. As a result, large quantities of observations and even complete sections of cities might be absent from the smart card database. We have developed a machine (supervised) learning method to impute missing boarding stops based on ordinal classification. In addition, we present a new metric, Pareto Accuracy, to evaluate algorithms where classes have an ordinal nature. Results are based on a case study in the city of Beer Sheva utilizing one month of data. We show that our proposed method significantly outperforms schedule-based imputation methods and can improve the accuracy and usefulness of large-scale transportation data. The implications for data imputation of smart card information is further discussed. © 2020, Springer Nature Switzerland AG.",10.1007/978-3-030-62362-3_3,Boarding stop imputation; Machine learning; Smart card,,
DBQ: A Differentiable Branch Quantizer for Lightweight Deep Neural Networks,"Dbouk H., Sanghvi H., Mehendale M., Shanbhag N.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020.0,"Deep neural networks have achieved state-of-the art performance on various computer vision tasks. However, their deployment on resource-constrained devices has been hindered due to their high computational and storage complexity. While various complexity reduction techniques, such as lightweight network architecture design and parameter quantization, have been successful in reducing the cost of implementing these networks, these methods have often been considered orthogonal. In reality, existing quantization techniques fail to replicate their success on lightweight architectures such as MobileNet. To this end, we present a novel fully differentiable non-uniform quantizer that can be seamlessly mapped onto efficient ternary-based dot product engines. We conduct comprehensive experiments on CIFAR-10, ImageNet, and Visual Wake Words datasets. The proposed quantizer (DBQ) successfully tackles the daunting task of aggressively quantizing lightweight networks such as MobileNetV1, MobileNetV2, and ShuffleNetV2. DBQ achieves state-of-the art results with minimal training overhead and provides the best (pareto-optimal) accuracy-complexity trade-off. © 2020, Springer Nature Switzerland AG.",10.1007/978-3-030-58583-9_6,Deep learning; Low-complexity neural networks; Quantization,,
Improved Adversarial Training via Learned Optimizer,"Xiong Y., Hsieh C.-J.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020.0,"Adversarial attack has recently become a tremendous threat to deep learning models. To improve the robustness of machine learning models, adversarial training, formulated as a minimax optimization problem, has been recognized as one of the most effective defense mechanisms. However, the non-convex and non-concave property poses a great challenge to the minimax training. In this paper, we empirically demonstrate that the commonly used PGD attack may not be optimal for inner maximization, and improved inner optimizer can lead to a more robust model. Then we leverage a learning-to-learn (L2L) framework to train an optimizer with recurrent neural networks, providing update directions and steps adaptively for the inner problem. By co-training optimizer’s parameters and model’s weights, the proposed framework consistently improves over PGD-based adversarial training and TRADES. © 2020, Springer Nature Switzerland AG.",10.1007/978-3-030-58598-3_6,Adversarial training; Learning to learn; Optimization,1.0,
FraudFox: Adaptable Fraud Detection in the Real World,"Butler M., Fan Y., Faloutsos C.",Communications in Computer and Information Science,2020.0,"The proposed method (FraudFox) provides solutions to adversarial attacks in a resource constrained environment. We focus on questions like the following: How suspicious is ‘Smith’, trying to buy $500 shoes, on Monday 3am? How to merge the risk scores, from a handful of risk-assessment modules (‘oracles’) in an adversarial environment? More importantly, given historical data (orders, prices, and what-happened afterwards), and business goals/restrictions, which transactions, like the ‘Smith’ transaction above, which ones should we ‘pass’, versus send to human investigators? The business restrictions could be: ‘at most x investigations are feasible’, or ‘at most $y lost due to fraud’. These are the two research problems we focus on, in this work. One approach to address the first problem (‘oracle-weighting’), is by using Extended Kalman Filters with dynamic importance weights, to automatically and continuously update our weights for each ‘oracle’. For the second problem, we show how to derive an optimal decision surface, and how to compute the Pareto optimal set, to allow what-if questions. An important consideration is adaptation: Fraudsters will change their behavior, according to our past decisions; thus, we need to adapt accordingly. The resulting system, FraudFox, is scalable, adaptable to changing fraudster behavior, effective, and already in production at Amazon. FraudFox augments a fraud prevention sub-system and has led to significant performance gains. © 2020, Springer Nature Switzerland AG.",10.1007/978-3-030-59621-7_3,Adversarial learning; Ensemble modeling; Fraud detection; Kalman filters,,
Intelligent algorithms with selection of hyperparameters for e-health applications powered by 5g wireless networks,"Turnea M., Arotaritei D., Fuior R.",eLearning and Software for Education Conference,2020.0,"Different from previous generations, the 5G networks has new capabilities due to servicebased architecture model and virtualization. The successful broadband networks must be able to handle the growth in the data traffic. The e-Health networks has additional issues as the continuous monitoring of patients suffering from chronic diseases (non-communicable diseases). The wearable devices used for monitoring are supposed to be used for balneo-physio-kinetotherapy (including the body gait index calculation) in the future and this will require and increasing traffic as users and data for 5G networks. Medical data and biomedical data are usually very large (especially for medical images) and the traffic can be critical in some situation when in order to take a decision due to alarms from generated by medical emergency when the data should be provided very fast to the physicians (hospitals, or clinics). An architecture for smart e-Health monitoring including the management of big database open the opportunity to use intelligent algorithm for complex problems, machine learning and artificial intelligence. The possibility to use of three algorithms in simulation and simulators for e-Health 5G wireless network is investigate in this paper. One of the key requirement is low energy consumption due to number of antenna elements at the access points and number of user terminals. The problem optimization address to a mix agglomeration: Dense urban area along with a set of dispersed locations in a rural area. The network planning is defined as optimization problem of configuration that depends on BS (Base Station) location and transmission power but as novelty, the constraints due to inclusion of rural area are also included in feasible solution. The constraints refer to two situations: The relief (that can be natural zone) or imposed black zone due external factors. Three algorithms are examined: Realcoded Genetic Algorithm for Variable Population - RCGAV), NSGA - II and Gossip, applied to modelling and optimization of power consumption in wireless access networks. Scenarios with simulation of the traffic between the client and the server are taken in to account using known models of distribution: Poisson, Pareto, and Weibull. © 2020, National Defence University-Carol I Printing House. All rights reserved.",10.12753/2066-026X-20-205,Broadband Wireless; Data traffic; Intelligent Algorithms; Machine Learning; Optimization Algorithms,,
Stressgan: A generative deep learning model for 2D stress distribution prediction,"Jiang H., Nie Z., Yeo R., Barati A., Levent F., Kara B.",Proceedings of the ASME Design Engineering Technical Conference,2020.0,"Using deep learning to analyze mechanical stress distributions has been gaining interest with the demand for fast stress analysis methods. Deep learning approaches have achieved excellent outcomes when utilized to speed up stress computation and learn the physics without prior knowledge of underlying equations. However, most studies restrict the variation of geometry or boundary conditions, making these methods difficult to be generalized to unseen configurations. We propose a conditional generative adversarial network (cGAN) model for predicting 2D von Mises stress distributions in solid structures. The cGAN learns to generate stress distributions conditioned by geometries, load, and boundary conditions through a two-player minimax game between two neural networks with no prior knowledge. By evaluating the generative network on two stress distribution datasets under multiple metrics, we demonstrate that our model can predict more accurate high-resolution stress distributions than a baseline convolutional neural network model, given various and complex cases of geometry, load and boundary conditions. Copyright © 2020 ASME.",10.1115/DETC2020-22682,,3.0,
A simple and effective methodology to perform multi-objective bayesian optimization: An application in the design of sandwich composite armors for blast mitigation,"Valladares H., Tovar A.",Proceedings of the ASME Design Engineering Technical Conference,2020.0,"Bayesian optimization is a versatile numerical method to solve global optimization problems of high complexity at a reduced computational cost. The efficiency of Bayesian optimization relies on two key elements: a surrogate model and an acquisition function. The surrogate model is generated on a Gaussian process statistical framework and provides probabilistic information of the prediction. The acquisition function, which guides the optimization, uses the surrogate probabilistic information to balance the exploration and the exploitation of the design space. In the case of multi-objective problems, current implementations use acquisition functions such as the multi-objective expected improvement (MEI). The evaluation of MEI requires a surrogate model for each objective function. In order to expand the Pareto front, such implementations perform a multi-variate integral over an intricate hypervolume, which require high computational cost. The objective of this work is to introduce an efficient multi-objective Bayesian optimization method that avoids the need for multi-variate integration. The proposed approach employs the working principle of multi-objective traditional methods, e.g., weighted sum and min-max methods, which transform the multi-objective problem into a single-objective problem through a functional mapping of the objective functions. Since only one surrogate is trained, this approach has a low computational cost. The effectiveness of the proposed approach is demonstrated with the solution of four problems: (1) an unconstrained version of the Binh and Korn test problem (convex Pareto front), (2) the Fonseca and Fleming test problem (non-convex Pareto front), (3) a three-objective test problem and (4) the design optimization of a sandwich composite armor for blast mitigation. The optimization algorithm is implemented in MATLAB and the finite element simulations are performed in the explicit, nonlinear finite element analysis code LS-DYNA. The results are comparable (or superior) to the results of the MEI acquisition function. © 2020 American Society of Mechanical Engineers (ASME). All rights reserved.",10.1115/DETC2020-22564,Bayesian Machine Learning; Bayesian Optimization; Blast Mitigation; Design Optimization; Multi-objective optimization; Response Surface Methodology; Sandwich Composites,2.0,
Nonconvex Nonseparable Sparse Nonnegative Matrix Factorization for Hyperspectral Unmixing,"Xiong F., Zhou J., Lu J., Qian Y.",IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,2020.0,"Hyperspectral unmixing is an important step to learn the material categories and corresponding distributions in a scene. Over the past decade, nonnegative matrix factorization (NMF) has been utilized for this task, thanks to its good physical interpretation. The solution space of NMF is very huge due to its nonconvex objective function for both variables simultaneously. Many convex and nonconvex sparse regularizations are embedded into NMF to limit the number of trivial solutions. Unfortunately, they either produce biased sparse solutions or unbiased sparse solutions with the sacrifice of the convex objective function of NMF with respect to individual variable. In this article, we enhance NMF by introducing a generalized minimax concave (GMC) sparse regularization. The GMC regularization is nonconvex and nonseparable, enabling promotion of unbiased and sparser results while simultaneously preserving the convexity of NMF for each variable separately. Therefore, GMC-NMF better avoids being trapped into local minimals, and thereby produce physically meaningful and accurate results. Extensive experimental results on synthetic data and real-world data verify its utility when compared with several state-of-the-art approaches. © 2008-2012 IEEE.",10.1109/JSTARS.2020.3028104,Generalized minimax concave (GMC) regularization; hyperspectral unmixing; nonnegative matrix factorization (NMF); sparse representation,3.0,
A task unloading strategy of IoT devices using deep reinforcement learning based on mobile cloud computing environment,"Qi H., Mu X., Shi Y.",Wireless Networks,2020.0,"Aiming at the task unloading mode in cloud computing environment, the task unloading problem for IoT devices is studied. Through theoretical analysis, we can know that in the task unloading problem, it is usually contradictory to improve the utilization of cloud resources and reduce the task delay. In order to solve this problem, a task unloading scheme for Internet of things devices using deep reinforcement learning algorithm is proposed. The deep reinforcement learning algorithm is used to model the task unloading problem. The return value with weight is introduced into the algorithm, and the utilization rate of cloud resources and the delay of unloading task are weighed by adjusting the return value of the weight. First of all, the improved k-means clustering algorithm with weighted density is used to cluster the physical machines. The physical machines of each cluster have similar bandwidth and task waiting time. Then, deep reinforcement learning is used to select the best physical machine cluster from the current unloading tasks. Finally, the improved PSO algorithm is used to select the optimal physical machine from the optimal cluster, and Pareto is used to improve the convergence speed. Experimental results show that compared with the traditional method, the proposed algorithm has a good performance, and can achieve the goal of increasing the utilization of physical machine resources and reducing task delay. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",10.1007/s11276-020-02471-4,Convergence speed; Deep reinforcement learning; K-means clustering algorithm; Mobile cloud computing; Physical machine; Task unloading,1.0,
Object detection in conditional GAN transferred sensor images,Bhatia A.,Proceedings of SPIE - The International Society for Optical Engineering,2020.0,"Object detection is a central theme for many Artificial Intelligence (AI) applications such as autonomous vehicles, surveillance etc. The algorithms providing this capability rely on training data being available in corresponding sensor mode. Having co-located data from multiple sensor modes enhances the detection confidence, but the availability of training data in desired sensor mode is not always readily available, which slows down progress. In this paper, we investigate the ability to translate images from one sensor mode to another, on a single fixed camera dataset, using conditional Generative Adversarial Network (cGAN). Specifically, images are transferred from Electro-Optical (EO) to Infra-Red (IR) images and vice-versa using cGAN models, which are generative models that learn the data distribution in a minimax game setting. To investigate the usability of such transferred images, we apply object detection algorithm on ground truth and transferred images and compare their performance. The results indicate that transferred images match closely to real images and object detection has good performance on transferred images, especially when the object size is large. © 2020 COPYRIGHT SPIE.",10.1117/12.2566885,Conditional GAN; GAN; Image transfer; Object detection,,
Modelling of pollutants and particulate matter in air using auto-tuned deep recurrent networks,"Inapakurthi R.K., Miriyala S.S., Mitra K.",IFAC-PapersOnLine,2020.0,"Atmospheric pollutants and Particulate Matter of size less than 10µm (PM10) are becoming dominant in the atmosphere due to human activities and natural calamities. To address their associated problems on human health, the interactions between pollutants and PM10 have to be envisaged. Machine learning techniques like Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) were successfully employed in establishing the interactions between various factors at play. However, these techniques are denounced for following a heuristic approach for determining network hyper-parameters. We propose a novel evolutionary multiobjective optimization algorithm which can optimally determine the hyper-parameters in deep recurrent neural networks. We test the algorithm to build optimal RNNs and LSTMs for modelling and forecasting the pollutants and PM10 data generated in northern Taiwan region during the year 2015. A state-of-the-art network training algorithm, Truncated Back Propagation Through Time was used in our study and single variable regression was done for CO, NOx, SO2, and PM10. Except for SO2 with RNN, model developed with the proposed algorithm gave high R2 values. LSTM was found to be superior than RNN in all the cases with R2 going as high as 0.9584 for PM10, while that attained by RNN is 0.93. © 2020, IFAC (International Federation of Automatic Control) Hosting by Elsevier Ltd. All rights reserved.",10.1016/j.ifacol.2020.06.089,Air quality monitoring; Auto tuning of deep neural networks; Hyper-parameter optimization; Machine learning; Pareto; Particulate matter,,
Random forests followed by computed abc analysis as a feature selection method for machine learning in biomedical data,"Lötsch J., Ultsch A.","Studies in Classification, Data Analysis, and Knowledge Organization",2020.0,"Background: Data from biomedical measurements usually include many parameters (variables/features). To reduce efforts of data acquisition or to enhance comprehension, a feature selection method is proposed that combines the ranking of the relative importance of each parameter in random forests classifiers with an item categorization provided by computed ABC analysis. Data: The input data space, comprising an example subset of plasma concentrations of d = 23 different lipid markers of various classes, acquired in Parkinson patients and healthy subjects (n = 100 each). Methods: Random forest classifiers were constructed with various different scenarios of the number of trees and the number of features in each tree. The relative importance of each feature calculated by the classifier was submitted to computed ABC analysis, a categorization technique for skewed distributions to identify the most important feature subset “A,” i.e., a reduced-set containing the important few items. Results: Using different parameters for the algorithms, the classification performance of all reduced-set random forest classifiers was almost as good as that of a random forest classifier using the full set of d = 23 lipid markers; all reaching 95% or better classification accuracy. When including additional “nonsense” features consisting of concentration data permutated across the subject groups, these features were never found in the ABC set “A.” The obtained features sets provided better classifiers than those obtained using classical regression methods. Conclusions: Random forests plus computed ABC analysis provided a feature selection without the necessity to predefine the number of features. A substantial reduction of the number of features, following the “80/20 rule,” was obtained. The classifiers using the A-class performed better than with a regression-based feature selection and were (nearly) as good as using the complete feature set. The obtained small feature sets are also well suited for domain experts’ interpretation. © Springer Nature Singapore Pte Ltd 2020.",10.1007/978-981-15-3311-2_5,Feature selection; Item categorization techniques; Machine learning,4.0,
A novel test case prioritization method based on problems of numerical software code statement defect prediction [Nowatorska metoda priorytetyzacji przypadków testowych oparta na prognozowaniu błędów instrukcji kodu oprogramowania numerycznego],"Shao Y., Liu B., Wang S., Xiao P.",Eksploatacja i Niezawodnosc,2020.0,"Test case prioritization (TCP) has been considerably utilized to arrange the implementation order of test cases, which contributes to improve the efficiency and resource allocation of software regression testing. Traditional coverage-based TCP techniques, such as statement-level, method/function-level and class-level, only leverages program code coverage to prioritize test cases without considering the probable distribution of defects. However, software defect data tends to be imbalanced following Pareto principle. Instinctively, the more vulnerable the code covered by the test case is, the higher the priority it is. Besides, statement-level coverage is a more fine-grained method than function-level coverage or class-level coverage, which can more accurately formulate test strategies. Therefore, we present a test case prioritization approach based on statement software defect prediction to tame the limitations of current coverage-based techniques in this paper. Statement metrics in the source code are extracted and data pre-processing is implemented to train the defect predictor. And then the defect detection rate of test cases is calculated by combining the prioritization strategy and prediction results. Finally, the prioritization performance is evaluated in terms of average percentage faults detected in four open source datasets. We comprehensively compare the performance of the proposed method under different prioritization strategies and predictors. The experimental results show it is a promising technique to improve the prevailing coverage-based TCP methods by incorporating statement-level defect-proneness. Moreover, it is also concluded that the performance of the additional strategy is better than that of max and total, and the choice of the defect predictor affects the efficiency of the strategy. © 2020, Polish Academy of Sciences Branch Lublin. All rights reserved.",10.17531/ein.2020.3.4,Code statement metrics; Machine learning; Software defect prediction; Software testing; Test case prioritization,1.0,
Combining Individual and Joint Networking Behavior for Intelligent IoT Analytics,"Jeyakumar J.V., Cherkasova L., Lajevardi S., Allan M., Zhao Y., Fry J., Srivastava M.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020.0,"The IoT vision of a trillion connected devices over the next decade requires reliable end-to-end connectivity and automated device management platforms. While we have seen successful efforts for maintaining small IoT testbeds, there are multiple challenges for the efficient management of large-scale device deployments. With Industrial IoT, incorporating millions of devices, traditional management methods do not scale well. In this work, we address these challenges by designing a set of novel machine learning techniques, which form a foundation of a new tool, IoTelligent, for IoT device management, using traffic characteristics obtained at the network level. The design of our tool is driven by the analysis of 1-year long networking data, collected from 350 companies with IoT deployments. The exploratory analysis of this data reveals that IoT environments follow the famous Pareto principle, such as: (i) 10% of the companies in the dataset contribute to 90% of the entire traffic; (ii) 7% of all the companies in the set own 90% of all the devices. We designed and evaluated CNN, LSTM, and Convolutional LSTM models for demand forecasting, with a conclusion of the Convolutional LSTM model being the best. However, maintaining and updating individual company models is expensive. In this work, we design a novel, scalable approach, where a general demand forecasting model is built using the combined data of all the companies with a normalization factor. Moreover, we introduce a novel technique for device management, based on autoencoders. They automatically extract relevant device features to identify device groups with similar behavior to flag anomalous devices. © 2020, Springer Nature Switzerland AG.",10.1007/978-3-030-59615-6_4,Deep learning; Device management; Forecasting,,
Multidisciplinary design and control optimization of a spherical robot for planetary exploration,"Kalita H., Thangavelautham J.",AIAA Scitech 2020 Forum,2020.0,"Missions targeting extreme and rugged environments such as caves, canyons, cliffs and crater rims of the Moon, Mars and icy moons are the next frontiers in solar system exploration. Exploring these sites will help ascertain the range of conditions that can support life and identify planetary processes that are responsible for generating and sustaining habitable worlds. Current landers and rovers are unable to access these areas of high interest due to limitations in precision landing techniques, need for large and sophisticated science instruments and a mission assurance and operations culture where risks are minimized at all costs. This research proposes using multiple spherical robots called SphereX for exploring these extreme environments. The design of SphereX is a complex task that involves a large number of design variables and multiple engineering disciplines. The methodology developed in this work uses Automated Multidisciplinary Design and Control Optimization (AMDCO) techniques to find near optimal design solutions in terms of mass, volume, power and control for SphereX for different mission scenarios. The implementation of AMDCO for SphereX design is a complex process because of complexity of modelling and implementation, discontinuities in the design space, and wide range of time scales and exploration objectives. We address these issues by using machine learning in the form of Evolutionary Algorithms integrated with gradient-based optimization techniques to search through the design space and find pareto optimal solutions for a given mission task. The design space is searched using a GA multi-objective optimizer at the system (global) level to find the Pareto-optimal results while gradient-based techniques are used to search at the discipline (local) level. The modeled disciplines are mobility system, power system, thermal system, shielding, communication system, avionics and shell. Using this technology, it is now possible to perform end to end automated preliminary design of planetary robots for surface exploration. © 2020 American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.",10.2514/6.2020-0065,,,
Learning Specifications for Labelled Patterns,"Basset N., Dang T., Mambakam A., Jarabo J.I.R.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020.0,"In this work, we introduce a supervised learning framework for inferring temporal logic specifications from labelled patterns in signals, so that the formulae can then be used to correctly detect the same patterns in unlabelled samples. The input patterns that are fed to the training process are labelled by a Boolean signal that captures their occurrences. To express the patterns with quantitative features, we use parametric specifications that are increasing, which we call Increasing Parametric Pattern Predictor (IPPP). This means that augmenting the value of the parameters makes the predicted pattern true on a larger set. A particular class of parametric specification formalisms that we use is Parametric Signal Temporal Logic (PSTL). One of the main contributions of this paper is the definition of a new measure, called count, to assess the quality of the learned formula. This measure enables us to compare two Boolean signals and, hence, quantifies how much the labelling signal induced by the formula differs from the true labelling signal (e.g. given by an expert). Therefore, the count can measure the number of mismatches (either false positives or false negatives) up to some error tolerance. Our supervised learning framework can be expressed by a multicriteria optimization problem with two objective functions: the minimization of false positives and false negatives given by the parametric formula on a signal. We provide an algorithm to solve this multi-criteria optimization problem. Our approach is demonstrated on two case studies involving characterization and classification of labeled ECG (electrocardiogram) data. © 2020, Springer Nature Switzerland AG.",10.1007/978-3-030-57628-8_5,Monotonic specification learning; Pareto multi-criteria optimization; Signal pattern matching; Signal Temporal Logic,,
Reliability-based design optimization of a door beam for fmvss214 requirements,"Swaroop K., Kakodkar N., Tripathy B.",Lecture Notes in Mechanical Engineering,2020.0,"Door beams are the principal components used in vehicle doors to satisfy FMVSS214 side impact load case and meet associated performance criteria. The beams are typically designed with multiple tailor rolled blanks (TRB) to minimize mass. This poses challenge to the designer to optimize the section sizes as well as thicknesses of different regions while meeting various performance requirements. One such requirement for the door beam is not to undergo buckling at places other than ram engagement location (called end buckling) in addition to other requirements such as adequate load resistance. Machine learning-based techniques can be used to effectively predict subjective performances like end buckling. Physical properties of the door beam such as thickness and material property can vary around the mean resulting in variations in performance. This calls for reliability-based design optimization (RBDO)—designs which would meet the required reliability in performance when the input variables undergo random variations. This piece of work combines the machine learning methods used to predict performance along with reliability analysis tools to develop a RBDO framework which can predict reliability of a design and can also come up with a design to meet required reliability figures. The framework developed is applied to a simplified door beam, and the results are presented. Different supervised machine learning models are investigated for predicting peak load, average load, mass and end buckling. These techniques are integrated with particle swarm optimization (PSO) technique to perform design optimization. A reliability analysis is done using a Monte Carlo Simulation within the PSO. The RBDO framework can develop a Pareto front of door beam mass as a function of reliability, which can help the designer to select a door beam to meet required performance with least mass. © Springer Nature Singapore Pte Ltd 2020.",10.1007/978-981-15-5432-2_13,Meta-heuristic optimization; RBDO; Supervised learning; Vehicle safety,,
Feature Selection Using PSO: A Multi Objective Approach,"Vashishtha J., Puri V.H., Mukesh",Communications in Computer and Information Science,2020.0,"Feature selection is a pre-processing technique in which a subset or a small number of features, which are relevant and non-redundant, are selected for better classification performance. Multi-objective optimization is applied in the fields where finest decisions need to be taken in presence of trade-offs between two or more differing objectives. Therefore, feature selection is considered as a multi-objective problem with conflicting measures like classification error rate and feature reduction rate. The existing algorithms, Non-dominated Sorting based particle swarm optimization for Feature Selection (NSPSOFS) and Crowding Mutation Dominance based particle swarm optimization for Feature Selection (CMDPSOFS) are the two multi-objective PSO algorithms for feature selection. This work presents the enhanced form of NSPSOFS and CMDPSOFS. A novel selection mechanism for gbest is incorporated and hybrid mutation is also added to the algorithms in order to generate a better pareto optimal front of non-dominated solutions. The experimental results show that the proposed algorithm generates non-dominated solutions and produce better result than existing algorithms. © 2020, Springer Nature Singapore Pte Ltd.",10.1007/978-981-15-6318-8_10,Feature selection; Multi-objective optimization; PSO,,
Ensemble Learning via Multimodal Multiobjective Differential Evolution and Feature Selection,"Wang J., Wang B., Liang J., Yu K., Yue C., Ren X.",Communications in Computer and Information Science,2020.0,"Ensemble learning is an important element in machine learning. However, two essential tasks, including training base classifiers and finding a suitable ensemble balance for the diversity and accuracy of these base classifiers, are need to be achieved. In this paper, a novel ensemble method, which utilizes a multimodal multiobjective differential evolution (MMODE) algorithm to select feature subsets and optimize base classifiers parameters, is proposed. Moreover, three methods including minimum error ensemble, all Pareto sets ensemble, and error reduction ensemble are employed to construct ensemble classifiers for executing classification tasks. Experimental results on several benchmark classification databases evidence that the proposed algorithm is valid. © 2020, Springer Nature Singapore Pte Ltd.",10.1007/978-981-15-3425-6_34,Classifier parameter; Ensemble learning; Feature selection; Multimodal multiobjective optimization,,
Inline Part Average Testing (I-PAT) for automotive die reliability,"Robinson J.C., Sherman K., Price D.W., Rathert J.",Proceedings of SPIE - The International Society for Optical Engineering,2020.0,"Semiconductor reliability in applications such as automotive is getting increased attention as design rules shrink to include 1Xnm, semiconductor content per vehicle continues to grow, applications become more critical and reliability requirements tighten. Current automotive requirements stipulate less than one defective part per million (DPPM). Approaches to address reliability include improving design, manufacturing and test. Process control in manufacturing is critical for reliability and includes continuous improvement for reducing process tool defectivity, excursion monitoring of process tools and product lines, golden or best performing tool methods [1], measurement system analysis (MSA) methods and screening. Inline defectivity is known to have an impact on both yield and reliability [2], and defects can impact reliability in one of two ways. Killer defects located in areas that are untested can result in so called Zero-Kilometer failures. In other cases, the same types of defects that cause yield loss can also cause latent reliability failures-the difference being size, location and density. Latent reliability defects become activated after test and can include defect types such as partial bridges, partial opens, and embedded particles. Current reliability engineering relies on outlier detection rules like parametric part average testing (P-PAT) [3], or geographic part average testing (G-PAT), both of which are derived from end-of-line screening data, which is based solely on electrical test data [4]. Inline Part Average Testing (I-PAT™) is enabled by multi-channel high-speed LED scanning inspection technology and offers an opportunity to apply fab data to reliability engineering. Defect inspection results are analyzed with machine learning (ML) to weigh the defectivity and create a die-level defectivity metric allowing the statistical identification of die which are a high reliability risk [5, 6]. Two case studies are described. The first case is a feasibility study based on historical fab defectivity data and includes a sample of ∼250,000 die, with eight inline defect inspections per wafer, including four front end of line (FEOL) and four back end of line (BEOL), on a high sensitivity broadband inspection system [7, 8]. Each defect is assigned a weight based on its impact to various ""ground truth"" indicators. The combined impact of all defects in a given die stacked across all inspections is aggregated into a die-level metric. Plotting the die-level I-PAT metrics for all the die as a Pareto chart allows outliers to be identified using accepted statistical methods [9]. I-PAT metrics can then be correlated to electrical wafer sort (EWS) yield or fallout rate, specific wafer-sort bins, EWS parametric test performance and post burn-in electrical test. Of key importance is that wafer test was not used to train the I-PAT model, and therefore this method is an independent validation of latent reliability. The second case study focuses on production screening feasibility with multi-channel high-speed LED scanning, and addresses overkill, or the over inking of potentially good die based on inline defectivity, which is a critical challenge that must be overcome for production implementation [10]. Using inspection enabled by high speed LED scanning technology, die screening is a critical component of a comprehensive automotive Zero Defect program. Applications include early detection of fab excursions, feedback for continuous improvement of inline defectivity, feedforward to optimize electrical test methods and screening of die containing possible latent reliability defects. The I-PAT methodology can be used to enhance standard end-of-line outlier detection rules such as P-PAT [3], which is based solely on parametric testing. © 2020 SPIE.",10.1117/12.2551539,I-PAT; Inline Inspection; Part Average Testing (PAT); Reliability; Screening,1.0,
Correcting nuisance variation using Wasserstein distance,"Tabak G., Fan M., Yang S., Hoyer S., Davis G.",PeerJ,2020.0,"Profiling cellular phenotypes from microscopic imaging can provide meaningful biological information resulting from various factors affecting the cells. One motivating application is drug development: morphological cell features can be captured from images, from which similarities between different drug compounds applied at different doses can be quantified. The general approach is to find a function mapping the images to an embedding space of manageable dimensionality whose geometry captures relevant features of the input images. An important known issue for such methods is separating relevant biological signal from nuisance variation. For example, the embedding vectors tend to be more correlated for cells that were cultured and imaged during the same week than for those from different weeks, despite having identical drug compounds applied in both cases. In this case, the particular batch in which a set of experiments were conducted constitutes the domain of the data; an ideal set of image embeddings should contain only the relevant biological information (e.g., drug effects). We develop a general framework for adjusting the image embeddings in order to ""forget"" domain-specific information while preserving relevant biological information. To achieve this, we minimize a loss function based on distances between marginal distributions (such as the Wasserstein distance) of embeddings across domains for each replicated treatment. For the dataset we present results with, the only replicated treatment happens to be the negative control treatment, for which we do not expect any treatment-induced cell morphology changes. We find that for our transformed embeddings (i) the underlying geometric structure is not only preserved but the embeddings also carry improved biological signal; and (ii) less domain-specific information is present. Copyright © 2020 Tabak et al.",10.7717/peerj.8594,Batch effect; Cellular phenotyping; Domain adaptation; Embedding; Minimax; Optimal transport; Wasserstein distance,2.0,
Burden Surface Decision Using MODE with TOPSIS in Blast Furnace Ironmkaing,"Li Y., Li H., Zhang J., Zhang S., Yin Y.",IEEE Access,2020.0,"Burden surface distribution plays a key role in achieving an energy-efficient status of blast furnace (BF). However, actual adjustment of burden surface usually depends on the operator's experience when the production status changes. Meanwhile, due to the characteristics of high dimension, strong coupling, and distributed parameters, it is difficult to establish the accurate mechanism model for BF ironmaking process. Considering the aforementioned issues, this paper proposes an integrated multi-objective optimization framework for optimizing burden surface distribution based on the analysis of BF operation characteristics. Firstly, data-driven models are constructed for two objectives, i.e., gas utilization ratio (GUR) and coke ratio (CR), and two constraints using adaptive particle swarm optimization (APSO) based extreme learning machine (ELM), named APSO-ELM. Multi-objective optimization is subsequently carried out between GUR and CR using the multi-objective differential evolution algorithm (MODE) to generate the Pareto optimal solutions. Finally, TOPSIS is applied to select a best compromise solution among the Pareto optimal solutions for this optimization problem. Comprehensive experiments are presented to illustrate the performance of the proposed integrated multi-objective optimization framework. The experimental results demonstrate that the proposed framework can give a reasonable burden surface profile according to the production status changes to guarantee the BF operation more efficient and stable. © 2013 IEEE.",10.1109/ACCESS.2020.2974882,Blast furnace; burden surface optimization; extreme learning machine; MODE; multi-objective optimization; TOPSIS,6.0,
Machine Learning Based Heuristic Technique for Multi-response Machining Process,"Ghosh T., Martinsen K.",Lecture Notes in Mechanical Engineering,2020.0,"Manufacturing process variables influence the quality of products substantially. It is unquestionably difficult to model the manufacturing processes that include a large number of variables and responses. Development of the multi-objective surrogate models for the manufacturing processes could be computationally and economically expensive. In this article, a generic multi-objective surrogate-coupled heuristic algorithm is employed that needs small amount of experimental data as input, and predicts precise responses with quick Pareto solutions. The proposed algorithm is verified with different cases collected from the literature based on the CNC turning, centerless cylindrical grinding, and micro milling machining and shown to produce some interesting results. © Springer Nature Switzerland AG 2020.",10.1007/978-3-030-37566-9_3,Heuristic algorithm; Manufacturing process optimization; Multi-objective optimization; Surrogate models,,
An improved ensemble approach for effective intrusion detection,Kumar G.,Journal of Supercomputing,2020.0,"Nowadays, one critical challenge of cybersecurity administrators is the protection of online resources from network intrusions. Despite several academic and industry research initiatives, full protection of online resources from these network intrusions is not feasible. Therefore, several techniques have been developed that use network audit data for accurate detection of network intrusions effectively and efficiently and are used in network intrusion detection systems (NIDSs). But, most of NIDSs reported low detection accuracy with high false alarm rate and provide a single solution that lacks in classification trade-offs. In this paper, the authors present a hybrid approach of multi-objective genetic algorithm and neural networks for creating a set of ensemble solutions for detecting network intrusions effectively. The proposed approach works in two phases that initially creates a set of non-dominating solutions or Pareto optimal solutions of base techniques and then creates ensemble solutions. In the outcome of individual solutions or models in the ensemble are aggregated using most popular method of majority voting. The proposed hybrid approach is evaluated using benchmark datasets of NSL_KDD and ISCX-2012 datasets for intrusion detection. The evaluation results using benchmark datasets demonstrate that the proposed hybrid approach enables detecting network intrusions effectively as compared to conventional ensemble approaches, namely bagging and boosting. The resultant ensemble solutions are non-dominating and provide classification trade-offs for cybersecurity administrators. The results also show that the proposed hybrid approach detects both minority and majority intrusion types accurately. The proposed hybrid approach demonstrated a detection accuracy of 97% and 88% with FPR of 2.4% and 2% for ISCX-2012 and NSL_KDD datasets, respectively. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",10.1007/s11227-019-03035-w,Genetic algorithm; Intrusion; Intrusion detection system; Machine learning; MOGA; Neural networks,8.0,
Multi-Objective-Based Radiomic Feature Selection for Lesion Malignancy Classification,"Zhou Z., Li S., Qin G., Folkert M., Jiang S., Wang J.",IEEE Journal of Biomedical and Health Informatics,2020.0,"Objective: accurately classifying the malignancy of lesions detected in a screening scan is critical for reducing false positives. Radiomics holds great potential to differentiate malignant from benign tumors by extracting and analyzing a large number of quantitative image features. Since not all radiomic features contribute to an effective classifying model, selecting an optimal feature subset is critical. Methods: this work proposes a new multi-objective based feature selection (MO-FS) algorithm that considers sensitivity and specificity simultaneously as the objective functions during feature selection. For MO-FS, we developed a modified entropy-based termination criterion that stops the algorithm automatically rather than relying on a preset number of generations. We also designed a solution selection methodology for multi-objective learning that uses the evidential reasoning approach (SMOLER) to automatically select the optimal solution from the Pareto-optimal set. Furthermore, we developed an adaptive mutation operation to generate the mutation probability in MO-FS automatically. Results: we evaluated the MO-FS for classifying lung nodule malignancy in low-dose CT and breast lesion malignancy in digital breast tomosynthesis. Conclusion: the experimental results demonstrated that the feature set selected by MO-FS achieved better classification performance than features selected by other commonly used methods. Significance: the proposed method is general and more effective radiomic feature selection strategy. © 2013 IEEE.",10.1109/JBHI.2019.2902298,evidential reasoning; feature selection; lesion malignancy classification; multi-objective evolutionary algorithm; Radiomics,18.0,
Signatures of criticality in mining accidents and recurrent neural network forecasting model,"Doss K., Hanshew A.S., Mauro J.C.",Physica A: Statistical Mechanics and its Applications,2020.0,"We report signatures of criticality in mining accident data obtained from the Mine Accident, Injury and Illness Report form (MSHA Form 7000-1). This work builds on the hypothesis that workplace accident statistics follow self-organized criticality (Mauro et al., 2018). “1/f noise,” a distinct feature of critical systems, is extracted from this database and is used to forecast accident trends using a long short-term memory (LSTM) recurrent neural network (RNN). The algorithm used for extracting this noise is applicable to data available in any standard worker's compensation database. We also report a Pareto distribution in the number of accidents in relation to employee mine experience, implying a strong correlation between experience and susceptibility to accidents. © 2019 Elsevier B.V.",10.1016/j.physa.2019.122656,Machine learning; Mining safety; Self-organized criticality; Time-series forecasting,3.0,
Multi-objective Solution of Traveling Salesman Problem with Time,Hameed I.A.,Advances in Intelligent Systems and Computing,2020.0,"The traveling salesman problem (TSP) is a challenging problem in combinatorial optimization. No general method of solution is known, and the problem is NP-hard. In this paper, we consider the multi-objective TSP which encompasses the optimization of two conflicting and competing objectives: here the dual minimization of the total travel distance and total travel time at various traffic flow conditions. It is well known that travellers can experience extra travel time during peak hours (i.e., congestion conditions) compared to free flow conditions (i.e., un-congested conditions), therefore and under some conditions, minimizing traveled time could conflict and compete with travel distance and vice versa. This problem has been studied in the form of a single objective problem, where either the two objectives have been combined in a single objective function or one of the objectives has been treated as a constraint. The purpose of this paper is to find a set of non-dominated solutions (i.e., the sequence of cities) using the notion of Pareto optimality where none of the objective functions can be improved in value without degrading one or more of the other objective values. The traveller then has the chance to choose a solution that fits his/her needs at each congestion level. In this paper, a multi-objective genetic algorithm (MOGA) for searching for efficient solutions is investigated. Here, an initial population composed of an approximation to the extreme supported efficient solutions is generated. A Pareto local search is then applied to all solutions of the initial population. The method is applied to a simulated problem and to a real-world problem where distances and real estimates of the travel duration for multiple origins and destinations for specific transport modes are obtained from Google Maps Platform using a Google Distance Matrix API. Results show that solving a TSP as a multi-objective optimization problem can provide more realistic solutions. The proposed approach can be used for recommending routes based on variable duration matrix and cost. © 2020, Springer Nature Switzerland AG.",10.1007/978-3-030-14118-9_13,Genetic algorithms; Optimization; TSP,5.0,
Weighted incremental minimax probability machine-based method for quality prediction in gasoline blending process,"He K., Zhong M., Du W.",Chemometrics and Intelligent Laboratory Systems,2020.0,"Near-infrared (NIR) spectroscopy is frequently used to predict quality-relevant variables that are difficult to measure online. This technology can be applied by developing the NIR model in advance. Obtaining a high-accuracy NIR model is difficult using traditional modeling methods because process data inherently contain uncertainties and present strong non-Gaussian characteristics. Considering the difficulty in obtaining precise prediction results, biased estimation is important in producing qualified products when NIR spectroscopy is used in a feedback quality control system. The present work proposes a biased estimation model based on probabilistic representation to address the aforementioned issues. Additionally, a novel weighted incremental strategy with “just-in-time” learning is proposed to improve model adaptiveness. In this way, the NIR model could be established and maintained without imposing any distribution hypothesis on process data, and biased estimation could be obtained in the form of probability. The performance of the proposed method is demonstrated on an actual data set from a gasoline blending process. © 2019 Elsevier B.V.",10.1016/j.chemolab.2019.103909,Biased estimation; Gasoline blending; Minimax probability machine; Near-infrared spectroscopy; Non-Gaussian,5.0,
Efficient Multiagent Policy Optimization Based on Weighted Estimators in Stochastic Cooperative Environments,"Zheng Y., Hao J.-Y., Zhang Z.-Z., Meng Z.-P., Hao X.-T.",Journal of Computer Science and Technology,2020.0,"Multiagent deep reinforcement learning (MA-DRL) has received increasingly wide attention. Most of the existing MA-DRL algorithms, however, are still inefficient when faced with the non-stationarity due to agents changing behavior consistently in stochastic environments. This paper extends the weighted double estimator to multiagent domains and proposes an MA-DRL framework, named Weighted Double Deep Q-Network (WDDQN). By leveraging the weighted double estimator and the deep neural network, WDDQN can not only reduce the bias effectively but also handle scenarios with raw visual inputs. To achieve efficient cooperation in multiagent domains, we introduce a lenient reward network and scheduled replay strategy. Empirical results show that WDDQN outperforms an existing DRL algorithm (double DQN) and an MA-DRL algorithm (lenient Q-learning) regarding the averaged reward and the convergence speed and is more likely to converge to the Pareto-optimal Nash equilibrium in stochastic cooperative environments. © 2020, Institute of Computing Technology, Chinese Academy of Sciences.",10.1007/s11390-020-9967-6,cooperative Markov game; deep reinforcement learning; lenient reinforcement learning; multiagent system; weighted double estimator,6.0,
Intelligent Navigation System for the Visually Impaired - A Deep Learning Approach,"Yadav D.K., Mookherji S., Gomes J., Patil S.","Proceedings of the 4th International Conference on Computing Methodologies and Communication, ICCMC 2020",2020.0,"Visually impaired individuals have been gradually claiming a significant stake in the population demographics. The proposed autonomous device aims to provide a holistic solution by engineering a smart navigation system that relentlessly scans the environment, detects and classifies neighboring objects using a 4 layered Convolutional Neural Network (CNN) that has been trained on a data set containing 2513 permutations of various images of household objects that an individual may encounter in daily life. The CNN follows the 80-20 rule for testing and training the self-learning model enabling it to learn recursively from the error rate. The proposed system then calculates distances of neighboring objects from the user and provides adaptive solutions in real time to manoeuvre the user to safety by providing auditory input in a simplistic manner which considers 10-24 frames per second while drafting the kinematic response for the user. The device has achieved an unprecedented success rate of serving within a response time of less than 50 ms. The accuracy of the CNN algorithm being at 94.6%, also sets a distinguished benchmark as an object detection algorithm thereby contributing to the success in simulations of the proposed device in a constrained environment. © 2020 IEEE.",10.1109/ICCMC48092.2020.ICCMC-000121,Convolutional Neural Networks; Deep Learning; Image Processing Algorithm; Image Scanning; Intelligent System; Internet of Things; Navigation System; Object Detection; Wireless Sensors,1.0,
Sparse feature learning of hyperspectral imagery via multiobjective-based extreme learning machine,"Fang X., Cai Y., Cai Z., Jiang X., Chen Z.",Sensors (Switzerland),2020.0,"Hyperspectral image (HSI) consists of hundreds of narrow spectral band components with rich spectral and spatial information. Extreme Learning Machine (ELM) has been widely used for HSI analysis. However, the classical ELM is difficult to use for sparse feature leaning due to its randomly generated hidden layer. In this paper, we propose a novel unsupervised sparse feature learning approach, called Evolutionary Multiobjective-based ELM (EMO-ELM), and apply it to HSI feature extraction. Specifically, we represent the task of constructing the ELM Autoencoder (ELM-AE) as a multiobjective optimization problem that takes the sparsity of hidden layer outputs and the reconstruction error as two conflicting objectives. Then, we adopt an Evolutionary Multiobjective Optimization (EMO) method to solve the two objectives, simultaneously. To find the best solution from the Pareto solution set and construct the best trade-off feature extractor, a curvature-based method is proposed to focus on the knee area of the Pareto solutions. Benefited from the EMO, the proposed EMO-ELM is less prone to fall into a local minimum and has fewer trainable parameters than gradient-based AEs. Experiments on two real HSIs demonstrate that the features learned by EMO-ELM not only preserve better sparsity but also achieve superior separability than many existing feature learning methods. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.",10.3390/s20051262,Autoencoder; Evolutionary multiobjective optimization; Extreme learning machine autoencoder; Hyperspectral imagery; Sparse feature learning,5.0,
Incorporating human and learned domain knowledge into training deep neural networks: A differentiable dose-volume histogram and adversarial inspired framework for generating Pareto optimal dose distributions in radiation therapy,"Nguyen D., McBeth R., Sadeghnejad Barkousaraie A., Bohara G., Shen C., Jia X., Jiang S.",Medical Physics,2020.0,"Purpose: We propose a novel domain-specific loss, which is a differentiable loss function based on the dose-volume histogram (DVH), and combine it with an adversarial loss for the training of deep neural networks. In this study, we trained a neural network for generating Pareto optimal dose distributions, and evaluate the effects of the domain-specific loss on the model performance. Methods: In this study, three loss functions — mean squared error (MSE) loss, DVH loss, and adversarial (ADV) loss — were used to train and compare four instances of the neural network model: (a) MSE, (b) MSE + ADV, (c) MSE + DVH, and (d) MSE + DVH+ADV. The data for 70 prostate patients, including the planning target volume (PTV), and the organs at risk (OAR) were acquired as 96 × 96 × 24 dimension arrays at 5 mm3 voxel size. The dose influence arrays were calculated for 70 prostate patients, using a 7 equidistant coplanar beam setup. Using a scalarized multicriteria optimization for intensity-modulated radiation therapy, 1200 Pareto surface plans per patient were generated by pseudo-randomizing the PTV and OAR tradeoff weights. With 70 patients, the total number of plans generated was 84 000 plans. We divided the data into 54 training, 6 validation, and 10 testing patients. Each model was trained for a total of 100,000 iterations, with a batch size of 2. All models used the Adam optimizer, with a learning rate of 1 × 10−3. Results: Training for 100 000 iterations took 1.5 days (MSE), 3.5 days (MSE+ADV), 2.3 days (MSE+DVH), and 3.8 days (MSE+DVH+ADV). After training, the prediction time of each model is 0.052 s. Quantitatively, the MSE+DVH+ADV model had the lowest prediction error of 0.038 (conformation), 0.026 (homogeneity), 0.298 (R50), 1.65% (D95), 2.14% (D98), and 2.43% (D99). The MSE model had the worst prediction error of 0.134 (conformation), 0.041 (homogeneity), 0.520 (R50), 3.91% (D95), 4.33% (D98), and 4.60% (D99). For both the mean dose PTV error and the max dose PTV, Body, Bladder and rectum error, the MSE+DVH+ADV outperformed all other models. Regardless of model, all predictions have an average mean and max dose error &lt;2.8% and 4.2%, respectively. Conclusion: The MSE+DVH+ADV model performed the best in these categories, illustrating the importance of both human and learned domain knowledge. Expert human domain-specific knowledge can be the largest driver in the performance improvement, and adversarial learning can be used to further capture nuanced attributes in the data. The real-time prediction capabilities allow for a physician to quickly navigate the tradeoff space for a patient, and produce a dose distribution as a tangible endpoint for the dosimetrist to use for planning. This is expected to considerably reduce the treatment planning time, allowing for clinicians to focus their efforts on the difficult and demanding cases. © 2019 American Association of Physicists in Medicine",10.1002/mp.13955,adversarial networks; deep learning; domain knowledge; dose volume histogram; intensity modulated radiation therapy; pareto optimality,19.0,
Designing non-linear minimax and related discriminants by disjoint tangent configurations applied to RBF networks,"Martínez-García J.-A., Sancho-Gómez J.-L., Sánchez-Morales A., Figueiras-Vidal A.R.",Neurocomputing,2020.0,"Non-linear classification machines seldom are trained under criteria that are usual and useful for linear discriminants, such as minimax, Fisher's, and other similar criteria. The reason is the learning difficulties that transformation-trainable machines suffer when applying such criteria. However, the possibility of using non-linear machines whose transformations are pre-designed merits attention. In this contribution, we propose and study an efficient and potentially effective option: Applying Disjoint Tangent Configurations (DTC), a formulation that includes discriminants such as Fisher's, Bayes for normal distributions, Minimax Probabilistic Decision Hyperplane (MPDH), and others, to the output of a Radial Basis Function (RBF) network which has been previously designed with a moderate number of nodes to reduce the computational load, but with a high quality centroid selection algorithm, Frequency Sensitive Competitive Learning (FSCL), which allows to obtain networks with high representation capabilities. Experiments demonstrate that this approach leads to good performance results with acceptable computational efforts. © 2019 Elsevier B.V.",10.1016/j.neucom.2019.12.016,Binary classification; Minimax; Non-linear discriminants; Radial basis functions; Single-hidden layer feedforward networks,,
Application of soft computing methods in predicting uniaxial compressive strength of the volcanic rocks with different weathering degree,"Ceryan N., Samui P.",Arabian Journal of Geosciences,2020.0,"Uniaxial compressive strength (UCS) of rock material is very important parameter for rock engineering applications such as rock mass classification, numerical modelling bearing capacity, mechanical excavation, slope stability and supporting with respect to the engineering behaviors’ of rock. UCS is obtained directly or can be predicted by different methods including using existing tables and diagrams, regression, Bayesian approach and soft computing methods. The main purpose of this study is to examine the applicability and capability of the Extreme Learning Machine (ELM), Minimax Probability Machine Regression (MPMR) for prediction of UCS of the volcanic rocks and to compare its performance with Least Square Support Vector Machine (LS-SVM). The samples tested were taken from the volcanic rock masses exposed at the eastern Pontides (NE Turkey). In the soft computing model to estimate UCS of the samples investigated, porosity and slake durability index were used as input parameters. In this study, the root mean square error (RMSE), variance account factor (VAF), maximum determination coefficient value (R2), adjusted determination coefficient (Adj. R2) and performance index (PI), regression error characteristic (REC) curve and Taylor diagram were used to determine the accuracy of the ELM, MPMR and LS-SVM models developed. © 2020, Saudi Society for Geosciences.",10.1007/s12517-020-5273-4,Extreme learning machine; Least square support vector machine; Minimax probability machine regression; Porosity; Slake durability index; Uniaxial compressive strength; Volcanic rock,13.0,
Sparse principal component analysis via axis-aligned random projections,"Gataric M., Wang T., Samworth R.J.",Journal of the Royal Statistical Society. Series B: Statistical Methodology,2020.0,"We introduce a new method for sparse principal component analysis, based on the aggregation of eigenvector information from carefully selected axis-aligned random projections of the sample covariance matrix. Unlike most alternative approaches, our algorithm is non-iterative, so it is not vulnerable to a bad choice of initialization. We provide theoretical guarantees under which our principal subspace estimator can attain the minimax optimal rate of convergence in polynomial time. In addition, our theory provides a more refined understanding of the statistical and computational trade-off in the problem of sparse principal component estimation, revealing a subtle interplay between the effective sample size and the number of random projections that are required to achieve the minimax optimal rate. Numerical studies provide further insight into the procedure and confirm its highly competitive finite sample performance. © 2020 The Authors Journal of the Royal Statistical Society: Series B (Statistical Methodology) Published by John Wiley & Sons Ltd on behalf of the Royal Statistical Society.",10.1111/rssb.12360,Dimensionality reduction; Eigenspace estimation; Ensemble learning; Sketching; Statistical and computational trade-offs,4.0,
Solving the ruin probabilities of some risk models with Legendre neural network algorithm,"Lu Y., Chen G., Yin Q., Sun H., Hou M.",Digital Signal Processing: A Review Journal,2020.0,"This paper studies a numerical method based on Legendre polynomials and extreme learning machine algorithm to solve the ruin probabilities in the classical risk model and the Erlang(2) risk model. In our method, the hidden layer is eliminated by expanding the input pattern using Legendre polynomials. The network parameters are obtained by solving a system of linear equations using extreme learning machine algorithm. The numerical experiments of some risk models under exponential distribution and Pareto distribution have been performed to validate the accuracy and reliability of our proposed Legendre neural network algorithm. Compared with the existing method, the results obtained by our proposed Legendre neural network model can achieve very high accuracy. Legendre neural network algorithm is well suited for solving the ruin probabilities of the risk models. © 2019 Elsevier Inc.",10.1016/j.dsp.2019.102634,Approximate solutions; Classical risk model; Erlang(2) risk model; Legendre neural network algorithm; Ruin probability,6.0,
Passive underwater target tracking: Conditionally minimax nonlinear filtering with bearing-doppler observations,"Borisov A., Bosov A., Miller B., Miller G.",Sensors (Switzerland),2020.0,"The paper presents an application of the Conditionally-Minimax Nonlinear Filtering (CMNF) algorithm to the online estimation of underwater vehicle movement given a combination of sonar and Doppler discrete-time noisy sensor observations. The proposed filter postulates recurrent “prediction–correction” form with some predefined basic prediction and correction terms, and then they are optimally fused. The CMNF estimates have the following advantageous features. First, the obtained estimates are unbiased. Second, the theoretical covariance matrix of CMNF errors meets the real values. Third, the CMNF algorithm gives a possibility to choose the preliminary observation transform, basic prediction, and correction functions in any specific case of the observation system to improve the estimate accuracy significantly. All the features of conditionally-minimax estimates are demonstrated by the regression example of random position estimate given the noisy bearing observations. The contribution of the paper is the numerical study of the CMNF algorithm applied to the underwater target tracking given bearing-only and bearing-Doppler observations. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.",10.3390/s20082257,Bearing-Doppler measurements; Bearing-only measurements; Conditionally minimax nonlinear filter; Machine learning; Nonlinear filtering; Port-starboard ambiguity; Underwater target tracking,4.0,
Multiple people tracking with articulation detection and stitching strategy,"Liu Y., Yin J., Yu D., Zhao S., Shen J.",Neurocomputing,2020.0,"Multiple people tracking in a monocular video of crowded scenes is a challenging problem, methods of which are mostly based on tracking-by-detection strategies. The result of detection preprocessing used by many tracking methods to avoid creating wrong targets, is likely to be contaminated when there are defective detections in datasets of benchmark. We propose an articulation-based detection selecting method to screen out detections unqualified for further processing. For the association part of tracking workflow, applying minimax operation can minimize the max intra-distance but results in discontinuous trajectories. We design a stitching strategy to link the tracklets created by minimax algorithm. The experimental results will demonstrate that the proposed method outperforms or is comparable to previous approaches. © 2019",10.1016/j.neucom.2019.12.037,Articulation detection; Multiple people tracking; Stitching strategy,6.0,
A bi-phased multi-objective genetic algorithm based classifier,"Dutta D., Sil J., Dutta P.",Expert Systems with Applications,2020.0,"This paper presents a novel Bi-Phased Multi-Objective Genetic Algorithm (BPMOGA) based classification method. It is a Learning Classifier System (LCS) designed for supervised learning tasks. Here we have used Genetic Algorithms (GAs) to discover optimal classifiers from data sets. The objective of the work is to find out a classifier or Complete Rule (CR) which comprises of several Class Specific Rules (CSRs). Phase-I of BPMOGA extracts optimized CSRs in IF−THEN form by following Michigan approach, without considering interaction among the rules. Phase-II of BPMOGA builds optimized CRs from CSRs by following Pittsburgh way. It combines the advantages of both approaches. Extracted CRs help to build CSRs for the next run of phase-I. Hence, phase-I and phase-II are cyclically related, which is one of the uniqueness of BPMOGA. With the help of twenty one benchmark data sets from the University of California at Irvine (UCI) machine learning repository we have compared performance of BPMOGA based classifier with fourteen GA and non-GA based classifiers. Statistical test shows that the performance of the proposed classifier is either superior or comparable to other classifiers. © 2019",10.1016/j.eswa.2019.113163,Classification rules mining; Elitist Multi-Objective Genetic Algorithm; Pareto approach; Statistical test,9.0,
A new steganography without embedding based on adversarial training,"Jiang W., Hu D., Yu C., Li M., Zhao Z.-Q.",ACM International Conference Proceeding Series,2020.0,"Steganography is an art to hide information in the carriers to prevent from being detected, while steganalysis is the opposite art to detect the presence of the hidden information. With the development of deep learning, several state-of-the-art steganography and steganalysis based on deep learning techniques have been proposed to improve hiding or detection capabilities. Generative Adversarial Networks (GANs) based steganography directly uses the minimax game between the generator and discriminator, to automatically generate steganography algorithms resisting being detected by powerful steganalysis. The steganography without embedding (SWE) based on GANs, where the generated cover images themselves are stego ones carrying secret information has shown its state-of-the-art steganography performance. However, SWE based on GANs has serious weaknesses, such as low information recovery accuracy, low steganography capacity and poor natural showing. To solve these problems, this paper proposes a new SWE based on adversarial training, with carefully designed generator, discriminator and extractor, as well as their loss functions and optimized training mode. The proposed method can achieve a very high information recovery accuracy (100% in some cases), and at the same time improve the steganography capacity and image quality. © 2020 ACM.",10.1145/3393527.3393564,Generative adversarial networks; Steganalysis; Steganography; Steganography without embedding,,
Imparting fairness to pre-trained biased representations,"Sadeghi B., Boddeti V.N.",IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops,2020.0,"Adversarial representation learning is a promising paradigm for obtaining data representations that are invariant to certain sensitive attributes while retaining the information necessary for predicting target attributes. Existing approaches solve this problem through iterative adversarial minimax optimization and lack theoretical guarantees. In this paper, we first study the ""linear"" form of this problem i.e., the setting where all the players are linear functions. We show that the resulting optimization problem is both non-convex and non-differentiable. We obtain an exact closed-form expression for its global optima through spectral learning. We then extend this solution and analysis to non-linear functions through kernel representation. Numerical experiments on UCI and CIFAR-100 datasets indicate that, (a) practically, our solution is ideal for ""imparting"" provable invariance to any biased pre-trained data representation, and (b) empirically, the trade-off between utility and invariance provided by our solution is comparable to iterative minimax optimization of existing deep neural network based approaches.Code is available at Human Analysis Lab. © 2020 IEEE.",10.1109/CVPRW50498.2020.00016,,3.0,
Multi-objective support vector regression reduces systematic error in moderate resolution maps of tree species abundance,"Legaard K., Simons-Legaard E., Weiskittel A.",Remote Sensing,2020.0,"When forest conditions are mapped from empirical models, uncertainty in remotely sensed predictor variables can cause the systematic overestimation of low values, underestimation of high values, and suppression of variability. This regression dilution or attenuation bias is a well-recognized problem in remote sensing applications, with few practical solutions. Attenuation is of particular concern for applications that are responsive to prediction patterns at the high end of observed data ranges, where systematic error is typically greatest. We addressed attenuation bias in models of tree species relative abundance (percent of total aboveground live biomass) based on multitemporal Landsat and topoclimatic predictor data. We developed a multi-objective support vector regression (MOSVR) algorithm that simultaneously minimizes total prediction error and systematic error caused by attenuation bias. Applied to 13 tree species in the Acadian Forest Region of the northeastern U.S., MOSVR performed well compared to other prediction methods including single-objective SVR (SOSVR) minimizing total error, Random Forest (RF), gradient nearest neighbor (GNN), and Random Forest nearest neighbor (RFNN) algorithms. SOSVR and RF yielded the lowest total prediction error but produced the greatest systematic error, consistent with strong attenuation bias. Underestimation at high relative abundance caused strong deviations between predicted patterns of species dominance/codominance and those observed at field plots. In contrast, GNN and RFNN produced dominance/codominance patterns that deviated little from observed patterns, but predicted species relative abundance with lower accuracy and substantial systematic error. MOSVR produced the least systematic error for all species with total error often comparable to SOSVR or RF. Predicted patterns of dominance/codominance matched observations well, though not quite as well as GNN or RFNN. Overall, MOSVR provides an effective machine learning approach to the reduction of systematic prediction error and should be fully generalizable to other remote sensing applications and prediction problems. © 2020 by the authors.",10.3390/rs12111739,Attenuation bias; Genetic algorithm; Multi-objective optimization; Pareto optimization; Regression; Regression dilution bias; Species abundance; Species distribution modeling; Species dominance; Support vector machines,3.0,
Deep open-set domain adaptation for cross-scene classification based on adversarial learning and pareto ranking,"Adayel R., Bazi Y., Alhichri H., Alajlan N.",Remote Sensing,2020.0,"Most of the existing domain adaptation (DA) methods proposed in the context of remote sensing imagery assume the presence of the same land-cover classes in the source and target domains. Yet, this assumption is not always realistic in practice as the target domain may contain additional classes unknown to the source leading to the so-called open set DA. Under this challenging setting, the problem turns to reducing the distribution discrepancy between the shared classes in both domains besides the detection of the unknown class samples in the target domain. To deal with the openset problem, we propose an approach based on adversarial learning and pareto-based ranking. In particular, the method leverages the distribution discrepancy between the source and target domains using min-max entropy optimization. During the alignment process, it identifies candidate samples of the unknown class from the target domain through a pareto-based ranking scheme that uses ambiguity criteria based on entropy and the distance to source class prototype. Promising results using two cross-domain datasets that consist of very high resolution and extremely high resolution images, show the effectiveness of the proposed method. © 2020 by the authors.",10.3390/rs12111716,Adversarial learning; Min-max entropy; Open-set domain adaptation; Pareto ranking; Scene classification,11.0,
Accurate energy and performance prediction for frequency-scaled GPU kernels,"Fan K., Cosenza B., Juurlink B.",Computation,2020.0,"Energy optimization is an increasingly important aspect of today's high-performance computing applications. In particular, dynamic voltage and frequency scaling (DVFS) has become a widely adopted solution to balance performance and energy consumption, and hardware vendors providemanagement libraries that allowthe programmer to change bothmemory and core frequencies manually to minimize energy consumption while maximizing performance. This article focuses on modeling the energy consumption and speedup of GPU applications while using different frequency configurations. The task is not straightforward, because of the large set of possible and uniformly distributed configurations and because of the multi-objective nature of the problem, which minimizes energy consumption and maximizes performance. This article proposes a machine learning-based method to predict the best core and memory frequency configurations on GPUs for an input OpenCL kernel. The method is based on two models for speedup and normalized energy predictions over the default frequency configuration. Those are later combined into a multi-objective approach that predicts a Pareto-set of frequency configurations. Results show that our approach is very accurate at predicting extema and the Pareto set, and finds frequency configurations that dominate the default configuration in either energy or performance. © 2020 by the authors.",10.3390/COMPUTATION8020037,Energy efficiency; Frequency scaling; GPU; Modeling,1.0,
Genetic programming-assisted multi-scale optimization for multi-objective dynamic performance of laminated composites: the advantage of more elementary-level analyses,"Kalita K., Mukhopadhyay T., Dey P., Haldar S.",Neural Computing and Applications,2020.0,"High-fidelity multi-scale design optimization of many real-life applications in structural engineering still remains largely intractable due to the computationally intensive nature of numerical solvers like finite element method. Thus, in this paper, an alternate route of metamodel-based design optimization methodology is proposed in multi-scale framework based on a symbolic regression implemented using genetic programming (GP) coupled with d-optimal design. This approach drastically cuts the computational costs by replacing the finite element module with appropriately constructed robust and efficient metamodels. Resulting models are compact, have good interpretability and assume a free-form expression capable of capturing the non-linearly, complexity and vastness of the design space. Two robust nature-inspired optimization algorithms, viz. multi-objective genetic algorithm and multi-objective particle swarm optimization, are used to generate Pareto optimal solutions for several test problems with varying complexity. TOPSIS, a multi-criteria decision-making approach, is then applied to choose the best alternative among the Pareto optimal sets. Finally, the applicability of GP in efficiently tackling multi-scale optimization problems of composites is investigated, where a real-life scenario is explored by varying fractions of pertinent engineering materials to bring about property changes in the final composite structure across two different scales. The study reveals that a microscale optimization leads to better optimized solutions, demonstrating the advantage of carrying out a multi-scale optimization without any additional computational burden. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",10.1007/s00521-019-04280-z,d-Optimal design; Genetic programming; Machine learning-based optimization; Multi-scale optimization; Robust composite structures; Symbolic regression,14.0,
Non-Asymptotic Analysis of Monte Carlo Tree Search,"Shah D., Xie Q., Xu Z.",SIGMETRICS Performance 2020 - Abstracts of the 2020 SIGMETRICS/Performance Joint International Conference on Measurement and Modeling of Computer Systems,2020.0,"In this work, we consider the popular tree-based search strategy within the framework of reinforcement learning, the Monte Carlo Tree Search (MCTS), in the context of infinite-horizon discounted cost Markov Decision Process (MDP) with deterministic transitions. While MCTS is believed to provide an approximate value function for a given state with enough simulations, cf. [Kocsis and Szepesvari 2006; Kocsis et al. 2006], the claimed proof of this property is incomplete. This is due to the fact that the variant of MCTS, the Upper Confidence Bound for Trees (UCT), analyzed in prior works utilizes ""logarithmic"" bonus term for balancing exploration and exploitation within the tree-based search, following the insights from stochastic multi-arm bandit (MAB) literature, cf. [Agrawal 1995; Auer et al. 2002]. In effect, such an approach assumes that the regret of the underlying recursively dependent non-stationary MABs concentrates around their mean exponentially in the number of steps, which is unlikely to hold as pointed out in [Audibert et al. 2009], even for stationary MABs. As the key contribution of this work, we establish polynomial concentration property of regret for a class of non-stationary multi-arm bandits. This in turn establishes that the MCTS with appropriate polynomial rather than logarithmic bonus term in UCB has the claimed property of [Kocsis and Szepesvari 2006; Kocsis et al. 2006]. Interestingly enough, empirically successful approaches (cf. [Silver et al. 2017]) utilize a similar polynomial form of MCTS as suggested by our result. Using this as a building block, we argue that MCTS, combined with nearest neighbor supervised learning, acts as a ""policy improvement"" operator, i.e., it iteratively improves value function approximation for all states, due to combining with supervised learning, despite evaluating at only finitely many states. In effect, we establish that to learn an ϵ-approximation of the value function for deterministic MDPs with respect to ĝ.,""∞ norm, MCTS combined with nearest neighbor requires a sample size scaling as Õ (ϵ-(d+4), where d is the dimension of the state space. This is nearly optimal due to a minimax lower bound of ĝ1/4ω (ϵ-(d+2) [Shah and Xie 2018], suggesting the strength of the variant of MCTS we propose here and our resulting analysis. © 2019 Owner/Author.",10.1145/3393691.3394202,monte carlo tree search; non stationary multi-arm bandit; reinforcement learning,4.0,
Multiobjective tree-structured parzen estimator for computationally expensive optimization problems,"Ozaki Y., Tanigaki Y., Watanabe S., Onishi M.",GECCO 2020 - Proceedings of the 2020 Genetic and Evolutionary Computation Conference,2020.0,"Practitioners often encounter computationally expensive multiobjective optimization problems to be solved in a variety of real-world applications. On the purpose of challenging these problems, we propose a new surrogate-based multiobjective optimization algorithm that does not require a large evaluation budget. It is called Multiobjective Tree-structured Parzen Estimator (MOTPE) and is an extension of the tree-structured Parzen estimator widely used to solve expensive single-objective optimization problems. Our empirical evidences reveal that MOTPE can approximate Pareto fronts of many benchmark problems better than existing methods with a limited budget. In this paper, we discuss furthermore the influence of MOTPE configurations to understand its behavior. © 2020 Owner/Author.",10.1145/3377930.3389817,Bayesian optimization; Computationally expensive optimization; Infill criteria; Machine learning; Multiobjective optimization; Surrogate modeling; Tree-structured parzen estimator,8.0,
GeneCAI: <u>gene</u>tic evolution for acquiring <u>c</u>ompact <u>AI</u>,"Javaheripi M., Samragh M., Javidi T., Koushanfar F.",GECCO 2020 - Proceedings of the 2020 Genetic and Evolutionary Computation Conference,2020.0,"In the contemporary big data realm, Deep Neural Networks (DNNs) are evolving towards more complex architectures to achieve higher inference accuracy. Model compression techniques can be leveraged to efficiently deploy these compute-intensive architectures on resource-limited mobile devices. Such methods comprise various hyperparameters that require per-layer customization to ensure high accuracy. Choosing the hyperparameters is cumbersome as the pertinent search space grows exponentially with model layers. This paper introduces GeneCAI, a novel optimization method that automatically learns how to tune per-layer compression hyperparameters. We devise a bijective translation scheme that encodes compressed DNNs to the genotype space. Each genotype's optimality is measured using a multi-objective score based on the accuracy and number of floating-point operations. We develop customized genetic operations to iteratively evolve the non-dominated solutions towards the optimal Pareto front, thus, capturing the optimal trade-off between model accuracy and complexity. GeneCAI optimization method is highly scalable and can achieve a near-linear performance boost on distributed multi-GPU platforms. Our extensive evaluations demonstrate that GeneCAI outperforms existing rule-based and reinforcement learning methods in DNN compression by finding models that lie on a better accuracy/complexity Pareto curve. © 2020 ACM.",10.1145/3377930.3390226,Computer aided/automated design; Deep learning; Genetic algorithms; Multi-objective optimization; Parallel optimization,3.0,
Finding EFL and EQL Allocations of Indivisible Goods,"Huang W., Huang W., Cai D.","Proceedings - 2020 International Conference on Computer Vision, Image and Deep Learning, CVIDL 2020",2020.0,"Fair resource allocation has become an emerging research topic in Computer Science and Artificial Intelligence. We can judge whether the allocation is 'fair' from two aspects. First, if each agent prefers his own set of bundle, that is, envy-free, then we say that the allocation is fair; If each agent's valuation of his own item set is equal to other agents' valuation of its own item-equitability (EQ), then we also say that this allocation is fair. Solving the problem of envy-free (EF) or equitability (EQ) fair allocation is proved to be NP-hard, so this paper mainly studies the problem of approximate fair allocation i.e., approximate envy-free and approximate equitability. Our contribution are as follows. 1. We proved that equitable up to one less-preferred good (EQL) allocation always exists and can be found in polynomial time; 2. We proved that the allocation that satisfies both equitable up to one less-preferred good (EQL) and Pareto optimality (PO) exists and provides a pseudo-polynomial time algorithm that can find the allocation when the valuation function is additive with strictly positive; 3. We proved that when the allocation with specific valuation that satisfies equitable up to one less-preferred good (EQL), envy-free up to one less-preferred good (EFL) and Pareto optimality (PO) exists then it can be found in polynomial time. © 2020 IEEE.",10.1109/CVIDL51233.2020.00-53,Computational Economics; Fair allocation; Optimal allocation,,
Reliability Evaluation of Grid Connected Roof Top Solar Photovoltaic Power Plant Using Markov Model Approach,"Raju K.K., Eswaramoorthy M.",Applied Solar Energy (English translation of Geliotekhnika),2020.0,"Abstract—: Reliability of the solar power plant depends on its performance and economics factor compared to the conventional fueled power plants. In this paper, reliability performance assessment of grid connected roof top solar photovoltaic power plant (GCRTSPP) are presented at site location 12.0950° N, 75.5451° E) by considering various operating factors of subcomponents of solar photovoltaic panels, diode, capacitor and controller using Markov model approach. Critical stress factors are identified and are improved upon to enhance the system reliability. The performance of the solar photovoltaic array and the subcomponent of GCRTSPP have been examined with the sensitivity results. Also, sensitivity analysis of failure rate of these components with respect to stress factors is performed and critical stress factors are identified. Pareto analysis as a tool, reliability studies of grid connected solar photovoltaic system have been carried out to compute the highest failure rate of component. It is found that electronic controller and diode is more sensitive item compared to solar photovoltaic panels and capacitor. These components failure rate sensitivity analysis with respect to stress factors is performed. The reliability on standalone and grid connected photovoltaic system also compared. The critical stress factors of the components are identified. Artificial neural network (ANN) and machine learning programme (MLP) proposed for further study. © 2020, Allerton Press, Inc.",10.3103/S0003701X2004009X,Markov model; reliability analysis; solar PV power plant,,
Handling Constrained Multi-Objective optimization with Objective Space Mapping to Decision Space Based on Extreme Learning Machine,"Zhang H., Tao K., Ma L., Yong Y.","2020 IEEE Congress on Evolutionary Computation, CEC 2020 - Conference Proceedings",2020.0,"Constrained multi-objective optimization is frequently encountered from the point of view of practical problem solving. The difficulty of constrained multi-objective optimization is how to offer guarantee of finding feasible optimal solutions within a specified number of iterations. To address the issue, this paper proposes an innovative optimization framework with objective space mapping to decision space for constrained multiobjective optimization and a novel multi-objective optimization algorithms are proposed based on this framework. Extreme learning machine implements prediction of decision variables from modified objective values with distance measure and adaptive penalty. This algorithm employs the framework of artificial bee colony to divide this optimization process into two phases: the employed bees and the onlooker bees. In the phase of employed bees, multi-objective strategy employs fast non-dominant sort and crowded distance to push the population toward Pareto front. In the phase of onlooker bees, multi-objective strategy employs Tchebycheff approach to enhance the population diversity. The experimental results on a series of benchmark problems suggest that our proposed algorithm is quite effective, in comparison to other state-of-the-art constrained multi-objective optimizers. © 2020 IEEE.",10.1109/CEC48606.2020.9185580,artificial bee colony; constrained multi-objective optimization; decomposition; extreme learning machine; nondomination,,
Generative Adversarial Networks are special cases of Artificial Curiosity (1990) and also closely related to Predictability Minimization (1991),Schmidhuber J.,Neural Networks,2020.0,"I review unsupervised or self-supervised neural networks playing minimax games in game-theoretic settings: (i) Artificial Curiosity (AC, 1990) is based on two such networks. One network learns to generate a probability distribution over outputs, the other learns to predict effects of the outputs. Each network minimizes the objective function maximized by the other. (ii) Generative Adversarial Networks (GANs, 2010-2014) are an application of AC where the effect of an output is 1 if the output is in a given set, and 0 otherwise. (iii) Predictability Minimization (PM, 1990s) models data distributions through a neural encoder that maximizes the objective function minimized by a neural predictor of the code components. I correct a previously published claim that PM is not based on a minimax game. © 2020 Elsevier Ltd",10.1016/j.neunet.2020.04.008,Artificial Curiosity; Generative Adversarial Networks; Predictability Minimization,24.0,
An evolutionary approach for constructing multi-stage classifiers,"Hamilton N.H., Fulp E.W.",GECCO 2020 Companion - Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion,2020.0,"Multi-stage classification is a supervised learning approach that distributes a set of features, each with an associated importance and cost of generation, across a series of classifiers (stages). Inputs are processed in a pipeline fashion through the stages, each of which utilizes only a subset of the complete feature set, until a confident classification decision can be made or until all features and stages have been exhausted. This design benefits from processing inputs in parallel and ensures that labels are assigned using only the necessary features, but the number and composition of stages used by the model can have significant impact on overall performance. Unfortunately, identifying these critical design aspects becomes more difficult as the number of features and possible stages increases, often making brute-force search or human intuition impractical. This paper introduces a novel evolutionary approach for discovering multi-stage configurations that provide high classification performance and fast processing times. Using this approach, multistage classifier configurations are modeled as chromosomes, and a series of selection, recombination, and mutation operations are iteratively performed to find better configurations. Since the problem has multiple objectives, a Pareto-based fitness measure is developed to rank chromosomes, where better chromosomes have high accuracy, high conclusiveness, and fast processing time. Experimental results indicate this approach is able to consistently find accurate and fast multi-stage classifier configurations under various conditions, including an increasing number of features and different feature synthesis time distributions. © 2020 ACM.",10.1145/3377929.3398088,,,
Software defect prediction based on stacked contractive autoencoder and multi-objective optimization,"Zhang N., Zhu K., Ying S., Wang X.","Computers, Materials and Continua",2020.0,"Software defect prediction plays an important role in software quality assurance. However, the performance of the prediction model is susceptible to the irrelevant and redundant features. In addition, previous studies mostly regard software defect prediction as a single objective optimization problem, and multi-objective software defect prediction has not been thoroughly investigated. For the above two reasons, we propose the following solutions in this paper: (1) we leverage an advanced deep neural network-Stacked Contractive AutoEncoder (SCAE) to extract the robust deep semantic features from the original defect features, which has stronger discrimination capacity for different classes (defective or non-defective). (2) we propose a novel multi-objective defect prediction model named SMONGE that utilizes the Multi-Objective NSGAII algorithm to optimize the advanced neural network-Extreme learning machine (ELM) based on state-of-the-art Pareto optimal solutions according to the features extracted by SCAE. We mainly consider two objectives. One objective is to maximize the performance of ELM, which refers to the benefit of the SMONGE model. Another objective is to minimize the output weight norm of ELM, which is related to the cost of the SMONGE model. We compare the SCAE with six state-of-the-art feature extraction methods and compare the SMONGE model with multiple baseline models that contain four classic defect predictors and the MONGE model without SCAE across 20 open source software projects. The experimental results verify that the superiority of SCAE and SMONGE on seven evaluation metrics. © 2020 Tech Science Press. All rights reserved.",10.32604/cmc.2020.011001,Deep neural network; Extreme learning machine; Multi-objective optimization; Software defect prediction; Stacked contractive autoencoder,6.0,
Minimax optimal rates for mondrian trees and forests,"Mourtada J., Gaïffas S., Scornet E.",Annals of Statistics,2020.0,"Introduced by Breiman (Mach. Learn. 45 (2001) 5–32), Random Forests are widely used classification and regression algorithms. While being initially designed as batch algorithms, several variants have been proposed to handle online learning. One particular instance of such forests is the Mondrian forest (In Adv. Neural Inf. Process. Syst. (2014) 3140–3148; In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS) (2016)), whose trees are built using the so-called Mondrian process, therefore allowing to easily update their construction in a streaming fashion. In this paper we provide a thorough theoretical study of Mondrian forests in a batch learning setting, based on new results about Mondrian partitions. Our results include consistency and convergence rates for Mondrian trees and forests, that turn out to be minimax optimal on the set of s-Hölder function with s ∈ (0, 1] (for trees and forests) and s ∈ (1, 2] (for forests only), assuming a proper tuning of their complexity parameter in both cases. Furthermore, we prove that an adaptive procedure (to the unknown s ∈ (0, 2]) can be constructed by combining Mondrian forests with a standard model aggregation algorithm. These results are the first demonstrating that some particular random forests achieve minimax rates in arbitrary dimension. Owing to their remarkably simple distributional properties, which lead to minimax rates, Mondrian trees are a promising basis for more sophisticated yet theoretically sound random forests variants. © Institute of Mathematical Statistics, 2020.",10.1214/19-AOS1886,Minimax rates; Nonparametric estimation; Random forests; Supervised learning,4.0,
A two-stage multi-objective deep reinforcement learning framework,"Chen D., Wang Y., Gao W.",Frontiers in Artificial Intelligence and Applications,2020.0,"In multi-objective decision making problems, multi-objective reinforcement learning (MORL) algorithms aim to approximate the Pareto frontier uniformly. A naive approach is to learn multiple policies by repeatedly running a single-objective reinforcement learning (RL) algorithm on scalarized rewards. The scalarization methods denote the preferences of objectives, which are different in each run. However, in this way, the model representation and computation are redundant. Furthermore, uniform preferences can not guarantee a uniformly approximated Pareto frontier. To address these problems and leverage the expressive power of deep neural networks, we propose a two-stage MORL framework integrating a multi-policy deep RL algorithm and an evolution strategy algorithm. Firstly, a multi-policy soft actor-critic algorithm is proposed to collaboratively learn multiple policies which are assigned with different scalarization weights. The lower layers of all policy networks are shared. The first-stage learning can be regarded as representation learning. Secondly, the multi-objective covariance matrix adaptation evolution strategy (MO-CMA-ES) is applied to fine-tune policy-independent parameters to approach a dense and uniform estimation of the Pareto frontier. Experimental results on two benchmarks (Deep Sea Treasure and Adaptive Streaming) show the superiority of the proposed method. © 2020 The authors and IOS Press.",10.3233/FAIA200202,,1.0,
Regularization methods based on the Lq-likelihood for linear models with heavy-tailed errors,Hirose Y.,Entropy,2020.0,"We propose regularization methods for linear models based on the Lq-likelihood, which is a generalization of the log-likelihood using a power function. Regularization methods are popular for the estimation in the normal linear model. However, heavy-tailed errors are also important in statistics and machine learning. We assume q-normal distributions as the errors in linear models. A q-normal distribution is heavy-tailed, which is defined using a power function, not the exponential function. We find that the proposed methods for linear models with q-normal errors coincide with the ordinary regularization methods that are applied to the normal linear model. The proposed methods can be computed using existing packages because they are penalized least squares methods. We examine the proposed methods using numerical experiments, showing that the methods perform well, even when the error is heavy-tailed. The numerical experiments also illustrate that our methods work well in model selection and generalization, especially when the error is slightly heavy-tailed. © 2020 by the author.",10.3390/E22091036,Least absolute shrinkage and selection operator (LASSO); Minimax concave penalty (MCP); Power function; Q-normal distribution; Smoothly clipped absolute deviation (SCAD); Sparse estimation,,
Learning pareto optimal solution of a multi-attribute bilateral negotiation using deep reinforcement,"Montazeri M., Kebriaei H., Araabi B.N.",Electronic Commerce Research and Applications,2020.0,"This paper aims to design an intelligent buyer to learn how to decide in an incomplete information multi-attribute bilateral simultaneous negotiation. The buyer does not know the negotiation strategy of the seller and only have access to the historical data of the previous negotiations. Using the historical data and clustering method, the type of seller is identified online during the negotiation. Then, the deep reinforcement learning method is utilized to support the buyer to learn its optimal decision. In the complete information case, we prove that the negotiation admits a unique Nash bargaining solution with possibly asymmetric negotiation powers. In comprehensive simulation studies, the efficiency of the proposed learning agent is evaluated in different scenarios and we show that the learning negotiation with incomplete information is converged to a Pareto optimal solution. Then, using the concept of the Nash bargaining solution, the negotiation power of the buyer is assessed in negotiation. © 2020 Elsevier B.V.",10.1016/j.elerap.2020.100987,Actor-critic; Bargaining power; Deep auto encoder; Multi-attribute negotiation; Nash bargaining solution,1.0,
Predicting patient specific Pareto fronts from patient anatomy only,"van der Bijl E., Wang Y., Janssen T., Petit S.",Radiotherapy and Oncology,2020.0,"Purpose: To demonstrate the feasibility of predicting the patient-specific treatment planning Pareto front (PF) for prostate cancer patients based only on delineations of PTV, rectum and body. Material/methods: Our methodology consists of four steps. First, using Erasmus-iCycle, the Pareto fronts of 112 prostate cancer patients were constructed by generating per patient 42 Pareto optimal treatment plans with different priorities. Dose parameters associated to homogeneity, conformity and dose to rectum were extracted. Second, a 3D convex function representing the PF spanned by the 42 plans was fitted for each patient using three patient-specific parameters. Third, ten features were extracted from the, aforementioned, structures to train a linear-regressor prediction algorithm to predict these three patient-specific parameters. Fourth, the quality of the predictions was assessed by calculating the average and maximum distances of the predicted PF to the 42 plans for patients in the validation cohort. Results: The prediction model was able to predict the clinically relevant PF within 2 Gy for 90% of the patients with a median average distance of 0.6 Gy. Conclusions: We demonstrate the feasibility of fast, accurate predictions of the patient-specific PF for prostate cancer patients based only on delineations of PTV, rectum and body. © 2020 Elsevier B.V.",10.1016/j.radonc.2020.05.050,Knowledge based planning (KBP); Pareto front; Prostate cancer; Treatment planning,2.0,
Multi-objective energy management of multiple microgrids under random electric vehicle charging,"Tan B., Chen H.",Energy,2020.0,"In view of the increasing development of decentralized power systems and electric vehicles, this paper seeks to improve the energy management performance of multiple microgrid systems under the uncertainty associated with electric vehicle charging. A multi-objective optimization model is established for minimizing the transmission losses, operating costs, and carbon emissions of multiple microgrid systems. Firstly, a novel method is proposed for forecasting electric vehicle charging loads based on a back propagation neural network improved by long short-term memory deep learning. Based on the forecast data, a double layer solution algorithm is proposed, which consists of an adaptive multi-objective evolutionary algorithm based on decomposition and differential evolution at the multiple microgrids layer and a modified consistency algorithm for fast economic scheduling at the single microgrid layer. Finally, a model system composed of four interconnected IEEE microgrids is simulated as a case study, and the performance of the proposed algorithm is compared with that of conventional multi-objective evolutionary algorithms based on decomposition. The simulation results demonstrate the superiority of the global search performance and the rapid convergence performance of the proposed improved algorithm. © 2020 Elsevier Ltd",10.1016/j.energy.2020.118360,Consistency algorithm; Electric vehicles; Long short-term memory; Multi-microgrids; Pareto optimality; Shannon-Wiener index,17.0,
Combining a gradient-based method and an evolution strategy for multi-objective reinforcement learning,"Chen D., Wang Y., Gao W.",Applied Intelligence,2020.0,"Multi-objective reinforcement learning (MORL) algorithms aim to approximate the Pareto frontier uniformly in multi-objective decision making problems. In the scenario of deep reinforcement learning (RL), gradient-based methods are often adopted to learn deep policies/value functions due to the fast convergence speed, while pure gradient-based methods can not guarantee a uniformly approximated Pareto frontier. On the other side, evolution strategies straightly manipulate in the solution space to achieve a well-distributed Pareto frontier, but applying evolution strategies to optimize deep networks is still a challenging topic. To leverage the advantages of both kinds of methods, we propose a two-stage MORL framework combining a gradient-based method and an evolution strategy. First, an efficient multi-policy soft actor-critic algorithm is proposed to learn multiple policies collaboratively. The lower layers of all policy networks are shared. The first-stage learning can be regarded as representation learning. Secondly, the multi-objective covariance matrix adaptation evolution strategy (MO-CMA-ES) is applied to fine-tune policy-independent parameters to approach a dense and uniform estimation of the Pareto frontier. Experimental results on three benchmarks (Deep Sea Treasure, Adaptive Streaming, and Super Mario Bros) show the superiority of the proposed method. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",10.1007/s10489-020-01702-7,Multi-objective reinforcement learning; Multi-policy reinforcement learning; Pareto frontier; Sampling efficiency,2.0,
Friend-or-Foe Deep Deterministic Policy Gradient,"Jiang H., Shi D., Xue C., Wang Y., Wang G., Zhang Y.","Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics",2020.0,"One of the toughest challenges in the multi-agent deep reinforcement learning (MADRL) is that when the opponents' policies change rapidly, the collaborative agents can't learn well to respond to the opponents' policies effectively. This may lead to a local optimum w.r.t. the learned policy of the collaborative agents may be only locally optimal to the opponents' current policies. To address this problem, we propose a novel algorithm termed Friend-or-Foe Deep Deterministic Policy Gradient (FD2PG), in which the cooperative agents can be trained more robust and have stronger cooperation ability in continuous action space. These collaborative agents can generalize easily and respond correctly, even if their opponents' policies alter. Inspired by the classic Friend-or-Foe Q-learning algorithm (FFQ), we introduce the idea of minimizing the foes and maximizing the friends into the centralized training distributed execution framework, multi-agent deep deterministic policy gradient algorithm (MADDPG), to enhance collaborative agents' robustness and cooperativity. Besides, we introduce a Minimax Multi-Agent Learning (MMAL) method to explore two special equilibriums (the adversarial equilibrium and the coordination equilibrium), which can guarantee the convergence of FD2PG and improve optimization. Extensive fine-grained experiments, including four representative scenario experiments and two scale-performance correlation experiments, were conducted to demonstrate the superior performance of FD2PG comparing with existing baselines. © 2020 IEEE.",10.1109/SMC42975.2020.9283033,Deep Deterministic Policy Gradient; Friend-or-Foe; Multi-Agent Deep Reinforcement Learning,1.0,
Designing a clean and efficient air conditioner with AI intervention to optimize energy-exergy interplay,"Saikia P., Gaurav, Rakshit D.",Energy and AI,2020.0,"Conventional refrigerants in air conditioners (A/Cs) although deliver localized comfort within dwellings, their overall detrimental impact on the environment is an alarming issue. This study investigates eco-friendly alternatives to the refrigerants R410A and R22 (which contribute to global warming and ozone depletion) through a comprehensive analysis of salient parameters such as coefficient of performance (COP), volumetric cooling capacity, and exergetic efficiency (ηex) featuring a sustainably retrofitted vapor compression based A/C. Diverse thermophysical properties of alternate refrigerants yield multiple options for contriving a sustainable A/C. Performance enhancement of the retrofitted system is then realized through a multifaceted genetic algorithm (GA) coupled with an artificial neural network (ANN). Subsequently, the incongruence of optimality between maximum system COP and maximum ηex is dealt by a dual ANN powered non dominated sorting genetic algorithm-2 (NSGA-2) optimizer which provides balanced output in terms of COP (4.37 for L20a and 4.238 for ARM71a refrigerant respectively) and ηex (26.208% for L20a and 25.413% for ARM71a refrigerant respectively). The artificial intelligence (AI) based approach helps comprehend the trade-off between different system performance indices (having different units/ranges of variation) during optimum design selection. Furthermore, the data-driven surrogate model reveals the dominating effect of energy performance over exergy performance of the system, urging for the higher priority of resource allocation for COP upgrade than ηex upgrade. Finally, the multi-objective optimization yields a broader set of Pareto optimal points which offer flexibility to the stakeholders to thrust the sustainable system towards higher COP or higher ηex mode of operation. © 2020",10.1016/j.egyai.2020.100029,Artificial neural network; Energy; Exergy; Genetic algorithm; Machine learning; Refrigerant,1.0,
PANDORA: An Architecture-Independent Parallelizing Approximation-Discovery Framework,"Stitt G., Campbell D.",ACM Transactions on Embedded Computing Systems,2020.0,"In this article, we introduce a parallelizing approximation-discovery framework, PANDORA, for automatically discovering application- and architecture-specialized approximations of provided code. PANDORA complements existing compilers and runtime optimizers by generating approximations with a range of Pareto-optimal tradeoffs between performance and error, which enables adaptation to different inputs, different user preferences, and different runtime conditions (e.g., battery life). We demonstrate that PANDORA can create parallel approximations of inherently sequential code by discovering alternative implementations that eliminate loop-carried dependencies. For a variety of functions with loop-carried dependencies, PANDORA generates approximations that achieve speedups ranging from 2.3x to 81x, with acceptable error for many usage scenarios. We also demonstrate PANDORA's architecture-specialized approximations via FPGA experiments, and highlight PANDORA's discovery capabilities by removing loop-carried dependencies from a recurrence relation with no known closed-form solution. © 2020 ACM.",10.1145/3391899,approximate computing; machine learning; Symbolic regression,,
Numerical solution of inverse problems by weak adversarial networks,"Bao G., Ye X., Zang Y., Zhou H.",Inverse Problems,2020.0,"In this paper, a weak adversarial network approach is developed to numerically solve a class of inverse problems, including electrical impedance tomography and dynamic electrical impedance tomography problems. The weak formulation of the partial differential equation for the given inverse problem is leveraged, where the solution and the test function are parameterized as deep neural networks. Then, the weak formulation and the boundary conditions induce a minimax problem of a saddle function of the network parameters. As the parameters are alternatively updated, the network gradually approximates the solution of the inverse problem. Theoretical justifications are provided on the convergence of the proposed algorithm. The proposed method is completely mesh-free without any spatial discretization, and is particularly suitable for problems with high dimensionality and low regularity on solutions. Numerical experiments on a variety of test inverse problems demonstrate the promising accuracy and efficiency of this approach. © 2020 IOP Publishing Ltd.",10.1088/1361-6420/abb447,Adversarial network; Deep learning; Inverse problem; Stochastic gradient; Weak formulation,1.0,
Learning adversarial attack policies through multi-objective reinforcement learning,"García J., Majadas R., Fernández F.",Engineering Applications of Artificial Intelligence,2020.0,"Deep Reinforcement Learning has shown promising results in learning policies for complex sequential decision-making tasks. However, different adversarial attack strategies have revealed the weakness of these policies to perturbations to their observations. Most of these attacks have been built on existing adversarial example crafting techniques used to fool classifiers, where an adversarial attack is considered a success if it makes the classifier outputs any wrong class. The major drawback of these approaches when applied to decision-making tasks is that they are blind for long-term goals. In contrast, this paper suggests that it is more appropriate to view the attack process as a sequential optimization problem, with the aim of learning a sequence of attacks, where the attacker must consider the long-term effects of each attack. In this paper, we propose that such an attack policy must be learned with two objectives in view. On the one hand, the attack must pursue the maximum performance loss of the attacked policy. On the other hand, it also should minimize the cost of the attacks. Therefore, in this paper we propose a novel modelization of the process of learning an attack policy as a Multi-objective Markov Decision Process with two objectives: maximizing the performance loss of the attacked policy and minimizing the cost of the attacks. We also reveal the conflicting nature of these two objectives and use a Multi-objective Reinforcement Learning algorithm to draw the Pareto fronts for four well-known tasks: the GridWorld, the Cartpole, the Mountain car and the Breakout. © 2020 Elsevier Ltd",10.1016/j.engappai.2020.104021,Adversarial reinforcement learning; Multi-objective reinforcement learning,6.0,
A multi-objective deep reinforcement learning framework,"Nguyen T.T., Nguyen N.D., Vamplew P., Nahavandi S., Dazeley R., Lim C.P.",Engineering Applications of Artificial Intelligence,2020.0,"This paper introduces a new scalable multi-objective deep reinforcement learning (MODRL) framework based on deep Q-networks. We develop a high-performance MODRL framework that supports both single-policy and multi-policy strategies, as well as both linear and non-linear approaches to action selection. The experimental results on two benchmark problems (two-objective deep sea treasure environment and three-objective Mountain Car problem) indicate that the proposed framework is able to find the Pareto-optimal solutions effectively. The proposed framework is generic and highly modularized, which allows the integration of different deep reinforcement learning algorithms in different complex problem domains. This therefore overcomes many disadvantages involved with standard multi-objective reinforcement learning methods in the current literature. The proposed framework acts as a testbed platform that accelerates the development of MODRL for solving increasingly complicated multi-objective problems. © 2020 Elsevier Ltd",10.1016/j.engappai.2020.103915,Deep learning; Multi-objective; Multi-policy; Reinforcement learning; Single-policy,6.0,
A hybrid intelligent system for designing optimal proportions of recycled aggregate concrete,"Zhang J., Huang Y., Aslani F., Ma G., Nener B.",Journal of Cleaner Production,2020.0,"The replacement of natural coarse aggregate (NCA) with recycled coarse aggregate (RCA) in concrete mixtures offers various advantages, including conservation of natural resources, reduction of CO2 emissions, and cost reduction. However, multiple related variables and objectives (e.g., mechanical, economic, and environmental objectives) need to be considered when optimizing mixtures of recycled aggregate concrete (RAC). This cannot be achieved through traditional laboratory- or statistics-based methods. This study proposes a hybrid intelligent system based on artificial intelligence (AI) and metaheuristic algorithms for designing optimal mixtures of RAC. To verify the proposed model, a data set containing 344 different RAC mixtures was collected from previous literature. A semi-supervised cotraining algorithm using two k-nearest neighbor (kNN) regressors with different distance metrics is developed to label the unlabeled data in the collected dataset. Different AI models are incorporated into the system for modeling the relationship between RAC strength and its influencing variables. A multi-objective optimization (MOO) model based on AI algorithms and on a multi-objective firefly algorithm is used to search for optimal mixtures of RAC. The results show that kNN-based semi-supervised cotraining can effectively exploit unlabeled data to improve the regression estimates. In the test set, A Random Forest and Backpropagation Neural Network achieve the best prediction accuracy for predicting, respectively, uniaxial compressive strength and splitting tensile strength of RAC, indicated by the highest correlation coefficients (0.9064 and 0.8387, respectively) and lowest root-mean-square errors (6.639 MPa and 0.5119 MPa, respectively). The Pareto fronts of the multi-objective mixture optimization problem are successfully obtained by the MOO model. The proposed system can also be used to optimize mixture proportions of other cementitious materials in civil engineering. © 2020 Elsevier Ltd",10.1016/j.jclepro.2020.122922,Artificial intelligence; Concrete mixture optimization; Firefly algorithm; Mechanical properties; Recycled aggregate concrete,26.0,
Applying Gradient Boosting Trees and Stochastic Leaf Evaluation to MCTS on Hearthstone,"Papagiannis T., Alexandridis G., Stafylopatis A.","Proceedings - 19th IEEE International Conference on Machine Learning and Applications, ICMLA 2020",2020.0,"Collectible card games are an interesting testing ground for artificial intelligence algorithms, mainly because of their stochasticity and high branching factor. In this work, the performance of a monte carlo tree search-based agent, enhanced with a gradient boosting tree classifier on the simulation phase, is investigated on Hearthstone. Furthermore, the impact of the combination of random simulations and the classifier's predictions is studied, as well as its correlation with the action space and the tree's depth. The aforementioned approach has been implemented in the Metastone framework and has been tested against the vanilla approach and the state-of-the-art algorithm, both provided by the framework itself. Over a set of evaluation games, it is demonstrated that the examined methodology significantly outperforms the vanilla-MCTS and is even matched with the heuristic-driven minimax algorithm. © 2020 IEEE.",10.1109/ICMLA51294.2020.00034,Gradient Boosting Trees; Hearthstone; Metastone; Monte Carlo Tree Search; Stochastic Leaf Evaluation,,
Analysis of the transferability and robustness of GANs evolved for Pareto set approximations,"Garciarena U., Mendiburu A., Santana R.",Neural Networks,2020.0,"The generative adversarial network (GAN) is a good example of a strong-performing, neural network-based generative model, even though it does have some drawbacks of its own. Mode collapsing and the difficulty in finding the optimal network structure are two of the most concerning issues. In this paper, we address these two issues at the same time by proposing a neuro-evolutionary approach with an agile evaluation method for the fast evolution of robust deep architectures that avoid mode collapsing. The computation of Pareto set approximations with GANs is chosen as a suitable benchmark to evaluate the quality of our approach. Furthermore, we demonstrate the consistency, scalability, and generalization capabilities of the proposed method, which shows its potential applications to many areas. We finally readdress the issue of designing this kind of models by analyzing the characteristics of the best performing GAN specifications, and conclude with a set of general guidelines. This results in a reduction of the many-dimensional problem of structural manual design or automated search. © 2020 Elsevier Ltd",10.1016/j.neunet.2020.09.003,Generative adversarial networks; Knowledge transferability; Multi-objective optimization; Neuro-evolution; Pareto front approximation,3.0,
New hybrid between SPEA/R with deep neural network: Application to predicting the multi-objective optimization of the stiffness parameter for powertrain mount systems,"Dao D.-N., Guo L.-X.",Journal of Low Frequency Noise Vibration and Active Control,2020.0,"In this study, a new methodology, hybrid Strength Pareto Evolutionary Algorithm Reference Direction (SPEA/R) with Deep Neural Network (HDNN&SPEA/R), has been developed to achieve cost optimization of stiffness parameter for powertrain mount systems. This problem is formalized as a multi-objective optimization problem involving six optimization objectives: mean square acceleration of a rear engine mount, mean square displacement of a rear engine mount, mean square acceleration of a front left engine mount, mean square displacement of a front left engine mount, mean square acceleration of a front right engine mount, and mean square displacement of a front right engine mount. A hybrid HDNN&SPEA/R is proposed with the integration of genetic algorithm, deep neural network, and a Strength Pareto evolutionary algorithm based on reference direction for multi-objective SPEA/R. Several benchmark functions are tested, and results reveal that the HDNN&SPEA/R is more efficient than the typical deep neural network. stiffness parameter for powertrain mount systems optimization with HDNN&SPEA/R is simulated, respectively. It proved the potential of the HDNN&SPEA/R for stiffness parameter for powertrain mount systems optimization problem. © The Author(s) 2019.",10.1177/1461348419868322,extreme learning machine; feed-forward artificial neural network; mounting system; multi-objective evolutionary algorithms; powertrain mount system stiffness; SPEA/R algorithm,1.0,
A novel dictionary learning method for sparse representation with nonconvex regularizations,"Tan B., Li Y., Zhao H., Li X., Ding S.",Neurocomputing,2020.0,"In dictionary learning, sparse regularization is used to promote sparsity and has played a major role in the developing of dictionary learning algorithms. ℓ1-norm is of the most popular sparse regularization due to its convexity and the related tractable convex optimization problems. However, ℓ1-norm leads to biased solutions and provides inferior performance on certain applications compared with nonconvex sparse regularizations. In this work, we propose a generalized minimax-concave (GMC) sparse regularization, which is nonconvex, to promote sparsity to design dictionary learning model. Applying the alternate optimization scheme, we use the forward–backward splitting (FBS) algorithm to solve the sparse coding problem. As the improvement, we incorporate Nesterov's acceleration technique and adaptive threshold scheme into the FBS algorithm to improve the convergence efficiency and performance. In the dictionary update step, we apply the difference of convex functions (DC) programming and the DC algorithm (DCA) to address the dictionary update. Two dictionary update algorithms are designed; one updates the dictionary atoms one by one, and the other one updates the dictionary atoms simultaneously. The presented dictionary learning algorithms perform robustly in dictionary recovery. Numerical experiments are designed to verify the performance of proposed algorithms and to compare with the state-of-the-art algorithms. © 2020",10.1016/j.neucom.2020.07.085,DC programming and DCA; Dictionary learning; Forward-backward splitting algorithm; GMC regularization; Nonconvex,1.0,
Chimera: A Hybrid Machine Learning-Driven Multi-Objective Design Space Exploration Tool for FPGA High-Level Synthesis,"Yu M., Huang S., Chen D.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021.0,"In recent years, hardware accelerators based on field programmable gate arrays (FPGA) have been widely applied and the high-level synthesis (HLS) tools were created to facilitate the design of these accelerators. However, achieving high performance with HLS is still time-consuming and requires expert knowledge. Therefore, we present Chimera, an automated design space exploration tool for applying HLS optimization directives. It utilizes a novel multi-objective exploration method that seamlessly integrates active learning, evolutionary algorithm, and Thompson sampling, which enables it to find a set of optimized designs on a Pareto curve by only evaluating a small number of design points. On the Rosetta benchmark suite, Chimera explored design points that have the same or superior performance compared to highly optimized hand-tuned designs created by expert HLS users in less than 24 h. Moreover, it explores a Pareto frontier, where the elbow point can save up to 26% of flip-flop resource with negligible performance overhead. © 2021, Springer Nature Switzerland AG.",10.1007/978-3-030-91608-4_52,,,
AutoML Technologies for the Identification of Sparse Models,"Liuliakov A., Hammer B.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021.0,"Automated machine learning (AutoML) technologies constitute promising tools to automatically infer model architecture, meta-parameters or processing pipelines for specific machine learning tasks given suitable training data. At present, the main objective of such technologies typically relies on the accuracy of the resulting model. Additional objectives such as sparsity can be integrated by pre-processing steps or according penalty terms in the objective function. Yet, sparsity and model accuracy are often contradictory goals, and optimum solutions form a Pareto front. Thereby, it is not guaranteed that solutions at different positions of the Pareto front share the same architectural choices, hence current AutoML technologies might yield sub-optimal results. In this contribution, we propose a novel method, based on the AutoML method TPOT, which enables an automated optimization of ML pipelines with sparse input features along the whole Pareto front. We demonstrate that, indeed, different architectures are found at different points of the Pareto front for benchmark examples from the domain of systems security. © 2021, Springer Nature Switzerland AG.",10.1007/978-3-030-91608-4_7,AutoML; Feature selection; TPOT,,
"Multi-objective optimization of GFRP injection molding process parameters, using GA-ELM, MOFA, and GRA-TOPSIS","Liu X., Fan X., Guo Y., Cao Y., Li C.",Transactions of the Canadian Society for Mechanical Engineering,2021.0,"Owing to the influence of the injection molding process, warpage and volume shrinkage are two common quality defects for products manufactured by glass fiber-reinforced plastic (GFRP) injection molding. To minimize these two defects, an extreme learning machine optimized with a genetic algorithm (GA-ELM), multi-objective firefly algorithm (MOFA), and a multi-objective decision-making method (GRA-TOPSIS) were implemented in this study. All of the experiments, based on Latin hypercubic sampling (LHS), were conducted using Moldflow software to obtain the results for warpage and volume shrinkage. The prediction accuracy of the defect-prediction models based on the extreme learning machine (ELM) and GA-ELM algorithms were compared. The results show that the GA-ELM models can better predict the defect values. Finally, MOFA was used to find the Pareto optimal front, and the GRA-TOPSIS method was used to find the optimum solution from the Pareto optimal front. According to the results of the simulation verification, the warpage and volume shrinkage were effectively reduced by 12.25% and 6.11%, respectively, compared with before optimization, which indicates the effectiveness and reliability of the optimization method. © 2021 The Author(s).",10.1139/tcsme-2021-0053,Evaluation strategy; Extreme learning machine; Injection molding; Latin hypercube sampling; Multi-objective optimization,,
BOOM-Explorer: RISC-V BOOM Microarchitecture Design Space Exploration Framework,"Bai C., Sun Q., Zhai J., Ma Y., Yu B., Wong M.D.E.","IEEE/ACM International Conference on Computer-Aided Design, Digest of Technical Papers, ICCAD",2021.0,"The microarchitecture design of a processor has been increasingly difficult due to the large design space and time-consuming verification flow. Previously, researchers rely on prior knowledge and cycle-accurate simulators to analyze the performance of different microarchitecture designs but lack sufficient discussions on methodologies to strike a good balance between power and performance. This work proposes an automatic framework to explore microarchitecture designs of the RISC-V Berkeley Out-of-Order Machine (BOOM), termed as BOOM-Explorer, achieving a good trade-off on power and performance. Firstly, the framework utilizes an advanced microarchitecture-aware active learning (MicroAL) algorithm to generate a diverse and representative initial design set. Secondly, a Gaussian process model with deep kernel learning functions (DKL-GP) is built to characterize the design space. Thirdly, correlated multi-objective Bayesian optimization is leveraged to explore Pareto-optimal designs. Experimental results show that BOOM-Explorer can search for designs that dominate previous arts and designs developed by senior engineers in terms of power and performance within a much shorter time. © 2021 IEEE",10.1109/ICCAD51958.2021.9643455,,1.0,
Evolutionary computing assisted deep reinforcement learning for multi-objective integrated energy system management,"Huang C., Wang L., Luo X., Zhang H., Song Y.","Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",2021.0,"This paper investigates the multi-objective optimal operation problem of an integrated energy system (IES) which integrates grid-connected photovoltaic (PV) generator, gas boiler, battery energy storage system, and thermal storage to satisfy energy demand in forms of electricity and heat. To handle the changes from the system uncertainty (e.g., PV generation, electrical loads, thermal loads, etc.) and unknown thermal dynamic model for temperature control, deep reinforcement learning-based model-free optimization method is proposed to solve the multi-objective optimization problem in which the multi-objective optimization problem is firstly formulated as a multi-objective Markov decision process (MDP) problem. The multi-objective MDP problem is converted to many single-objective MDP problems by the sum technique which are solved by multi-agent deep deterministic policy gradient (DDPG) algorithm. To improve the performance of multi-agent DDPG algorithm, evolutionary computing-based parameter-tuning method is further proposed to fine-tune the policy parameters in DDPG algorithm. The proposed methods are verified on real data. Experiments results illustrate that the multi-agent DDPG algorithm can efficiently solve the multi-objective optimal operation problem of the IES while the evolutionary computing-based policy parameter-tuning method can further improve the approximation of Pareto frontier. © 2021 IEEE.",10.1109/ICTAI52525.2021.00082,deep reinforcement learning; evolutionary computing; integrated energy system; multi-objective optimization,,
Soft Computing Tools for Multiobjective Optimization of Offshore Crude Oil and Gas Separation Plant for the Best Operational Condition,"Mendoza J.H., Tariq R., Espinosa L.F.S., Anguebes F., Bassam A.","CCE 2021 - 2021 18th International Conference on Electrical Engineering, Computing Science and Automatic Control",2021.0,"The selection of operating conditions in the oil and gas separation plants is obtained through data monitoring or through the experience of the operating personnel which can bring operational difficulties leading to inefficiencies and capital loss. The modern techniques of soft computing including artificial intelligence and genetic algorithm-based optimization can add value to this operation. In this work, five key controllable design variables of an oil and gas separation plant are optimized considering two performance indicators (oil flow productivity and gas compression power). The physical model of the plant is simulated using ASPEN HYSYS, and a digital twin model is generated using an artificial neural network. It is followed by a multiobjective optimization using non-dominating sorting genetic algorithm II to obtain the Pareto front. The results have indicated that operational optimization can enhance oil production by up to ∼6.2% and decrease the compression work by ∼3.2%. It is concluded that the proposed operation of the plant is energy-efficient and can increase productivity. © 2021 IEEE.",10.1109/CCE53527.2021.9633049,artificial neural network; data science; oil and gas separation; optimization; simulation; soft computing,,
Comparing the Forecast Performance of Advanced Statistical and Machine Learning Techniques Using Huge Big Data: Evidence from Monte Carlo Experiments,"Khan F., Urooj A., Khan S.A., Alsubie A., Almaspoor Z., Muhammadullah S.",Complexity,2021.0,"This research compares factor models based on principal component analysis (PCA) and partial least squares (PLS) with Autometrics, elastic smoothly clipped absolute deviation (E-SCAD), and minimax concave penalty (MCP) under different simulated schemes like multicollinearity, heteroscedasticity, and autocorrelation. The comparison is made with varying sample size and covariates. We found that in the presence of low and moderate multicollinearity, MCP often produces superior forecasts in contrast to small sample case, whereas E-SCAD remains better. In the case of high multicollinearity, the PLS-based factor model remained dominant, but asymptotically the prediction accuracy of E-SCAD significantly enhances compared to other methods. Under heteroscedasticity, MCP performs very well and most of the time beats the rival methods. In some circumstances under large samples, Autometrics provides a similar forecast as MCP. In the presence of low and moderate autocorrelation, MCP shows outstanding forecasting performance except for the small sample case, whereas E-SCAD produces a remarkable forecast. In the case of extreme autocorrelation, E-SCAD outperforms the rival techniques under both the small and medium samples, but further augmentation in sample size enables MCP forecast more accurate comparatively. To compare the predictive ability of all methods, we split the data into two halves (i.e., data over 1973-2007 as training data and data over 2008-2020 as testing data). Based on the root mean square error and mean absolute error, the PLS-based factor model outperforms the competitor models in terms of forecasting performance. © 2021 Faridoon Khan et al.",10.1155/2021/6117513,,1.0,
"Optimization of the exergy efficiency, exergy destruction, and engine noise index in an engine with two direct injectors using NSGA-II and artificial neural network","Shirvani S., Shirvani S., Jazayeri S.A., Reitz R.",International Journal of Engine Research,2021.0,"Direct Dual Fuel Stratification (DDFS) strategy is a novel Low Temperature Combustion (LTC) strategy that has comparable thermal efficiency to the Reactivity Controlled Compression Ignition (RCCI) strategy, while it offers more control over the combustion process and the rate of heat release. The DDFS strategy uses two direct injectors for the low- and high-reactivity fuels (gasoline and diesel) to benefit from the RCCI concept. In this study, the injection strategy of the injectors of a gasoline/diesel DDFS engine was optimized from the thermodynamic perspective to maximize exergy efficiency and minimize exergy destruction and an engine noise index. An artificial neural network was developed with 576 samples from a CFD code to predict the DDFS mode behavior, and the non-dominated sorting genetic algorithm (NSGA-II) was used to obtain the Pareto Front and the optimal solutions. Compared to the base case, the exergy efficiency of the optimal cases increased by up to 2%, exergy destruction and Peak Pressure Rise Rate (PPRR) reduced by about 2.3%, and 2 bar/deg, respectively, in the optimal solutions. NOX and soot emissions were reduced by 40% and 35%, respectively, in the best-case scenarios. © IMechE 2021.",10.1177/14680874211057752,ANN; Direct dual fuel stratification; machine learning; NSGA-II; optimization,,
Training-Free Multi-objective Evolutionary Neural Architecture Search via Neural Tangent Kernel and Number of Linear Regions,"Do T., Luong N.H.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021.0,"A newly introduced training-free neural architecture search (TE-NAS) framework suggests that candidate network architectures can be ranked via a combined metric of expressivity and trainability. Expressivity is measured by the number of linear regions in the input space that can be divided by a network. Trainability is assessed based on the condition number of the neural tangent kernel (NTK), which affects the convergence rate of training a network with gradient descent. These two measurements have been found to be correlated with network test accuracy. High-performance architectures can thus be searched for without incurring the intensive cost of network training as in a typical NAS run. In this paper, we suggest that TE-NAS can be incorporated with a multi-objective evolutionary algorithm (MOEA), in which expressivity and trainability are kept separate as two different objectives rather than being combined. We also add the minimization of floating-point operations (FLOPs) as the third objective to be optimized simultaneously. On NAS-Bench-101 and NAS-Bench-201 benchmarks, our approach achieves excellent efficiency in finding Pareto fronts of a wide range of architectures exhibiting optimal trade-offs among network expressivity, trainability, and complexity. Network architectures obtained by our approach on CIFAR-10 also show high transferability on CIFAR-100 and ImageNet. © 2021, Springer Nature Switzerland AG.",10.1007/978-3-030-92270-2_29,Deep learning; Evolutionary computation; Multi-objective optimization; Neural architecture search; Neural tangent kernels,,
A Survey of Long-Tail Item Recommendation Methods,Qin J.,Wireless Communications and Mobile Computing,2021.0,"Recommender systems represent a critical field of AI technology applications. The core function of a recommender system is to recommend items of interest to users, but if it is only user history-based (purchasing or browsing data), it can only recommend similar products to a user, which makes the user feel fatigued (creating so-called ""Information Cocoons""). Besides, transaction data (purchasing or browsing data) in various fields usually follow Pareto distributions. Accordingly, 20% of products are purchased or viewed a greater number of times (short-head items), while the remaining 80% of products are purchased or viewed less frequently (long-tail items). Using the traditional recommendation method, considering only the accuracy of recommendations, the coverage rate is relatively low, and most of the recommended items are short-head items. The long-tail item recommendation method not only considers the recommendation of short-head items but also considers recommending more long-tail items to users, thus improving the coverage and diversity of the recommendation results. Long-tail item recommendation research has become a frontier issue in recommendation systems in recent years. While the current research paper is still scarce, there have been related research achievements in top-level conferences in the field of computers, such as VLDB and IJCAI. Due to the fact that there is no review literature in this field, to allow readers to better understand the research status of the long-tail item recommendation method, this paper summarizes the progress of the research on long-tail item recommendation methods (from clustering-based, which began in 2008, to deep learning-based methods, which began in 2020) and the future directions associated with this research. © 2021 Jing Qin.",10.1155/2021/7536316,,,
Enhanced Innovized Progress Operator for Evolutionary Multi-and Many-objective Optimization,"Mittal S., Saxena D.K., Deb K., Goodman E.D.",IEEE Transactions on Evolutionary Computation,2021.0,"Innovization is a task of learning common relationships among some or all of the Pareto-optimal (PO) solutions in multi-and many-objective optimization problems. A recent study has shown that a chronological sequence of non-dominated solutions obtained along the successive generations of an optimizer possesses salient patterns that can be learnt using a Machine Learning (ML) model, and can help the offspring solutions progress in useful directions. This paper enhances each constitutive module of the above approach, including novel interventions on management of the convergence-diversity tradeoff while mapping the solutions from the previous and current generation; use of a computationally more efficient ML method, namely Random Forest; and changing the manner and extent to which the learnt ML model is utilized towards advancement of the offspring. The proposed modules constitute what is called the enhanced innovized progress (IP2) operator. To investigate the search efficacy provided by the IP2 operator, it is integrated with multi-and many-objective optimization algorithms, such as NSGA-II, NSGA-III, MOEA/D, and MaOEA-IGD, and tested on a range of two-to ten-objective test problems, and five real-world problems. Since the IP2 operator utilizes the history of gradual and progressive improvements in solutions over generations, without requiring any additional solution evaluations, it opens up a new direction for ML-assisted evolutionary optimization. IEEE",10.1109/TEVC.2021.3131952,Electrooculography; History; Innovization; Innovized Progress; IP networks; Learning-assisted Optimization; Machine Learning; Maintenance engineering; Multiobjective Optimization; Online Innovization.; Optimization; Radio frequency; Search problems,,
Machine Learning for Sensor Transducer Conversion Routines,"Newton T., Meech J.T., Marbell P.S.",IEEE Embedded Systems Letters,2021.0,"Sensors with digital outputs require software conversion routines to transform the unitless analogue-to-digital converter samples to physical quantities with correct units. These conversion routines are computationally complex given the limited computational resources of low-power embedded systems. This article presents a set of machine learning methods to learn new, less-complex conversion routines that do not sacrifice accuracy for the BME680 environmental sensor. We present a Pareto analysis of the tradeoff between accuracy and computational overhead for the models and models that reduce the computational overhead of the existing industry-standard conversion routines for temperature, pressure, and humidity by 62%, 71%, and 18% respectively. The corresponding RMS errors are 0:0114&#x2218;C, 0:0280KPa, and 0:0337%. These results show that machine learning methods for learning conversion routines can produce conversion routines with reduced computational overhead which maintain good accuracy. IEEE",10.1109/LES.2021.3129892,Ash; Computational modeling; Data models; Humidity; Interpolation; Machine Learning; Random access memory; Regression; Sensor.; Temperature measurement,,
Triple Generative Adversarial Networks,"Li C., Xu K., Zhu J., Liu J., Zhang B.",IEEE Transactions on Pattern Analysis and Machine Intelligence,2021.0,"We propose a unified game-theoretical framework to perform classification and conditional image generation given limited supervision. It is formulated as a three-player minimax game consisting of a generator, a classifier and a discriminator, and therefore is referred to as Triple Generative Adversarial Network (Triple-GAN). The generator and the classifier characterize the conditional distributions between images and labels to perform conditional generation and classification, respectively. The discriminator solely focuses on identifying fake image-label pairs. Theoretically, the three-player formulation guarantees consistency. Namely, under a nonparametric assumption, the unique equilibrium of the game is that the distributions characterized by the generator and the classifier converge to the data distribution. As a byproduct of the three-player formulation, Triple-GAN is flexible to incorporate different semi-supervised classifiers and GAN architectures. We evaluate Triple-GAN in two challenging settings, namely, semi-supervised learning and the extreme low data regime. In both settings, Triple-GAN can achieve excellent classification results and generate meaningful samples in a specific class simultaneously. In particular, using a commonly adopted 13-layer CNN classifier, Triple-GAN outperforms extensive semi-supervised learning methods substantially on several benchmarks no matter data augmentation is applied or not. IEEE",10.1109/TPAMI.2021.3127558,conditional image generation; deep generative model; Entropy; extremely low data regime; Games; Generative adversarial network; Generative adversarial networks; Generators; Linear programming; semi-supervised learning; Semisupervised learning; Task analysis,,
Iteratively Reweighted Minimax-Concave Penalty Minimization for Accurate Low-rank Plus Sparse Matrix Decomposition,"Pokala P.K., Hemadri R.V., Seelamantula C.S.",IEEE Transactions on Pattern Analysis and Machine Intelligence,2021.0,"Low-rank plus sparse matrix decomposition (LSD) is an important problem in computer vision and machine learning. It has been solved using convex relaxations of the matrix rank and l0-pseudo-norm, which are the nuclear norm and l1-norm, respectively. Convex approximations are known to result in biased estimates, to overcome which, nonconvex regularizers such as weighted nuclear-norm minimization and weighted Schatten p-norm minimization have been proposed. However, works employing these regularizers have used heuristic weight-selection strategies. We propose weighted minimax-concave penalty (WMCP) as the nonconvex regularizer and show that it admits an equivalent representation that enables weight adaptation. Similarly, an equivalent representation to the weighted matrix gamma norm (WMGN) enables weight adaptation for the low-rank part. The optimization algorithms are based on the alternating direction method of multipliers technique. We show that the optimization frameworks relying on the two penalties, WMCP and WMGN, coupled with a novel iterative weight update strategy, result in accurate low-rank plus sparse matrix decomposition. The algorithms are also shown to satisfy descent properties and convergence guarantees. On the applications front, we consider the problem of foreground-background separation in video sequences. Simulation experiments and validations on standard datasets, namely, I2R, CDnet 2012, and BMC 2012 show that the proposed techniques outperform the benchmark techniques. IEEE",10.1109/TPAMI.2021.3122259,Convergence; Costs; equivalent minimax-concave penalty; Image reconstruction; low-rank and sparse matrix decomposition; Matrix decomposition; Minimax-concave penalty; Minimization; nonconvex penalty; Nuclear-norm minimization; Optimization; Sparse matrices; weighted 1 minimization,,
A Gaussian mixture variational autoencoder-based approach for designing phononic bandgap metamaterials,"Wang Z., Xian W., Baccouche M.R., Lanzerath H., Li Y., Xu H.",Proceedings of the ASME Design Engineering Technical Conference,2021.0,"Phononic bandgap metamaterials, which consist of periodic cellular structures, are capable of absorbing energy within a certain frequency range. Designing metamaterials that trap waves across a wide wave frequency range is still a challenging task. In this study, we proposed a deep feature learning-based framework to design cellular metamaterial structures considering two design objectives: bandgap width and stiffness. A Gaussian mixture variational autoencoder (GM-VAE) is employed to extract structural features and a Gaussian Process (GP) model is employed to enable property-driven structure optimization. By comparing the GM-VAE and a regular variational autoencoder (VAE), we demonstrate that (i) GM-VAE has the advantage of learning capability, and (ii) GM-VAE discovers a more diversified design set (in terms of the distribution in the performance space) in the unsupervised learning-based generative design. Two supervised learning strategies, building independent single-response GP models for each output and building an all-in-one multi-response GP model for all outputs, are employed and compared to establish the relationship between the latent features and the properties of interest. Multi-objective design optimization is conducted to obtain the Pareto frontier with respect to bandgap width and stiffness. The effectiveness of the proposed design framework is validated by comparing the performances of newly discovered designs with existing designs. The caveats to designing phonic bandgap metamaterials are summarized. © 2021 by ASME",10.1115/DETC2021-67629,Gaussian mixture variational autoencoder; Gaussian process; Metamaterial; Optimization; Phononic bandgap,,
Adaptive Diagnosis of Lung Cancer by Deep Learning Classification Using Wilcoxon Gain and Generator,"Obulesu O., Kallam S., Dhiman G., Patan R., Kadiyala R., Raparthi Y., Kautish S.",Journal of Healthcare Engineering,2021.0,"Cancer is a complicated worldwide health issue with an increasing death rate in recent years. With the swift blooming of the high throughput technology and several machine learning methods that have unfolded in recent years, progress in cancer disease diagnosis has been made based on subset features, providing awareness of the efficient and precise disease diagnosis. Hence, progressive machine learning techniques that can, fortunately, differentiate lung cancer patients from healthy persons are of great concern. This paper proposes a novel Wilcoxon Signed-Rank Gain Preprocessing combined with Generative Deep Learning called Wilcoxon Signed Generative Deep Learning (WS-GDL) method for lung cancer disease diagnosis. Firstly, test significance analysis and information gain eliminate redundant and irrelevant attributes and extract many informative and significant attributes. Then, using a generator function, the Generative Deep Learning method is used to learn the deep features. Finally, a minimax game (i.e., minimizing error with maximum accuracy) is proposed to diagnose the disease. Numerical experiments on the Thoracic Surgery Data Set are used to test the WS-GDL method's disease diagnosis performance. The WS-GDL approach may create relevant and significant attributes and adaptively diagnose the disease by selecting optimal learning model parameters. Quantitative experimental results show that the WS-GDL method achieves better diagnosis performance and higher computing efficiency in computational time, computational complexity, and false-positive rate compared to state-of-the-art approaches. © 2021 O. Obulesu et al.",10.1155/2021/5912051,,2.0,
LoCoMOBO: A Local Constrained Multi-Objective Bayesian Optimization for Analog Circuit Sizing,"Touloupas K., Sotiriadis P.P.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,2021.0,"A Local Constrained Multi-Objective Bayesian Optimization (LoCoMOBO) method is introduced to address automatic sizing and trade-off exploration for analog and RF integrated circuits (IC). LoCoMOBO applies to constrained optimization problems utilizing multiple Gaussian Process (GP) models that approximate the objective and constraint functions locally in the search space. It searches for potential pareto optimal solutions within trust regions of the search space using only a few time-consuming simulations. The trust regions are adaptively updated during the optimization process based on feasibility and Hypervolume metrics. In contrast to mainstream Bayesian Optimization approaches, LoCoMOBO uses a new acquisition function that can provide multiple query points, therefore allowing for parallel execution of costly simulations. GP inference is also enhanced by using GPU acceleration in order to handle highly constrained problems that require large sample budgets. Combined with a framework for schematic parametrization and simulator calls, LoCoMOBO provides improved performance trade-offs and sizing results on three real-world circuit examples, while reducing the total run-time up to &#x00D7;43 times compared to state-of-the-art methods. IEEE",10.1109/TCAD.2021.3121263,Analog Sizing; Bayes methods; Bayesian Methods.; Computational modeling; Gaussian processes; Integrated circuit modeling; Machine Learning; Mathematical models; Optimization; Optimization; Radio frequency,,
Direct-Optimization-Based DC Dictionary Learning With the MCP Regularizer,"Li Z., Yang Z., Zhao H., Xie S.",IEEE Transactions on Neural Networks and Learning Systems,2021.0,"Direct-optimization-based dictionary learning has attracted increasing attention for improving computational efficiency. However, the existing direct optimization scheme can only be applied to limited dictionary learning problems, and it remains an open problem to prove that the whole sequence obtained by the algorithm converges to a critical point of the objective function. In this article, we propose a novel direct-optimization-based dictionary learning algorithm using the minimax concave penalty (MCP) as a sparsity regularizer that can enforce strong sparsity and obtain accurate estimation. For solving the corresponding optimization problem, we first decompose the nonconvex MCP into two convex components. Then, we employ the difference of the convex functions algorithm and the nonconvex proximal-splitting algorithm to process the resulting subproblems. Thus, the direct optimization approach can be extended to a broader class of dictionary learning problems, even if the sparsity regularizer is nonconvex. In addition, the convergence guarantee for the proposed algorithm can be theoretically proven. Our numerical simulations demonstrate that the proposed algorithm has good convergence performances in different cases and robust dictionary-recovery capabilities. When applied to sparse approximations, the proposed approach can obtain sparser and less error estimation than the different sparsity regularizers in existing methods. In addition, the proposed algorithm has robustness in image denoising and key-frame extraction. IEEE",10.1109/TNNLS.2021.3114400,Approximation algorithms; Convergence; Convergence analysis; Convex functions; Dictionaries; dictionary learning; direct optimization; Machine learning; minimax concave penalty (MCP) regularizer.; Optimization; Signal processing algorithms,1.0,
Adaptive constraint handling in optimization of complex structures by using machine learning,"Cai Y., Jelovica J.",Proceedings of the International Conference on Offshore Mechanics and Arctic Engineering - OMAE,2021.0,"Optimization of complex systems requires robust and computationally efficient global search algorithms. Constraints make this a very difficult task, significantly slowing down an algorithm, and can even prevent finding the true Pareto front. This study continues the development of a recently proposed repair approach that exploits infeasible designs to increase computational efficiency of a prominent genetic algorithm, and to find a wider spread of the Pareto front. This paper proposes adaptive and automatized discovery of sensitivity of constraints to variables, i.e. the link, which needed direct designer's input in the previous version of the repair approach. This is achieved by using machine learning in the form of artificial neural networks (ANN). A surrogate model is afterwards utilized in optimization based on ANN. The proposed approach is used for the recently proposed constraint handling implemented into NSGA-II optimization algorithm. The proposed framework is compared with two other constraint handling methods. The performance is analyzed on a structural optimization of a 178 m long chemical tanker which needs to fulfil class society's criteria for strength. The results show that the proposed framework is competitive in terms of convergence and spread of the front. This is achieved while discovering the link automatically using ANN, without an input from a user. In addition, computational time is reduced by 60%. © 2021 by ASME",10.1115/OMAE2021-62304,Artificial neural networks; Constraint-handling; Deep learning; Machine learning; Multi-objective optimization; Structural optimization,,
Ssdan: Multi-source semi-supervised domain adaptation network for remote sensing scene classification,"Lasloum T., Alhichri H., Bazi Y., Alajlan N.",Remote Sensing,2021.0,"We present a new method for multi-source semi-supervised domain adaptation in remote sensing scene classification. The method consists of a pre-trained convolutional neural network (CNN) model, namely EfficientNet-B3, for the extraction of highly discriminative features, followed by a classification module that learns feature prototypes for each class. Then, the classification module computes a cosine distance between feature vectors of target data samples and the feature prototypes. Finally, the proposed method ends with a Softmax activation function that converts the distances into class probabilities. The feature prototypes are also divided by a temperature parameter to normalize and control the classification module. The whole model is trained on both the unlabeled and labeled target samples. It is trained to predict the correct classes utilizing the standard cross-entropy loss computed over the labeled source and target samples. At the same time, the model is trained to learn domain invariant features using another loss function based on entropy computed over the unlabeled target samples. Unlike the standard cross-entropy loss, the new entropy loss function is computed on the model’s predicted probabilities and does not need the true labels. This entropy loss, called minimax loss, needs to be maximized with respect to the classification module to learn features that are domain-invariant (hence removing the data shift), and at the same time, it should be minimized with respect to the CNN feature extractor to learn discriminative features that are clustered around the class prototypes (in other words reducing intra-class variance). To accomplish these maximization and minimization processes at the same time, we use an adversarial training approach, where we alternate between the two processes. The model combines the standard cross-entropy loss and the new minimax entropy loss and optimizes them jointly. The proposed method is tested on four RS scene datasets, namely UC Merced, AID, RESISC45, and PatternNet, using two-source and three-source domain adaptation scenarios. The experimental results demonstrate the strong capability of the proposed method to achieve impressive performance despite using only a few (six in our case) labeled target samples per class. Its performance is already better than several state-of-the-art methods, including RevGrad, ADDA, Siamese-GAN, and MSCN. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",10.3390/rs13193861,Convolutional neural networks; Deep learning; Domain adaptation; EfficientNet-B3; Remote sensing; Semi-supervised scene classification,4.0,
On the Treatment of Optimization Problems with L1 Penalty Terms via Multiobjective Continuation,"Bieker K., Gebken B., Peitz S.",IEEE Transactions on Pattern Analysis and Machine Intelligence,2021.0,"We present a novel algorithm that allows us to gain detailed insight into the effects of sparsity in linear and nonlinear optimization. Sparsity is of great importance in many scientific areas such as image and signal processing, medical imaging, compressed sensing, and machine learning, as it ensures robustness against noisy data and yields models that are easier to interpret due to the small number of relevant terms. It is common practice to enforce sparsity by adding the l1-norm as a penalty term. In order to gain a better understanding and to allow for an informed model selection, we directly solve the corresponding multiobjective optimization problem (MOP) that arises when minimizing the main objective and the l1-norm simultaneously. As this MOP is in general non-convex for nonlinear objectives, the penalty method will fail to provide all optimal compromises. To avoid this issue, we present a continuation method specifically tailored to MOPs with two objective functions one of which is the l1-norm. Our method can be seen as a generalization of homotopy methods for linear regression problems to the nonlinear case. Several numerical examples - including neural network training- demonstrate our theoretical findings and the additional insight gained by this multiobjective approach. Author",10.1109/TPAMI.2021.3114962,Linear programming; Machine Learning; Mathematical models; Multiobjective Optimization; Neural networks; Nonsmooth Optimization; Optimization; Pareto optimization; Signal processing; Sparsity; Training,,
On-design component-level multiple-objective optimization of a small-scale cavity-stabilized combustor,"Briones A.M., Erdmann T.J., Rankin B.A.",Proceedings of the ASME Turbo Expo,2021.0,"This work presents an on-design component-level multiple-objective optimization of a small-scaled uncooled cavity-stabilized combustor. Optimization is performed at the maximum power condition of the engine thermodynamic cycle. The CFD simulations are managed by a supervised machine learning algorithm to divide a continuous and deterministic design space into non-dominated Pareto frontier and dominated design points. Steady, compressible three-dimensional simulations are performed using a multi-phase Realizable k-∈ RANS and non-adiabatic FPV combustion model. Conjugate heat transfer through the combustor liner is also considered. There are fifteen geometrical input parameters and four objective functions viz., maximization of combustion efficiency, and minimization of total pressure losses, pattern factor, and critical liner area factor. The baseline combustor design is based on engineering guidelines developed over the past two decades. The small-scale baseline design performs remarkably well. Direct optimization calculations are performed on this baseline design. In terms of Pareto optimality, the baseline design remains in the Pareto frontier throughout the optimization. However, the optimization calculations show improvement from an initial design point population to later iteration design points. The optimization calculations report other non-dominated designs in the Pareto frontier. The Euclidean distance from design points to the utopic point is used to select a ""best""and ""worst""design point for future fabrication and experimentation. The methodology to perform CFD optimization calculations of a small-scale uncooled combustor is expected to be useful for guiding the design and development of future gas turbine combustors. Copyright © 2021 by The United States Government.",10.1115/GT2021-60102,,1.0,
Multi-task Learning with Riemannian Optimization,"Cai T., Song L., Li G., Liao M.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021.0,"Multi-task learning (MTL) is a promising research field of machine learning, in which the training process of the neural network is equivalent to multi-objective optimization. On one hand, MTL trains all the network weights simultaneously to converge the multi-task loss. On the other hand, multi-objective optimization aims to find the optimum solution, which satisfies the constraints and optimizes the vector of objective functions. Therefore, the performance of MTL is dominated by the computation of the multi-objective solution. This paper proposes a method based on Riemannian optimization to solve the multi-objective optimization in MTL. Firstly, multi-objective optimization is reduced to its Karush-Kuhn-Tucker (KKT) condition as the optimum solution of constrained quadratic optimization. Secondly, by mapping the Euclidean space of the constraint into manifold, the quadratic optimization is transformed to an unconstrained problem. Finally, Riemannian optimization algorithm is used to compute the solution of this problem, which gives a Pareto direction towards the KKT condition. We perform experiments on the MultiMNIST and Fashion MNIST datasets, and the experimental results demonstrate the efficiency of our method. © 2021, Springer Nature Switzerland AG.",10.1007/978-3-030-84529-2_42,Multi-objective optimization; Multi-task learning; Riemannian optimization,,
Facing Many Objectives for Fairness in Machine Learning,"Villar D., Casillas J.",Communications in Computer and Information Science,2021.0,"Fairness is an increasingly important topic in the world of Artificial Intelligence. Machine learning techniques are widely used nowadays to solve huge amounts of problems, but those techniques may be biased against certain social groups due to different reasons. Using fair classification methods we can attenuate this discrimination source. Nevertheless, there are lots of valid fairness definitions which may be mutually incompatible. The aim of this paper is to propose a method which generates fair solutions for machine learning binary classification problems with one sensitive attribute. As we want accurate, fair and interpretable solutions, our method is based on Many Objective Evolutionary Algorithms (MaOEAs). The decision space will represent hyperparameters for training our classifiers, which will be decision trees, while the objective space will be a four-dimensional space representing the quality of the classifier in terms of an accuracy measure, two contradictory fairness criteria and an interpretability indicator. Experimentation have been done using four well known fairness datasets. As we will see, our algorithm generates good solutions compared to previous work, and a presumably well populated pareto-optimal population is found so that different classifiers could be used depending on our needs. © 2021, Springer Nature Switzerland AG.",10.1007/978-3-030-85347-1_27,Decision trees; Fairness in machine learning; Many objective evolutionary algorithm,,
Many-Objective Distribution Network Reconfiguration via Deep Reinforcement Learning Assisted Optimization Algorithm,"Li Y., Hao G., Liu Y., Yu Y., Ni Z., Zhao Y.",IEEE Transactions on Power Delivery,2021.0,"With the increasing penetration of renewable energy (RE), the operation of distribution network is threatened and some issues may appear, i.e., large voltage deviation, deterioration of statistic voltage stability, high power loss, etc. In turn, RE accommodation would be significantly impacted. Therefore, we propose a many-objective distribution network reconfiguration (MDNR) model, with the consideration of RE curtailment, voltage deviation, power loss, statistic voltage stability, and generation cost. This aims to assess the trade-off among these objectives for better operations of distribution networks. As this proposed model is a non-convex, non-linear, many-objective optimization problem, it is difficult to be solved. We further propose a deep reinforcement learning (DRL) assisted multi-objective bacterial foraging optimization (DRLMBFO) algorithm. This algorithm combines the advantages of DRL and MBFO, and is targeted to find the Pareto front of proposed MDNR model with better searching efficiency. Finally, case study based on a modified IEEE 33-bus distribution system verifies the effectiveness of MDNR model and outperformance of DRL-MBFO. IEEE",10.1109/TPWRD.2021.3107534,deep reinforcement learning; Distribution network reconfiguration; Distribution networks; Generators; many-objective optimization; Microorganisms; Optimization; Power system stability; Reinforcement learning; renewable energy; Renewable energy sources,2.0,
Minimax Approximation of Sign Function by Composite Polynomial for Homomorphic Comparison,"Lee E., Lee J., Kim Y., No J.",IEEE Transactions on Dependable and Secure Computing,2021.0,"The comparison operation for two numbers is one of the most frequently used operations in several applications, including deep learning. As such, lots of research has been conducted with the goal of efficiently evaluating the comparison operation in homomorphic encryption schemes. Recently, Cheon et al. (Asiacrypt 2020) proposed new comparison methods that approximated the sign function on homomorphically encrypted data using composite polynomials and proved that these methods had optimal asymptotic complexity. In this paper, we propose a practically optimal method that approximates the sign function using compositions of minimax approximation polynomials. We prove that this approximation method is optimal with respect to depth consumption and the number of non-scalar multiplications. In addition, we propose a polynomial-time algorithm that determines the optimal composition of minimax approximation polynomials for the proposed homomorphic comparison operation using dynamic programming. The numerical analysis demonstrates that when minimizing runtime, the proposed comparison operation reduces the runtime by approximately 45% on average when compared to the previous algorithm. Likewise, when minimizing depth consumption, the proposed algorithm reduces the runtime by approximately 41% on average. In addition, when high precision in the comparison operation is required, the previous algorithm does not achieve 128-bit security, while the proposed algorithm does due to its small depth consumption. IEEE",10.1109/TDSC.2021.3105111,Approximation algorithms; Cheon-Kim-Kim-Song (CKKS) scheme; Deep learning; Encryption; fully homomorphic encryption; Heuristic algorithms; homomorphic comparison operation; minimax approximation polynomial; Remez algorithm; Runtime; sign function; Sorting; Upper bound,,
Semantic Regularized Class-Conditional GANs for Semi-Supervised Fine-Grained Image Synthesis,"Chen T., Wu S., Yang X., Xu Y., Wong H.",IEEE Transactions on Multimedia,2021.0,"Learning effective generative models for natural image synthesis is a promising way to reduce the dependence of deep models on massive training data. This work focuses on Fine-Grained Image Synthesis (FGIS) in the semi-supervised setting where a small number of training instances are labeled. Different from generic image synthesis tasks, the available fine-grained data may be inadequate, and the differences among the object categories are typically subtle. To address these issues, we propose a Semantic Regularized class-conditional Generative Adversarial Network, which is referred to as SReGAN. We incorporate an additional discriminator and classifier into the generator-discriminator minimax game. Competing with two discriminators enforces the generator to model both marginal and class-conditional data distributions, which alleviates the problem of limited training data and labels. However, the discriminators may overlook the class separability. To induce the generator to discover the distinctions between classes, we construct semantically congruent and incongruent pairs in the generation process, and further regularize the generator by encouraging high similarities of congruent pairs, while penalizing that of incongruent ones in the classifier's feature space. We have conducted extensive experiments to verify the capability of SReGAN in generating high-fidelity images on a variety of FGIS benchmarks. IEEE",10.1109/TMM.2021.3091859,Data models; fine-grained image synthesis; Generative adversarial networks; generative adversarial networks; Generators; Image synthesis; semantic regularization; Semantics; Semi-supervised learning; Task analysis; Training,,
An Online Machine Learning-Based Prediction Strategy for Dynamic Evolutionary Multi-objective Optimization,"Liu M., Chen D., Zhang Q., Jiang L.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021.0,"Due to the impact of environmental changes, dynamic evolutionary multi-objective optimization algorithms need to track the time-varying Pareto optimal solution set of dynamic multi-objective optimization problems (DMOPs) as soon as possible by effectively mining historical data. Since online machine learning can help algorithms dynamically adapt to new patterns in the data in machine learning community, this paper introduces Passive-Aggressive Regression (PAR, a common online learning technology) into dynamic evolutionary multi-objective optimization research area. Specifically, a PAR-based prediction strategy is proposed to predict the new Pareto optimal solution set of the next environment. Furthermore, we integrate the proposed prediction strategy into the multi-objective evolutionary algorithm based on decomposition with a differential evolution operator (MOEA/D-DE) to handle DMOPs. Finally, the proposed prediction strategy is compared with three state-of-the-art prediction strategies under the same dynamic MOEA/D-DE framework on CEC2018 dynamic optimization competition problems. The experimental results indicate that the PAR-based prediction strategy is promising for dealing with DMOPs. © 2021, Springer Nature Switzerland AG.",10.1007/978-3-030-72062-9_16,Dynamic environment; Evolutionary multi-objective optimization; Online machine learning; Prediction strategy,1.0,
Predicting longitudinal dispersion coefficient in natural streams using minimax probability machine regression and multivariate adaptive regression spline,"Roy S.S., Samui P.",International Journal of Advanced Intelligence Paradigms,2021.0,"This article employs minimax probability machine regression (MPMR) and multivariate adaptive regression spline (MARS) for prediction of longitudinal dispersion coefficient in natural streams. The variables of hydraulic features such as channel width (B), flow depth (H), flow velocity (U), shear velocity (u*) and geometric features such as channel sinuosity (σ) and channel shape parameter (β) were taken as the input. The dispersion coefficient Kx was the decision parameter for the proposed machine learning models. MARS does not assume any functional relationship between inputs and output. The MARS model is a non-parametric regression model that splits the data and fits each interval into a basis function. MPMR is a probabilistic model which maximises the minimum probability of predicted output. MPMR also provides output within some bound of the true regression function. The proposed study gives an equation for prediction of longitudinal dispersion coefficient based on the developed MARS. The developed MARS has been compared with proposed MPMR. Finally, the performances of the models have been measured by different performance metrics. Copyright © 2021 Inderscience Enterprises Ltd.",10.1504/IJAIP.2021.115244,Longitudinal dispersion coefficient; MARS; Minimax probability machine regression; MPMR; Multivariate adaptive regression spline; Natural streams; Prediction,,
Resetting Weight Vectors in MOEA/D for Multiobjective Optimization Problems With Discontinuous Pareto Front,"Zhang C., Gao L., Li X., Shen W., Zhou J., Tan K.C.",IEEE Transactions on Cybernetics,2021.0,"When a multiobjective evolutionary algorithm based on decomposition (MOEA/D) is applied to solve problems with discontinuous Pareto front (PF), a set of evenly distributed weight vectors may lead to many solutions assembling in boundaries of the discontinuous PF. To overcome this limitation, this article proposes a mechanism of resetting weight vectors (RWVs) for MOEA/D. When the RWV mechanism is triggered, a classic data clustering algorithm DBSCAN is used to categorize current solutions into several parts. A classic statistical method called principal component analysis (PCA) is used to determine the ideal number of solutions in each part of PF. Thereafter, PCA is used again for each part of PF separately and virtual targeted solutions are generated by linear interpolation methods. Then, the new weight vectors are reset according to the interrelationship between the optimal solutions and the weight vectors under the Tchebycheff decomposition framework. Finally, taking advantage of the current obtained solutions, the new solutions in the decision space are updated via a linear interpolation method. Numerical experiments show that the proposed MOEA/D-RWV can achieve good results for bi-objective and tri-objective optimization problems with discontinuous PF. In addition, the test on a recently proposed MaF benchmark suite demonstrates that MOEA/D-RWV also works for some problems with other complicated characteristics. IEEE",10.1109/TCYB.2021.3062949,Clustering algorithms; DBSCAN; Evolutionary computation; Heuristic algorithms; Machine learning; Machine learning algorithms; multiobjective evolutionary algorithm (MOEA); multiobjective evolutionary algorithm based on decomposition (MOEA/D); Optimization; Principal component analysis; principal component analysis (PCA); weight vectors,,
Learning Implicit Generative Models by Teaching Density Estimators,"Xu K., Du C., Li C., Zhu J., Zhang B.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021.0,"Implicit generative models are difficult to train as no explicit density functions are defined. Generative adversarial nets (GANs) present a minimax framework to train such models, which however can suffer from mode collapse due to the nature of the JS-divergence. This paper presents a learning by teaching (LBT) approach to learning implicit models, which intrinsically avoids the mode collapse problem by optimizing a KL-divergence rather than the JS-divergence in GANs. In LBT, an auxiliary density estimator is introduced to fit the implicit model’s distribution while the implicit model teaches the density estimator to match the data distribution. LBT is formulated as a bilevel optimization problem, whose optimal generator matches the true data distribution. LBT can be naturally integrated with GANs to derive a hybrid LBT-GAN that enjoys complimentary benefits. Finally, we present a stochastic gradient ascent algorithm with unrolling to solve the challenging learning problems. Experimental results demonstrate the effectiveness of our method. © 2021, Springer Nature Switzerland AG.",10.1007/978-3-030-67661-2_15,Deep generative models; Generative adversarial nets; Mode collapse problem,1.0,
A Posteriori Preference Multi-objective Optimization Using Machine Learning,"Sun Z., Huang Y., Sun W., Chen Z.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021.0,"As a widely accepted way to solve multi-objective optimization problems (MOPs), evolutionary algorithms (EAs) can produce a well converged and well diverse Pareto Front (PF). However, only the partial PF around the decision maker (DM) preference is crucial in making decisions. The paper proposed an a posteriori method to help DMs to find solutions of interest (SOIs), i.e. solutions DMs interested in and to make well decisions. The proposed method is divided into three parts: the optimization part, learning part and operation part. With an EA, the optimization part works out optimal nondominated solutions. Then, the learning part trains an inverse mapping model according to the solutions. In the operation part, a set of probable preference vectors (PPVs) are generated to predict more SOIs. Finally, the feasibility of the proposed method is verified with the experiments on 2- and 3-objective test problems. © 2021, Springer Nature Switzerland AG.",10.1007/978-3-030-68884-4_40,A posteriori; Decision maker; Evolutionary computation; Multi-objective optimization; Preference; Random forest,,
Evaluating Theoretical Baselines for ML Benchmarking Across Different Accelerators,"Blott M., Vasilciuc A., Leeser M., Doyle L.",IEEE Design and Test,2021.0,"Benchmarking in Machine Learning (ML) becomes increasingly important as the design space complexity escalates, with a growing number of ML algorithms, optimization techniques and spectrum of novel hardware architectures. QuTiBench is a quantized, tiered benchmarking suite for ML on heterogeneous hardware. It is unique in that it provides a theoretical baseline for performance predictions across a broad spectrum of hardware architecture and takes optimizations into account. The goal of this theoretical baseline is to minimize the amount of experiments required and provide fast guidance on which hardware platforms with which optimizations work best for a given design task. In this article, we analyze how well this theoretical analysis can represent actual system behaviour. Our evaluation shows that the theoretical baselines can predict performance with a correlation coefficient between 0.64 and 0.96 across the selected hardware platforms. The optimization techniques are successfully represented and pareto-optimal design points instantly identified. Furthermore, we highlight a number of suggestions on how to improve prediction accuracy in the future. Thus, we believe that theoretical baselines can bring significant benefits to ML benchmarking in the future. IEEE",10.1109/MDAT.2021.3063340,Benchmark testing; Benchmarking; Deep Learning; FPGA; GPU; Hardware; Micromechanical devices; Neural Nets; Optimization; Performance evaluation; Task analysis; Topology,,
A novel approach for displacement interval forecasting of landslides with step-like displacement pattern,"Ge Q., Sun H., Liu Z., Yang B., Lacasse S., Nadim F.",Georisk,2021.0,"Quantifying the uncertainties in the prediction of landslide displacement is important for making reliable predictions and for managing landslide risk. This study develops a novel approach for the interval prediction (i.e. uncertainty) of landslide with step-like displacement pattern in the Three Gorges Reservoir (TGR) area using Density-Based Spatial Clustering of Applications with Noise (DBSCAN), Synthetic Minority Oversampling Technique and Edited Nearest Neighbor (SMOTEENN) based Random Forest (RF) and bootstrap-Multilayer Perceptron (MLPs). DBSCAN was employed to carry out clustering analysis for different deformation states of the landslide with step-like displacement pattern. The SMOTEENN based RF classifier was trained to deal with imbalanced classification problems. A dynamic switching prediction scheme to construct high-quality Prediction Intervals (PIs) using bootstrap-MLPs was established. The concepts of Pareto front and Knee point were adopted to select the PIs that could provide the best compromise between reliability and accuracy. The proposed DBSCAN-RF-bootstrap-MLP method is illustrated and verified with one typical landslide with step-like displacement pattern, the Bazimen landslide from the TGR area in China. The method showed to perform well and provides the uncertainties associated with landslide displacement prediction for decision making. © 2021 Informa UK Limited, trading as Taylor & Francis Group.",10.1080/17499518.2021.1892769,Landslide displacement prediction; landslide risk; machine learning; prediction intervals; switched prediction; uncertainty,2.0,
"Regional flood frequency analysis, using L-moments, artificial neural networks and ols regression, of various sites of Khyber-Pakhtunkhwa, Pakistan","Khan M.S.R., Hussain Z., Ahmad I.",Applied Ecology and Environmental Research,2021.0,"This study provides the results of flood frequency analysis adopting a regional approach using annual maxima’s of peak flows (APF) of eight catchments located on various small rivers of Khyber-Pakhtunkhwa, Pakistan. Initial screening reveals that the recorded data of APF for all catchments are independent, random, free from significant trend and identically distributed. L-moments based heterogeneity measure indicates that the study region is homogeneous. The results of |Z-Dist| statistic and L-moment ratio diagram being goodness of fit measures are in favor of Generalized Pareto (GPA) distribution among five candidates of regional distribution. For the ungauged sites, flood quantiles have been estimated through OLS regression and artificial neural networks (ANN). The estimated quantiles using ANN method are relatively accurate compared to OLS regression. The historical assessment indicates that quantile estimates obtained through ANN and index flood method are close to the highest recorded APF values for shorter as well as longer return periods for each site. © 2021, ALÖKI Kft., Budapest, Hungary.",10.15666/aeer/1901_471489,Annual maximum peaks; GPA distribution; L-moments; Least squares regression; Machine learning methods; Ungauged sites,,
An Adaptive Control for Surrogate Assisted Multi-objective Evolutionary Algorithms,"Nguyen D.D., Nguyen L.",Advances in Intelligent Systems and Computing,2021.0,"Multi-objective problems (MOPs), a class of optimization problems in the real-world, have multiple conflicting objectives. Multi-objective evolutionary algorithms (MOEAs) are known as great potential algorithms to solve difficult MOPs. With MOEAs, based on the principle of population, we have a set of optimal solutions (feasible solution set) after the search. We often use the concept of dominance relationship in population, and it is not difficult to find out set of Pareto optimal solutions during generations. However, with expensive optimization problems in the real world, it has to use a lot of fitness function evaluations during the search. To avoid expensive physical experiments, we can use computer simulations methods to solve the difficult MOPs. In fact, this way often costs expensive in computation and times for the simulation. In these cases, researchers discussed on the usage of surrogate models for evolutionary algorithms, especially for MOEAs to minimize the number of fitness callings. There are a series of proposals which were introduced with the usage of RBF, PRS, Kriging, SVM models. With the concept of machine learning, these MOEAs can solve expensive MOPS effectively. However, with our analysis, we found that using a fixed ratio of fitness functions and surrogate functions may make the unbalance of exploitation and exploration of the evolutionary process. In this paper, we suggest to use an adaptive control to determine the effective ratio during the search. The proposal is confirmed though an experiment with standard measurements on well-known benchmark sets. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",10.1007/978-981-15-8289-9_12,Adaptive control; K-RVEA; Kriging; Surrogate,1.0,
A semi-supervised method for the characterization of degradation of nuclear power plants steam generators,"Pinciroli L., Baraldi P., Shokry A., Zio E., Seraoui R., Mai C.",Progress in Nuclear Energy,2021.0,"The digitalization of nuclear power plants, with the rapid growth of information technology, opens the door to the development of new methods of condition-based maintenance. In this work, a semi-supervised method for characterizing the level of degradation of nuclear power plant components using measurements collected during plant operational transients is proposed. It is based on the fusion of selected features extracted from the monitored signals. Feature selection is formulated as a multi-objective optimization problem. The objectives are the maximization of the feature monotonicity and trendability, and the maximization of a novel measure of correlation between the feature values and the results of non-destructive tests performed to assess the component degradation. The features of the Pareto optimal set are normalized and the component degradation level is defined as the median of the obtained values. The developed method is applied to real data collected from steam generators of pressurized water reactors. It is shown able to identify degradation level with errors comparable to those obtained by ad-hoc non-destructive tests. © 2020 Elsevier Ltd",10.1016/j.pnucene.2020.103580,Condition-based maintenance; Degradation assessment; Feature selection; Nuclear power plant; Semi-supervised; Steam generator,3.0,
Communication of Design Space Relationships Learned by Bayesian Networks,"Wincott C., Collette M.",Lecture Notes in Civil Engineering,2021.0,"Modern ship design often involves the automated creation of thousands of design alternatives; even when provided a Pareto front of optimal solutions designers may struggle to understand the differences in designs and the relationships of design variables. Machine learning Bayesian networks from automatically developed design data can allow us to analyze the designs, understand the variable relationships that drive their differences and optimalities, and lead engineers to better designs. However, the information about variable relationships in Bayesian network are encoded in difficult to interpret conditional probability tables (CPTs). Translation of a Bayesian network’s CPTs into simpler edge weights defining the strength of relationship between nodes allows engineers to more easily interpret and use the complex information encoded in the network through standard network analysis techniques. Bayesian networks developed from a multi-objective bulk carrier design problem developed by Sen are transformed to network adjacency matrices for such analysis in this work. © 2021, Springer Nature Singapore Pte Ltd.",10.1007/978-981-15-4680-8_2,Bayesian networks; Design space exploration,,
Dimensionality reduction and the strange case of categorical data for predicting defective water meter devices,"Roccetti M., Casini L., Delnevo G., Bonfante S.",Advances in Intelligent Systems and Computing,2021.0,"Further to an experiment conducted with a deep learning (DL) model, tailored to predict whether a water meter device would fail with passage of time, we came across a very strange case, occurring when we tried to strengthen the training activity of our classifier by using, besides the numerical measurements of consumed water, also other contextual available information, of categorical type. Surprisingly, that further categorical information did not improve the prediction accuracy, which instead fell down, sensibly. Recognized the problem as a case of an excessive increase of the dimensions of the space of data under observation, with a correspondent loss of statistical significance, we changed the training strategy. Observing that every categorical variable followed a quasi-Pareto distribution, we re-trained our DL models, for each single categorical variable, only on that fraction of meter devices (and corresponding measurements of consumed water) that exhibited the most frequent qualitative values for that categorical variable. This new strategy yielded a prediction accuracy level never reached before, amounting to a value of 87–88% on average. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2021.",10.1007/978-3-030-55307-4_24,Human data science; Human-machine-bigdata interaction loop; Machine learning design; Water metering and consumption,1.0,
A new framework of multi-objective evolutionary algorithms for feature selection and multi-label classification of video data,"Karagoz G.N., Yazici A., Dokeroglu T., Cosar A.",International Journal of Machine Learning and Cybernetics,2021.0,"There are few studies in the literature to address the multi-objective multi-label feature selection for the classification of video data using evolutionary algorithms. Selecting the most appropriate subset of features is a significant problem while maintaining/improving the accuracy of the prediction results. This study proposes a framework of parallel multi-objective Non-dominated Sorting Genetic Algorithms (NSGA-II) for exploring a Pareto set of non-dominated solutions. The subsets of non-dominated features are extracted and validated by multi-label classification techniques, Binary Relevance (BR), Classifier Chains (CC), Pruned Sets (PS), and Random k-Labelset (RAkEL). Base classifiers such as Support Vector Machines (SVM), J48-Decision Tree (J48), and Logistic Regression (LR) are performed in the classification phase of the algorithms. Comprehensive experiments are carried out with local feature descriptors extracted from two multi-label data sets, the well-known MIR-Flickr dataset and a Wireless Multimedia Sensor (WMS) dataset that we have generated from our video recordings. The prediction accuracy levels are improved by 6.36% and 25.7% for the MIR-Flickr and WMS datasets respectively while the number of features is significantly reduced. The results verify that the algorithms presented in this new framework outperform the state-of-the-art algorithms. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",10.1007/s13042-020-01156-w,Evolutionary; Feature selection; Machine learning; Multi-label classification; Multi-objective optimization,6.0,
Reinforcement learning-based optimal complete water-blasting for autonomous ship hull corrosion cleaning system,"Le A.V., Kyaw P.T., Veerajagadheswar P., Muthugala M.A.V.J., Elara M.R., Kumar M., Khanh Nhan N.H.",Ocean Engineering,2021.0,"Routine cleaning of the corroded ship hulls in dry dock maintenance guarantees the smooth operation of the shipping industry. Deploying the autonomous system to remove the corrosion by water-blasting is a feasible approach to ease the burden in manual operation and to reduce water, time, and energy consumption. In this paper, the water-blasting framework is proposed for a novel robot platform named Hornbill with the adhesion mechanism by permanent magnetic, self-localization by sensor fusion to navigate smoothly on a vertical surface. Hence, we propose a complete waypoint path planning (CWPP) to re-blast the self-synthesizing deep convolutional neural network (DCNN) based corrosion heatmap by initial-blasting. The optimal CWPP problem, including the shortest travel distance and shortest travel time to save water, power while ensuring visiting all predefined waypoints by benchmarking output, is modeled as the classic Travel Salesman Problem (TSP). Further, the Pareto-optimal trajectory for given TSP has been driven by the reinforcement learning (RL) technique with a proposed reward function based on the robot's operation during blasting. From the experimental results at the shipyard site, the proposed RL-based CWPP generates the Pareto-optimal trajectory that enables the water-blasting robot to spend about 10% of energy and 9% of water less than the second-best evolutionary-based optimization method in various workspaces. © 2020 Elsevier Ltd",10.1016/j.oceaneng.2020.108477,Benchmarking blasting quality; Corrosion cleaning; Path planning; Reinforcement learning; Ship maintenance industry,7.0,
The multi-task learning with an application of Pareto improvement,"Cai T., Gao X., Song L., Liao M.",ACM International Conference Proceeding Series,2021.0,"Multi-task learning is a promising field in machine learning, which aims to improve the performance of multiple related learning tasks by taking advantage of useful information between them. Multi-task learning is essentially equivalent to multi-objective optimization problem, the purpose is to find the most appropriate weight, and because the performance of many deep learning systems based on multi-task learning largely depends on the relative weight of each task loss. It's a problem that we need to study how to calculate the weight value under some constraint conditions by reasonable method. Therefore, this paper employs a powerful method based on convex optimization theory, whose purpose is to find the Pareto optimal solution and get the specific task loss weight. The optimization process is closely related to the gradient in deep learning. In addition, to improve the accuracy, we add the modules of gradient normalization and weight standardization. The experimental results show that the performance of our method is better than that of single task experiment or multi-task experiment under fixed weight, and multi-task experiment based on uncertainty based adaptive learning, and the accuracy is further improved after adding the above modules. © 2021 ACM.",10.1145/3448734.3450463,multi-objective optimization; Multi-task learning; Pareto improvement,,
Synthesizing optimal collective algorithms,"Cai Z., Liu Z., Maleki S., Musuvathi M., Mytkowicz T., Nelson J., Saarikivi O.","Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPOPP",2021.0,"Collective communication algorithms are an important component of distributed computation. Indeed, in the case of deep-learning, collective communication is the Amdahl's bottleneck of data-parallel training. This paper introduces SCCL (for Synthesized Collective Communication Library), a systematic approach to synthesizing collective communication algorithms that are explicitly tailored to a particular hardware topology. SCCL synthesizes algorithms along the Pareto-frontier spanning from latency-optimal to bandwidth-optimal implementations of a collective. The paper demonstrates how to encode the synthesis problem as a quantifier-free SMT formula which can be discharged to a theorem prover. We show how our carefully built encoding enables SCCL to scale. We synthesize novel latency and bandwidth optimal algorithms not seen in the literature on two popular hardware topologies. We also show how SCCL efficiently lowers algorithms to implementations on two hardware architectures (NVIDIA and AMD) and demonstrate competitive performance with hand optimized collective communication libraries. © 2021 ACM.",10.1145/3437801.3441620,collective communication; GPU; interconnection; network; synthesis,,
Adaptative DNN emulator-enabled multi-objective optimization to manage aquifer−sea flux interactions in a regional coastal aquifer,"Yu X., Sreekanth J., Cui T., Pickett T., Xin P.",Agricultural Water Management,2021.0,"This study focuses on the analyses of influx and efflux of groundwater at the aquifer−sea interface in response to the total groundwater extraction from a regional coastal aquifer. The groundwater planning and management goal is formulated as a multi-objective optimization problem to optimize the total pumping, groundwater influx and efflux through the coastal boundary. A four-stage optimization strategy is implemented for solving this optimization problem, whereby the first three stages are the iterative optimizations using the proposed Multi-Objective Particle Swarm Optimization algorithm and the analysis of Pareto-optimal solutions, and the last stage is the selection of one bargaining solution using the Kalai-Smorodinsky approach considering compromises among multiple objectives. In order to improve the efficiency of the simulation-optimization model, Deep Neural Networks emulators are fitted to approximate individual optimization objective, and a novel dynamic sampling strategy is applied to adaptively improve the accuracy of the emulators. This study demonstrates that accurate and efficient emulators can be achieved for the regional coastal aquifer with zone-based pumping rate multipliers from eight bands (Band 1 to Band 8) of increasing distance from the coastal boundary as the decision variables. The results from the Pareto-front suggest that the abstractions of Band 2 (close to the sea) can be reduced to the lower boundary of the rate multiplier (0.5) whereas the abstraction of farther bands can be significantly enhanced. The water influx through the coastal boundary was decreased by 15.69% under the slight compromise of the total pumping and water efflux by the selected compromising pumping pattern in the regional model. The final Pareto-optimal solution set and the compromised solution provide valuable information for the groundwater manager to plan sustainable groundwater use. The proposed simulation-optimization approach used in this study can be applied for a wide range of groundwater management problems. © 2020 Elsevier B.V.",10.1016/j.agwat.2020.106571,Deep neural networks; Kalai−Smorodinsky bargaining solution; Machine learning; Multi-Objective Particle Swarm Optimization; Pumping optimization; Regional groundwater modelling,3.0,
Random forest swarm optimization-based for heart diseases diagnosis,"Asadi S., Roshan S., Kattan M.W.",Journal of Biomedical Informatics,2021.0,"Heart disease has been one of the leading causes of death worldwide in recent years. Among diagnostic methods for heart disease, angiography is one of the most common methods, but it is costly and has side effects. Given the difficulty of heart disease prediction, data mining can play an important role in predicting heart disease accurately. In this paper, by combining the multi-objective particle swarm optimization (MOPSO) and Random Forest, a new approach is proposed to predict heart disease. The main goal is to produce diverse and accurate decision trees and determine the (near) optimal number of them simultaneously. In this method, an evolutionary multi-objective approach is used instead of employing a commonly used approach, i.e., bootstrap, feature selection in the Random Forest, and random number selection of training sets. By doing so, different training sets with different samples and features for training each tree are generated. Also, the obtained solutions in Pareto-optimal fronts determine the required number of training sets to build the random forest. By doing so, the random forest's performance can be enhanced, and consequently, the prediction accuracy will be improved. The proposed method's effectiveness is investigated by comparing its performance over six heart datasets with individual and ensemble classifiers. The results suggest that the proposed method with the (near) optimal number of classifiers outperforms the random forest algorithm with different classifiers. © 2021 Elsevier Inc.",10.1016/j.jbi.2021.103690,Data mining; Diversity; Ensemble learning; Heart disease; Random forest,5.0,
SAP-cGAN: Adversarial learning for breast mass segmentation in digital mammogram based on superpixel average pooling,"Li Y., Zhao G., Zhang Q., Lin Y., Wang M.",Medical Physics,2021.0,"Purpose: Breast mass segmentation is a prerequisite step in the use of computer-aided tools designed for breast cancer diagnosis and treatment planning. However, mass segmentation remains challenging due to the low contrast, irregular shapes, and fuzzy boundaries of masses. In this work, we propose a mammography mass segmentation model for improving segmentation performance. Methods: We propose a mammography mass segmentation model called SAP-cGAN, which is based on an improved conditional generative adversarial network (cGAN). We introduce a superpixel average pooling layer into the cGAN decoder, which utilizes superpixels as a pooling layout to improve boundary segmentation. In addition, we adopt a multiscale input strategy to enable the network to learn scale-invariant features with increased robustness. The performance of the model is evaluated with two public datasets: CBIS-DDSM and INbreast. Moreover, ablation analysis is conducted to evaluate further the individual contribution of each block to the performance of the network. Results: Dice and Jaccard scores of 93.37% and 87.57%, respectively, are obtained for the CBIS-DDSM dataset. The Dice and Jaccard scores for the INbreast dataset are 91.54% and 84.40%, respectively. These results indicate that our proposed model outperforms current state-of-the-art breast mass segmentation methods. The superpixel average pooling layer and multiscale input strategy has improved the Dice and Jaccard scores of the original cGAN by 7.8% and 12.79%, respectively. Conclusions: Adversarial learning with the addition of a superpixel average pooling layer and multiscale input strategy can encourage the Generator network to generate masks with increased realism and improve breast mass segmentation performance through the minimax game between the Generator network and Discriminator network. © 2020 American Association of Physicists in Medicine",10.1002/mp.14671,breast mass segmentation; generative adversarial network; multiscale features; superpixel pooling,2.0,
Analysis of the rate of convergence of fully connected deep neural network regression estimates with smooth activation function,Langer S.,Journal of Multivariate Analysis,2021.0,"This article contributes to the current statistical theory of deep neural networks (DNNs). It was shown that DNNs are able to circumvent the so-called curse of dimensionality in case that suitable restrictions on the structure of the regression function hold. In most of those results the tuning parameter is the sparsity of the network, which describes the number of non-zero weights in the network. This constraint seemed to be the key factor for the good rate of convergence results. Recently, the assumption was disproved. In particular, it was shown that simple fully connected DNNs can achieve the same rate of convergence. Those fully connected DNNs are based on the unbounded ReLU activation function. In this article we extend the results to smooth activation functions, i.e., to the sigmoid activation function. It is shown that estimators based on fully connected DNNs with sigmoid activation function also achieve the minimax rates of convergence (up to lnn-factors). In our result the number of hidden layers is fixed, the number of neurons per layer tends to infinity for sample size tending to infinity and a bound for the weights in the network is given. © 2020 Elsevier Inc.",10.1016/j.jmva.2020.104695,Curse of dimensionality; Deep learning; Neural networks; Nonparametric regression; Rate of convergence,7.0,
Mitigating bias in set selection with noisy protected attributes,"Mehrotra A., Celis L.E.","FAccT 2021 - Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",2021.0,"Subset selection algorithms are ubiquitous in AI-driven applications, including, online recruiting portals and image search engines, so it is imperative that these tools are not discriminatory on the basis of protected attributes such as gender or race. Currently, fair subset selection algorithms assume that the protected attributes are known as part of the dataset. However, protected attributes may be noisy due to errors during data collection or if they are imputed (as is often the case in real-world settings). While a wide body of work addresses the effect of noise on the performance of machine learning algorithms, its effect on fairness remains largely unexamined. We find that in the presence of noisy protected attributes, in attempting to increase fairness without considering noise, one can, in fact, decrease the fairness of the result! Towards addressing this, we consider an existing noise model in which there is probabilistic information about the protected attributes (e.g., [19, 32, 44, 56]), and ask is fair selection possible under noisy conditions? We formulate a ""denoised""selection problem which functions for a large class of fairness metrics; given the desired fairness goal, the solution to the denoised problem violates the goal by at most a small multiplicative amount with high probability. Although this denoised problem turns out to be NP-hard, we give a linear-programming based approximation algorithm for it. We evaluate this approach on both synthetic and real-world datasets. Our empirical results show that this approach can produce subsets which significantly improve the fairness metrics despite the presence of noisy protected attributes, and, compared to prior noise-oblivious approaches, has better Pareto-tradeoffs between utility and fairness. © 2021 ACM.",10.1145/3442188.3445887,,,
Agent Decision Processes Using Double Deep Q-Networks + Minimax Q- Learning,"Fitch N., Clancy D.",IEEE Aerospace Conference Proceedings,2021.0,"Aerospace battle scenarios represent a challenging modeling effort, often requiring large, continuous, and simultaneous state and/ or action spaces with imperfect information. We model a battle as a Multi-Stage Markov Stochastic Game (MSMSG) and facilitate agent decision making using a Double Deep Q-Network (DDQN) paradigm with Minimax Q-Learning. We demonstrate our model performance in contrast with a DDQN agent trained using a traditional Q-learning algorithm in a 1D dynamic battle environment. Preliminary findings suggest that the DDQN + Minimax-Q agent is more robust to parameter tuning and can learn true optimal mixed strategies compared to its traditional Q-learning counterpart. © 2021 IEEE.",10.1109/AERO50100.2021.9438149,,,
Coverage path planning using reinforcement learning-based tsp for htetran — A polyabolo-inspired self-reconfigurable tiling robot,"Le A.V., Veerajagadheswar P., Kyaw P.T., Elara M.R., Nhan N.H.K.",Sensors,2021.0,"One of the critical challenges in deploying the cleaning robots is the completion of covering the entire area. Current tiling robots for area coverage have fixed forms and are limited to cleaning only certain areas. The reconfigurable system is the creative answer to such an optimal coverage problem. The tiling robot’s goal enables the complete coverage of the entire area by reconfiguring to different shapes according to the area’s needs. In the particular sequencing of navigation, it is essential to have a structure that allows the robot to extend the coverage range while saving energy usage during navigation. This implies that the robot is able to cover larger areas entirely with the least required actions. This paper presents a complete path planning (CPP) for hTetran, a polyabolo tiled robot, based on a TSP-based reinforcement learning optimization. This structure simultaneously produces robot shapes and sequential trajectories whilst maximizing the reward of the trained reinforcement learning (RL) model within the predefined polyabolo-based tileset. To this end, a reinforcement learning-based travel sales problem (TSP) with proximal policy optimization (PPO) algorithm was trained using the complementary learning computation of the TSP sequencing. The reconstructive results of the proposed RL-TSP-based CPP for hTetran were compared in terms of energy and time spent with the conventional tiled hypothetical models that incorporate TSP solved through an evolutionary based ant colony optimization (ACO) approach. The CPP demonstrates an ability to generate an ideal Pareto optima trajectory that enhances the robot’s navigation inside the real environment with the least energy and time spent in the company of conventional techniques. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",10.3390/s21082577,"Energy-aware reward function; Reconfigurable system; Reinforcement learning TSP, complete path planning; Tiling robotic",6.0,
Cyber claim analysis using Generalized Pareto regression trees with applications to insurance,"Farkas S., Lopez O., Thomas M.",Insurance: Mathematics and Economics,2021.0,"With the rise of the cyber insurance market, there is a need for better quantification of the economic impact of this risk and its rapid evolution. Due to the heterogeneity of cyber claims, evaluating the appropriate premium and/or the required amount of reserves is a difficult task. In this paper, we propose a method for cyber claim analysis based on regression trees to identify criteria for claim classification and evaluation. We particularly focus on severe/extreme claims, by combining a Generalized Pareto modeling – legitimate from Extreme Value Theory – and a regression tree approach. Coupled with an evaluation of the frequency, our procedure allows computations of central scenarios and of extreme loss quantiles for a cyber portfolio. Finally, the method is illustrated on a public database. © 2021 Elsevier B.V.",10.1016/j.insmatheco.2021.02.009,Clustering; Cyber insurance; Extreme value analysis; Generalized Pareto distribution; Machine learning; Regression trees,3.0,
Multi-objective genetic programming for feature learning in face recognition,"Bi Y., Xue B., Zhang M.",Applied Soft Computing,2021.0,"Face recognition is a challenging task due to high variations of pose, expression, ageing, and illumination. As an effective approach to face recognition, feature learning can be formulated as a multi-objective optimisation task of maximising classification accuracy and minimising the number of learned features. However, most of the existing algorithms focus on improving classification accuracy without considering the number of learned features. In this paper, we propose new multi-objective genetic programming (GP) algorithms for feature learning in face recognition. To achieve effective face feature learning, a new individual representation is developed to allow GP to select informative regions from the input image, extract features using various descriptors, and combine the extracted features for classification. Then two new multi-objective genetic programming (GP) algorithms, one with the idea of non-dominated sorting (NSGPFL) and the other with the idea of Strength Pareto (SPGPFL), are proposed to simultaneously optimise these two objectives. NSGPFL and SPGPFL are compared with a single-objective GP for feature learning (GPFL), a single-objective GP for weighting two objectives (GPFLW), and a large number of baseline methods. The experimental results show the effectiveness of the NSGPFL and SPGPFL algorithms by achieving better or comparable classification performance and learning a small number of features. © 2021 Elsevier B.V.",10.1016/j.asoc.2021.107152,Evolutionary computation; Face recognition; Feature learning; Genetic programming; Multi-objective optimisation,3.0,
Exploring multiobjective training in multiclass classification,"Raimundo M.M., Drumond T.F., Marques A.C.R., Lyra C., Rocha A., Von Zuben F.J.",Neurocomputing,2021.0,"Multinomial logistic loss and L2 regularization are often conflicting objectives as more robust regularization leads to restrained multinomial parameters. For many practical problems, leveraging the best of both worlds would be invaluable for better decision-making processes. This research proposes a novel framework to obtain representative and diverse L2-regularized multinomial models, based on valuable trade-offs between prediction error and model complexity. The framework relies upon the Non-Inferior Set Estimation (NISE) method – a deterministic multiobjective solver. NISE automatically implements hyperparameter tuning in a multiobjective context. Given the diverse set of efficient learning models, model selection and aggregation of the multiple models in an ensemble framework promote high performance in multiclass classification. Additionally, NISE uses the weighted sum method as scalarization, thus being able to deal with the learning formulation directly. Its deterministic nature and the convexity of the learning problem confer scalability to the proposal. The experiments show competitive performance in various setups, taking a broad set of multiclass classification methods as contenders. © 2021 Elsevier B.V.",10.1016/j.neucom.2020.12.087,Diversity of Pareto-optimal models; Ensemble learning; Multiclass classification; Multiobjective optimization,1.0,
DeResolver: A decentralized negotiation and conflict resolution framework for smart city services,"Yuan Y., Ma M., Han S., Zhang D., Miao F., Stankovic J., Lin S.",ICCPS 2021 - Proceedings of the 2021 ACM/IEEE 12th International Conference on Cyber-Physical Systems (with CPS-IoT Week 2021),2021.0,"As various smart services are increasingly deployed in modern cities, many unexpected conflicts arise due to various physical world couplings. Existing solutions for conflict resolution often rely on centralized control to enforce predetermined and fixed priorities of different services, which is challenging due to the inconsistent and private objectives of the services. Also, the centralized solutions miss opportunities to more effectively resolve conflicts according to their spatiotemporal locality of the conflicts. To address this issue, we design a decentralized negotiation and conflict resolution framework named DeResolver, which allows services to resolve conflicts by communicating and negotiating with each other to reach a Pareto-optimal agreement autonomously and efficiently. Our design features a two-level semi-supervised learning-based algorithm to predict acceptable proposals and their rankings of each opponent through the negotiation. Our design is evaluated with a smart city case study of three services: intelligent traffic light control, pedestrian service, and environmental control. In this case study, a data-driven evaluation is conducted using a large data set consisting of the GPS locations of 246 surveillance cameras and an automatic traffic monitoring system with more than 3 million records per day to extract real-world vehicle routes. The evaluation results show that our solution achieves much more balanced results, i.e., only increasing the average waiting time of vehicles, the measurement metric of intelligent traffic light control service, by 6.8% while reducing the weighted sum of air pollutant emission, measured for environment control service, by 12.1%, and the pedestrian waiting time, the measurement metric of pedestrian service, by 33.1%, compared to priority-based solution. © 2021 ACM.",10.1145/3450267.3450538,conflicts across services; decentralized resolution; multiple services negotiation; smart services,,
Designing optimized ternary catalytic alloy electrode for efficiency improvement of semiconductor gas sensors using a machine learning approach,"Ghosal S., Dey S., Chattopadhyay P.P., Datta S., Bhattacharyya P.",Decision Making: Applications in Management and Engineering,2021.0,"Catalytic noble metal (s) or its alloy (s) has long been used as the electrode material to enhance the sensing performance of the semiconducting oxide-based gas sensors. In the present paper, optimized ternary metal alloy electrode has been designed, while the database is in pure or binary alloy compositions, using a machine learning methodology is reported for detection of CH4 gas as a test case. Pure noble metals or their binary alloys as the electrode on the semiconducting ZnO sensing layer were investigated by the earlier researchers to enhance the sensitivity towards CH4. Based on those research findings, an artificial neural network (ANN) model was developed considering the three main features of the gas sensor devices, viz. response magnitude, response time and recovery time as a function of ZnO particle size and the composition of the catalytic alloy. A novel methodology was introduced by using ANN models considered for optimized ternary alloy with enriched presentation through the multi-objective genetic algorithm (GA) wherever the generated Pareto front was used. The prescriptive data analytics methodology seems to offer more or less convinced evidence for future experimental studies. © 2018 by the authors.",10.31181/DMAME210402126G,Artificial neural network; Genetic algorithm; Multi-objective optimization; Oxide based gas sensor; Sensing parameters; Ternary alloy catalyst design,1.0,
Lexicographically fair learning: Algorithms and generalization,"Diana E., Gill W., Globus-Harris I., Kearns M., Roth A., Sharifi-Malvajerdi S.","Leibniz International Proceedings in Informatics, LIPIcs",2021.0,"We extend the notion of minimax fairness in supervised learning problems to its natural conclusion: lexicographic minimax fairness (or lexifairness for short). Informally, given a collection of demographic groups of interest, minimax fairness asks that the error of the group with the highest error be minimized. Lexifairness goes further and asks that amongst all minimax fair solutions, the error of the group with the second highest error should be minimized, and amongst all of those solutions, the error of the group with the third highest error should be minimized, and so on. Despite its naturalness, correctly defining lexifairness is considerably more subtle than minimax fairness, because of inherent sensitivity to approximation error. We give a notion of approximate lexifairness that avoids this issue, and then derive oracle-efficient algorithms for finding approximately lexifair solutions in a very general setting. When the underlying empirical risk minimization problem absent fairness constraints is convex (as it is, for example, with linear and logistic regression), our algorithms are provably efficient even in the worst case. Finally, we show generalization bounds - approximate lexifairness on the training sample implies approximate lexifairness on the true distribution with high probability. Our ability to prove generalization bounds depends on our choosing definitions that avoid the instability of naive definitions. © Emily Diana, Wesley Gill, Ira Globus-Harris, Michael Kearns, Aaron Roth, and Saeed Sharifi-Malvajerdi; licensed under Creative Commons License CC-BY 4.0 2nd Symposium on Foundations of Responsible Computing (FORC 2021).",10.4230/LIPIcs.FORC.2021.6,Fair learning; Game theory; Lexicographic fairness; Online learning,,
Deep Reinforcement Learning for Multiobjective Optimization,"Li K., Zhang T., Wang R.",IEEE Transactions on Cybernetics,2021.0,"This article proposes an end-to-end framework for solving multiobjective optimization problems (MOPs) using deep reinforcement learning (DRL), that we call DRL-based multiobjective optimization algorithm (DRL-MOA). The idea of decomposition is adopted to decompose the MOP into a set of scalar optimization subproblems. Then, each subproblem is modeled as a neural network. Model parameters of all the subproblems are optimized collaboratively according to a neighborhood-based parameter-transfer strategy and the DRL training algorithm. Pareto-optimal solutions can be directly obtained through the trained neural-network models. Specifically, the multiobjective traveling salesman problem (MOTSP) is solved in this article using the DRL-MOA method by modeling the subproblem as a Pointer Network. Extensive experiments have been conducted to study the DRL-MOA and various benchmark methods are compared with it. It is found that once the trained model is available, it can scale to newly encountered problems with no need for retraining the model. The solutions can be directly obtained by a simple forward calculation of the neural network; thereby, no iteration is required and the MOP can be always solved in a reasonable time. The proposed method provides a new way of solving the MOP by means of DRL. It has shown a set of new characteristics, for example, strong generalization ability and fast solving speed in comparison with the existing methods for multiobjective optimizations. The experimental results show the effectiveness and competitiveness of the proposed method in terms of model performance and running time. © 2013 IEEE.",10.1109/TCYB.2020.2977661,Deep reinforcement learning (DRL); multiobjective optimization; Pointer Network; traveling salesman problem,9.0,
TPOT-NN: augmenting tree-based automated machine learning with neural network estimators,"Romano J.D., Le T.T., Fu W., Moore J.H.",Genetic Programming and Evolvable Machines,2021.0,"Automated machine learning (AutoML) and artificial neural networks (ANNs) have revolutionized the field of artificial intelligence by yielding incredibly high-performing models to solve a myriad of inductive learning tasks. In spite of their successes, little guidance exists on when to use one versus the other. Furthermore, relatively few tools exist that allow the integration of both AutoML and ANNs in the same analysis to yield results combining both of their strengths. Here, we present TPOT-NN—a new extension to the tree-based AutoML software TPOT—and use it to explore the behavior of automated machine learning augmented with neural network estimators (AutoML+NN), particularly when compared to non-NN AutoML in the context of simple binary classification on a number of public benchmark datasets. Our observations suggest that TPOT-NN is an effective tool that achieves greater classification accuracy than standard tree-based AutoML on some datasets, with no loss in accuracy on others. We also provide preliminary guidelines for performing AutoML+NN analyses, and recommend possible future directions for AutoML+NN methods research, especially in the context of TPOT. © 2021, The Author(s).",10.1007/s10710-021-09401-z,Artificial neural networks; Automated machine learning; Evolutionary algorithms; Genetic programming; Pareto optimization,3.0,
Pareto-optimal progressive neural architecture search,"Lomurno E., Samele S., Matteucci M., Ardagna D.",GECCO 2021 Companion - Proceedings of the 2021 Genetic and Evolutionary Computation Conference Companion,2021.0,"Neural Architecture Search (NAS) is the process of automating architecture engineering, searching for the best deep learning configuration. One of the main NAS approaches proposed in the literature, Progressive Neural Architecture Search (PNAS), seeks for the architectures with a sequential model-based optimization strategy: it defines a common recursive structure to generate the networks, whose number of building blocks rises through iterations. However, NAS algorithms are generally designed for an ideal setting without considering the needs and the technical constraints imposed by practical applications. In this paper, we propose a new architecture search named Pareto-Optimal Progressive Neural Architecture Search (POPNAS) that combines the benefits of PNAS to a time-accuracy Pareto optimization problem. POPNAS adds a new time predictor to the existing approach to carry out a joint prediction of time and accuracy for each candidate neural network, searching through the Pareto front. This allows us to reach a trade-off between accuracy and training time, identifying neural network architectures with competitive accuracy in the face of a drastically reduced training time. © 2021 ACM.",10.1145/3449726.3463146,convolution; deep learning; machine learning; NAS; Pareto optimality; PNAS; POPNAS,1.0,
An approximate MIP-DoM calculation for multi-objective optimization using affinity propagation clustering algorithm,"Lopes C.L.V., Martins F.V.C., Wanner E.F., Deb K.",GECCO 2021 Companion - Proceedings of the 2021 Genetic and Evolutionary Computation Conference Companion,2021.0,"Dominance move (DoM) is a quality indicator that compares two solution sets in a Pareto-optimal sense. The main issue related to DoM is its computational expense. A recent paper proposed a mixed-integer programming (MIP) approach for computing DoM that exhibited a computational complexity that is linear to the number of objectives and polynomial to the number of solutions. Even with this property, considering practical situations, the MIP-DoM calculation on some problems may take many hours. This paper presents an approximation method to deal with the problem using a cluster-based and divide-and-conquer strategy. Some experiments are tested, showing that the cluster based-algorithm is computationally much faster and makes a small percentage error from the original DoM value. © 2021 Owner/Author.",10.1145/3449726.3459445,cluster algorithms; computationally expensive optimization; evolutionary multi-objective optimization; machine learning; multi-objective optimization,,
Rawlsian Fair Adaptation of Deep Learning Classifiers,"Shah K., Gupta P., Deshpande A., Bhattacharyya C.","AIES 2021 - Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",2021.0,"Group-fairness in classification aims for equality of a predictive utility across different sensitive sub-populations, e.g., race or gender. Equality or near-equality constraints in group-fairness often worsen not only the aggregate utility but also the utility for the least advantaged sub-population. In this paper, we apply the principles of Pareto-efficiency and least-difference to the utility being accuracy, as an illustrative example, and arrive at the Rawls classifier that minimizes the error rate on the worst-off sensitive sub-population. Our mathematical characterization shows that the Rawls classifier uniformly applies a threshold to an ideal score of features, in the spirit of fair equality of opportunity. In practice, such a score or a feature representation is often computed by a black-box model that has been useful but unfair. Our second contribution is practical Rawlsian fair adaptation of any given black-box deep learning model, without changing the score or feature representation it computes. Given any score function or feature representation and only its second-order statistics on the sensitive sub-populations, we seek a threshold classifier on the given score or a linear threshold classifier on the given feature representation that achieves the Rawls error rate restricted to this hypothesis class. Our technical contribution is to formulate the above problems using ambiguous chance constraints, and to provide efficient algorithms for Rawlsian fair adaptation, along with provable upper bounds on the Rawls error rate. Our empirical results show significant improvement over state-of-the-art group-fair algorithms, even without retraining for fairness. © 2021 ACM.",10.1145/3461702.3462592,fair adaptation; fairness for deep learning classifiers; Rawlsian fairness,,
Minimax Group Fairness: Algorithms and Experiments,"Diana E., Gill W., Kearns M., Kenthapadi K., Roth A.","AIES 2021 - Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",2021.0,"We consider a recently introduced framework in which fairness is measured by worst-case outcomes across groups, rather than by the more standard differences between group outcomes. In this framework we provide provably convergent oracle-efficient learning algorithms (or equivalently, reductions to non-fair learning) for minimax group fairness. Here the goal is that of minimizing the maximum loss across all groups, rather than equalizing group losses. Our algorithms apply to both regression and classification settings and support both overall error and false positive or false negative rates as the fairness measure of interest. They also support relaxations of the fairness constraints, thus permitting study of the tradeoff between overall accuracy and minimax fairness. We compare the experimental behavior and performance of our algorithms across a variety of fairness-sensitive data sets and show empirical cases in which minimax fairness is strictly and strongly preferable to equal outcome notions. © 2021 ACM.",10.1145/3461702.3462523,fair machine learning; game theory; minimax fairness,3.0,
Advancement of vehicle occupant restraint system design by integration of artificial intelligence technologieS,Horii H.,International Journal of Transport Development and Integration,2021.0,"In order to improve the design method of vehicle occupant restraint systems, it is necessary to reduce the computational load of simulations, to improve the global search capability, and to examine and integrate analytical methods to understand the complex interaction between design variables and objective functions. Therefore, in this study, we integrated the following three artificial intelligence technologies and applied them to the design of a vehicle occupant restraint system: (1) construction of a highly accurate approximate model by machine learning, (2) improvement of global search capability by evolutionary multi-objective optimization and (3) visualization and knowledge acquisition of multidimensional information using multivariate analysis methods. First, we obtained the minimum number of actual calculation samples using a crash analysis model with the design of experiments, and then used these samples to construct a highly accurate approximate model using machine learning. Next, we used the approximate model to perform a global search in the design space by evolutionary multi-objective optimization to obtain a Pareto solution set that takes into account the trade-off relationship between the objective functions. Finally, multivariate analysis using cluster analysis and self-organizing maps was performed on the Pareto solution set. As a result, a fast global search was realized by substituting the evaluation calculation of evolutionary multi-objective optimization with a highly accurate approximate model. The Pareto solution set obtained therein was then partitioned into clusters by cluster analysis, and the partitioned clusters were analysed by self-organizing maps, which provided perceptual information on the factors governing the trade-offs between the objective functions and the interactions between the design variables, and were useful for design engineers' insights. © 2021 WIT Press.",10.2495/TDI-V5-N3-242-253,Cluster analysis; Evolutionary computation; Machine learning; Multi-objective optimization; Self-organizing maps; Vehicle occupant restraint system,1.0,
Multi-object aerodynamic design optimization using deep reinforcement learning,"Hui X., Wang H., Li W., Bai J., Qin F., He G.",AIP Advances,2021.0,"Aerodynamic design optimization is a key aspect in aircraft design. The further evolution of advanced aircraft derivatives requires a powerful optimization toolbox. Reinforcement learning (RL) is a powerful optimization tool but has rarely been utilized in the aerodynamic design. It can potentially obtain results similar to those of a human designer, by accumulating experience from training. In this work, a popular RL method called proximal policy optimization (PPO) is proposed to investigate multi-object aerodynamic design optimization. By observing the aerodynamic performances of different airfoils, the PPO updates a reasonable policy to generate the optimal airfoils in a single step. In a Pareto optimization problem with constraints, the PPO requires only 15% of the computational time of the non-dominated sorted genetic algorithm (II) to achieve the same accuracy. The results from testing show that the agent learns a policy that can achieve ∼4.3%-10.1% improvements of the aerodynamic performance compared with the results of baseline. © 2021 Author(s).",10.1063/5.0058088,,,
On Pareto-Optimal Boolean Logical Patterns for Numerical Data,"Guo C., Ryoo H.S.",Applied Mathematics and Computation,2021.0,"This paper clarifies the difference between intrinsically 0–1 data and binarized numerical data for Boolean logical patterns and strengthens mathematical results and methods from the literature on Pareto-optimal LAD patterns. Toward this end, we select suitable pattern definitions from the literature and adapt them with attention given to unique characteristics of individual patterns and the disparate natures of Boolean and numerical data. Next, we propose a set of revised criteria and definitions by which useful LAD patterns are clearly characterized for both 0–1 and real-valued data. Furthermore, we fortify recent pattern generation optimization models and demonstrate how earlier results on Pareto-optimal patterns can be adapted in accordance with revised pattern definitions. A numerical study validates practical benefits of the results of this paper through optimization-based pattern generation experiments. © 2021",10.1016/j.amc.2021.126153,Boolean logical pattern; Knowledge discovery; Logical analysis of data; Pareto-optimal pattern; Supervised learning,3.0,
Non-Uniform Time-Step Deep Q-Network for Carrier-Sense Multiple Access in Heterogeneous Wireless Networks,"Yu Y., Liew S.C., Wang T.",IEEE Transactions on Mobile Computing,2021.0,"This paper investigates a new class of carrier-sense multiple access (CSMA) protocols that employ deep reinforcement learning (DRL) techniques, referred to as carrier-sense deep-reinforcement learning multiple access (CS-DLMA). The goal of CS-DLMA is to enable efficient and equitable spectrum sharing among a group of co-located heterogeneous wireless networks. Existing CSMA protocols, such as the medium access control (MAC) protocol of WiFi, are designed for a homogeneous network in which all nodes adopt the same protocol. Such protocols suffer from severe performance degradation in a heterogeneous environment where there are nodes adopting other MAC protocols. CS-DLMA aims to circumvent this problem by making use of DRL. In particular, this paper adopts $\alpha$α-fairness as the general objective of CS-DLMA. With $\alpha$α-fairness, CS-DLMA can achieve a range of different objectives (e.g., maximizing sum throughput, achieving proportional fairness, or achieving max-min fairness) when coexisting with other MACs by changing the value of $\alpha$α. A salient feature of CS-DLMA is that it can achieve these objectives without knowing the coexisting MACs through a learning process based on DRL. The underpinning DRL technique in CS-DLMA is deep Q-network (DQN). However, the conventional DQN algorithms are not suitable for CS-DLMA due to their uniform time-step assumption. In CSMA protocols, time steps are non-uniform in that the time duration required for carrier sensing is smaller than the duration of data transmission. This paper introduces a non-uniform time-step formulation of DQN to address this issue. Our simulation results show that CS-DLMA can achieve the general $\alpha$α-fairness objective when coexisting with TDMA, ALOHA, and WiFi protocols by adjusting its own transmission strategy. Interestingly, we also find that CS-DLMA is more Pareto efficient than other CSMA protocols, e.g., p-persistent CSMA, when coexisting with WiFi. Although this paper focuses on the use of our non-uniform time-step DQN formulation in wireless networking, we believe this new DQN formulation can also find use in other domains. © 2002-2012 IEEE.",10.1109/TMC.2020.2990399,Deep reinforcement learning; Heterogeneous wireless networks; Medium access control (MAC); α-fairness,3.0,
Use of a pre‐trained neural network for automatic classification of arterial doppler flow waveforms: A proof of concept,"Guilcher A., Laneelle D., Mahé G.",Journal of Clinical Medicine,2021.0,"Background: Arterial Doppler flow waveform analysis is a tool recommended for the management of lower extremity peripheral arterial disease (PAD). To standardize the waveform analysis, classifications have been proposed. Neural networks have shown a great ability to categorize data. The aim of the present study was to use an existing neural network to evaluate the potential for categorization of arterial Doppler flow waveforms according to a commonly used classification. Methods: The Pareto efficient ResNet‐101 (ResNet‐101) neural network was chosen to categorize 424 images of arterial Doppler flow waveforms according to the Simplified Saint‐Bonnet classification. As a reference, the inter‐operator variability between two trained vascular medicine physicians was also assessed. Accuracy was expressed in percentage, and agreement was assessed using Cohen’s Kappa coefficient. Results: After retraining, ResNet‐101 was able to categorize waveforms with 83.7 ± 4.6% accuracy resulting in a kappa coefficient of 0.79 (0.75–0.83) (CI 95%), compared with a kappa coefficient of 0.83 (0.79–0.87) (CI 95%) between the two physicians. Conclusion: This study suggests that the use of transfer learning on a pre‐trained neural network is feasible for the automatic classification of images of arterial Doppler flow waveforms. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",10.3390/jcm10194479,Doppler waveform; Neural network classification; Peripheral artery disease,,
Deep reinforcement learning and parameter transfer based approach for the multi-objective agile earth observation satellite scheduling problem,"Wei L., Chen Y., Chen M., Chen Y.",Applied Soft Computing,2021.0,"The agile earth observation satellite scheduling problem (AEOSSP) consists of selecting and scheduling a number of tasks from a set of user requests in order to optimize one or multiple criteria. In this paper, we consider a multi-objective version of AEOSSP (called MO-AEOSSP) where the failure rate and the timeliness of scheduled requests are optimized simultaneously. Due to its NP-hardness, traditional iterative problem-tailored heuristic methods are sensitive to problem instances and require massive computational overhead. We thus propose a deep reinforcement learning and parameter transfer based approach (RLPT) to tackle the MO-AEOSSP in a non-iterative manner. RLPT first decomposes the MO-AEOSSP into a number of scalarized sub-problems by a weight sum approach where each sub-problem can be formulated as a Markov Decision Process (MDP). RLPT then applies an encoder–decoder structure neural network (NN) trained by a deep reinforcement learning procedure to producing a high-quality schedule for each sub-problem. The resulting schedules of all scalarized sub-problems form an approximate pareto front for the MO-AEOSSP. Once a NN of a subproblem is trained, RLPT applies a parameter transfer strategy to reducing the training expenses for its neighboring sub-problems. Experimental results on a large set of randomly generated instances show that RLPT outperforms three classical multi-objective evolutionary algorithms (MOEAs) in terms of solution quality, solution distribution and computational efficiency. Results on various-size instances also show that RLPT is highly general and scalable. To the best of our knowledge, this study is the first attempt that applies deep reinforcement learning to a satellite scheduling problem considering multiple objectives. © 2021 Elsevier B.V.",10.1016/j.asoc.2021.107607,Agile satellite scheduling; Deep reinforcement learning; MOEA; Multi-objective,1.0,
Shape optimization of GFRP elastic gridshells by the weighted Lagrange ε-twin support vector machine and multi-objective particle swarm optimization algorithm considering structural weight,"Kookalani S., Cheng B., Xiang S.",Structures,2021.0,"Elastic gridshell is a type of free-form structure usually with double-curved shape and it is constructed by deforming an initially flat grid to achieve the final structural form. Determining a reasonable structural shape considering structural performance is an essential task in the design of such structures. This paper presents a shape optimization method for elastic gridshells considering structural weight, aiming to minimize the structural stress and deformation. Support vector machine is utilized to predict the structural performance in the optimization process in order to avoid the time-consuming structural analysis. The least square support vector machine (LSSVM), weighted least square support vector machine (WLSSVM), a combination of particle swarm optimization and least square support vector machine (PSO-LSSVM), and weighted Lagrange ε-twin support vector machine (WL-ε-TSVM) are first compared for predicting the structural analysis results. The WL-ε-TSVM algorithm shows superior performance and is further adopted in the optimization method. The k-fold cross validation is implemented during the validation process of this algorithm to improve the predictive performance. Based on predicted structural performance, the optimal shape of gridshell is provided by using the multi-objective particle swarm optimization (MOPSO) algorithm. The Taguchi technique is applied to tune the parameters of the MOPSO algorithm. Afterwards, the technique for order preference by similarity to ideal solution (TOPSIS) is implemented to determine the most desirable solution from the Pareto optimal set. The presented method is validated through an example and the structural behavior of the optimized structure is further assessed by finite element analysis. Results demonstrate that the presented method is applicable for finding the optimum shape of gridshells with high structural performance. © 2021 Institution of Structural Engineers",10.1016/j.istruc.2021.05.077,Cross validation; Gridshell structure; Machine learning; MOPSO; Particle swarm optimization; Structural optimization; Support vector machine; Taguchi; TOPSIS,1.0,
Task Allocation on Layered Multiagent Systems: When Evolutionary Many-Objective Optimization Meets Deep Q-Learning,"Li M., Wang Z., Li K., Liao X., Hone K., Liu X.",IEEE Transactions on Evolutionary Computation,2021.0,"This article is concerned with the multitask multiagent allocation problem via many-objective optimization for multiagent systems (MASs). First, a novel layered MAS model is constructed to address the multitask multiagent allocation problem that includes both the original task simplification and the many-objective allocation. In the first layer of the model, the deep Q-learning method is introduced to simplify the prioritization of the original task set. In the second layer of the model, the modified shift-based density estimation (MSDE) method is put forward to improve the conventional strength Pareto evolutionary algorithm 2 (SPEA2) in order to achieve many-objective optimization on task assignments. Then, an MSDE-SPEA2-based method is proposed to tackle the many-objective optimization problem with objectives including task allocation, makespan, agent satisfaction, resource utilization, task completion, and task waiting time. As compared with the existing allocation methods, the developed method in this article exhibits an outstanding feature that the task assignment and the task scheduling are carried out simultaneously. Finally, extensive experiments are conducted to: 1) verify the validity of the proposed model and the effectiveness of two main algorithms and 2) illustrate the optimal solution for task allocation and efficient strategy for task scheduling under different scenarios. © 1997-2012 IEEE.",10.1109/TEVC.2021.3049131,Deep Q-learning (DQL); evolutionary computation; many-objective optimization; multiagent systems (MAS); task allocation.,3.0,
APOLLO: An automated power modeling framework for runtime power introspection in high-volume commercial microprocessors,"Xie Z., Xu X., Walker M., Knebel J., Palaniswamy K., Hebert N., Hu J., Yang H., Chen Y., Das S.","Proceedings of the Annual International Symposium on Microarchitecture, MICRO",2021.0,"Accurate power modeling is crucial for energy-efficient CPU design and runtime management. An ideal power modeling framework needs to be accurate yet fast, achieve high temporal resolution (ideally cycle-accurate) yet with low runtime computational overheads, and easily extensible to diverse designs through automation. Simultaneously satisfying such conflicting objectives is challenging and largely unattained despite significant prior research. In this paper, we propose APOLLO, an automated per-cycle power modeling framework that serves as the basis for both a design-time power estimator and a low-overhead runtime on-chip power meter (OPM). APOLLO uses the minimax concave penalty (MCP)-based feature selection algorithm to automatically select less than 0.05% of RTL signals as power proxies. The power estimation achieves R2 > 0.95 on Arm Neoverse N1 [3] and R2 > 0.94 on Arm Cortex-A77 [2] microprocessors, respectively. When integrated with an emulator-assisted flow, APOLLO finishes per-cycle power estimation on millions-of-cycles benchmark in minutes for million-gate industrial CPU designs. Furthermore, the power model is synthesized and integrated into the microprocessor implementation as a runtime OPM. APOLLO's accuracy further improves when coarse-grained temporal resolution is preferred. To our best knowledge, this is the first runtime OPM that simultaneously achieves percycle temporal resolution and < 1% area/power overhead without compromising accuracy, which is validated on high-performance, out-of-order industrial CPU designs. © 2021 Association for Computing Machinery.",10.1145/3466752.3480064,Commercial microprocessors; Machine learning; On-chip power meter; Power modeling and estimation; Voltage droop,,
Profiling Neural Blocks and Design Spaces for Mobile Neural Architecture Search,"Mills K.G., Han F.X., Zhang J., Changiz Rezaei S.S., Chudak F., Lu W., Lian S., Jui S., Niu D.","International Conference on Information and Knowledge Management, Proceedings",2021.0,"Neural architecture search automates neural network design and has achieved state-of-the-art results in many deep learning applications. While recent literature has focused on designing networks to maximize accuracy, little work has been conducted to understand the compatibility of architecture design spaces to varying hardware. In this paper, we analyze the neural blocks used to build Once-for-All (MobileNetV3), ProxylessNAS and ResNet families, in order to understand their predictive power and inference latency on various devices, including Huawei Kirin 9000 NPU, RTX 2080 Ti, AMD Threadripper 2990WX, and Samsung Note10. We introduce a methodology to quantify the friendliness of neural blocks to hardware and the impact of their placement in a macro network on overall network performance via only end-to-end measurements. Based on extensive profiling results, we derive design insights and apply them to hardware-specific search space reduction. We show that searching in the reduced search space generates better accuracy-latency Pareto frontiers than searching in the original search spaces, customizing architecture search according to the hardware. Moreover, insights derived from measurements lead to notably higher ImageNet top-1 scores on all search spaces investigated. © 2021 ACM.",10.1145/3459637.3481944,design space; latency measurement; neural architecture search,,
Modeling the influence of fused filament fabrication processing parameters on the mechanical properties of ABS parts,"Alafaghani A., Ablat M.A., Abedi H., Qattawi A.",Journal of Manufacturing Processes,2021.0,"Modeling the influence of the processing parameters of fused filament fabrication (FFF) on the mechanical properties of FFF fabricated parts is a challenging task due to the complex dynamics and the large number of factors that affect the quality of the fabricated parts. Therefore, optimizing the mechanical properties of parts fabricated using FFF usually requires a considerable number of test samples. Most past studies have focused on the main effects of the processing parameters and have ignored the interactions between the parameters or their non-linear effects on the mechanical properties of FFF fabricated parts. In the work presented, the effects of the layer thickness, nozzle temperature, infill percentage, and infill pattern are investigated to achieve the fabricated parts' optimum strength and stiffness. A group of Artificial Neural Networks (ANN) was used to model the influence of these processing parameters on the mechanical properties of FFF fabricated ABS parts. The Design of Experiments (DOE) approach was utilized to minimize the number of tests needed to study the investigated parameters. Response Surface Methodology (RSM) and Taguchi's orthogonal arrays were used to generate the training and testing data sets used to develop a group of ANN models and evaluate their performance. Furthermore, the fitness of ANN models was compared to the regression models of the RSM and Taguchi's DOE. It was found that some parameters exhibit strong interaction and nonlinear effects on the strength, stiffness, and ductility of FFF fabricated parts. The coefficient of determination R2 indicates that the ANN models were more accurate at predicting the mechanical properties than DOE regression models. The contour plots show that generally, increasing the layer thickness and infill percentage increase the tensile strength and the elastic modulus and that increasing the nozzle temperature is important when thick layers are used. For the ductility, the infill pattern is the most significant parameter, with linear infill yielding the highest ductility and triangular yielding the lowest. Lower nozzle temperature generally improves the ductility which is the exact opposite of what is required to maximize the elastic modulus and tensile strength. Finally, a multi-objective particle swarm optimization algorithm was used to obtain the non-dominated Pareto-optimal solutions that can lead to an optimal combination of the mechanical properties. © 2021 The Society of Manufacturing Engineers",10.1016/j.jmapro.2021.09.057,Additive manufacturing; Artificial neural network; Fused deposition modeling; Machine learning; Multi-objective optimization; Particle swarm optimization; Response surface methodology,,
Assessment of importance-based machine learning feature selection methods for aggregate size distribution measurement in a 3D binocular vision system,"Sun Z., Liu H., Huyan J., Li W., Guo M., Hao X., Pei L.",Construction and Building Materials,2021.0,"Aggregate size is usually measured by manual sampling and sieving. Machine vision techniques can provide fast, non-invasive measurement. However, the traditional imaging method using a single size descriptor to discriminate different sieve-size classes of coarse aggregates might not yield high-precision classification results. To determine the optimum supervised machine learning model for coarse aggregates sieve-size measurement, 17 methods were evaluated and compared. To train our model, a new dataset named MFCA27 (Multiple Features of Coarse Aggregate 27) was introduced, which contains 27 features of aggregates based on aggregate three-dimensional (3D) top-surface object. In addition, a feature selection approach for investigating how accuracy varied with the datasets under different feature sets was developed, where feature selection was performed according to the impurity-based feature importance score measured using an extremely randomized tree model. Experiments demonstrated that the Gaussian process classifier (GPC) was the best-performing method on the datasets with two- or three-dimensional (2D/3D) feature sets in terms of accuracy and robustness. The results also showed that, compared with the traditional aggregate sieve-size measurement method, which is based on a single size descriptor, GPC can achieve an accuracy of 95.06% on the test dataset of MFCA27 in the aggregate sieve-size class measurement task. © 2021",10.1016/j.conbuildmat.2021.124894,Aggregates; Digital sieving; Importance-based feature selection; Size distribution; Supervised machine learning,1.0,
Modular supply chain optimization considering demand uncertainty to manage risk,"Bhosekar A., Badejo O., Ierapetritou M.",AIChE Journal,2021.0,"Supply chain under demand uncertainty has been a challenging problem due to increased competition and market volatility in modern markets. Flexibility in planning decisions makes modular manufacturing a promising way to address this problem. In this work, the problem of multiperiod process and supply chain network design is considered under demand uncertainty. A mixed integer two-stage stochastic programming problem is formulated with integer variables indicating the process design and continuous variables to represent the material flow in the supply chain. The problem is solved using a rolling horizon approach. Benders decomposition is used to reduce the computational complexity of the optimization problem. To promote risk-averse decisions, a downside risk measure is incorporated in the model. The results demonstrate the several advantages of modular designs in meeting product demands. A pareto-optimal curve for minimizing the objectives of expected cost and downside risk is obtained. © 2021 American Institute of Chemical Engineers.",10.1002/aic.17367,feasibility analysis; machine learning; modular manufacturing; stochastic mixed integer programming; supply chain optimization,3.0,
Multi-objective downscaling of precipitation time series by genetic programming,"Zerenner T., Venema V., Friederichs P., Simmer C.",International Journal of Climatology,2021.0,"We use symbolic regression to estimate daily precipitation amounts at six stations in the Alpine region from a global reanalysis. Symbolic regression only prescribes the set of mathematical expressions allowed in the regression model, but not its structure. The regression models are generated by genetic programming (GP) in analogy to biological evolution. The two conflicting objectives of a low root-mean-square error (RMSE) and consistency in the distribution between model and observations are treated as a multi-objective optimization problem. This allows us to derive a set of downscaling models that represents different achievable trade-offs between the two conflicting objectives, a so-called Pareto set. Our GP setup limits the size of the regression models and uses an analytical quotient instead of a standard or protected division operator. With this setup we obtain models that have a generalization performance comparable with generalized linear regression models (GLMs), which are used as a benchmark. We generate deterministic and stochastic downscaling models with GP. The deterministic downscaling models with low RMSE outperform the respective stochastic models. The stochastic models with low IQD, however, perform slightly better than the respective deterministic models for the majority of cases. No approach is uniquely superior. The stochastic models with optimal IQD provide useful distribution estimates that capture the stochastic uncertainty similar to or slightly better than the GLM-based downscaling. © 2021 The Authors International Journal of Climatology published by John Wiley & Sons Ltd on behalf of Royal Meteorological Society.",10.1002/joc.7172,genetic programming; machine learning; Pareto optimality; stochastic downscaling,,
Multi-objective jellyfish search optimizer for efficient power system operation based on multi-dimensional OPF framework,"Shaheen A.M., El-Sehiemy R.A., Alharthi M.M., Ghoneim S.S.M., Ginidi A.R.",Energy,2021.0,"An enhanced multi-objective Quasi-Reflected Jellyfish Search Optimizer (MOQRJFS) is presented in this article for solving multi-dimensional Optimal Power Flow (MDOPF) issue with diverse objectives which display the minimization of economic fuel cost, total emissions, and the active power loss with satisfying operational constraints. Despite the simple structure of JFS with control of exploitation and exploration, searching capability of the JFS requires more support. Hence, two modifications are performed on the standard JFS algorithm. The first modification is that a cluster with a random size has been proposed which illustrates the social community that can share the data in the cluster and are dissimilar from one to another. The second modification is that a quasi-opposition-based learning is emerged in JFS to support the exploration phase. As selection criteria for the best solutions, a fuzzy decision-making strategy is joint into MOQRJFS optimizer. Additionally, the Pareto optimality concept is added to extract the non-dominated solutions. The superiority of the MOQRJFS is proved throughout application on IEEE 30-bus system, IEEE 57-bus system, the West Delta Region System of 52 bus (WDRS-52) in Egypt, and a large scale 118-bus system. Thirteen cases with economic, environmental, and technical objectives of MDOPF are included in this study. The outcomes of the proposed MOQRJFS have been compared with the conventional MOJFS and the reported techniques in the literature. It is clearly observed that the MOQRJFS give the minimum values compared with these techniques which reveals its robustness, effectiveness, and superiority when handling MDOPF among other techniques. © 2021",10.1016/j.energy.2021.121478,Fuel cost; Multi-dimension optimal power flow; Quasi-reflected jellyfish search optimizer; Real power loss; Total emissions,5.0,
The Need for Transparent Demographic Group Trade-Offs in Credit Risk and Income Classification,"Balashankar A., Lees A.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2022.0,"Prevalent methodology towards constructing fair machine learning (ML) systems, is to enforce a strict equality metric for demographic groups based on protected attributes like race and gender. While definitions of fairness in philosophy are varied, mitigating bias in ML classifiers often relies on demographic parity-based constraints across sub-populations. However, enforcing such constraints blindly can lead to undesirable trade-offs between group-level accuracy if groups possess different underlying sampled population metrics, an occurrence that is surprisingly common in real-world applications like credit risk and income classification. Similarly, attempts to relax hard constraints may lead to unintentional degradation in classification performance, without benefit to any demographic group. In these increasingly likely scenarios, we make the case for transparent human intervention in making the trade-offs between the accuracies of demographic groups. We propose that transparency in trade-offs between demographic groups should be a key tenet of ML design and implementation. Our evaluation demonstrates that a transparent human-in-the-loop trade-off technique based on the Pareto principle increases both overall and group-level accuracy by 9.5% and 9.6% respectively, in two commonly explored UCI datasets for credit risk and income classification. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",10.1007/978-3-030-96957-8_30,,,
Response Time Estimate for a Fork-Join System with Pareto Distributed Service Time as a Model of a Cloud Computing System Using Neural Networks,"Gorbunova A.V., Lebedev A.V.",Communications in Computer and Information Science,2022.0,"A cloud computing system that receives complex user tasks involving several subtasks is studied from the point of view of the response time. In order to reduce the service time, the tasks are divided into smaller components and processed in parallel. As a cloud center model we use a fork-join queuing system with Pareto distribution of the service time on the servers. To analyze the mean response time and its standard deviation, a new approach is used combining simulation modeling with one of the machine learning methods. The estimates obtained are much more accurate than the earlier analytical results on fork-join systems. © 2022, Springer Nature Switzerland AG.",10.1007/978-3-030-97110-6_25,Artificial neural networks; Cloud computing; Machine learning; Mean response time; Parallel computing; Parallel processing; Queuing system,,
Cascaded Classifier for Pareto-Optimal Accuracy-Cost Trade-Off Using Off-the-Shelf ANNs,"Latotzke C., Loh J., Gemmeke T.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2022.0,"Machine-learning classifiers provide high quality of service in classification tasks. Research now targets cost reduction measured in terms of average processing time or energy per solution. Revisiting the concept of cascaded classifiers, we present a first of its kind analysis of optimal pass-on criteria between the classifier stages. Based on this analysis, we derive a methodology to maximize accuracy and efficiency of cascaded classifiers. On the one hand, our methodology allows cost reduction of 1.32 × while preserving reference classifier’s accuracy. On the other hand, it allows to scale cost over two orders while gracefully degrading accuracy. Thereby, the final classifier stage sets the top accuracy. Hence, the multi-stage realization can be employed to optimize any state-of-the-art classifier. © 2022, Springer Nature Switzerland AG.",10.1007/978-3-030-95470-3_32,Cascaded classifier; Design methodology; Edge devices; Machine learning; Pareto analysis; Preliminary classifier,,
A Logarithmic Distance-Based Multi-Objective Genetic Programming Approach for Classification of Imbalanced Data,"Kumar A., Goel S., Sinha N., Bhardwaj A.",Communications in Computer and Information Science,2022.0,"Standard classification algorithms give biased results when data sets are imbalanced. Genetic Programming, a machine learning algorithm based on the evolution of species in nature, also suffers from the same issue. In this research work, we introduced a logarithmic distance-based multi-objective genetic programming (MOGP) approach for classifying imbalanced data. The proposed approach utilizes the logarithmic value of the distance between predicted and expected values. This logarithmic value for the minority and the majority classes is treated as two separate objectives while learning. In the final generation, the proposed approach generated a Pareto-front of classifiers with a balanced surface representing the majority and the minority class accuracies for binary classification. The primary advantage of the MOGP technique is that it can produce a set of good-performing classifiers in a single experimental execution. Against the MOGP approach, the canonical GP method requires multiple experimental runs and a priori objective-based fitness function. Another benefit of MOGP is that it explicitly includes the learning bias into the algorithms. For evaluation of the proposed approach, we performed extensive experimentation of five imbalanced problems. The proposed approach’s results have proven its superiority over the traditional method, where the minority and majority class accuracies are taken as two separate objectives. © 2022, Springer Nature Switzerland AG.",10.1007/978-3-030-95502-1_23,Fitness function; Genetic programming; Imbalanced data classification; Multi-objective optimization; Pareto front,,
Meta-Learning-Based Deep Reinforcement Learning for Multiobjective Optimization Problems,"Zhang Z., Wu Z., Zhang H., Wang J.",IEEE Transactions on Neural Networks and Learning Systems,2022.0,"Deep reinforcement learning (DRL) has recently shown its success in tackling complex combinatorial optimization problems. When these problems are extended to multiobjective ones, it becomes difficult for the existing DRL approaches to flexibly and efficiently deal with multiple subproblems determined by the weight decomposition of objectives. This article proposes a concise meta-learning-based DRL approach. It first trains a meta-model by meta-learning. The meta-model is fine-tuned with a few update steps to derive submodels for the corresponding subproblems. The Pareto front is then built accordingly. Compared with other learning-based methods, our method can greatly shorten the training time of multiple submodels. Due to the rapid and excellent adaptability of the meta-model, more submodels can be derived so as to increase the quality and diversity of the found solutions. The computational experiments on multiobjective traveling salesman problems and multiobjective vehicle routing problems with time windows demonstrate the superiority of our method over most of the learning-based and iteration-based approaches. IEEE",10.1109/TNNLS.2022.3148435,Deep reinforcement learning (DRL); Learning systems; meta-learning; multiobjective optimization; Optimization; Pareto optimization; Task analysis; Training; traveling salesman problem; Traveling salesman problems; Vehicle routing; vehicle routing problem.,,
MOSAIC: Multi-objective Optimization Strategy for AI-aided Internet-of-Things Communications,"Lee H., Lee S.H., Quek T.Q.S.",IEEE Internet of Things Journal,2022.0,"Future Internet-of-Things (IoT) communication trends toward heterogeneous services and diverse quality-of-service requirements pose fundamental challenges for network management strategies. In particular, multi-objective optimization is necessary in resolving the competition among different nodes sharing limited wireless network resources. A unified coordination mechanism is essential such that individual nodes conduct the opportunistic maximization of heterogeneous local objectives for efficient distributed resource allocation. To such a problem, this paper proposes an artificial intelligence (AI) based framework which is termed as multi-objective optimization strategy for AI-aided Internet-of-Things communications (MOSAIC). This framework enables to tackle numerous MOO tasks in IoT network management with simple reconfiguration of learning rules. In this strategy, a component unit associated with an individual network node includes a pair of DNNs to learn optimal local functions responsible for calculation and distributed coordination, respectively. The resultant AI module swarm called DNN tiles realizes the node cooperation that collectively seeks distributed MOO calculation rules. The advantage of MOSAIC is characterized by Pareto tradeoffs among conflicting performance metrics in diverse wireless networking configurations subject to severe interference and distinct criteria for multiple targets. IEEE",10.1109/JIOT.2022.3150747,deep learning; distributed network management; Evolutionary computation; Internet of Things; Linear programming; Multi-objective optimization; Optimization; primal-dual training.; Task analysis; Training; Wireless communication,,
Evaluating the Performance of Feature Selection Methods Using Huge Big Data: A Monte Carlo Simulation Approach,"Khan F., Urooj A., Khan S.A., Khosa S.K., Muhammadullah S., Almaspoor Z.",Mathematical Problems in Engineering,2022.0,"In this article, we compare autometrics and machine learning techniques including Minimax Concave Penalty (MCP), Elastic Smoothly Clipped Absolute Deviation (E-SCAD), and Adaptive Elastic Net (AEnet). For simulation experiments, three kinds of scenarios are considered by allowing the multicollinearity, heteroscedasticity, and autocorrelation conditions with varying sample sizes and the varied number of covariates. We found that all methods show improved their performance for a large sample size. In the presence of low and moderate multicollinearity and low and moderate autocorrelation, the considered methods retain all relevant variables. However, for low and moderate multicollinearity, excluding AEnet, all methods keep many irrelevant predictors as well. In contrast, under low and moderate autocorrelation, along with AEnet, the Autometrics retain less irrelevant predictors. Considering the case of extreme multicollinearity, AEnet retains more than 93 percent correct variables with an outstanding gauge (zero percent). However, the potency of remaining techniques, specifically MCP and E-SCAD, tends towards unity with augmenting sample size but capturing massive irrelevant predictors. Similarly, in case of high autocorrelation, E-SCAD has shown good performance in the selection of relevant variables for a small sample, while in gauge, Autometrics and AEnet are performed better and often retained less than 5 percent irrelevant variables. In the presence of heteroscedasticity, all techniques often hold all relevant variables but also suffer from overspecification problems except AEnet and Autometrics which circumvent the irrelevant predictors and establish the true model precisely. For an empirical application, we take into account the workers' remittance data for Pakistan along its twenty-seven determinants spanning from 1972 to 2020 for Pakistan. The AEnet selected thirteen relevant covariates of workers' remittance while E-SCAD and MCP suffered from an overspecification problem. Hence, the policymakers and practitioners should focus on the relevant variables selected by AEnet to improve workers' remittance in the case of Pakistan. In this regard, the Pakistan government has devised policies that make it easy to transfer remittances legally and mitigate the cost of transferring remittances from abroad. The AEnet approach can help policymakers arrive at relevant variables in the presence of a huge set of covariates, which in turn produce accurate predictions. © 2022 Faridoon Khan et al.",10.1155/2022/6607330,,,
Shear Wave Velocity-Based Liquefaction Susceptibility of Soil Using Extreme Learning Machine (ELM) with Strength Pareto Evolutionary Algorithm (SPEA 2),"Mohanty R., Das S.K., Mohanty M.",Lecture Notes in Civil Engineering,2022.0,"In the present study, the multi-objective optimization problems observed in the prediction of shear wave velocity (Vs)-based liquefaction susceptibility of soil are solved in the framework of multi-objective feature selection (MOFS) algorithms. The learning algorithm, extreme learning machine, ELM and a multi-objective evolutionary algorithm (MOEA) algorithm, strength Pareto evolutionary algorithm (SPEA 2) are unified to form the MOFS algorithm. The proposed MOFS model is equally proficient in predicting the liquefied and non-liquefied cases for a highly unbalanced database of Vs with the ratio of liquefaction (L) to non-liquefaction (NL) cases being 287:124 (L/NL = 2.31). The additional merit of the present study is the identification of important input features; cyclic stress ratio (CSR), Vs, and moment magnitude of the earthquake (Mw). The representation of the results as Pareto front facilitates in the decision-making process. ELM + SPEA 2 is also found to be more efficient in equal prediction of majority and minority class instances; thus, it outperforms ELM + non-dominated sorting genetic algorithm (NSGA-II) as a classifier model. It was also observed that for training to testing ratios of 0.70:0.30 and 0.75:0.25, ELM + SPEA 2 has better generalization capacity than ELM + NSGA-II. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",10.1007/978-981-16-5669-9_3,Extreme learning machine (ELM); Liquefaction; Multi-objective feature selection (MOFS); Shear wave velocity (Vs); Strength Pareto evolutionary algorithm (SPEA 2),,
Multi-objective optimisation with hybrid machine learning strategy for complex catalytic processes,"Tai X.Y., Ocone R., Christie S.D.R., Xuan J.",Energy and AI,2022.0,"Catalytic chemical processes such as hydrocracking, gasification and pyrolysis play a vital role in the renewable energy and net zero transition. Due to the complex and non-linear behaviours during operation, catalytic chemical processes require a powerful modelling tool for prediction and optimisation for smart operation, speedy green process routes discovery and rapid process design. However, challenges remain due to the lack of an effective modelling and optimisation toolbox, which requires not only a precise analysis but also a fast optimisation. Here, we propose a hybrid machine learning strategy by embedding the physics-based continuum lumping kinetic model into the data-driven artificial neural network framework. This hybrid model is adopted as the surrogate model in the multi-objective optimisation and demonstrated in the benchmarking of a hydrocracking process. The results show that the novel hybrid surrogate model exhibits the mean square error less than 0.01 by comparing with the physics-based simulation results. This well-trained hybrid model was then integrated with non-dominated-sort genetic algorithm (NSGA-II) as the surrogate model to evaluate and optimise the yield and selectivity of the hydrocracking process. The Pareto front from the multi-objective optimisation was able to identify the trade-off curve between the objective functions which is essential for the decision-making during process design. Our work indicates that adopting the hybrid machine learning strategy as the surrogate model in the multi-objective optimisation is a promising approach in various complex catalytic chemical processes to enable an accurate computation as well as a rapid optimisation. © 2021 The Author(s)",10.1016/j.egyai.2021.100134,Catalytic chemical process; Hybrid machine learning; Hybrid surrogate model; Multi-objective optimisation,,
Knowledge-Based Reinforcement Learning and Estimation of Distribution Algorithm for Flexible Job Shop Scheduling Problem,"Du Y., Li J., Chen X., Duan P., Pan Q.",IEEE Transactions on Emerging Topics in Computational Intelligence,2022.0,"In this study, a flexible job shop scheduling problem with time-of-use electricity price constraint is considered. The problem includes machine processing speed, setup time, idle time, and the transportation time between machines. Both maximum completion time and total electricity price are optimized simultaneously. A hybrid multi-objective optimization algorithm of estimation of distribution algorithm and deep Q-network is proposed to solve this. The processing sequence, machine assignment, and processing speed assignment are all described using a three-dimensional solution representation. Two knowledge-based initialization strategies are designed for better performance. In the estimation of distribution algorithm component, three probability matrices corresponding to solution representation are provided. In the deep Q-network component, 34 state features are selected to describe the scheduling situation, while nine knowledge-based actions are defined to refine the scheduling solution, and the reward based on the two objectives is designed. As the knowledge for initialization and optimization strategies, five properties of the considered problem are proposed. The proposed mixed integer linear programming model of the problem is validated by exact solver CPLEX. The results of the numerical testing on wide-range scale instances show that the proposed hybrid algorithm is efficient and effective at solving the integrated flexible job shop scheduling problem. IEEE",10.1109/TETCI.2022.3145706,deep reinforcement learning; Estimation; estimation of distribution algorithm; Flexible job shop scheduling problem; Indexes; Job shop scheduling; Knowledge based systems; multi-objective optimization; Optimization; Standards; Transportation,1.0,
Minimax Combined with Machine Learning to Cope with Uncertainties in Medical Application,"Nakonechnyi O., Martsenyuk V., Klos-Witkowska A., Zhehestovska D.",Lecture Notes in Networks and Systems,2022.0,"The work considers ML problems in medical application and presents a minimax approach for developing ML models that would be resistant to aleatoric and epistemic uncertainties. Aleatoric uncertainties are presented with the help of different resampling strategies whereas epistemic ones are a variety of models. The main methods applied are based on linear regression, SVM, random forest for ML, PCA for dimension reduction, and cross-validation as a resampling strategy. The approach which is offered is presented with the help of the flowchart which includes the basic steps of ML model development under uncertainties, including import and primary processing of the clinical data, the statement of task, resampling strategy including the dimension reduction, the choice of methods (learners), tuning their parameters, and models comparison on the basis of minimax criterion. As a clinical example, we consider the problem of the development of learners for lifetime prediction for cardiac patients. The software implementation of the ML model development under uncertainties is offered in the package mlr with the help of the benchmark of learners. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",10.1007/978-981-16-2102-4_64,Cardiac pathology; Cross-validation; Machine learning; Medical research; Minimax; mlr; Resampling strategy; Uncertainty,,
Torsion design of CFRP-CFST columns using a data-driven optimization approach,"Huang H., Xue C., Zhang W., Guo M.",Engineering Structures,2022.0,"A challenging issue of utilizing the merit of the machine learning model to the multi-objective optimization (MOO) problem is that sufficient physical experiments data are hard to get. With the limited training data, overfitting of the surrogate model is inevitable, which may mislead the search engine. A data-driven model based on simulations commonly has a better performance for addressing overfitting problems. However, there is a gap between the numerical model and the real structure/physical experiments. In this paper, a framework called data-driven optimization is proposed for structural performance optimization. First, the transfer learning algorithm (two-stage TrAdaBoost) is designed to reweight the simulation data points that have more significant residuals predicted by a base learner (i.e., ensemble machine learning model), which goal is to select the “accuracy” of simulation data points as supplements to the real structure/physical experiments data points, instead of trying to reduce the gap by model-updating methods, as the traditional methods do. In this way, simulation data points relevant to the real structure/physical experiments will be assigned a large weight value. Then, the generated two-stage TrAdaBoost model incorporated with nondominated sorting genetic algorithm II (NSGA-II) is used to optimize structure design. Finally, this paper used the proposed framework to conduct feature impact analysis, fast predict, and optimize torsion design for the concrete-filled steel tube (CFST) column subjected to combined compression-bending-torsion. The results showed that the two-stage TrAdaBoost algorithm performs better and outperforms the baseline model, an extreme gradient boosting (XGBoost). By typical examples, the proposed framework can be a viable tool for the preliminary design of the CFST column. The Pareto fronts of the two objectives (ultimate torsion strength and cost) are successfully obtained. © 2021 Elsevier Ltd",10.1016/j.engstruct.2021.113479,CFST columns; Combined loads; Multi-objective optimization; Overfitting; SHAP values; Transfer learning,9.0,
An efficient primal-dual method for solving non-smooth machine learning problem,"Lyaqini S., Nachaoui M., Hadri A.","Chaos, Solitons and Fractals",2022.0,"This paper deals with the machine learning model as a framework of regularized loss minimization problem in order to obtain a generalized model. Recently, some studies have proved the success and the efficiency of nonsmooth loss function for supervised learning problems Lyaqini et al. [1]. Motivated by the success of this choice, in this paper we formulate the supervised learning problem based on L1 fidelity term. To solve this nonsmooth optimization problem we transform it into a mini-max one. Then we propose a Primal-Dual method that handles the mini-max problem. This method leads to an efficient and significantly faster numerical algorithm to solve supervised learning problems in the most general case. To illustrate the effectiveness of the proposed approach we present some experimental-numerical validation examples, which are made through synthetic and real-life data. Thus, we show that our approach is outclassing existing methods in terms of convergence speed, quality, and stability of the predicted models. © 2021 Elsevier Ltd",10.1016/j.chaos.2021.111754,ECG; Kernel methods EMG; Non-smooth optimization; Primal-dual algorithm; Supervised learning,,
Towards stacking fault energy engineering in FCC high entropy alloys,"Khan T.Z., Kirk T., Vazquez G., Singh P., Smirnov A.V., Johnson D.D., Youssef K., Arróyave R.",Acta Materialia,2022.0,"Stacking Fault Energy (SFE) is an intrinsic alloy property that governs much of the plastic deformation mechanisms observed in fcc alloys. While SFE has been recognized for many years as a key intrinsic mechanical property, its inference via experimental observations or prediction using, for example, computationally intensive first-principles methods is challenging. This difficulty precludes the explicit use of SFE as an alloy design parameter. In this work, we combine DFT calculations (with necessary configurational averaging), machine-learning (ML) and physics-based models to predict the SFE in the fcc CoCrFeMnNiV-Al high-entropy alloy space. The best-performing ML model is capable of accurately predicting the SFE of arbitrary compositions within this 7-element system. This efficient model along with a recently developed model to estimate intrinsic strength of fcc HEAs is used to explore the strength–SFE Pareto front, predicting new-candidate alloys with particularly interesting mechanical behavior. © 2021 Acta Materialia Inc.",10.1016/j.actamat.2021.117472,Alloy design; High entropy alloys; Machine learning; Stacking fault energy,1.0,
Learning-to-ensemble by contextual rank aggregation in e-commerce,"Wang X., Huzhang G., Lin Q., Da Q.",WSDM 2022 - Proceedings of the 15th ACM International Conference on Web Search and Data Mining,2022.0,"Ensemble models in E-commerce combine predictions from multiple sub-models for ranking and revenue improvement. Industrial ensemble models are typically deep neural networks, following the supervised learning paradigm to infer conversion rate given inputs from sub-models. However, this process has the following two problems. Firstly, the point-wise scoring approach disregards the relationships between items and leads to homogeneous displayed results, while diversified display benefits user experience and revenue. Secondly, the learning paradigm focuses on the ranking metrics and does not directly optimize the revenue. In our work, we propose a new Learning-To-Ensemble (LTE) framework RA-EGO, which replaces the ensemble model with a contextual Rank Aggregator (RA) and explores the best weights of sub-models by the Evaluator-Generator Optimization (EGO). To achieve the best online performance, we propose a new rank aggregation algorithm TournamentGreedy as a refinement of classic rank aggregators, which also produces the best average weighted Kendall Tau Distance (KTD) amongst all the considered algorithms with quadratic time complexity. Under the assumption that the best output list should be Pareto Optimal on the KTD metric for sub-models, we show that our RA algorithm has higher efficiency and coverage in exploring the optimal weights. Combined with the idea of Bayesian Optimization and gradient descent, we solve the online contextual Black-Box Optimization task that finds the optimal weights for sub-models given a chosen RA model. RA-EGO has been deployed in our online system and has improved the revenue significantly. © 2022 ACM.",10.1145/3488560.3498401,Contextual black-box optimization; Rank aggregation,,
A multi-objective optimisation approach for activity excitation of waste glass mortar,"Sun J., Tang Y., Wang J., Wang X., Wang J., Yu Z., Cheng Q., Wang Y.",Journal of Materials Research and Technology,2022.0,"Waste glass is promising to be recycled and reused in construction for sustainability. Silicon dioxide is the main component of glass, however, its pozzolanic activity is latent mainly due to its stable silica tetrahedron structure. To excite the activation of waste glass, chemical activation and mechanical grinding of waste glass powder (WGP) were investigated. As the supplementary, hydrothermal and combined (mechanical-chemical-hydrothermal) treatments were conducted on part of the WGP samples. The unconfined compression strength (UCS), expansion caused by alkali–silica reaction (ASR), and the microstructural morphology of WGP were investigated. The results showed the dosage threshold (around 2%) of the chemical activators (alkali and sodium sulfate) and the combined activation were optimal. Besides, a firefly algorithm (FA) based multi-objective optimisation model (MOFA) was applied to seek the Pareto fronts based on three objectives: UCS, ASR expansion, and Cost of mixture proportion. The objective functions of UCS and expansion were established through training the machine learning (ML) models where FA was used to tune the hyperparameters. The cost was calculated by a polynomial function. The ultimate values of root mean square error (RMSE) and correlation coefficient (R) showed the robustness of the ML models. Moreover, the Pareto fronts for mortars containing 300 μm and 75 μm WGPs were successfully obtained, which contributed to the practical application of waste glass in mortar production. In addition, the sensitivity analysis was conducted to rank the importance of input variables. The results showed that curing time, activator's content, and WGP particle size were three essential parameters. © 2022",10.1016/j.jmrt.2022.01.066,Activation methodology; Alkali–silica reaction; Compressive strength; Machine learning; Multi-objective optimisation; Waste glass,,
Online Minimax Q Network Learning for Two-Player Zero-Sum Markov Games,"Zhu Y., Zhao D.",IEEE Transactions on Neural Networks and Learning Systems,2022.0,"The Nash equilibrium is an important concept in game theory. It describes the least exploitability of one player from any opponents. We combine game theory, dynamic programming, and recent deep reinforcement learning (DRL) techniques to online learn the Nash equilibrium policy for two-player zero-sum Markov games (TZMGs). The problem is first formulated as a Bellman minimax equation, and generalized policy iteration (GPI) provides a double-loop iterative way to find the equilibrium. Then, neural networks are introduced to approximate Q functions for large-scale problems. An online minimax Q network learning algorithm is proposed to train the network with observations. Experience replay, dueling network, and double Q-learning are applied to improve the learning process. The contributions are twofold: 1) DRL techniques are combined with GPI to find the TZMG Nash equilibrium for the first time and 2) the convergence of the online learning algorithm with a lookup table and experience replay is proven, whose proof is not only useful for TZMGs but also instructive for single-agent Markov decision problems. Experiments on different examples validate the effectiveness of the proposed algorithm on TZMG problems. © 2012 IEEE.",10.1109/TNNLS.2020.3041469,Deep reinforcement learning (DRL); generalized policy iteration (GPI); Markov game (MG); Nash equilibrium; Q network; zero sum,3.0,
Multi-objective optimization-based reactive nitrogen transport modeling for the water-environment-agriculture nexus in a basin-scale coastal aquifer,"Yin Z., Wu J., Song J., Yang Y., Zhu X., Wu J.",Water Research,2022.0,"The quantification of trade-offs between social-economic and environmental effects is of great importance, especially in the semi-arid coastal areas with highly developed agriculture. The study presents an integrated multi-objective simulation-optimization (S-O) framework to evaluate the basin-scale water-environment-agriculture (WEA) nexus. First, the variable-density groundwater model (SEAWAT) is coupled to the reactive transport model (RT3D) for the first attempt to simulate the environmental effects subject to seawater intrusion (SWI) and nitrate pollution (NP). Then, the surrogate assisted multi-objective optimization algorithm is utilized to investigate the trade-offs between the net agricultural benefits and extents of SWI and NP while considering the water supply, food security, and land availability simultaneously. The S-O modeling methodology is applied to the Dagu River Basin (DRB), a typical SWI region with intensive agricultural irrigation in China. It is shown that the three-objective space based on Pareto-optimal front can be achieved by optimizing planting area in the irrigation districts, indicating the optimal evolution of the WEA nexus system. The Pareto-optimal solutions generated by multi-objective S-O model are more realistic and pragmatic, avoiding the decision bias that may often lead to cognitive myopia caused by the low-dimensional objectives. Although the net agricultural benefits in Pareto-optimal solutions are declined to some extent, the environmental objectives (the extents of SWI and NP) are improved compared to those in the pre-optimized scheme. Therefore, the proposed multi-objective S-O framework can be applied to the WEA nexus in the river basin with intensive agriculture development, which is significant to implement the integrated management of water, food, and environment, especially for the semi-arid coastal aquifers. © 2022 Elsevier Ltd",10.1016/j.watres.2022.118111,Kernel extreme learning machine (KELM); Multi-objective trade-off; Reactive nitrogen transport; Seawater intrusion (SWI); Simulation-optimization (S-O); Water-environment-agriculture (WEA) nexus,,
An extended multi-model regression approach for compressive strength prediction and optimization of a concrete mixture,"Motlagh S.A.T., Naghizadehrokni M.",Construction and Building Materials,2022.0,"Due to the significant delay and cost associated with experimental tests, a model-based evaluation of concrete compressive strength is of high value, both for the purpose of strength prediction as well as the mixture optimization. In contrast to the prior recent studies employing a single regression model, in this paper, we present a combined multi-model framework where the regression methods based on artificial neural network, random forest regression and polynomial regression are jointly implemented for compressive strength prediction with a higher accuracy. The outcomes of the individual regression models are combined via a linear weighting strategy and optimized over the training data set as a quadratic convex optimization problem. It is worth mentioning that due to the convexity of the formulated problem, the globally optimum weighting strategy is obtained via standard numerical solvers. Afterwards, employing the obtained regression model, a multi-objective genetic algorithm-based method is proposed for mixture optimization under practical constraints, where a Pareto front of the cost-CS trade-off has been obtained employing the available data set. Numerical evaluations show that the proposed multi-model regression achieves a significantly higher prediction accuracy, i.e., approximately 18% reduction in the obtained prediction mean squared error, without weight optimization, and roughly 30% reduction in the obtained prediction mean squared error with an optimized combination following a convex quadratic optimization, compared to the best single model regression method employing a multi-layered artificial neural network. © 2022 Elsevier Ltd",10.1016/j.conbuildmat.2022.126828,Compressive strength prediction; Concrete; Convex optimization; Deep neural network; Genetic algorithm; Heuristic optimization; Machine learning; Multi-model regression; Predictive regression,,
Hour-ahead photovoltaic generation forecasting method based on machine learning and multi objective optimization algorithm,"Wang J., Zhou Y., Li Z.",Applied Energy,2022.0,"As the penetration rate of solar energy in the grid continues to enhance, solar power photovoltaic generation forecasts have become an indispensable aspect of mechanism mobilization and maintenance of the stability of the power system. In this regard, many researchers have done a lot of study, and put forward some predictive models. However, many individual prediction systems only consider the prediction accuracy rate without further considering the prediction utility and stability. To fill this gap, a comprehensive system is designed in this paper, which is on the basis of automatic optimization of variational mode decomposition mechanism, and the weight of system is determined by multi objective intelligent optimization algorithm. In particular, it can be proved theoretically that the developed predictive system can achieve the pareto optimal solution. And the designed system is shown to be very effective in forecasting the 2021 photovoltaic power data obtained from Belgium. The empirical study reports that the combination of variational mode decomposition strategy based on genetic algorithm and multi objective grasshopper optimization algorithm is found to be the satisfactory strategy to optimize the predictive system compared with other common mechanism. And the results of several numerical studies show that the designed predictive system achieves the superior performance as compared to the control systems, and in multi-step forecasting, the designed system has better stability than the comparison systems. © 2022 Elsevier Ltd",10.1016/j.apenergy.2022.118725,Data preprocessing method; Machine learning; Multi objective optimization algorithm; Photovoltaic power forecast,,
Deep learning based optimization under uncertainty for surfactant-enhanced DNAPL remediation in highly heterogeneous aquifers,"Du J., Shi X., Mo S., Kang X., Wu J.",Journal of Hydrology,2022.0,"The optimization for surfactant-enhanced aquifer remediation (SEAR) of dense non-aqueous phase liquid (DNAPL)-contaminated aquifers is usually accompanied by uncertainties, which may arise from the characterization of complex aquifer heterogeneity and DNAPL source zone architecture (SZA) due to measurement sparsity. Optimization under uncertainty is computationally expensive as it involves an enormous number of model runs. The huge computational burden can be alleviated by utilizing a surrogate model for repeated model evaluations. However, most of the developed surrogates are often limited to low-dimensional optimization problems that only consider simplified aquifer heterogeneity. In this study, we developed a multi-objective simulation-optimization framework to optimize the SEAR schemes considering the characterization uncertainties from both highly heterogeneous aquifer permeability and complex SZA. A fast-to-run convolutional neural network (CNN)-based surrogate model was developed to approximate the high-dimensional and highly complex input-output mapping of the DNAPL multiphase flow simulation model. We first used the rejection sampling strategy to generate random realizations of permeability and SZA conditioning on their limited measurements and then formulated a multi-objective optimization under uncertainty problem based on these realizations. The developed 3-D CNN was trained and used as the surrogate for repeated model runs in optimization to identify the optimal SEAR schemes under uncertainty. A 3-D numerical experiment was used to test the performance of the CNN-based simulation-optimization framework. Comprehensive analysis on the obtained Pareto fronts demonstrates that the proposed framework can efficiently identify reliable Pareto-optimal solutions with a 99.8% speedup compared to the traditional optimization coupled with the forward model. Moreover, the optimization considering multiple realizations enables us to perform the risk assessment to locate the risk zone where the NAPL phase possibly exists after remediation, which provides useful information for decision-making. © 2022 Elsevier B.V.",10.1016/j.jhydrol.2022.127639,Deep learning; DNAPL; Heterogeneous permeability; Optimization under uncertainty; Remediation strategy design; Risk assessment,,
Efficient and sparse neural networks by pruning weights in a multiobjective learning approach,"Reiners M., Klamroth K., Heldmann F., Stiglmayr M.",Computers and Operations Research,2022.0,"Overparameterization and overfitting are common concerns when designing and training deep neural networks, that are often counteracted by pruning and regularization strategies. However, these strategies remain secondary to most learning approaches and suffer from time and computational intensive procedures. We suggest a multiobjective perspective on the training of neural networks by treating its prediction accuracy and the network complexity as two individual objective functions in a biobjective optimization problem. As a showcase example, we use the cross entropy as a measure of the prediction accuracy while adopting an l1-penalty function to assess the total cost (or complexity) of the network parameters. The latter is combined with an intra-training pruning approach that reinforces complexity reduction and requires only marginal extra computational cost. From the perspective of multiobjective optimization, this is a truly large-scale optimization problem. We compare two different optimization paradigms: On the one hand, we adopt a scalarization-based approach that transforms the biobjective problem into a series of weighted-sum scalarizations. On the other hand we implement stochastic multi-gradient descent algorithms that generate a single Pareto optimal solution without requiring or using preference information. In the first case, favorable knee solutions are identified by repeated training runs with adaptively selected scalarization parameters. Numerical results on exemplary convolutional neural networks confirm that large reductions in the complexity of neural networks with negligible loss of accuracy are possible. © 2022 Elsevier Ltd",10.1016/j.cor.2021.105676,Automated machine learning; l1-regularization; Multiobjective learning; Stochastic multi-gradient descent; Unstructured pruning,,
Multi-label learning via minimax probability machine,"Rastogi (nee Khemchandani) R., Jain S.",International Journal of Approximate Reasoning,2022.0,"In this paper, we propose Minimax Probability Machine for Multi-label data classification and is termed as Multi-Label Minimax Probability Machine (MLMPM). Based on data mean and covariance information, MLMPM builds a classifier that minimizes an upper bound on the mis-classification probability of unseen future data. For capturing label correlation we have considered asymmetric co-occurrency matrix into the model. The proposed model has also been extended to non-linear settings using the Mercer Kernel trick. To accelerate the training procedure, iterative weighted least squares is used to train the underlying optimization model efficiently. Extensive experimental comparisons of our proposed method with related multi-label algorithms on synthetic as well as real world multi-label datasets, along with Amazon rainforest satellite images dataset, prove its efficacy. © 2022 Elsevier Inc.",10.1016/j.ijar.2022.02.002,Label correlation; Minimax probability machine; Multi-label classification; Second order cone programming problem; Weighted least squares,,
TensorOpt: Exploring the Tradeoffs in Distributed DNN Training with Auto-Parallelism,"Cai Z., Yan X., Ma K., Wu Y., Huang Y., Cheng J., Su T., Yu F.",IEEE Transactions on Parallel and Distributed Systems,2022.0,"Effective parallelization strategies are crucial for the performance of distributed deep neural network (DNN) training. Recently, several methods have been proposed to search parallelization strategies but they all optimize a single objective (e.g., execution time, memory consumption) and produce only one strategy. We propose Frontier Tracking (FT), an efficient algorithm that finds a set of Pareto-optimal parallelization strategies to explore the best trade-off among different objectives. FT can minimize the memory consumption when the number of devices is limited and fully utilize additional resources to reduce the execution time. Based on FT, we develop a user-friendly system, called TensorOpt, which allows users to run their distributed DNN training jobs without caring the details about searching and coding parallelization strategies. Experimental results show that TensorOpt is more flexible in adapting to resource availability compared with existing frameworks. © 1990-2012 IEEE.",10.1109/TPDS.2021.3132413,Deep learning; distributed systems; large-scale model training,,
Multi-Objective Evolutionary Algorithms Embedded with Machine Learning — A Survey,"Fan, Zhun and Hu, Kaiwen and Li, Fang and Rong, Yibiao and Li, Wenji and Lin, Huibiao",IEEE Press,2016,"Multi-objective evolutionary algorithms (MOEAs) have been widely used in solving multi-objective optimization problems. A great number of the-state-of-art MOEAs have been proposed. These MOEAs can be classified into the following categories: decomposition-based, domination-based, indicator-based, and probability-based methods. Among them, the first four categories belong to non-model based methods, while the fifth one is considered to be model-based method, in which machine learning techniques are often used to build the models. Recently, embedding machine learning mechanisms into MOEAs is becoming popular and promising. In this paper, a relatively thorough review on both traditional MOEAs and those equipped with machine learning mechanisms are made, with the aim of shedding light on the future development of this emerging research field.",10.1109/CEC.2016.7743932,,,
Accurate Multi-Criteria Decision Making Methodology for Recommending Machine Learning Algorithm,"Ali, Rahman and Lee, Sungyoung and Chung, Tae Choong","Pergamon Press, Inc.",2017,"A multi-criteria decision making methodology is proposed to select best classifier.NGT-based method is adapted to choose suitable classifier's evaluation metrics.Accuracy, time complexity and consistency based new ranking criteria is designed.AHP-based method is adapted to estimate relative weights for evaluation metrics.Classifiers are ranked based on relative closeness score, computed using TOPSIS. ObjectiveManual evaluation of machine learning algorithms and selection of a suitable classifier from the list of available candidate classifiers, is highly time consuming and challenging task. If the selection is not carefully and accurately done, the resulting classification model will not be able to produce the expected performance results. In this study, we present an accurate multi-criteria decision making methodology (AMD) which empirically evaluates and ranks classifiers' and allow end users or experts to choose the top ranked classifier for their applications to learn and build classification models for them. Methods and materialExisting classifiers performance analysis and recommendation methodologies lack (a) appropriate method for suitable evaluation criteria selection, (b) relative consistent weighting mechanism, (c) fitness assessment of the classifiers' performances, and (d) satisfaction of various constraints during the analysis process. To assist machine learning practitioners in the selection of suitable classifier(s), AMD methodology is proposed that presents an expert group-based criteria selection method, relative consistent weighting scheme, a new ranking method, called optimum performance ranking criteria, based on multiple evaluation metrics, statistical significance and fitness assessment functions, and implicit and explicit constraints satisfaction at the time of analysis. For ranking the classifiers performance, the proposed ranking method integrates Wgt.Avg.F-score, CPUTimeTesting, CPUTimeTraining, and Consistency measures using the technique for order performance by similarity to ideal solution (TOPSIS). The final relative closeness score produced by TOPSIS, is ranked and the practitioners select the best performance (top-ranked) classifier for their problems in-hand. FindingsBased on the extensive experiments performed on 15 publically available UCI and OpenML datasets using 35 classification algorithms from heterogeneous families of classifiers, an average Spearman's rank correlation coefficient of 0.98 is observed. Similarly, the AMD method has showed improved performance of 0.98 average Spearman's rank correlation coefficient as compared to 0.83 and 0.045 correlation coefficient of the state-of-the-art ranking methods, performance of algorithms (PAlg) and adjusted ratio of ratio (ARR). Conclusion and implicationThe evaluation, empirical analysis of results and comparison with state-of-the-art methods demonstrate the feasibility of AMD methodology, especially the selection and weighting of right evaluation criteria, accurate ranking and selection of optimum performance classifier(s) for the user's application's data in hand. AMD reduces expert's time and efforts and improves system performance by designing suitable classifier recommended by AMD methodology.",10.1016/j.eswa.2016.11.034,"Classification algorithms, Algorithm recommendation, Ranking classifiers, TOPSIS, Classifiers recommendation, Multi-criteria decision making, Algorithm selection",,
Multi-View and Multi-Objective Semi-Supervised Learning for HMM-Based Automatic Speech Recognition,"Cui, Xiaodong and Huang, Jing and Chien, Jen-Tzung",IEEE Press,2012,"Current hidden Markov acoustic modeling for large-vocabulary continuous speech recognition (LVCSR) heavily relies on the availability of abundant labeled transcriptions. Given that speech labeling is both expensive and time-consuming while there is a huge amount of unlabeled data easily available nowadays, the semi-supervised learning (SSL) from both labeled and unlabeled data aiming to reduce the development cost for LVCSR becomes more important than ever. In this paper, a new SSL approach is proposed which exploits the cross-view transfer learning for LVCSR through a committee machine consisting of multiple views learned from different acoustic features and randomized decision trees. In addition, a multi-objective learning scheme is developed in each view by maximizing a hybrid information-theoretic criterion which is established by the relative entropy between labeled data and their labels and the entropy of unlabeled data. The multi-objective scheme is then generalized to a unified SSL framework which can be interpreted into a variety of learning strategies under different weighting schemes. Experiments conducted on English Broadcast News using 50 hours of transcribed speech with 50 hours and 150 hours of untranscribed speech show the benefits of proposed approaches.",10.1109/TASL.2012.2191955,,,
Mining Cross Product Line Rules with Multi-Objective Search and Machine Learning,"Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat",Association for Computing Machinery,2017,"Nowadays, an increasing number of systems are being developed by integrating products (belonging to different product lines) that communicate with each other through information networks. Cost-effectively supporting Product Line Engineering (PLE) and in particular enabling automation of configuration in PLE is a challenge. Capturing rules is the key for enabling automation of configuration. Product configuration has a direct impact on runtime interactions of communicating products. Such products might be within or across product lines and there usually don't exist explicitly specified rules constraining configurable parameter values of such products. Manually specifying such rules is tedious, time-consuming, and requires expert's knowledge of the domain and the product lines. To address this challenge, we propose an approach named as SBRM that combines multi-objective search with machine learning to mine rules. To evaluate the proposed approach, we performed a real case study of two communicating Video Conferencing Systems belonging to two different product lines. Results show that SBRM performed significantly better than Random Search in terms of fitness values, Hyper-Volume, and machine learning quality measurements. When comparing with rules mined with real data, SBRM performed significantly better in terms of Failed Precision (18%), Failed Recall (72%), and Failed F-measure (59%).",10.1145/3071178.3071261,"product line, multi-objective search, configuration, machine learning, rule mining",,
Using Multi-Objective Search and Machine Learning to Infer Rules Constraining Product Configurations,"Safdar, Safdar Aqeel and Yue, Tao and Ali, Shaukat and Lu, Hong",Kluwer Academic Publishers,2020,"Modern systems are being developed by integrating multiple products within/across product lines that communicate with each other through information networks. Runtime behaviors of such systems are related to product configurations and information networks. Cost-effectively supporting Product Line Engineering (PLE) of such systems is challenging mainly because of lacking the support of automation of the configuration process. Capturing rules is the key for automating the configuration process in PLE. However, there does not exist explicitly-specified rules constraining configurable parameter values of such products and product lines. Manually specifying such rules is tedious and time-consuming. To address this challenge, in this paper, we present an improved version (named as SBRM+) of our previously proposed Search-based Rule Mining (SBRM) approach. SBRM+ incorporates two machine learning algorithms (i.e., C4.5 and PART) and two multi-objective search algorithms (i.e., NSGA-II and NSGA-III), employs a clustering algorithm (i.e., k means) for classifying rules as high or low confidence rules, which are used for defining three objectives to guide the search. To evaluate SBRM+ (i.e., SBRMNSGA-II+-C45, SBRMNSGA-III+-C45, SBRMNSGA-II+-PART, and SBRMNSGA-III+-PART), we performed two case studies (Cisco and Jitsi) and conducted three types of analyses of results: difference analysis, correlation analysis, and trend analysis. Results of the analyses show that all the SBRM+ approaches performed significantly better than two Random Search-based approaches (RBRM+-C45 and RBRM+-PART) in terms of fitness values, six quality indicators, and 17 machine learning quality measurements (MLQMs). As compared to RBRM+ approaches, SBRM+ approaches have improved the quality of rules based on MLQMs up to 27% for the Cisco case study and 28% for the Jitsi case study.",10.1007/s10515-019-00266-2,"Rule mining, Configuration, Interacting products, Machine learning, Multi-objective search, Product line",,
Analyzing Tennis Game through Sensor Data with Machine Learning and Multi-Objective Optimization,"Mlakar, Miha and Lu\v{s}trek, Mitja",Association for Computing Machinery,2017,"Wearable devices are heavily used in many sports. However, the existing sports wearables are either not tennis-specific, or are limited to information on shots. We therefore added tennis-specific information to a leading commercial device. Firstly, we developed a method for classifying shot types into forehand, backhand and serve. Secondly, we used multi-objective optimization to distinguish active play from the time in-between points. By combining both parts with the general movement information already provided by the device, we get comprehensive metrics for professional players and coaches to objectively measure a player's performance and enable in-depth tactical analysis.",10.1145/3123024.3123163,"wearable analytics, shot detection, tennis, optimization",,
An Intelligent Telugu Handwritten Character Recognition Using Multi-Objective Mayfly Optimization with Deep Learning Based DenseNet Model,"Sonthi, Vijaya Krishna and Nagarajan, S. and Krishnaraj, N.",Association for Computing Machinery,2021,"Handwritten character recognition process has gained significant attention among research communities due to the application in assistive technologies for visually impaired people, human robot interaction, automated registry for business document, and so on. Handwritten character recognition of Telugu language is hard owing to the absence of massive dataset and trained convolution neural network (CNN). Therefore, this paper introduces an intelligent Telugu character recognition using multi-objective mayfly optimization with deep learning (MOMFO-DL) model. The proposed MOMFO-DL technique involves DenseNet-169 model as a feature extractor to generate a useful set of feature vectors. Moreover, functional link neural network (FLNN) is used as a classification model to recognize and classify the printer characters. The design of MOMFO technique for the parameter optimization of DenseNet model and FLNN model shows the novelty of the work. The use of MOMFO technique helps to optimally tune the parameters in such a way that the overall performance can be improved. The extensive experimental analysis takes place on benchmark datasets and the outcomes are examined with respect to different measures. The experimental results pointed out the supremacy of the MOMFO technique over the recent state of art methods.",10.1145/3520439,,,
Multi-Objective Model Type Selection,"Rosales-P\'{e}rez, Alejandro and Gonzalez, Jesus A. and Coello Coello, Carlos A. and Escalante, Hugo Jair and Reyes-Garcia, Carlos A.",Elsevier Science Publishers B. V.,2014,"Classification is a mainstream within the machine learning community. As a result, a large number of learning algorithms have been proposed. The performance of many of these could highly depend on the chosen values of their hyper-parameters. This paper introduces a novel method for addressing the model selection problem for a given classification task. In our model selection formulation, both the learning algorithm and its hyper-parameters are considered. In our proposed approach, model selection is tackled as a multi-objective optimization problem. The empirical error, or training error, and the model complexity are defined as the objectives. We adopt a multi-objective evolutionary algorithm as the search engine, due to its high performance and its advantages for solving multi-objective problems. The model complexity is estimated experimentally, in a general fashion, for any learning algorithm, through the VC dimension. Strategies for choosing a single model or for constructing an ensemble of models from the resulting non-dominated set are also proposed. Experimental results on benchmark data sets indicate the effectiveness of the proposed approach. Furthermore, a comparative study shows that the obtained models are highly competitive, in terms of generalization performance, with other methods in the state of the art that focus on a single-learning algorithm, or a single-objective approach.",10.1016/j.neucom.2014.05.077,"VC dimension, Multi-objective optimization, Model type selection, Ensemble methods",,
A Multi-Objective Load Balancing Algorithm for Virtual Machine Placement in Cloud Data Centers Based on Machine Learning,"Ghasemi, Arezoo and Toroghi Haghighat, Abolfazl",Springer-Verlag,2020,"Cloud computing provides utility computing in which clients pay the cost according to their demands and service use. There are some challenges to this technology. One of these issues in data centers is virtual machine (VM) placement so that mapping of these VMs to hosts is executed for a variety of objectives such as load balancing, reducing energy consumption, increasing resource utilization, shortening response time, etc. In this paper, a strategy is presented based on machine learning for VM replacement which aims to balance the load in host machines (HM). In this proposed strategy, the learning agent, in each learning episode by selecting an action from among the permissible actions and executing it on the environment receives a reward according to the desirability of the solution obtained by doing that action in the environment. Receiving a reward from the environment and updating the action value table enable the learner agent to learn in the following episodes that in each environment state, selecting and executing which action is better in the environment and this leads to further enhancement. Our proposed algorithm has, on average, improved the inter-HM load balance in terms of processor, memory, and bandwidth by 25%, 34%, and 32%, respectively, prior to the implementation of the algorithm. Our strategy was compared from diffrent aspects in three scenarios to the MOVMrB strategy. Finally, it was concluded that our proposed algorithm can be more effective in load balancing by having much less runtime and turning off more HMs.",10.1007/s00607-020-00813-w,"68T05, Virtual machine placement, 68Q99, 68M14, Cloud computing, Load balancing, Machine learning",,
Dimensionality Reduction for Multi-Criteria Problems: An Application to the Decommissioning of Oil and Gas Installations,"Martins, Isabelle D. and Bahiense, Laura and Infante, Carlos E.D. and Arruda, Edilson F.","Pergamon Press, Inc.",2020,,10.1016/j.eswa.2020.113236,"Dimensionality reduction, Machine learning, Multi-criteria decision analysis, Oil and gas, Decommissioning, Feature selection",,
A Multi-Objective Optimization Design Framework for Ensemble Generation,"Ribeiro, Victor Henrique Alves and Reynoso-Meza, Gilberto",Association for Computing Machinery,2018,"Machine learning algorithms have found to be useful for the solution of complex engineering problems. However, due to problem's characteristics, such as class imbalance, classical methods may not be formidable. The authors believe that the application of multi-objective optimization design can improve the results of machine learning algorithms on such scenarios. Thus, this paper proposes a novel methodology for the creation of ensembles of classifiers. To do so, a multi-objective optimization design approach composed of two steps is used. The first step focus on generating a set of diverse classifiers, while the second step focus on the selection of such classifiers as ensemble members. The proposed method is tested on a real-world competition data set, using both decision trees and logistic regression classifiers. Results show that the ensembles created with such technique outperform the best ensemble members.",10.1145/3205651.3208219,"logistic regression, decision trees, multi-objective optimization, ensemble methods",,
Optimal Set of Overlapping Clusters Using Multi-Objective Genetic Algorithm,"Das, Sunanda and Chaudhuri, Shreya and Das, Asit K.",Association for Computing Machinery,2017,"Clustering is an important unsupervised machine learning techniqueused in diverse fields to explore the inherent structure of the data. In most of the real life datasets, one object resides in many clusters with different membership values. Many clustering algorithms have been proposed for finding such overlapping clusters for knowledge extraction and future trend prediction. In the paper, multi-objective genetic algorithm based cluster analysis technique is proposed for finding the optimal set of overlapping clusters. As most of the real world search and optimization problems involve multiple objectives, multi-objective Genetic Algorithm is an obvious choice for capturing multiple optimal solutions. Thus the usefulness of applying the multi-objective Genetic Algorithm is to grouping the objects based on different objective functions for finding optimal set of overlapping clusters. The advantage of this algorithm is that it assigns a membership value only to the objects which are the members of several clusters, instead of assigning membership values for all clusters like fuzzy clustering algorithm. If any object positively belongs only to a single cluster, its membership value for this cluster is ' 1' and '0' for all other clusters. The overall performance of the method is investigated on some popular UCI and microarray datasets and the optimality of the clusters is measured by some important cluster validation indices. The experimental results show the effectiveness of the proposed method.",10.1145/3055635.3056653,"Cluster Analysis, Cluster validation index, Fuzzy Clustering, Multi-objective Genetic Algorithm, Overlapping Cluster",,
Multi-Objective Deep CNN for Outdoor Auto-Navigation,"Wei, Wu and He, Shuai and Wang, DongLiang and Yeboah, Yao",Association for Computing Machinery,2018,"Target-guided navigation establishes the foundation for efficiently addressing vision-based multi-agent coordination for robotics. This work proposes a multi-objective deep convolution network which consists of two parallel branches built atop a shared feature extractor. The proposed network is capable of concurrently constructing semantic maps while achieving efficient visual detection of a designated guider robot or landmark towards outdoor navigation. In order to achieve the low latency requirements of the navigation controller, the structure and parameters of the network have been meticulously designed to boost run-time performance. The model is trained and tested on an altered version of the Cityscape outdoor dataset. We further finetune using a collected dataset in order to improve generalization performance on unseen outdoor scenes. Experimental results on an outdoor navigation robot equipped with an RGBD camera and GPU mini PC verifies the feasibility of the model.",10.1145/3234804.3234823,"Single-shot-detector, Object detection, Deeplab, Semantic segmentation, Outdoor navigation, MobileNet",,
Automatic Multi-Objective Clustering Based on Game Theory,"Heloulou, Imen and Radjef, Mohammed Said and Kechadi, Mohand Tahar","Pergamon Press, Inc.",2017,"Novel multi-objective clustering algorithm based-sequential games was proposed.It optimizes simultaneously and efficiently multiple conflicting objectives.Proposed approach can automatically calculate the optimal number of clusters.This algorithm shows good performance of generating high-quality solutions.Experimental study demonstrates the effectiveness of our algorithm over others. Data clustering is a very well studied problem in machine learning, data mining, and related disciplines. Most of the existing clustering methods have focused on optimizing a single clustering objective. Often, several recent disciplines such as robot team deployment, ad hoc networks, multi-agent systems, facility location, etc., need to consider multiple criteria, often conflicting, during clustering. Motivated by this, in this paper, we propose a sequential game theoretic approach for multi-objective clustering, called ClusSMOG-II. It is specially designed to optimize simultaneously intrinsically conflicting objectives, which are inter-cluster/intra-cluster inertia and connectivity. This technique has an advantage of keeping the number of clusters dynamic. The approach consists of three main steps. The first step sets initial clusters with their representatives, whereas the second step calculates the correct number of clusters by resolving a sequence of multi-objective multi-act sequential two-player games for conflict-clusters. Finally, the third step constructs homogenous clusters by resolving sequential two-player game between each cluster representative and the representative of its nearest neighbor. For each game, we define payoff functions that correspond to the model objectives. We use a methodology based on backward induction to calculate a pure Nash equilibrium for each game. Experimental results confirm the effectiveness of the proposed approach over state-of-the-art clustering algorithms.",10.1016/j.eswa.2016.09.008,"Sequential game, Backward induction, Multi-objective clustering, Nash equilibrium",,
A Time Series Forecasting Based Multi-Criteria Methodology for Air Quality Prediction,"Espinosa, Raquel and Palma, Jos\'{e} and Jim\'{e}nez, Fernando and Kami\'{n}ska, Joanna and Sciavicco, Guido and Lucena-S\'{a}nchez, Estrella",Elsevier Science Publishers B. V.,2021,,10.1016/j.asoc.2021.107850,"Multivariate time series forecasting, Deep learning, Multi-criteria decision support systems, Air quality",,
Video Trajectory Analysis Using Unsupervised Clustering and Multi-Criteria Ranking,"Sekh, Arif Ahmed and Dogra, Debi Prosad and Kar, Samarjit and Roy, Partha Pratim",Springer-Verlag,2020,"Surveillance camera usage has increased significantly for visual surveillance. Manual analysis of large video data recorded by cameras may not be feasible on a larger scale. In various applications, deep learning-guided supervised systems are used to track and identify unusual patterns. However, such systems depend on learning which may not be possible. Unsupervised methods relay on suitable features and demand cluster analysis by experts. In this paper, we propose an unsupervised trajectory clustering method referred to as t-Cluster. Our proposed method prepares indexes of object trajectories by fusing high-level interpretable features such as origin, destination, path, and deviation. Next, the clusters are fused using multi-criteria decision making and trajectories are ranked accordingly. The method is able to place abnormal patterns on the top of the list. We have evaluated our algorithm and compared it against competent baseline trajectory clustering methods applied to videos taken from publicly available benchmark datasets. We have obtained higher clustering accuracies on public datasets with significantly lesser computation overhead.",10.1007/s00500-020-04967-9,"Unsupervised clustering, Object trajectory, Motion analysis",,
A Multi-Objective Optimization Model for Music Styles,"Malandrino, Delfina and Zaccagnino, Rocco and Zizza, Rosalba",Association for Computing Machinery,2018,"Style recognition is one of the problems mostly faced by Computational Intelligence techniques. Most of them were defined ad-hoc for a specific music genre and so not generalizable and applicable to any style. A music style, both of soloists performer and of musical collectives, is the result of aesthetic goals, i.e., experience and preferences, functional rules, i.e., rules used to produce music, and external influence, i.e., the choices depending by the simultaneous presence of other musicians. We propose a new model of style, defined in terms of a multi-objective problem, where the objective is to minimize the distance between the style of each musician and the stylistic features derived by other musicians. Such a model is general since it is applicable to any type of style. We also propose a new approach for both recognition and automatic composition of styles based on such a model, which exploits a machine learning recognizer and a splicing composer. To assess the effectiveness and the generalization capability of our system we performed several tests using a large set of Jazz transcriptions and a corpus of 4-voice music by J. S. Bach. We show that our classifier is able to achieve a recognition accuracy of 97.1%. With regard to the composition process, we measured the capability of our system to capture both aesthetic goals by collecting subjective perceptions from domain experts, and functional rules by computing the average percentage of (1) typical harmonic progressions in the Jazz music produced and (2) forbidden exceptions, which occur in the 4-voice music, produced.",10.1145/3297156.3297205,"Machine learning, Splicing systems, Music Style recognition",,
Multi-Objective Hyperparameter Tuning and Feature Selection Using Filter Ensembles,"Binder, Martin and Moosbauer, Julia and Thomas, Janek and Bischl, Bernd",Association for Computing Machinery,2020,"Both feature selection and hyperparameter tuning are key tasks in machine learning. Hyperparameter tuning is often useful to increase model performance, while feature selection is undertaken to attain sparse models. Sparsity may yield better model interpretability and lower cost of data acquisition, data handling and model inference. While sparsity may have a beneficial or detrimental effect on predictive performance, a small drop in performance may be acceptable in return for a substantial gain in sparseness. We therefore treat feature selection as a multi-objective optimization task. We perform hyperparameter tuning and feature selection simultaneously because the choice of features of a model may influence what hyperparameters perform well.We present, benchmark, and compare two different approaches for multi-objective joint hyperparameter optimization and feature selection: The first uses multi-objective model-based optimization. The second is an evolutionary NSGA-II-based wrapper approach to feature selection which incorporates specialized sampling, mutation and recombination operators. Both methods make use of parameterized filter ensembles.While model-based optimization needs fewer objective evaluations to achieve good performance, it incurs computational overhead compared to the NSGA-II, so the preferred choice depends on the cost of evaluating a model on given data.",10.1145/3377930.3389815,"model-based optimization, multiobjective optimization, hyperparameter optimization, evolutionary algorithms, feature selection",,
Deep Belief Networks Ensemble with Multi-Objective Optimization for Failure Diagnosis,"Zhang, Chong and Sun, Jia Hui and Tan, Kay Chen",IEEE Press,2015,"Early diagnosis that can detect faults from some symptoms accurately is critical, because it provides the potential benefits such as reducing maintenance costs, improving productivity and avoiding serious damages. Degradation pattern classification for early diagnosis has not been explored in many researches yet. This paper will use hybrid ensemble model for degradation pattern classification. Supervised training of deep models (e.g. Many-layered Neural Nets) is difficult for optimization problem with unlabeled datasets or insufficient data sample. Shallow models (SVMs, Neural Networks, etc...) are unlikely candidates for learning high-level abstractions, since they are affected by the curse of dimensionality. Therefore, deep learning network (DBN), an unsupervised learning model, in diagnosis problem has been investigated to do classification. Few researches have been done for exploring the effects of DBN in diagnosis. In this paper, an ensemble of DBNs with MOEA/D has been applied for diagnosis to handle failure degradation with multivariate sensory data. Turbofan engine degradation dataset is employed to demonstrate the efficacy of the proposed model. We believe that deep learning with multi-objective ensemble for degradation pattern classification can shed new light on failure diagnosis, and our work presented the applicability of this method to diagnosis as well as prognostics.",10.1109/SMC.2015.19,,,
Correlated Multi-Objective Multi-Fidelity Optimization for HLS Directives Design,"Sun, Qi and Chen, Tinghuan and Liu, Siting and Chen, Jianli and Yu, Hao and Yu, Bei",Association for Computing Machinery,2022,"High-level synthesis (HLS) tools have gained great attention in recent years because it emancipates engineers from the complicated and heavy hardware description language writing and facilitates the implementations of modern applications (e.g., deep learning models) on Field-programmable Gate Array (FPGA), by using high-level languages and HLS directives. However, finding good HLS directives is challenging, due to the time-consuming design processes, the balances among different design objectives, and the diverse fidelities (accuracies of data) of the performance values between the consecutive FPGA design stages.To find good HLS directives, a novel automatic optimization algorithm is proposed to explore the Pareto designs of the multiple objectives while making full use of the data with different fidelities from different FPGA design stages. Firstly, a non-linear Gaussian process (GP) is proposed to model the relationships among the different FPGA design stages. Secondly, for the first time, the GP model is enhanced as correlated GP (CGP) by considering the correlations between the multiple design objectives, to find better Pareto designs. Furthermore, we extend our model to be a deep version deep CGP (DCGP) by using the deep neural network to improve the kernel functions in Gaussian process models, to improve the characterization capability of the models, and learn better feature representations. We test our design method on some public benchmarks (including general matrix multiplication and sparse matrix-vector multiplication) and deep learning-based object detection model iSmart2 on FPGA. Experimental results show that our methods outperform the baselines significantly and facilitate the deep learning designs on FPGA.",10.1145/3503540,"Gaussian process, multi-fidelity optimization, High-level synthesis, correlated multi-objective optimization, design space exploration",,
Surrogate-Assisted Multi-Objective Model Selection for Support Vector Machines,"Rosales-P\'{e}rez, Alejandro and Gonzalez, Jesus A. and Coello Coello, Carlos A. and Escalante, Hugo Jair and Reyes-Garcia, Carlos A.",Elsevier Science Publishers B. V.,2015,"Classification is one of the most well-known tasks in supervised learning. A vast number of algorithms for pattern classification have been proposed so far. Among these, support vector machines (SVMs) are one of the most popular approaches, due to the high performance reached by these methods in a wide number of pattern recognition applications. Nevertheless, the effectiveness of SVMs highly depends on their hyper-parameters. Besides the fine-tuning of their hyper-parameters, the way in which the features are scaled as well as the presence of non-relevant features could affect their generalization performance. This paper introduces an approach for addressing model selection for support vector machines used in classification tasks. In our formulation, a model can be composed of feature selection and pre-processing methods besides the SVM classifier. We formulate the model selection problem as a multi-objective one, aiming to minimize simultaneously two components that are closely related to the error of a model: bias and variance components, which are estimated in an experimental fashion. A surrogate-assisted evolutionary multi-objective optimization approach is adopted to explore the hyper-parameters space. We adopted this approach due to the fact that estimating the bias and variance could be computationally expensive. Therefore, by using surrogate-assisted optimization, we expect to reduce the number of solutions evaluated by the fitness functions so that the computational cost would also be reduced. Experimental results conducted on benchmark datasets widely used in the literature, indicate that highly competitive models with a fewer number of fitness function evaluations are obtained by our proposal when it is compared to state of the art model selection methods.",10.1016/j.neucom.2014.08.075,"Support vector machines, Surrogate-assisted optimization, Model selection, Multi-objective optimization",,
Multi-Criteria Decision Analysis Approaches for Selecting and Evaluating Digital Learning Objects,"Ba\c{s}aran, Seren",Elsevier Science Publishers B. V.,2016,"The wide spread of internet resources has emerged new paradigms of learning and knowledge delivery to facilitate and enhance learning particularly in e-learning systems. The important milestones of these new paradigms are known as digital learning objects (DLO) which are smallest packed bits for learning. The abundance of these DLOs raises an important question on how to select effectively high quality reusable learning objects. There exist many soft computing approaches such as; fuzzy computing, neural networks, evolutionary computing, support vector machines, machine learning and probabilistic reasoning. In this paper, fuzzy multi-criteria decision-making methods in choosing suitable DLOs were reviewed. This paper discusses recent variety of soft computing methods used in selecting and evaluating digital learning objects through multi criteria decision analysis approaches used; for selecting metrics like basic topical, course similarity, internal topical, basic and user similarity personal, and context similarity situational relevance and for evaluation; scalarization method, employing triangular, trapezoidal and distance based similarity (adapted from TOPSIS techniques). As DLOs continue to evolve it is inevitable to utilize multi-criteria techniques for selecting and or improving quality. The abundance of DLOs has increased the need for applying practical soft computing techniques to retrieve high quality, reusable DLOs.",10.1016/j.procs.2016.09.398,"Digital Learning Objects(DLOs), multi-criteria decision making, soft computing, expert evaluation, fuzzy approach",,
Data Mining Methods for Knowledge Discovery in Multi-Objective Optimization,"Bandaru, Sunith and Ng, Amos H.C. and Deb, Kalyanmoy","Pergamon Press, Inc.",2017,"Data mining methods for extracting knowledge from multi-objective optimization are reviewed.Methods are classified based on the type and form of knowledge generated.Descriptive statistics, visual data mining and machine learning methods are discussed.Limitations of existing methods are discussed.A generic framework for knowledge-driven optimization is proposed. Real-world optimization problems typically involve multiple objectives to be optimized simultaneously under multiple constraints and with respect to several variables. While multi-objective optimization itself can be a challenging task, equally difficult is the ability to make sense of the obtained solutions. In this two-part paper, we deal with data mining methods that can be applied to extract knowledge about multi-objective optimization problems from the solutions generated during optimization. This knowledge is expected to provide deeper insights about the problem to the decision maker, in addition to assisting the optimization process in future design iterations through an expert system. The current paper surveys several existing data mining methods and classifies them by methodology and type of knowledge discovered. Most of these methods come from the domain of exploratory data analysis and can be applied to any multivariate data. We specifically look at methods that can generate explicit knowledge in a machine-usable form. A framework for knowledge-driven optimization is proposed, which involves both online and offline elements of knowledge discovery. One of the conclusions of this survey is that while there are a number of data mining methods that can deal with data involving continuous variables, only a few ad hoc methods exist that can provide explicit knowledge when the variables involved are of a discrete nature. Part B of this paper proposes new techniques that can be used with such datasets and applies them to discrete variable multi-objective problems related to production systems.",10.1016/j.eswa.2016.10.015,"Data mining, Machine learning, Multi-objective optimization, Descriptive statistics, Visual data mining, Knowledge-driven optimization",,
Multi-Criteria Optimization of Wireless Connectivity over Sparse Networks,"Ojog, Cristian-Octavian and Marin, Radu-Corneliu and Ciobanu, Radu-Ioan and Dobre, Ciprian","Elsevier North-Holland, Inc.",2016,"Opportunistic networking shows great promise in providing an infrastructure for MNP, through its unique perspective over mobility.Our previous studies show that WiFi is the most feasible media for opportunistic contacts between peers connected to the same wireless access point.We propose a machine learning algorithm that aims to increase the number of contacts between mobile nodes by using a smarter WiFi access point choosing heuristic.The algorithm properly balances signal strength, latency, bandwidth, and the number of friends predicted to connect to the respective access point.We show through simulations based on real-life tracing data-sets that our proposed solution not only increases the likelihood of opportunistic contacts, but it also evenly distributes social subgraphs of users over wireless networks while improving the overall hit rate. Opportunistic networking is at the basis of cyber-physical Mobile Networks in Proximity (MNP), through its unique perspective over mobility and the incorporation of socio-inspired networking algorithms. However, results in the field are mostly theoretical, proven to account for stricter hit rate and latency requirements in specific environments. They generally assume that two devices being in proximity automatically see one-another, an assumption which might not stand under real-world conditions (Bluetooth assumes a peering session and close-proximity, WiFi Direct implementations are different between manufacturers, etc.).Our previous studies in the area show that WiFi is still the most feasible media for opportunistic contacts. WiFi-enabled devices, with out-of-the-box networking capabilities, can connect in an ad-hoc opportunistic network, over wireless routers, and thus support a cyber-physical infrastructure for opportunistically spreading information.In this article, we propose a machine learning algorithm that aims to increase the number of contacts between mobile nodes by using a smarter WiFi access point selection heuristic. The algorithm is based on properly balancing signal strength, latency, bandwidth, and, most importantly, the number of friends predicted to connect to the respective access point. We show through simulations based on real-life tracing data-sets that our proposed solution not only increases the likelihood of opportunistic contacts, but it also evenly distributes social subgraphs of users over wireless networks while improving the overall hit rate.",10.1016/j.comnet.2016.07.010,"Learning, Wifi, Social, Machine, Networking, Opportunistic",,
Particle Swarm Optimization with Deep Learning for Human Action Recognition,"Berlin, S. Jeba and John, Mala",Kluwer Academic Publishers,2020,"A novel method for human action recognition using a deep learning network with features optimized using particle swarm optimization is proposed. The binary histogram, Harris corner points and wavelet coefficients are the features extracted from the spatiotemporal volume of the video sequence. In order to reduce the computational complexity of the system, the feature space is reduced by particle swarm optimization technique with the multi-objective fitness function. Finally, the performance of the system is evaluated using deep learning neural network (DLNN). Two autoencoders are trained independently and the knowledge embedded in the autoencoders are transferred to the proposed DLNN for human action recognition. The proposed framework achieves an average recognition rate of 91% on UT interaction set 1, 88% on UT interaction set 2, 91% on SBU interaction dataset and 94% on Weizmann dataset.",10.1007/s11042-020-08704-0,"Human action recognition, Video surveillance, Autoencoder, Deep learning network, Particle swarm optimization",,
Multi-Criteria Feature Selection on Cost-Sensitive Data with Missing Values,"Shu, Wenhao and Shen, Hong",Elsevier Science Inc.,2016,"Feature selection plays an important role in pattern recognition and machine learning. Confronted with high dimensional data in many data analysis tasks, feature selection techniques are designed to find a relevant feature subset of the original features which can facilitate classification. However, in many real-world applications, missing feature values that contribute to test and misclassification costs are emerging to be an issue of increasing concern for most data sets, particularly dealing with big data. The existing feature selection approaches do not address this issue effectively. In this paper, based on rough set theory we address the problem of feature selection for cost-sensitive data with missing values. We first propose a multi-criteria evaluation function to characterize the significance of candidate features, by taking into consideration not only the power in the positive region and boundary region but also their associated costs. On this basis, we develop a forward greedy feature selection algorithm for selecting a feature subset of minimized cost that preserves the same information as the whole feature set. In addition, to improve the efficiency of this algorithm, we implement the selection of candidate features in a dwindling object set. Finally, we demonstrate the superior performance of the proposed algorithm to the existing feature selection algorithms through experimental results on different data sets. HighlightsA multi-criteria based evaluation function is proposed for measuring features from different viewpoints.A dwindling universe is provided to accelerate the feature selection process.A feature selection algorithm is developed on cost-sensitive data with missing values.The efficiency and effectiveness of the proposed algorithm are demonstrated on different data sets.",10.1016/j.patcog.2015.09.016,"Cost-sensitive data, Feature selection, Incomplete data, Algorithm MCFSMulti-criteria based feature selection algorithm on cost-sensitive data with missing values, Multi-criteria, Rough sets",,
Detection of XSS Attack and Defense of REST Web Service – Machine Learning Perspective,"Ivanova, Malinka and Rozeva, Anna",Association for Computing Machinery,2021,"The paper presents a machine learning approach for detection of stored XSS attack and for defense of REST web service. For this purpose, a XML-based REST web service is developed in JAVA, which is tested and attacked in specially created test-bed simulation environment, consisting of IntelliJ IDEA environment, Postman and web browser. The obtained data sets are processed resulting in the selection of 30 out of 171 features for further treatment. Supervised machine learning classifiers: Random Forest, Random Tree, Decision Tree and Gradient Boosted Tree are used for the detection of known attacks and clustering algorithm k-Means for the identification of unknown threats. The efficiency of implementing machine learning algorithms is evaluated and the results confirm their high accuracy. In addition fuzzy sets and fuzzy logic theory is utilized for solving multi-criteria task in support of decision making for web service defense.",10.1145/3453800.3453805,"machine learning, REST web service defense, XSS stored attack, fuzzy logic",,
A Spam Filtering Multi-Objective Optimization Study Covering Parsimony Maximization and Three-Way Classification,"Basto-Fernandes, Vitor and Yevseyeva, Iryna and M\'{e}ndez, Jos\'{e} R. and Zhao, Jiaqi and Fdez-Riverola, Florentino and T.M. Emmerich, Michael",Elsevier Science Publishers B. V.,2016,"Display Omitted Advances on applications of multi-objective optimization to anti-SPAM filtering.Parsimony maximization of rule-based SPAM classifiers.Three-way classification balancing user effort and confidence level.Indicator-based/machine learning/decomposition-based evolutionary optimization. Classifier performance optimization in machine learning can be stated as a multi-objective optimization problem. In this context, recent works have shown the utility of simple evolutionary multi-objective algorithms (NSGA-II, SPEA2) to conveniently optimize the global performance of different anti-spam filters. The present work extends existing contributions in the spam filtering domain by using three novel indicator-based (SMS-EMOA, CH-EMOA) and decomposition-based (MOEA/D) evolutionary multi-objective algorithms. The proposed approaches are used to optimize the performance of a heterogeneous ensemble of classifiers into two different but complementary scenarios: parsimony maximization and e-mail classification under low confidence level. Experimental results using a publicly available standard corpus allowed us to identify interesting conclusions regarding both the utility of rule-based classification filters and the appropriateness of a three-way classification system in the spam filtering domain.",10.1016/j.asoc.2016.06.043,"Spam filtering, Three-way classification, Multi-objective optimization, Rule-based classifiers, SpamAssassin, Parsimony",,
Parallel Alternatives for Evolutionary Multi-Objective Optimization in Unsupervised Feature Selection,"Kimovski, Dragi and Ortega, Julio and Ortiz, Andr\'{e}s and Ba\~{n}os, Ra\'{u}l","Pergamon Press, Inc.",2015,"Multiobjective unsupervised feature selection with many decision variables is tackled.EEG signals for Brain-Computer Interface (BCI) applications are used as benchmarks.Cooperative evolutionary algorithms for multiobjective optimization are given.Parallel implementations obtain quality results in terms of hypervolume and speedup.Superlinear speedups are justified by adjusting models to experimental results. Many machine learning and pattern recognition applications require reducing dimensionality to improve learning accuracy while irrelevant inputs are removed. This way, feature selection has become an important issue on these researching areas. Nevertheless, as in past years the number of patterns and, more specifically, the number of features to be selected have grown very fast, parallel processing constitutes an important tool to reach efficient approaches that make possible to tackle complex problems within reasonable computing times. In this paper we propose parallel multi-objective optimization approaches to cope with high-dimensional feature selection problems. Several parallel multi-objective evolutionary alternatives are proposed, and experimentally evaluated by using some synthetic and BCI (Brain-Computer Interface) benchmarks. The experimental results show that the cooperation of parallel evolving subpopulations provides improvements in the solution quality and computing time speedups depending on the parallel alternative and data profile.",10.1016/j.eswa.2015.01.061,"Parallel evolutionary algorithms, Unsupervised classification, Multi-objective clustering, Feature selection, High-dimensional data, Speedup models",,
"Feature Weighting for Na\""{\i}ve Bayes Using Multi Objective Artificial Bee Colony Algorithm","Chaudhuri, Abhilasha and Sahu, Tirath Prasad",Inderscience Publishers,2021,"Na\""{\i}ve Bayes (NB) is a widely used classifier in the field of machine learning. However, its conditional independence assumption does not hold true in real-world applications. In literature, various feature weighting approaches have attempted to alleviate this assumption. Almost all of these approaches consider the relationship between feature-class (relevancy) and feature-feature (redundancy) independently, to determine the weights of features. We argue that these two relationships are mutually dependent and both cannot be improved simultaneously, i.e., form a trade-off. This paper proposes a new paradigm to determine the feature weight by formulating it as a multi-objective optimisation problem to balance the trade-off between relevancy and redundancy. Multi-objective artificial bee colony-based feature weighting technique for na\""{\i}ve Bayes (MOABC-FWNB) is proposed. An extensive experimental study was conducted on 20 benchmark UCI datasets. Experimental results show that MOABC-FWNB outperforms NB and other existing state-of-the-art feature weighting techniques.",10.1504/ijcse.2021.113655,"na\""{\i}ve Bayes, multi objective optimisation, artificial bee colony, feature weighting",,
Empirical Mode Decomposition Based Multi-Objective Deep Belief Network for Short-Term Power Load Forecasting,"Fan, Chaodong and Ding, Changkun and Zheng, Jinhua and Xiao, Leyi and Ai, Zhaoyang",Elsevier Science Publishers B. V.,2020,,10.1016/j.neucom.2020.01.031,"Multi-objective optimization algorithm, Ensemble learning, Power load forecasting, Deep belief network, Empirical Mode Decomposition",,
Exploring Multi-Objective Exercise Recommendations in Online Education Systems,"Huang, Zhenya and Liu, Qi and Zhai, Chengxiang and Yin, Yu and Chen, Enhong and Gao, Weibo and Hu, Guoping",Association for Computing Machinery,2019,"Recommending suitable exercises to students in an online education system is highly useful. Existing approaches usually rely on machine learning techniques to mine large amounts of student interaction log data accumulated in the systems to select the most suitable exercises for each student. Generally, they mainly aim to optimize a single objective, i.e., recommending non-mastered exercises to address the immediate weakness of students. While this is a reasonable objective, there exist more beneficial multiple objectives in the long-term learning process that need to be addressed including Review &amp; Explore, Smoothness of difficulty level and Engagement. In this paper, we propose a novel Deep Reinforcement learning framework, namely DRE, for adaptively recommending Exercises to students with optimization of above three objectives. In the framework, we propose two different Exercise Q-Networks for the agent, i.e., EQNM and EQNR, to generate recommendations following Markov property and Recurrent manner, respectively. We also propose novel reward functions to formally quantify those three objectives so that DRE could update and optimize its recommendation strategy by interactively receiving students' performance feedbacks (e.g., score). We conduct extensive experiments on two real-world datasets. Experimental results clearly show that the proposed DRE can effectively learn from the student interaction data to optimize multiple objectives in a single unified framework and adaptively recommend suitable exercises to students.",10.1145/3357384.3357995,"recommendation, multiple objectives, deep reinforcement learning",,
Neural Network for Learning and Analyzing Preferences for Multi-Criteria Services,"Haddar, Imane and Raouyane, Brahim and Bellafkih, Mostafa",Association for Computing Machinery,2020,"In the latest years, service selection is becoming more and more important due to the significant effect of internet based services in the telecom industry. When it comes to selecting the best service, different candidate services with similar settings are proposed by different service providers. The selection should take into consideration the respect of the constraints of consumers in terms of Service Level Agreement contracts, what makes the modelling of the preferences of decision-makers for choice problems the main focus of this work. In order to model these preferences, we propose contextual preference functions based on machine learning techniques from neural networks. It will therefore be possible to further explain and decode preferences in order to facilitate negotiation and thus decision-making, thereby improving the quality of service providers while being on customer preferences.",10.1145/3419604.3419799,"Decision aid, Preferences, Neural network, Service selection",,
A Particle Swarm Optimization Approach to Multi Criteria Recommender System Utilizing Effective Similarity Measures,"Choudhary, Priyankar and Kant, Vibhor and Dwivedi, Pragya",Association for Computing Machinery,2017,"Recommender system (RS), a web personalization tool, attempts to generate suitable recommendations to users based on their preferences. Generally, recommender system works on overall ratings but these ratings do not reflect the actual user preferences. Therefore, incorporation of multiple criteria ratings into RS can capture the user preferences accurately and produce effective recommendations to users. Multi criteria recommender systems (MCRS) generate recommendations to users based on the aggregation of similarities computed on multiple criteria using collaborative filtering. However, capturing optimal weights of various users on different criteria in the process of similarity aggregation is a major concern. Further selection of appropriate similarity measure is another challenge for employing collaborative filtering. Our work in this paper is an attempt towards developing multi criteria recommender systems by utilizing various similarity measures and particle swarm optimization to learn optimal weights. Experimental results reveal that our proposed approaches outperform other traditional approaches.",10.1145/3055635.3056619,"collaborative filtering, particle swarm optimization, similarity measures, Multi criteria recommender system",,
Multi-Objective Search of Robust Neural Architectures against Multiple Types of Adversarial Attacks,"Liu, Jia and Jin, Yaochu",Elsevier Science Publishers B. V.,2021,,10.1016/j.neucom.2021.04.111,"Adversarial attacks, Robustness, Multi-objective evolutionary algorithm, Neural architecture search",,
An Ensemble Multi-Objective Particle Swarm Optimization Approach for Exchange Rates Forecasting Problem,"Dinh, Thi Thu Huong and Vu, Van Truong and Bui, Lam Thu",Association for Computing Machinery,2020,"In this paper, the authors propose an ensemble multi-objective particle swarm optimisation approach (named EMPSO) for forecasting the currency exchange rate chain. The proposed algorithm consists of two main phases. The first phase uses a multi-objective particle swarm optimisation algorithm to find a set of the best optimal particles (named leaders). The second phase then uses these leaders to jointly calculate the final results by using the soft voting ensemble method. The two objective functions used here are predictive error and particle diversity. The empirical data used in this study are six different sets of currency exchange rates. Through comparison results with other evolutionary algorithms and other multi-objective PSO algorithms, the proposed algorithm shows that it can achieve better as well as more stability results on experimental data sets.",10.1145/3380688.3380717,"multi-objective PSO, ensemble learning, Time series forecasting, PSO",,
Immunotherapy Treatment Outcome Prediction in Metastatic Melanoma through an Automated Multi-Objective Delta-Radiomics Model,"Chen, Xi and Zhou, Meijuan and Wang, Zhilong and Lu, Si and Chang, Shaojie and Zhou, Zhiguo","Pergamon Press, Inc.",2021,,10.1016/j.compbiomed.2021.104916,"Ensemble learning, Genetic algorithms, Delta radiomics, Multi-objective learning, Outcome prediction",,
Developing a Deep Learning Framework with Two-Stage Feature Selection for Multivariate Financial Time Series Forecasting,"Niu, Tong and Wang, Jianzhou and Lu, Haiyan and Yang, Wendong and Du, Pei","Pergamon Press, Inc.",2020,,10.1016/j.eswa.2020.113237,"Multi-objective optimization, Multivariate financial time series, Deep learning, Forecasting, Feature selection",,
Multi-Objective Exploration for Practical Optimization Decisions in Binary Translation,"Park, Sunghyun and Wu, Youfeng and Lee, Janghaeng and Aupov, Amir and Mahlke, Scott",Association for Computing Machinery,2019,"In the design of mobile systems, hardware/software (HW/SW) co-design has important advantages by creating specialized hardware for the performance or power optimizations. Dynamic binary translation (DBT) is a key component in co-design. During the translation, a dynamic optimizer in the DBT system applies various software optimizations to improve the quality of the translated code. With dynamic optimization, optimization time is an exposed run-time overhead and useful analyses are often restricted due to their high costs. Thus, a dynamic optimizer needs to make smart decisions with limited analysis information, which complicates the design of optimization decision models and often causes failures in human-made heuristics. In mobile systems, this problem is even more challenging because of strict constraints on computing capabilities and memory size.To overcome the challenge, we investigate an opportunity to build practical optimization decision models for DBT by using machine learning techniques. As the first step, loop unrolling is chosen as the representative optimization. We base our approach on the industrial strength DBT infrastructure and conduct evaluation with 17,116 unrollable loops collected from 200 benchmarks and real-life programs across various domains. By utilizing all available features that are potentially important for loop unrolling decision, we identify the best classification algorithm for our infrastructure with consideration for both prediction accuracy and cost. The greedy feature selection algorithm is then applied to the classification algorithm to distinguish its significant features and cut down the feature space. By maintaining significant features only, the best affordable classifier, which satisfies the budgets allocated to the decision process, shows 74.5% of prediction accuracy for the optimal unroll factor and realizes an average 20.9% reduction in dynamic instruction count during the steady-state translated code execution. For comparison, the best baseline heuristic achieves 46.0% prediction accuracy with an average 13.6% instruction count reduction. Given that the infrastructure is already highly optimized and the ideal upper bound for instruction reduction is observed at 23.8%, we believe this result is noteworthy.",10.1145/3358185,Loop unrolling<!--?clr?-->,,
Steady State IBEA Assisted by MLP Neural Networks for Expensive Multi-Objective Optimization Problems,"Azzouz, Nessrine and Bechikh, Slim and Ben Said, Lamjed",Association for Computing Machinery,2014,"Several engineering problems involve simultaneously several objective functions where at least one of them is expensive to evaluate. This fact has yielded to a new class of Multi-Objective Problems (MOPs) called expensive MOPs. Several attempts have been conducted in the literature with the goal to minimize the number of expensive evaluations by using surrogate models stemming from the machine learning field. Usually, researchers substitute the expensive objective function evaluation by an estimation drawn from the used surrogate. In this paper, we propose a new way to tackle expensive MOPs. The main idea is to use Neural Networks (NNs) within the Indicator-Based Evolutionary Algorithm (IBEA) in order to estimate the contribution of each generated offspring in terms of hypervolume. After that, only fit individuals with respect to the estimations are exactly evaluated. Our proposed algorithm called NN-SS-IBEA (Neural Networks assisted Steady State IBEA) have been demonstrated to provide good performance with a low number of function evaluations when compared against the original IBEA and MOEA/D-RBF on a set of benchmark problems in addition to the airfoil design problem.",10.1145/2576768.2598271,"multi-objective optimization, neural networks, surrogate model, fitness approximation",,
Collaborative Filtering Recommender System Base on the Interaction Multi-Criteria Decision with Ordered Weighted Averaging Operator,"Huynh, Tri Minh and Huynh, Hung Huu and Tran, Vu The and Huynh, Hiep Xuan",Association for Computing Machinery,2018,"In the recommender system, the most important is the decision-making solutionto consulte for user. Depending on the type and size of data stored, decision-making will always be improved to produce the best possible result.. The main task in implementing the model is to use methods to find the most valuable product or service for the user. In this paper, we propose a new approach to building a multi-user based collaborative filtering model using the interaction multi-criteria decision with ordered weighted averaging operator. This model demonstrates the synergy and interplay between user criteria for decision making. The model was evaluated through experimentation with the multirecsys tool on three datasets: MovieLense 100K, MSWeb and Jester5k. The experiment illustrated the model comparison with some other interactive multi-criteria counseling methods that have been researchedon both sparse datasets and thick datasets. In addition, the model is compared and evaluated with item-base collaborative filtering model using the interaction multi-criteria decision with ordered weighted averaging operator on two types of datasets. Consultancy results of the proposed model are quite effective compared to some traditional consulting models and some models with other operator. This counseling model can be applied well in a variety of contexts, especially in the case of sparse data, this model will give result in improved counseling. In addition, with the above method, the user-base model is always more efficient than item-base on all datasets.",10.1145/3184066.3184075,"the interaction multi-criteria decision, item-base, user-base, ordered weighted averaging operator, collaborative filtering recommender system",,
A Learning-Based <i>Innovized</i> Progress Operator for Faster Convergence in Evolutionary Multi-Objective Optimization,"Mittal, Sukrit and Saxena, Dhish Kumar and Deb, Kalyanmoy and Goodman, Erik D.",Association for Computing Machinery,2021,"Learning effective problem information from already explored search space in an optimization run, and utilizing it to improve the convergence of subsequent solutions, have represented important directions in Evolutionary Multi-objective Optimization (EMO) research. In this article, a machine learning (ML)-assisted approach is proposed that: (a) maps the solutions from earlier generations of an EMO run to the current non-dominated solutions in the decision space; (b) learns the salient patterns in the mapping using an ML method, here an artificial neural network (ANN); and (c) uses the learned ML model to advance some of the subsequent offspring solutions in an adaptive manner. Such a multi-pronged approach, quite different from the popular surrogate-modeling methods, leads to what is here referred to as the Innovized Progress (IP) operator. On several test and engineering problems involving two and three objectives, with and without constraints, it is shown that an EMO algorithm assisted by the IP operator offers faster convergence behavior, compared to its base version independent of the IP operator. The results are encouraging, pave a new path for the performance improvement of EMO algorithms, and set the motivation for further exploration on more challenging problems.",10.1145/3474059,"artificial neural networks, Multiobjective optimization, online innovization, learning-based optimization, innovization",,
Modeling and Evolutionary Optimization for Multi-Objective Vehicle Routing Problem with Real-Time Traffic Conditions,"Xiao, Long and Li, Changhe and Wang, Junchen and Mavrovouniotis, Michalis and Yang, Shengxiang and Dan, Xiaorong",Association for Computing Machinery,2020,"The study of the vehicle routing problem (VRP) is of outstanding significance for reducing logistics costs. Currently, there is little VRP considering real-time traffic conditions. In this paper, we propose a more realistic and challenging multi-objective VRP containing real-time traffic conditions. Besides, we also offer an adaptive local search algorithm combined with a dynamic constrained multi-objective evolutionary framework. In the algorithm, we design eight local search operators and select them adaptively to optimize the initial solutions. Experimental results show that our algorithm can obtain an excellent solution that satisfies the constraints of the vehicle routing problem with real-time traffic conditions.",10.1145/3383972.3384041,"Local Search, Multi-objective Optimization, Constrained Optimization, Vehicle Routing Problem",,
Interpretable and Accurate Medical Data Classification - a Multi-Objective Genetic-Fuzzy Optimization Approach,"Gorza\l{}czany, Marian B. and Rudzi\'{n}ski, Filip","Pergamon Press, Inc.",2017,"Novel fuzzy rule-based system (FRBS) for medical data classification tasks is proposed.FRBSs are designed using multi-objective evolutionary optimization algorithms.Set of FRBSs with various levels of accuracy-interpretability trade-off is generated.Benchmark medical data sets are used to evaluate the effectiveness of the system.Highly interpretable and accurate medical decision support is provided by our approach. In medical decision support systems, both the accuracy (i.e., the ability to adequately represent the decision making processes) as well as the transparency and interpretability (i.e., the ability to provide a domain user with compact and understandable explanation and justification of the proposed decisions) play essential roles. This paper presents an approach for automatic design of fuzzy rule-based classification systems (FRBCSs) from medical data using multi-objective evolutionary optimization algorithms (MOEOAs). Our approach generates, in a single run, a collection of solutions (medical FRBCSs) characterized by various levels of accuracy-interpretability trade-off. We propose a new complexity-related interpretability measure and we address the semantics-related interpretability issue by means of efficient implementation of the so-called strong fuzzy partitions of attribute domains. We also introduce a special-coding-free representation of the rule base and original genetic operators for its processing as well as we implement our ideas in the context of well-known and one of the presently most advanced MOEOAs, i.e., Non-dominated Sorting Genetic Algorithm II (NSGA-II). An important part of the paper is devoted to a broad comparative analysis of our approach and as many as 26 alternative techniques arranged in 32 experimental set-ups and applied to three well-known benchmark medical data sets (Breast Cancer Wisconsin (Original), Pima Indians Diabetes, and Heart Disease (Cleveland)) available from the UCI repository of machine learning databases (http://archive.ics.uci.edu/ml). A number of useful in medical applications performance measures including accuracy, sensitivity, specificity, and several interpretability measures are employed. The results of such a broad comparative analysis demonstrate that our approach significantly outperforms the alternative methods in terms of the interpretability of the obtained FRBCSs while remaining either competitive or superior in terms of their accuracy. It is worth stressing that the overwhelming majority of the existing medical classification methods concentrate almost exclusively on the accuracy issues.",10.1016/j.eswa.2016.11.017,"Accuracy and interpretability of medical classification systems, Fuzzy rule-based systems, Genetic computations, Multi-objective evolutionary optimization, Medical decision support",,
Using the Contextual Language Model BERT for Multi-Criteria Classification of Scientific Articles,"Ambalavanan, Ashwin Karthik and Devarakonda, Murthy V.",Elsevier Science,2020,,10.1016/j.jbi.2020.103578,"Screening scientific articles, Text classification, BERT, Neural networks, Biomedical natural language processing, SciBERT, Machine learning",,
A Methodology for Evaluating Multi-Objective Evolutionary Feature Selection for Classification in the Context of Virtual Screening,"Jim\'{e}nez, Fernando and P\'{e}rez-S\'{a}nchez, Horacio and Palma, Jos\'{e} and S\'{a}nchez, Gracia and Mart\'{\i}nez, Carlos",Springer-Verlag,2019,"Virtual screening (VS) methods have been shown to increase success rates in many drug discovery campaigns, when they complement experimental approaches, such as high-throughput screening methods or classical medicinal chemistry approaches. Nevertheless, predictive capability of VS is not yet optimal, mainly due to limitations in the underlying physical principles describing drug binding phenomena. One approach that can improve VS methods is the aid of machine learning methods. When enough experimental data are available to train such methods, predictive capability can considerably increase. We show in this research work how a multi-objective evolutionary search strategy for feature selection, which can provide with small and accurate decision trees that can be very easily understood by chemists, can drastically increase the applicability and predictive ability of these techniques and therefore aid considerable in the drug discovery problem. With the proposed methodology, we find classification models with accuracy between 0.9934 and 1.00 and area under ROC between 0.96 and 1.00 evaluated in full training sets, and accuracy between 0.9849 and 0.9940 and area under ROC between 0.89 and 0.93 evaluated with tenfold cross-validation over 30 iterations, while substantially reducing the model size.",10.1007/s00500-018-3479-0,"Feature selection, Multi-objective evolutionary algorithms, Virtual screening, Classification, Drug discovery, Decision trees",,
Multi-Objective Optimal Design of Excitation Systems of Synchronous Condensers for HVDC Systems Based on MOEA/D,"Shi, Fan and Wang, Honghua and Lu, Tianhang and Wang, Chengliang",Association for Computing Machinery,2021,"In order to optimize the reactive power characteristics of synchronous condensers and improve the capability of condensers to support the voltage of AC systems, in this paper, the outer loop control of the reactive power of condensers and the outer loop control of the voltage of AC systems are introduced into the design of the main excitation systems of condensers in high voltage direct current (HVDC) systems. Meanwhile, taking the integral values, peak values and steady-state values of voltage deviations of AC systems as objective functions, the multi-objective optimization design of the proportional adjustment coefficients in the outer loop control of the reactive power of condensers and the voltage of AC systems is carried out via utilizing a multi-objective evolutionary algorithm based on decomposition (MOEA/D) combining with fuzzy control method. Its purpose is to alleviate the overvoltage problems of power grids caused by the feedback of the reactive power of condensers and the voltage of AC systems. Lastly, the simulation model of ±100 kV HVDC system with a synchronous condenser is established. The simulation results show that the optimal design method of excitation systems of synchronous condensers proposed in this paper can optimize the reactive power characteristics of the condenser, ensure the rapid regulation of the voltage of the AC system by the condenser, and solve the overvoltage problem in the AC system caused by the reactive power regulation of the condenser which can not change suddenly and the feedback links of the reactive power of the condenser and the voltage of the AC system in the excitation system.",10.1145/3457682.3457770,"Synchronous condenser, Optimal design, Fuzzy control, HVDC, Multi-objective optimization",,
"A Multi-Objective Genetic Optimization for Fast, Fuzzy Rule-Based Credit Classification with Balanced Accuracy and Interpretability","Gorza\l{}czany, Marian B. and Rudzi\'{n}ski, Filip",Elsevier Science Publishers B. V.,2016,"Graphical abstractDisplay Omitted HighlightsNovel fuzzy rule-based classifiers (FRBCs) for financial data classification tasks are proposed.FRBCs are designed using multi-objective evolutionary optimization algorithms.Collection of FRBCs with various levels of accuracy-interpretability trade-off is generated.Benchmark financial data sets are used to evaluate the effectiveness of the proposed system.Highly accurate, interpretable and fast financial decision support is provided by our approach. Credit classification is an important component of critical financial decision making tasks such as credit scoring and bankruptcy prediction. Credit classification methods are usually evaluated in terms of their accuracy, interpretability, and computational efficiency. In this paper, we propose an approach for automatic designing of fuzzy rule-based classifiers (FRBCs) from financial data using multi-objective evolutionary optimization algorithms (MOEOAs). Our method generates, in a single experiment, an optimized collection of solutions (financial FRBCs) characterized by various levels of accuracy-interpretability trade-off. In our approach we address the complexity- and semantics-related interpretability issues, we introduce original genetic operators for the classifier's rule base processing, and we implement our ideas in the context of Non-dominated Sorting Genetic Algorithm II (NSGA-II), i.e., one of the presently most advanced MOEOAs. A significant part of the paper is devoted to an extensive comparative analysis of our approach and 24 alternative methods applied to three standard financial benchmark data sets, i.e., Statlog (Australian Credit Approval), Statlog (German Credit Approval), and Credit Approval (also referred to as Japanese Credit) sets available from the UCI repository of machine learning databases (http://archive.ics.uci.edu/ml). Several performance measures including accuracy, sensitivity, specificity, and some number of interpretability measures are employed in order to evaluate the obtained systems. Our approach significantly outperforms the alternative methods in terms of the interpretability of the obtained financial data classifiers while remaining either competitive or superior in terms of their accuracy and the speed of decision making.",10.1016/j.asoc.2015.11.037,"Fuzzy rule-based systems, Multi-objective evolutionary optimization, Accuracy and interpretability of credit classification systems, Financial decision support, Genetic computations",,
Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent,"Chen, Yudong and Su, Lili and Xu, Jiaming",Association for Computing Machinery,2018,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server and m working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of the m working machines suffer Byzantine faults -- a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks. In this paper, based on the geometric median of means of the gradients, we propose a simple variant of the classical gradient descent method. We show that our method can tolerate q Byzantine failures up to 2(1+ε)q \l{}e m for an arbitrarily small but fixed constant ε&gt;0. The parameter estimate converges in O(\l{}og N) rounds with an estimation error on the order of max √dq/N, ~√d/N , which is larger than the minimax-optimal error rate √d/N in the centralized and failure-free setting by at most a factor of √q . The total computational complexity of our algorithm is of O((Nd/m) log N) at each working machine and O(md + kd log 3 N) at the central server, and the total communication cost is of O(m d log N). We further provide an application of our general results to the linear regression problem. A key challenge arises in the above problem is that Byzantine failures create arbitrary and unspecified dependency among the iterations and the aggregated gradients. To handle this issue in the analysis, we prove that the aggregated gradient, as a function of model parameter, converges uniformly to the true gradient function.",10.1145/3292040.3219655,"byzantine adversaries, learning, security, distributed systems",,
Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent,"Chen, Yudong and Su, Lili and Xu, Jiaming",Association for Computing Machinery,2018,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server and m working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of the m working machines suffer Byzantine faults -- a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks. In this paper, based on the geometric median of means of the gradients, we propose a simple variant of the classical gradient descent method. We show that our method can tolerate q Byzantine failures up to 2(1+ε)q \l{}e m for an arbitrarily small but fixed constant ε&gt;0. The parameter estimate converges in O(\l{}og N) rounds with an estimation error on the order of max √dq/N, ~√d/N , which is larger than the minimax-optimal error rate √d/N in the centralized and failure-free setting by at most a factor of √q . The total computational complexity of our algorithm is of O((Nd/m) log N) at each working machine and O(md + kd log 3 N) at the central server, and the total communication cost is of O(m d log N). We further provide an application of our general results to the linear regression problem. A key challenge arises in the above problem is that Byzantine failures create arbitrary and unspecified dependency among the iterations and the aggregated gradients. To handle this issue in the analysis, we prove that the aggregated gradient, as a function of model parameter, converges uniformly to the true gradient function.",10.1145/3219617.3219655,"distributed systems, security, byzantine adversaries, learning",,
Research on the Optimization of the Supplier Intelligent Management System for Cross-Border e-Commerce Platforms Based on Machine Learning,"Yang, Ying",Springer-Verlag,2020,"At present, with the continuous development of the intelligent system, it is used in many industries. In e-commerce industry, the intelligent system has also been used, especially in supplier management. Based on the machine learning theory, this paper studies the optimization of the supplier management intelligent system of cross-border e-commerce platforms. Based on the wisdom algorithm and machine learning perspective, the optimization of cross-border e-commerce platform supplier credit system is studied in this paper. Firstly, the calculation of the traditional supplier credit evaluation is optimized by introducing the decision matrix algorithm of the difference matrix and the cloud model evaluation method. Then a multi-objective joint decision model of supplier selection and order allocation is established, and the multi-objective evolutionary algorithm combined with actual examples is applied to verify the effectiveness and feasibility of the algorithm and model. Finally, the decision makers’ preferences are integrated into the intelligent decision-making, and the cloud model evaluation method is adopted. The rough set and gray relational analysis mathematical tools are used to construct the procurement supply evaluation system. The research results show that the comparison of the three general indicators of the procurement supply chain can be obtained through the cloud model evaluation calculation, which indirectly reflects the preference decision weights of the three objective functions of the cross-border e-commerce supplier selection and order allocation multi-objective optimization model. This indicates that the procurement supply evaluation system constructed in this paper has achieved the purpose of scientific evaluation and selection of suppliers, and has played a theoretical reference role for supplier management of cross-border e-commerce platform.",10.1007/s10257-019-00402-1,"Machine learning, System optimization, Cross-border e-commerce",,
Quantitative Interactive Investment Algorithm Based on Machine Learning and Data Mining,"Cheng, Fangyuan and Jia, Junmin",Association for Computing Machinery,2021,"Quantitative investment is a mean to predict the development of securities and conduct transactions by using computer algorithms, but it usually ignores the guiding role of investors' personal preferences in deciding investment plans. Thus, we suggest a quantitative interactive investment algorithm to integrate the rational goals of investment with the individual preference indicators of decision-making investors. The interactive multi-objective solution algorithm creatively combines the decision tree algorithm which mine the investor's individual preference index from the investor's decision data with the second-generation non-dominated sorting genetic algorithm (NSGA-II). This method solves the problem of the lack of personalized choices in current quantitative investment methods by adding decision-makers’ preference indicators.",10.1145/3478905.3478982,"Quantitative investment, multi-objective optimization, machine learning, decision maker preference",,
Closed-Loop Restoration Approach to Blurry Images Based on Machine Learning and Feedback Optimization,"Yousaf, Saqib and Shiyin Qin",IEEE Press,2015,"Blind image deconvolution (BID) aims to remove or reduce the degradations that have occurred during the acquisition or processing. It is a challenging ill-posed problem due to a lack of enough information in degraded image for unambiguous recovery of both point spread function (PSF) and clear image. Although recently many powerful algorithms appeared; however, it is still an active research area due to the diversity of degraded images as well as degradations. Closed-loop control systems are characterized with their powerful ability to stabilize the behavior response and overcome external disturbances by designing an effective feedback optimization. In this paper, we employed feedback control to enhance the stability of BID by driving the current estimation quality of PSF to the desired level without manually selecting restoration parameters and using an effective combination of machine learning with feedback optimization. The foremost challenge when designing a feedback structure is to construct or choose a suitable performance metric as a controlled index and a feedback information. Our proposed quality metric is based on the blur assessment of deconvolved patches to identify the best PSF and computing its relative quality. The Kalman filter-based extremum seeking approach is employed to find the optimum value of controlled variable. To find better restoration parameters, learning algorithms, such as multilayer perceptron and bagged decision trees, are used to estimate the generic PSF support size instead of trial and error methods. The problem is modeled as a combination of pattern classification and regression using multiple training features, including noise metrics, blur metrics, and low-level statistics. Multi-objective genetic algorithm is used to find key patches from multiple saliency maps which enhance performance and save extra computation by avoiding ineffectual regions of the image. The proposed scheme is shown to outperform corresponding open-loop schemes, which often fails or needs many assumptions regarding images and thus resulting in sub-optimal results.",10.1109/TIP.2015.2492825,"bagged decision trees, learning, blur metric, blind deconvolution, closed-loop, image quality, Feedback optimization",,
Quality Assessment Methodology Based on Machine Learning with Small Datasets: Industrial Castings Defects,"Pastor-L\'{o}pez, Iker and Sanz, Borja and Tellaeche, Alberto and Psaila, Giuseppe and de la Puerta, Jos\'{e} Gaviria and Bringas, Pablo G.",Elsevier Science Publishers B. V.,2021,,10.1016/j.neucom.2020.08.094,"Artificial vision, Surface defect detection, Machine-learning, Defect categorization",,
Machine Learning Techniques to Discover Genes with Potential Prognosis Role in Alzheimers Disease Using Different Biological Sources,"Martnez-Ballesteros, Mara and Garca-Heredia, Jos M. and Nepomuceno-Chamorro, Isabel A. and Riquelme-Santos, Jos C.",Elsevier Science Publishers B. V.,2017,"Analyze Alzheimer disease gene expression profiles by changes in expression levels.Integration of machine learning methods: decision tree, associations and clustering.Fusion of external information sources: microarray, PubMed, GO and PPI network.Significant set of down/up regulated genes highly related with Alzheimer disease.Gene expression patterns and deep knowledge into relevant biological functions. Alzheimers disease is a complex progressive neurodegenerative brain disorder, being its prevalence expected to rise over the next decades. Unconventional strategies for elucidating the genetic mechanisms are necessary due to its polygenic nature. In this work, the input information sources are five: a public DNA microarray that measures expression levels of control and patient samples, repositories of known genes associated to Alzheimers disease, additional data, Gene Ontology and finally, a literature review or expert knowledge to validate the results. As methodology to identify genes highly related to this disease, we present the integration of three machine learning techniques: particularly, we have used decision trees, quantitative association rules and hierarchical cluster to analyze Alzheimers disease gene expression profiles to identify genes highly linked to this neurodegenerative disease, through changes in their expression levels between control and patient samples. We propose an ensemble of decision trees and quantitative association rules to find the most suitable configurations of the multi-objective evolutionary algorithm GarNet, in order to overcome the complex parametrization intrinsic to this type of algorithms. To fulfill this goal, GarNet has been executed using multiple configuration settings and the well-known C4.5 has been used to find the minimum accuracy to be satisfied. Then, GarNet is rerun to identify dependencies between genes and their expression levels, so we are able to distinguish between healthy individuals and Alzheimers patients using the configurations that overcome the minimum threshold of accuracy defined by C4.5 algorithm. Finally, a hierarchical cluster analysis has been used to validate the obtained gene-Alzheimers Disease associations provided by GarNet. The results have shown that the obtained rules were able to successfully characterize the underlying information, grouping relevant genes for Alzheimer Disease. The genes reported by our approach provided two well defined groups that perfectly divided the samples between healthy and Alzheimers Disease patients. To prove the relevance of the obtained results, a statistical test and gene expression fold-change were used. Furthermore, this relevance has been summarized in a volcano plot, showing two clearly separated and significant groups of genes that are up or down-regulated in Alzheimers Disease patients. A biological knowledge integration phase was performed based on the information fusion of systematic literature review, enrichment Gene Ontology terms for the described genes found in the hippocampus of patients. Finally, a validation phase with additional data and a permutation test is carried out, being the results consistent with previous studies.",10.1016/j.inffus.2016.11.005,"Biological knowledge integration, Gene expression profiles, Ensemble learning, Association rules, Alzheimers disease, Statistical significant genes",,
MQAPViz: A Divide-and-Conquer Multi-Objective Optimization Algorithm to Compute Large Data Visualizations,"Sanhueza, Claudio and Jim\'{e}nez, Francia and Berretta, Regina and Moscato, Pablo",Association for Computing Machinery,2018,"Algorithms for data visualizations are essential tools for transforming data into useful narratives. Unfortunately, very few visualization algorithms can handle the large datasets of many real-world scenarios. In this study, we address the visualization of these datasets as a Multi-Objective Optimization Problem. We propose mQAPViz, a divide-and-conquer multi-objective optimization algorithm to compute large-scale data visualizations. Our method employs the Multi-Objective Quadratic Assignment Problem (mQAP) as the mathematical foundation to solve the visualization task at hand. The algorithm applies advanced sampling techniques originating from the field of machine learning and efficient data structures to scale to millions of data objects. The algorithm allocates objects onto a 2D grid layout. Experimental results on real-world and large datasets demonstrate that mQAPViz is a competitive alternative to existing techniques.",10.1145/3205455.3205457,"visualization, large datasets, multi-objective optimization",,
NSGA-Net: Neural Architecture Search Using Multi-Objective Genetic Algorithm,"Lu, Zhichao and Whalen, Ian and Boddeti, Vishnu and Dhebar, Yashesh and Deb, Kalyanmoy and Goodman, Erik and Banzhaf, Wolfgang",Association for Computing Machinery,2019,"This paper introduces NSGA-Net --- an evolutionary approach for neural architecture search (NAS). NSGA-Net is designed with three goals in mind: (1) a procedure considering multiple and conflicting objectives, (2) an efficient procedure balancing exploration and exploitation of the space of potential neural network architectures, and (3) a procedure finding a diverse set of trade-off network architectures achieved in a single run. NSGA-Net is a population-based search algorithm that explores a space of potential neural network architectures in three steps, namely, a population initialization step that is based on prior-knowledge from hand-crafted architectures, an exploration step comprising crossover and mutation of architectures, and finally an exploitation step that utilizes the hidden useful knowledge stored in the entire history of evaluated neural architectures in the form of a Bayesian Network. Experimental results suggest that combining the dual objectives of minimizing an error metric and computational complexity, as measured by FLOPs, allows NSGA-Net to find competitive neural architectures. Moreover, NSGA-Net achieves error rate on the CIFAR-10 dataset on par with other state-of-the-art NAS methods while using orders of magnitude less computational resources. These results are encouraging and shows the promise to further use of EC methods in various deep-learning paradigms.",10.1145/3321707.3321729,"deep learning, image classification, multi objective, bayesian optimization, neural architecture search",,
What's inside the Black-Box? A Genetic Programming Method for Interpreting Complex Machine Learning Models,"Evans, Benjamin P. and Xue, Bing and Zhang, Mengjie",Association for Computing Machinery,2019,"Interpreting state-of-the-art machine learning algorithms can be difficult. For example, why does a complex ensemble predict a particular class? Existing approaches to interpretable machine learning tend to be either local in their explanations, apply only to a particular algorithm, or overly complex in their global explanations. In this work, we propose a global model extraction method which uses multi-objective genetic programming to construct accurate, simplistic and model-agnostic representations of complex black-box estimators. We found the resulting representations are far simpler than existing approaches while providing comparable reconstructive performance. This is demonstrated on a range of datasets, by approximating the knowledge of complex black-box models such as 200 layer neural networks and ensembles of 500 trees, with a single tree.",10.1145/3321707.3321726,"interpretable machine learning, evolutionary multi-objective optimisation, explainable artificial intelligence",,
WGNCS: A Robust Hybrid Cross-Version Defect Model via Multi-Objective Optimization and Deep Enhanced Feature Representation,"Zhang, Nana and Ying, Shi and Ding, Weiping and Zhu, Kun and Zhu, Dandan",Elsevier Science Inc.,2021,,10.1016/j.ins.2021.05.008,"Wasserstein GAN with Gradient Penalty, Convolutional neural network, Cross-version defect prediction, Multi-objective feature selection, Deep learning techniques",,
Breast Cancer Diagnosis Using Thermal Image Analysis: A Data-Driven Approach Based on Swarm Intelligence and Supervised Learning for Optimized Feature Selection,"Macedo, Mariana and Santana, Maira and dos Santos, Wellington P. and Menezes, Ronaldo and Bastos-Filho, Carmelo",Elsevier Science Publishers B. V.,2021,,10.1016/j.asoc.2021.107533,"Swarm intelligence, Feature selection, Breast cancer image diagnosis, Convolutional Neural Networks, Feature extraction, Fish school search",,
A SVM Optimization Tool and FPGA System Architecture Applied to NMPC,"Santos, Carlos Eduardo and Coelho, Leandro dos S. and Sampaio, Renato Coral and Jacobi, Ricardo and Ayala, Helon and Llanos, Carlos H.",Association for Computing Machinery,2017,"Support Vector Machines (SVMs) are supervised learning models of the machine learning field whose performance strongly depended on its hyperparameters. The Bio-inspired Optimization Tool for SVM (BIOTS) tool is based on a Multi-Objective Particle Swarm Algorithm (MOPSO) to tune hyperparameters of SVMs. In this work, BIOTS is proposed along with a custom hardware design generator (VHDL) that implements the SVM in a Field-Programmable Gate Array (FPGA). Both tools are combined to create an approximate Nonlinear Model Predictive Controller (NMPC) applied to a single-link robotic arm. The result is a generated SVM implemented in a FPGA yielding better results in terms of speed and simplicity compared to our previous work that addressed the same problem with a Radial Basis Functions Neural Networks (RBFNN).",10.1145/3109984.3110007,"SVM/SVR training tool, NMPC, FPGA",,
"Ensemble Classification and Regression-Recent Developments, Applications and Future Directions [Review Article]","Ren, Ye and Zhang, Le and Suganthan, P.N.",IEEE Press,2016,"Ensemble methods use multiple models to get better performance. Ensemble methods have been used in multiple research fields such as computational intelligence, statistics and machine learning. This paper reviews traditional as well as state-of-the-art ensemble methods and thus can serve as an extensive summary for practitioners and beginners. The ensemble methods are categorized into conventional ensemble methods such as bagging, boosting and random forest, decomposition methods, negative correlation learning methods, multi-objective optimization based ensemble methods, fuzzy ensemble methods, multiple kernel learning ensemble methods and deep learning based ensemble methods. Variations, improvements and typical applications are discussed. Finally this paper gives some recommendations for future research directions. ",10.1109/MCI.2015.2471235,,,
Leveraging a Predictive Model of the Workload for Intelligent Slot Allocation Schemes in Energy-Efficient HPC Clusters,"Coca\~{n}a-Fern\'{a}ndez, Alberto and S\'{a}nchez, Luciano and Ranilla, Jos\'{e}","Pergamon Press, Inc.",2016,"A proactive mechanism to learn an efficient strategy for adaptive resource clusters is proposed. In contrast to reactive techniques, that rescale the cluster to fit the past load, a predictive strategy is adopted. The cluster incoming workload is forecasted and an optimization problem is defined whose solution is the optimal action according to a utility function. Genetic-based machine learning techniques are used, including multi-objective evolutionary algorithms under the distal supervised learning setup. Experimental evaluations show that the proactive system presented in this work improves either the energetic efficiency or the number of reconfigurations of previous approaches without a loss in the quality of service. Depending on the predictability of the workload, in real world cluster scenarios additional energy savings of up to approximately 40% were measured over the best previous approach, with a 2 factor increment in the number of reconfigurations.",10.1016/j.engappai.2015.10.003,"Distal learning, Multi-criteria decision making, Energy-efficient cluster computing, Evolutionary algorithms",,
Spam Detection on Social Networks Using Cost-Sensitive Feature Selection and Ensemble-Based Regularized Deep Neural Networks,"Barushka, Aliaksandr and Hajek, Petr",Springer-Verlag,2020,"Spam detection on social networks is increasingly important owing to the rapid growth of social network user base. Sophisticated spam filters must be developed to deal with this complex problem. Traditional machine learning approaches such as neural networks, support vector machines and Na\""{\i}ve Bayes classifiers are not effective enough to process and utilize complex features present in high-dimensional data on social network spam. Moreover, the traditional objective criteria of social network spam filters cannot cope with different costs assigned to type I and type II errors. To overcome these problems, here we propose a novel cost-sensitive approach to social network spam filtering. The proposed approach is composed of two stages. In the first stage, multi-objective evolutionary feature selection is used to minimize both the misclassification cost of the proposed model and the number of attributes necessary for spam filtering. Then, the approach uses cost-sensitive ensemble learning techniques with regularized deep neural networks as base learners. We demonstrate that this approach is effective for social network spam filtering on two benchmark datasets. We also show that the proposed approach outperforms other popular algorithms used in social network spam filtering, such as random forest, Na\""{\i}ve Bayes or support vector machines.",10.1007/s00521-019-04331-5,"Neural network, Regularization, Ensemble learning, Social networks, Misclassification cost",,
Towards Enhancing Fault Tolerance in Neural Networks,"Duddu, Vasisht and Rao, D Vijay and Balas, Valentina",Association for Computing Machinery,2020," Deep Learning Accelerators and Neuromorphic hardware, used in many real-time safety-critical applications, are prone to faults that manifest in the form of errors in Neural Networks. Fault Tolerance in Neural Networks is a critical attribute for applications that require reliable computation for long duration such as IoT and mobile devices. The inherent fault tolerance of Neural Networks can be improved with regularization, however, the current techniques exhibit a trade-off between generalization and classification accuracy. To this extent, in this work, a Neural Network is modelled as two distinct functional components: a Feature Extractor with an unsupervised learning objective and a Fully Connected Classifier with a supervised learning objective. Traditional approaches to train the entire network using a single supervised learning objective are insufficient to achieve the objectives of the individual functional goals optimally. In this work, a novel two phase framework with multi-criteria objective function combining unsupervised training of the Feature Extractor followed by supervised training of the Classifier Network is proposed. In the Phase I, the unsupervised training of the Feature Extractor is modeled using two games solved simultaneously in the presence of Neural Networks with conflicting objectives. The first game with a generative model, trains the Feature Extractor to generate robust features for the input image by minimizing a reconstruction loss between the input and reconstructed image. The second game with a binary classification network, updates the Feature Extractor to smoothen the feature space and match with a prior Gaussian distribution. In Phase II, the resultant Feature Extractor, which is strongly regularized, is combined with the Fully Connected Classifier for fine-tuning on the classification task. The proposed two phase training algorithm is evaluated on four architectures with varying model complexity on standard image classification datasets: FashionMNIST and CIFAR10. The proposed framework is scalable and independent of the network architecture that provides superior tolerance to stuck at “0” faults as compared to existing regularization functions without loss in classification accuracy.",10.1145/3448891.3448936,"Fault Tolerance, Regularization., Neural Networks, Adversarial Game, Classification",,
Leveraging the VTA-TVM Hardware-Software Stack for FPGA Acceleration of 8-Bit ResNet-18 Inference,"Moreau, Thierry and Chen, Tianqi and Ceze, Luis",Association for Computing Machinery,2018,"We present a full-stack design to accelerate deep learning inference with FPGAs. Our contribution is two-fold. At the software layer, we leverage and extend TVM, the end-to-end deep learning optimizing compiler, in order to harness FPGA-based acceleration. At the the hardware layer, we present the Versatile Tensor Accelerator (VTA) which presents a generic, modular, and customizable architecture for TPU-like accelerators. Our results take a ResNet-18 description in MxNet and compiles it down to perform 8-bit inference on a 256-PE accelerator implemented on a low-cost Xilinx Zynq FPGA, clocked at 100MHz. Our full hardware acceleration stack will be made available for the community to reproduce, and build upon at http://github.com/uwsaml/vta.",10.1145/3229762.3229766,,,
A Regularized Root-Quartic Mixture of Experts for Complex Classification Problems,"Abbasi, Elham and Shiri, Mohammad Ebrahim and Ghatee, Mehdi",Elsevier Science Publishers B. V.,2016,"Mixture of experts is a neural network based ensemble learning approach consisting of several experts and a gating network. In this paper, we introduce regularized root-quartic mixture of experts (R-RTQRT-ME) by incorporating a regularization term into the error function to control the complexity of model and to increase robustness in confronting with over-fitting and noise. The average of the results of R-RTQRT-ME on 20 classification benchmark datasets, shows that this algorithm performs 1.75%, 2.50%, 2.29% better than multi objective regularized negative correlation learning, multi objective negative correlation learning and multi objective neural network, respectively. Also, the average of improvements of R-RTQRT-ME is 1.16%, 2.31%, 3.40%, 3.39% in comparison with root-quartic mixture of experts, mixture of negatively correlated experts, mixture of experts and negative correlation learning, respectively. Furthermore, the effect of the regularization penalty term in R-RTQRT-ME on noisy data is analyzed which shows the robustness of R-RTQRT-ME in these situations.",10.1016/j.knosys.2016.07.018,"Negative correlation learning, Mixture of experts, Generalization ability, Ensemble learning, Diversity, Regularization",,
Toward a Reliable Anomaly-Based Intrusion Detection in Real-World Environments,"Viegas, Eduardo K. and Santin, Altair O. and Oliveira, Luiz S.","Elsevier North-Holland, Inc.",2017,"A popular approach for detecting network intrusion attempts is to monitor the network traffic for anomalies. Extensive research effort has been invested in anomaly-based network intrusion detection using machine learning techniques; however, in general these techniques remain a research topic, rarely being used in real-world environments. In general, the approaches proposed in the literature lack representative datasets and reliable evaluation methods that consider real-world network properties during the system evaluation. In general, the approaches adopt a set of assumptions about the training data, as well as about the validation methods, rendering the created system unreliable for open-world usage. This paper presents a new method for creating intrusion databases. The objective is that the databases should be easy to update and reproduce with real and valid traffic, representative, and publicly available. Using our proposed method, we propose a new evaluation scheme specific to the machine learning intrusion detection field. Sixteen intrusion databases were created, and each of the assumptions frequently adopted in studies in the intrusion detection literature regarding network traffic behavior was validated. To make machine learning detection schemes feasible, we propose a new multi-objective feature selection method that considers real-world network properties. The results show that most of the assumptions frequently applied in studies in the literature do not hold when using a machine learning detection scheme for network-based intrusion detection. However, the proposed multi-objective feature selection method allows the system accuracy to be improved by considering real-world network properties during the model creation process.",10.1016/j.comnet.2017.08.013,"Multi-objective feature selection, Machine learning-based intrusion detection, Intrusion databases, Anomaly-based classifier",,
Automated Feature Engineering for Algorithmic Fairness,"Salazar, Ricardo and Neutatz, Felix and Abedjan, Ziawasch",VLDB Endowment,2021,"One of the fundamental problems of machine ethics is to avoid the perpetuation and amplification of discrimination through machine learning applications. In particular, it is desired to exclude the influence of attributes with sensitive information, such as gender or race, and other causally related attributes on the machine learning task. The state-of-the-art bias reduction algorithm Capuchin breaks the causality chain of such attributes by adding and removing tuples. However, this horizontal approach can be considered invasive because it changes the data distribution. A vertical approach would be to prune sensitive features entirely. While this would ensure fairness without tampering with the data, it could also hurt the machine learning accuracy. Therefore, we propose a novel multi-objective feature selection strategy that leverages feature construction to generate more features that lead to both high accuracy and fairness. On three well-known datasets, our system achieves higher accuracy than other fairness-aware approaches while maintaining similar or higher fairness.",10.14778/3461535.3463474,,,
RF Specification Test Compaction Using Learning Machines,"Stratigopoulos, Haralampos-G. and Drineas, Petros and Slamani, Mustapha and Makris, Yiorgos",IEEE Educational Activities Department,2010,"We present a machine learning approach to the problem of RF specification test compaction. The proposed compaction flow relies on a multi-objective genetic algorithm, which searches in the power-set of specification tests to select appropriate subsets, and a classifier, which makes pass/fail decisions based solely on these subsets. The method is demonstrated on production test data from an RF device fabricated by IBM. The results indicate that machine learning can identify intricate correlations between specification tests, which allows us to infer the outcome of all tests from a subset of tests. Thereby, the number of tests that need to be explicitly carried out and the corresponding cost are reduced significantly without adversely impacting test accuracy.",10.1109/TVLSI.2009.2017196,"RFICs, Artificial intelligence, circuit testing, artificial intelligence",,
Learning-Based Application-Agnostic 3D NoC Design for Heterogeneous Manycore Systems,"Joardar, Biresh Kumar and Kim, Ryan Gary and Doppa, Janardhan Rao and Pande, Partha Pratim and Marculescu, Diana and Marculescu, Radu",IEEE Computer Society,2019,"The rising use of deep learning and other big-data algorithms has led to an increasing demand for hardware platforms that are computationally powerful, yet energy-efficient. Due to the amount of data parallelism in these algorithms, high-performance three-dimensional (3D) manycore platforms that incorporate both CPUs and GPUs present a promising direction. However, as systems use heterogeneity (e.g., a combination of CPUs, GPUs, and accelerators) to improve performance and efficiency, it becomes more pertinent to address the distinct and likely conflicting communication requirements (e.g., CPU memory access latency or GPU network throughput) that arise from such heterogeneity. Unfortunately, it is difficult to quickly explore the hardware design space and choose appropriate tradeoffs between these heterogeneous requirements. To address these challenges, we propose the design of a 3D Network-on-Chip (NoC) for heterogeneous manycore platforms that considers the appropriate design objectives for a 3D heterogeneous system and explores various tradeoffs using an efficient machine learning (ML)-based multi-objective optimization (MOO) technique. The proposed design space exploration considers the various requirements of its heterogeneous components and generates a set of 3D NoC architectures that efficiently trades off these design objectives. Our findings show that by jointly considering these requirements (latency, throughput, temperature, and energy), we can achieve 9.6 percent better Energy-Delay Product on average at nearly iso-temperature conditions when compared to a thermally-optimized design for 3D heterogeneous NoCs. More importantly, our results suggest that our 3D NoCs optimized for a few applications can be generalized for unknown applications as well. Our results show that these generalized 3D NoCs only incur a 1.8 percent (36-tile system) and 1.1 percent (64-tile system) average performance loss compared to application-specific NoCs.",10.1109/TC.2018.2889053,,,
Attention Based End to End Speech Recognition for Voice Search in Hindi and English,"Joshi, Raviraj and Kannan, Venkateshan",Association for Computing Machinery,2021," We describe here our work with automatic speech recognition (ASR) in the context of voice search functionality on the Flipkart e-Commerce platform. Starting with the deep learning architecture of Listen-Attend-Spell (LAS), we build upon and expand the model design and attention mechanisms to incorporate innovative approaches including multi-objective training, multi-pass training, and external rescoring using language models and phoneme based losses. We report a relative WER improvement of 15.7% on top of state-of-the-art LAS models using these modifications. Overall, we report an improvement of 36.9% over the phoneme-CTC system on the Flipkart Voice Search dataset. The paper also provides an overview of different components that can be tuned in a LAS based system.",10.1145/3503162.3503173,"encoder-decoder models, listen attend spell, automatic speech recognition, attention",,
HYDRA: Large-Scale Social Identity Linkage via Heterogeneous Behavior Modeling,"Liu, Siyuan and Wang, Shuhui and Zhu, Feida and Zhang, Jinbo and Krishnan, Ramayya",Association for Computing Machinery,2014,"We study the problem of large-scale social identity linkage across different social media platforms, which is of critical importance to business intelligence by gaining from social data a deeper understanding and more accurate profiling of users. This paper proposes HYDRA, a solution framework which consists of three key steps: (I) modeling heterogeneous behavior by long-term behavior distribution analysis and multi-resolution temporal information matching; (II) constructing structural consistency graph to measure the high-order structure consistency on users' core social structures across different platforms; and (III) learning the mapping function by multi-objective optimization composed of both the supervised learning on pair-wise ID linkage information and the cross-platform structure consistency maximization. Extensive experiments on 10 million users across seven popular social network platforms demonstrate that HYDRA correctly identifies real user linkage across different platforms, and outperforms existing state-of-the-art algorithms by at least 20% under different settings, and 4 times better in most settings.",10.1145/2588555.2588559,"user linkage, heterogeneous behavior model, social networks, multiple information",,
Self-Adaptive Step Fruit Fly Algorithm Optimized Support Vector Regression Model for Dynamic Response Prediction of Magnetorheological Elastomer Base Isolator,"Yu, Yang and Li, Yancheng and Li, Jianchun and Gu, Xiaoyu",Elsevier Science Publishers B. V.,2016,"Parameter optimization of support vector regression (SVR) plays a challenging role in improving the generalization ability of machine learning. Fruit fly optimization algorithm (FFOA) is a recently developed swarm optimization algorithm for complicated multi-objective optimization problems and is also suitable for optimizing SVR parameters. In this work, parameter optimization in SVR using FFOA is investigated. In view of problems of premature and local optimum in FFOA, an improved FFOA algorithm based on self-adaptive step update strategy (SSFFOA) is presented to obtain the optimal SVR model. Moreover, the proposed method is utilized to characterize magnetorheological elastomer (MRE) base isolator, a typical hysteresis device. In this application, the obtained displacement, velocity and current level are used as SVR inputs while the output is the shear force response of the device. Experimental testing of the isolator with two types of excitations is applied for model performance evaluation. The results demonstrate that the proposed SSFFOA-optimized SVR (SSFFOA_SVR) has perfect generalization ability and more accurate prediction accuracy than other machine learning models, and it is a suitable and effective method to predict the dynamic behaviour of MRE isolator.",10.1016/j.neucom.2016.02.074,"Support vector regression, Magnetorheological elastomer base isolator, Self-adaptive step, Dynamic response prediction, Fruit fly optimization algorithm",,
Evolutionary Discovery of Coresets for Classification,"Barbiero, Pietro and Squillero, Giovanni and Tonda, Alberto",Association for Computing Machinery,2019,"When a machine learning algorithm is able to obtain the same performance given a complete training set, and a small subset of samples from the same training set, the subset is termed coreset. As using a coreset improves training speed and allows human experts to gain a better understanding of the data, by reducing the number of samples to be examined, coreset discovery is an active line of research. Often in literature the problem of coreset discovery is framed as i. single-objective, attempting to find the candidate coreset that best represents the training set, and ii. independent from the machine learning algorithm used. In this work, an approach to evolutionary coreset discovery is presented. Building on preliminary results, the proposed approach uses a multi-objective evolutionary algorithm to find compromises between two conflicting objectives, i. minimizing the number of samples in a candidate coreset, and ii. maximizing the accuracy of a target classifier, trained with the coreset, on the whole original training set. Experimental results on popular classification benchmarks show that the proposed approach is able to identify candidate coresets with better accuracy and generality than state-of-the-art coreset discovery algorithms found in literature.",10.1145/3319619.3326846,"evolutionary algorithms, multi-objective, explain AI, coreset discovery, classification, machine learning",,
FTT-NAS: Discovering Fault-Tolerant Neural Architecture,"Li, Wenshuo and Ning, Xuefei and Ge, Guangjun and Chen, Xiaoming and Wang, Yu and Yang, Huazhong",IEEE Press,2020,"With the fast evolvement of deep-learning specific embedded computing systems, applications powered by deep learning are moving from the cloud to the edge. When deploying NNs onto the edge devices under complex environments, there are various types of possible faults: soft errors caused by atmospheric neutrons and radioactive impurities, voltage instability, aging, temperature variations, and malicious attackers. Thus the safety risk of deploying neural networks at edge computing devices in safety-critic applications is now drawing much attention. In this paper, we implement the random bit-flip, Gaussian, and Salt-and-Pepper fault models and establish a multi-objective fault-tolerant neural architecture search framework. On top of the NAS framework, we propose Fault-Tolerant Neural Architecture Search (FT-NAS) to automatically discover convolutional neural network (CNN) architectures that are reliable to various faults in nowadays edge devices. Then we incorporate fault-tolerant training (FTT) in the search process to achieve better results, which we called FTT-NAS. Experiments show that the discovered architecture FT-NAS-Net and FTT-NAS-Net outperform other hand-designed baseline architectures (58.1%/86.6% VS. 10.0%/52.2%), with comparable FLOPs and less parameters. What is more, the architectures trained under a single fault model can also defend against other faults. By inspecting the discovered architecture, we find that there are redundant connections learned to protect the sensitive paths. This insight can guide future fault-tolerant neural architecture design, and we verify it by a modification on ResNet-20–ResNet-M.",10.1109/ASP-DAC47756.2020.9045324,,,
Semi-Supervised Extensions of Multi-Task Tree Ensembles,"Ad\i{}yeke, Esra and Baydo\u{g}an, Mustafa G\""{o}k\c{c}e",Elsevier Science Inc.,2022,,10.1016/j.patcog.2021.108393,"Ensemble learning, Multi-task learning, Totally randomized trees, Semi-supervised learning, Multi-objective trees",,
NSABC,"Kishor, Avadh and Singh, Pramod Kumar and Prakash, Jay",Elsevier Science Publishers B. V.,2016,"This paper presents a non-dominated sorting based multi-objective artificial bee colony algorithm NSABC to solve multi-objective optimization problems. It is an extension of the artificial bee colony algorithm ABC, which is a single objective optimization algorithm, to the multi-objective optimization domain. It uses a novel approach in the employee bee phase to steer the solutions to simultaneously achieve both the orthogonal goals in the multi-objective optimization convergence and diversity. The onlooker bee phase is similar to the ABC except for the fitness computation to exploit the promising solutions whereas there is no change in the scout bee phase, which is used to get rid of bad solutions and add diversity in the swarm by introducing random solutions. Along with a novel way of exploring new solutions, it uses non-dominated sorting and crowding distance, inspired by the NSGA-II, to maintain the best and diverse solutions in the swarm. It is tested on the 10 two-objective and three-objective unconstrained benchmark problems of varying nature and complexities from the CEC09 suite of test problems and is found better than or commensurable to thirteen state-of-the-art significant multi-objective optimization algorithms as well as other multi-objective variants of the ABC. Further, it is tested on the nine real-life data clustering problems considered from the UCI machine learning repository and proved itself better in comparison to the NSGA-II, MOVGA, and a recent multi-objective variant of the ABC named MOABC. Thus, it is observed that the NSABC is comparatively a simple, light, and powerful algorithm to solve multi-objective problems.",10.1016/j.neucom.2016.08.003,"Crowding distance, Non-dominated sorting, Multi-objective optimization, Artificial bee colony algorithm, Augmented population",,
Place-Type Detection in Location-Based Social Networks,"Hasanuzzaman, Mohammed and Way, Andy",Association for Computing Machinery,2017,"Determining the type of places in location-based social networks will contribute to the success of various downstream tasks such as POI recommendation, location search, automatic place name database creation, and data cleaning.In this paper, we propose a multi-objective ensemble learning framework that (i) allows the accurate tagging of places into one of the three categories: public, private, or virtual, and (ii) identifying a set of solutions thus offering a wide range of possible applications. Based on the check-in records, we compute two types of place features from (i) specific patterns of individual places and (ii) latent relatedness among similar places. The features extracted from specific patterns (SP) are derived from all check-ins at a specific place. The features from latent relatedness (LR) are computed by building a graph of related places where similar types of places are connected by virtual edges. We conduct an experimental study based on a dataset of over 2.7M check-in records collected by crawling Foursquare-tagged tweets from Twitter. Experimental results demonstrate the effectiveness of our approach to this new problem and show the strength of taking various methods into account in feature extraction. Moreover, we demonstrate how place type tagging can be beneficial for place name recommendation services.",10.1145/3078714.3078722,"poi recommendation, location-based social networks, place-type tagging",,
On Learning of Weights through Preferences,"Aggarwal, Manish",Elsevier Science Inc.,2015,"We present a method to learn the criteria weights in multi-criteria decision making (MCDM) by applying emerging learning-to-rank machine learning techniques. Given the pairwise preferences by a decision maker (DM), we learn the weights that the DM attaches to the multiple criteria, characterizing each alternative. As the training information, our method requires the pairwise preferences of alternatives, as revealed by the DM. Once, the DM's decision model is learnt in terms of the criteria weights, it can be applied to predict his choices for any new set of alternatives. The empirical validation of the proposed approach is done on a collection of 12 standard datasets. The accuracy values are compared with those obtained for the state-of-the-art methods such as ranking-SVM and TOPSIS.",10.1016/j.ins.2015.05.034,"Multi-criteria decision making, Preference learning, OWA, Learning-to-rank, Learning weights, Ordered weighted averaging",,
Adaptive Decision Making in Multi-Cloud Management,"Samreen, Faiza and Blair, Gordon S. and Rowe, Matthew",Association for Computing Machinery,2014,"The more cloud providers in the market the more information users have to handle to choose the best and suitable option for their application or business. The diversity in cloud services is a challenge for automated decision making in the multi-cloud environment. These decisions become more complex when the application's requirements and the application owner's constraints need to be satisfied throughout the application life cycle.This paper presents the concept of an Adaptive Decision Making Broker (ADMB) for multi-cloud management. ADMB aims to provide multi-criteria decision making using machine learning in a multi-cloud environment. In this context, we believe that our proposed methodology has the potential to provide optimal solutions as well as handle trade-offs between the functional and the non-functional requirements of given application.",10.1145/2676662.2676676,"machine learning, multi-criteria decision making, multi-cloud management",,
GPTune: Multitask Learning for Autotuning Exascale Applications,"Liu, Yang and Sid-Lakhdar, Wissam M. and Marques, Osni and Zhu, Xinran and Meng, Chang and Demmel, James W. and Li, Xiaoye S.",Association for Computing Machinery,2021,"Multitask learning has proven to be useful in the field of machine learning when additional knowledge is available to help a prediction task. We adapt this paradigm to develop autotuning frameworks, where the objective is to find the optimal performance parameters of an application code that is treated as a black-box function. Furthermore, we combine multitask learning with multi-objective tuning and incorporation of coarse performance models to enhance the tuning capability. The proposed framework is parallelized and applicable to any application, particularly exascale applications with a small number of function evaluations. Compared with other state-of-the-art single-task learning frameworks, the proposed framework attains up to 2.8X better code performance for at least 80% of all tasks using up to 2048 cores.",10.1145/3437801.3441621,"multitask learning, autotuning, exascale computing project, machine learning, bayesian optimization",,
A Block Object Detection Method Based on Feature Fusion Networks for Autonomous Vehicles,"Meng, Qiao and Song, Huansheng and Li, Gang and Zhang, Yu’an and Zhang, Xiangqing and Deng, Ke","John Wiley &amp; Sons, Inc.",2019,"Nowadays, automatic multi-objective detection remains a challenging problem for autonomous vehicle technologies. In the past decades, deep learning has been demonstrated successful for multi-objective detection, such as the Single Shot Multibox Detector (SSD) model. The current trend is to train the deep Convolutional Neural Networks (CNNs) with online autonomous vehicle datasets. However, network performance usually degrades when small objects are detected. Moreover, the existing autonomous vehicle datasets could not meet the need for domestic traffic environment. To improve the detection performance of small objects and ensure the validity of the dataset, we propose a new method. Specifically, the original images are divided into blocks as input to a VGG-16 network which add the feature map fusion after CNNs. Moreover, the image pyramid is built to project all the blocks detection results at the original objects size as much as possible. In addition to improving the detection method, a new autonomous driving vehicle dataset is created, in which the object categories and labelling criteria are defined, and a data augmentation method is proposed. The experimental results on the new datasets show that the performance of the proposed method is greatly improved, especially for small objects detection in large image. Moreover, the proposed method is adaptive to complex climatic conditions and contributes a lot for autonomous vehicle perception and planning.",10.1155/2019/4042624,,,
COUGAR: Clustering of Unknown Malware Using Genetic Algorithm Routines,"Wilkins, Zachary and Zincir-Heywood, Nur",Association for Computing Machinery,2020,"Through malware, cyber criminals can leverage our computing resources to disrupt our work, steal our information, and even hold it hostage. Security professionals seek to classify these malicious software so as to prevent their distribution and execution, but the sheer volume of malware complicates these efforts. In response, machine learning algorithms are actively employed to alleviate the workload. One such approach is evolutionary computation, where solutions are bred, rather than built. In this paper, we design, develop and evaluate a system, COUGAR, to reduce high-dimensional malware behavioural data, and optimize clustering behaviour using a multi-objective genetic algorithm. Evaluations demonstrate that each of our chosen clustering algorithms can successfully highlight groups of malware. We also present an example real-world scenario, based on the testing data, to demonstrate practical applications.",10.1145/3377930.3390151,"clustering, multi-objective optimization, evolution, malware, machine learning, cyber attack, cyber security",,
Retooling Fitness for Noisy Problems in a Supervised Michigan-Style Learning Classifier System,"Urbanowicz, Ryan and Moore, Jason",Association for Computing Machinery,2015,"An accuracy-based rule fitness is a hallmark of most modern Michigan-style learning classifier systems (LCS), a powerful, flexible, and largely interpretable class of machine learners. However, rule-fitness based solely on accuracy is not ideal for identifying 'optimal' rules in supervised learning. This is particularly true for noisy problem domains where perfect rule accuracy essentially guarantees over-fitting. Rule fitness based on accuracy alone is unreliable for reflecting the global 'value' of a given rule since rule accuracy is based on a subset of the training instances. While moderate over-fitting may not dramatically hinder LCS classification or prediction performance, the interpretability of the solution is likely to suffer. Additionally, over-fitting can impede algorithm learning efficiency and leads to a larger number of rules being required to capture relationships. The present study seeks to develop an intuitive multi-objective fitness function that will encourage the discovery, preservation, and identification of 'optimal' rules through accuracy, correct coverage of training data, and the prior probability of the specified attribute states and class expressed by a given rule. We demonstrate the advantages of our proposed fitness by implementing it into the ExSTraCS algorithm and performing evaluations over a large spectrum of complex, noisy, simulated datasets.",10.1145/2739480.2754756,"multiple solutions, bioinformatics, multi-objective optimization, pattern recognition and classification, classifier systems",,
Predictive Models with End User Preference,"Zhao, Yifan and Yang, Xian and Bolnykh, Carolina and Harenberg, Steve and Korchiev, Nodirbek and Yerramsetty, Saavan Raj and Vellanki, Bhanu Prasad and Kodumagulla, Ramakanth and Samatova, Nagiza F.","John Wiley &amp; Sons, Inc.",2022,"Classical machine learning models typically try to optimize the model based on the most discriminatory features of the data; however, they do not usually account for end user preferences. In certain applications, this can be a serious issue as models not aware of user preferences could become costly, untrustworthy, or privacy‐intrusive to use, thus becoming irrelevant and/or uninterpretable. Ideally, end users with domain knowledge could propose preferable features that the predictive model could then take into account. In this paper, we propose a generic modeling method that respects end user preferences via a relative ranking system to express multi‐criteria preferences and a regularization term in the model's objective function to incorporate the ranked preferences. In a more generic perspective, this method is able to plug user preferences into existing predictive models without creating completely new ones. We implement this method in the context of decision trees and are able to achieve a comparable classification accuracy while reducing the use of undesirable features.",10.1002/sam.11545,"decision tree, predictive model, child support, relative ranking, user preference, regularization",,
A Generalized Framework for Anaphora Resolution in Indian Languages,"Kumar Sikdar, Utpal and Ekbal, Asif and Saha, Sriparna",Elsevier Science Publishers B. V.,2016,"In this paper, we propose a joint model of feature selection and ensemble learning for anaphora resolution in the resource-poor environment like the Indian languages. The proposed approach is based on multi-objective differential evolution (DE) that optimises five coreference resolution scorers, namely Muc, Bcub, Ceafm, Ceafe and Blanc. The main goal is to determine the best combination of different mention classifiers and the most relevant set of features for anaphora resolution. The proposed method is evaluated for three leading Indian languages, namely Hindi, Bengali and Tamil. Experiments on the benchmark datasets of ICON-2011 Shared Task on Anaphora Resolution in Indian Languages show that our proposed approach attains good level of accuracies, which are often better with respect to the state-of-the-art systems. It achieves the F-measure values of 71.89%, 59.61%, 52.55% 34.45% and 72.52% for Muc, Bcub, Ceafm, Ceafe and Blanc, respectively, for Bengali language. For Hindi we obtain the F-measure values of 33.27%, 63.06%, 49.59%, 49.06% and 55.45% for Muc, Bcub, Ceafm, Ceafe and Blanc metrics, respectively. In order to further show the efficacy of our proposed algorithm, we evaluate with Tamil, a language that belongs to a different family. This shows the F-measure values of 31.79%, 64.67%, 46.81%, 45.29% and 52.80% for Muc, Bcub, Ceafm, Ceafe and Blanc metrics, respectively. Experiments on Dutch show the F-measure values of 17.67%, 74.43%, 58.08%, 59.21% and 55.58% for Muc, Bcub, Ceafm, Ceafe and Blanc metrics, respectively.",10.1016/j.knosys.2016.06.033,"Single objective optimization (SOO), Support vector machine (SVM), Conditional random field (CRF), Multiobjective optimization (MOO)",,
K-Hit Query: Top-k Query with Probabilistic Utility Function,"Peng, Peng and Wong, Raymong Chi-Wing",Association for Computing Machinery,2015,"Multi-criteria decision making problem has been well studied for many years. One popular query for multi-criteria decision making is top-k queries which require each user to specify an exact utility function. In many cases, the utility function of a user is probabilistic and finding the distribution on the utility functions has been widely explored in the machine learning areas, such as user's recommender systems, Bayesian learning models and user's preference elicitation, for improving user's experience. Motivated by this, we propose a new type of queries called k-hit queries, which has not been studied before. Given a set D of tuples in the database, the distribution θ on utility functions and a positive integer k, we would like to select a set of k tuples from D in order to maximize the probability that at least one of tuples in the selection set is the favorite of a user. All applications for top-k queries can naturally be used in k-hit queries. In this paper, we present various interesting properties of k-hit queries. Besides, based on these properties, we propose a novel algorithm called k-hit_Alg for k-hit queries. Finally, we conducted comprehensive experiments to show that the performance of our proposed method, k hit_Alg, is superior compared with other existing algorithms which were originally used to answer other existing queries.",10.1145/2723372.2723735,"sampling, top-k query, algorithm, geometry, preference query, skyline query",,
Searching for Fast Demosaicking Algorithms,"Ma, Karima and Gharbi, Michael and Adams, Andrew and Kamil, Shoaib and Li, Tzu-Mao and Barnes, Connelly and Ragan-Kelley, Jonathan",Association for Computing Machinery,2022,"We present a method to automatically synthesize efficient, high-quality demosaicking algorithms, across a range of computational budgets, given a loss function and training data. It performs a multi-objective, discrete-continuous optimization which simultaneously solves for the program structure and parameters that best trade off computational cost and image quality. We design the method to exploit domain-specific structure for search efficiency. We apply it to several tasks, including demosaicking both Bayer and Fuji X-Trans color filter patterns, as well as joint demosaicking and super-resolution. In a few days on 8 GPUs, it produces a family of algorithms that significantly improves image quality relative to the prior state-of-the-art across a range of computational budgets from 10s to 1000s of operations per pixel (1dB–3dB higher quality at the same cost, or 8.5–200 \texttimes{} higher throughput at same or better quality). The resulting programs combine features of both classical and deep learning-based demosaicking algorithms into more efficient hybrid combinations, which are bandwidth-efficient and vectorizable by construction. Finally, our method automatically schedules and compiles all generated programs into optimized SIMD code for modern processors.",10.1145/3508461,"differentiable programming, data driven methods, domain specific programming, demosaicking, neural architecture search, super-resolution",,
The Smart Black Box: A Value-Driven High-Bandwidth Automotive Event Data Recorder,"Yao, Yu and Atkins, Ella",IEEE Press,2021,"Autonomous vehicles require reliable and resilient sensor suites and ongoing validation through fleet-wide data collection. This paper proposes a Smart Black Box (SBB) to augment traditional low-bandwidth data logging with value-driven high-bandwidth data capture. The SBB caches short-term histories of data as buffers through a deterministic Mealy machine based on data value and similarity. Compression quality for each frame is determined by optimizing the trade-off between value and storage cost. With finite storage, prioritized data recording discards low-value buffers to make room for new data. This paper formulates SBB compression decision making as a constrained multi-objective optimization problem with novel value metrics and filtering. The SBB has been evaluated on a traffic simulator which generates trajectories containing events of interest (EOIs) and corresponding first-person view videos. SBB compression efficiency is assessed by comparing storage requirements with different compression quality levels and event capture ratios. Performance is evaluated by comparing results with a traditional first-in-first-out (FIFO) recording scheme. Deep learning performance on images recorded at different compression levels is evaluated to illustrate the reproducibility of SBB recorded data.",10.1109/TITS.2020.2971385,,,
Performance Optimality or Reproducibility: That is the Question,"Patki, Tapasya and Thiagarajan, Jayaraman J. and Ayala, Alexis and Islam, Tanzima Z.",Association for Computing Machinery,2019,"The era of extremely heterogeneous supercomputing brings with itself the devil of increased performance variation and reduced reproducibility. There is a lack of understanding in the HPC community on how the simultaneous consideration of network traffic, power limits, concurrency tuning, and interference from other jobs impacts application performance.In this paper, we design a methodology that allows both HPC users and system administrators to understand the trade-off space between optimal and reproducible performance. We present a first-of-its-kind dataset that simultaneously varies multiple system- and user-level parameters on a production cluster, and introduce a new metric, called the desirability score, which enables comparison across different system configurations. We develop a novel, model-agnostic machine learning methodology based on the graph signal theory for comparing the influence of parameters on application predictability, and using a new visualization technique, make practical suggestions for best practices for multi-objective HPC environments.",10.1145/3295500.3356217,"graph signal analysis, machine learning, performance reproducibility, visualization",,
ARMCO,"Pop, Florin and Potop-Butucaru, Maria",Elsevier Science Publishers B. V.,2016,"Cloud Computing can be seen as one of the latest major evolution in computing offering unlimited possibility to use ICT in various domains: business, smart cities, medicine, environmental computing, mobile systems, design and implementation of cyber-infrastructures. The recent expansion of Cloud Systems has led to adapting resource management solutions for large number of wide distributed and heterogeneous datacenters. The adaptive methods used in this context are oriented on: self-stabilizing, self-organizing and autonomic systems; dynamic, adaptive and machine learning based distributed algorithms; fault tolerance, reliability, availability of distributed systems. The pay-per-use economic model of Cloud Computing comes with a new challenge: maximizing the profit for service providers, minimizing the total cost for customers and being friendly with the environment.This special issue presents advances in virtual machine assignment and placement, multi-objective and multi-constraints job scheduling, resource management in federated Clouds and in heterogeneous environments, dynamic topology for data distribution, workflow performance improvement, energy efficiency techniques and assurance of Service Level Agreements.",10.1016/j.future.2015.07.016,"Ubiquitous systems, Adaptive methods, Task scheduling, Resource management, Cloud computing",,
Software Effort Estimation Using FAHP and Weighted Kernel LSSVM Machine,"Sehra, Sumeet Kaur and Brar, Yadwinder Singh and Kaur, Navdeep and Sehra, Sukhjit Singh",Springer-Verlag,2019,"In the life cycle of software product development, the software effort estimation&nbsp;(SEE) has always been a critical activity. The researchers have proposed numerous estimation methods since the inception of software engineering as a research area. The diversity of estimation approaches is very high and increasing, but it has been interpreted that no single technique performs consistently for each project and environment. Multi-criteria decision-making (MCDM) approach generates more credible estimates, which is subjected to expert’s experience. In this paper, a hybrid model has been developed to combine MCDM (for handling uncertainty) and machine learning algorithm (for handling imprecision) approach to predict the effort more accurately. Fuzzy analytic hierarchy process (FAHP) has been used effectively for feature ranking. Ranks generated from FAHP have been integrated into weighted kernel least square support vector machine for effort estimation. The model developed has been empirically validated on data repositories available for SEE. The combination of weights generated by FAHP and the radial basis function (RBF) kernel has resulted in more accurate effort estimates in comparison with bee colony optimisation and basic RBF kernel-based model.",10.1007/s00500-018-3639-2,"Software effort estimation, Fuzzy analytic hierarchy process, Least square support vector machine",,
TelMED: Dynamic User Clustering Resource Allocation Technique for MooM Datasets Under Optimizing Telemedicine Network,"Ahmed, Syed Thouheed and Sandhya, M. and Sankar, Sharmila",Kluwer Academic Publishers,2020,"Tele-Medical data communication via general purpose networking protocols and techniques are major set-back under low line channels and adequate resources such as bandwidth, frequency, power spectrum for transmission and impacts the Quality of Data (QoD) on transmission line. In this paper, a heterogeneous multi-input multi-out (MIMO) based dynamic user clustering technique is proposed and the protocol is termed as TelMED. The proposed technique introduces machine learning terminology on networking nodes for dynamic user grouping and classification resulting in the formation of clusters with reflective similarity indexing ratio. The dynamic clustered users of TelMED protocol are allocated with resources for the transmission of Multi-Objective Optimized Medical datasets resulting in creation of virtual telemedicine networking environment with a given typical network space. The technique is designed on clustered user grouping size of maximum 32 users for reliable results over a fixed networking space and optimized resources for low line transmission channels of rural or remote networks. The resulting technique proves an efficiency of 92.3% over dynamic MIMO user grouping.",10.1007/s11277-020-07091-x,"Telemedicine networks, Dynamic user grouping, Optimized resource allocation, Low line channel transmission",,
A Wrapper-Filter Feature Selection Technique Based on Ant Colony Optimization,"Ghosh, Manosij and Guha, Ritam and Sarkar, Ram and Abraham, Ajith",Springer-Verlag,2020,"Ant colony optimization (ACO) is a well-explored meta-heuristic algorithm, among whose many applications feature selection (FS) is an important one. Most existing versions of ACO are either wrapper based or filter based. In this paper, we propose a wrapper-filter combination of ACO, where we introduce subset evaluation using a filter method instead of using a wrapper method to reduce computational complexity. A memory to keep the best ants and feature dimension-dependent pheromone update has also been used to perform FS in a multi-objective manner. Our proposed approach has been evaluated on various real-life datasets, taken from UCI Machine Learning repository and NIPS2003 FS challenge, using K-nearest neighbors and multi-layer perceptron classifiers. The experimental outcomes have been compared to some popular FS methods. The comparison of results clearly shows that our method outperforms most of the state-of-the-art algorithms used for FS. For measuring the robustness of the proposed model, it has been additionally evaluated on facial emotion recognition and microarray datasets.",10.1007/s00521-019-04171-3,"Ant colony optimization, NIPS2003 challenge, Feature selection, Wrapper-filter method",,
A Novel Resource Optimization Algorithm&nbsp;Based on Clustering and Improved Differential Evolution Strategy Under a Cloud Environment,"Zhou, Zhou and Li, Fangmin and Yang, Shuiqiao",Association for Computing Machinery,2021,"Resource optimization algorithm based on clustering and improved differential evolution strategy, as a new global optimized algorithm, has wide applications in language translation, language processing, document understanding, cloud computing, and edge computing due to high efficiency. With the development of deep learning technology and the rise of big data, the resource optimization algorithm encounters a series of challenges, such as the workload imbalance and low resource utilization. To address the preceding problems, this study proposes a novel resource optimization algorithm based on clustering and an improved differential evolution strategy (Multi-objective Task Scheduling Strategy (MTSS)). Three indexes, namely task completion time, execution cost, and workload, of virtual machines are selected and used to build the fitness function of the MTSS algorithm. At the same time, the preprocessing state is set up to cluster according to the resource and task characteristics to reduce the magnitude of their matching scale. Moreover, to solve the workload imbalance among different resource sets, local resource tasks are reallocated using the Q-value method in the MTSS strategy to achieve workload balance of global resources and improve the resource utilization rate. Experiments are carried out to evaluate the effectiveness of the proposed algorithm. Results show that the proposed algorithm outperforms other algorithms in terms of task completion time, execution cost, and workload balancing.",10.1145/3462761,"low resource utilization, deep learning, multi-objective optimization, Natural language processing, resource optimization",,
Benchmarking and Performance Modelling of Dataflow with Cycles,"Ceesay, Sheriffo and Lin, Yuhui and Barker, Adam",Association for Computing Machinery,2021," Over the years, the popularity of iterative data-intensive applications such as machine learning applications has grown immensely. Unlike batch applications, iterative applications such as k-means, regression or classification algorithms require multiple access to the input data to train it sufficiently for convergence. In the context of big data, these applications are executed on distributed computing frameworks such as Apache Spark. These frameworks are simple to deploy and use, however, under the hood they are complex and highly configurable. To perform an exhaustive study of the impact of these ubiquitous parameters on application performance would be cumbersome due to the exponential amount of their combinations. In this paper, we group applications based on a common dataflow and communication pattern. We then present a multi-objective performance prediction framework to model the performance of these applications. The models can predict the execution time of a given application with high accuracy. The framework can be used to infer optimal configuration parameters to meet application execution deadlines. Based on these optimal configurable values, we recommend the best EC2 instances in terms of cost. The average error rate of the prediction results is ± 14% from the measured value.",10.1145/3492324.3494159,"Dataflow With Cycles, Modelling, Communication Patterns, Machine Learning, Big Data",,
A Deep Neural Networks Ensemble Workflow from Hyperparameter Search to Inference Leveraging GPU Clusters,"Pochelu, Pierrick and Petiton, Serge G. and Conche, Bruno",Association for Computing Machinery,2022," Automated Machine Learning with ensembling (or AutoML with ensembling) seeks to automatically build ensembles of Deep Neural Networks (DNNs) to achieve qualitative predictions. Ensemble of DNNs are well known to avoid over-fitting but they are memory and time consuming approaches. Therefore, an ideal AutoML would produce in one single run time different ensembles regarding accuracy and inference speed. While previous works on AutoML focus to search for the best model to maximize its generalization ability, we rather propose a new AutoML to build a larger library of accurate and diverse individual models to then construct ensembles. First, our extensive benchmarks show asynchronous Hyperband is an efficient and robust way to build a large number of diverse models to combine them. Then, a new ensemble selection method based on a multi-objective greedy algorithm is proposed to generate accurate ensembles by controlling their computing cost. Finally, we propose a novel algorithm to optimize the inference of the DNNs ensemble in a GPU cluster based on allocation optimization. The produced AutoML with ensemble method shows robust results on two datasets using efficiently GPU clusters during both the training phase and the inference phase.",10.1145/3492805.3492819,neural networks,,
OpenTuner: An Extensible Framework for Program Autotuning,"Ansel, Jason and Kamil, Shoaib and Veeramachaneni, Kalyan and Ragan-Kelley, Jonathan and Bosboom, Jeffrey and O'Reilly, Una-May and Amarasinghe, Saman",Association for Computing Machinery,2014,"Program autotuning has been shown to achieve better or more portable performance in a number of domains. However, autotuners themselves are rarely portable between projects, for a number of reasons: using a domain-informed search space representation is critical to achieving good results; search spaces can be intractably large and require advanced machine learning techniques; and the landscape of search spaces can vary greatly between different problems, sometimes requiring domain specific search techniques to explore efficiently.This paper introduces OpenTuner, a new open source framework for building domain-specific multi-objective program autotuners. OpenTuner supports fully-customizable configuration representations, an extensible technique representation to allow for domain-specific techniques, and an easy to use interface for communicating with the program to be autotuned. A key capability inside OpenTuner is the use of ensembles of disparate search techniques simultaneously; techniques that perform well will dynamically be allocated a larger proportion of tests. We demonstrate the efficacy and generality of OpenTuner by building autotuners for 7 distinct projects and 16 total benchmarks, showing speedups over prior techniques of these projects of up to 2.8x with little programmer effort.",10.1145/2628071.2628092,"optimization, autotuner",,
Feature-Based Selection for Open-Source Video Conferencing Software System Using Fuzzy Analytical Hierarchy Process (FAHP),"Qayyum, Shazib and Fan, Xiaoping and Faizan Ali, Rao",Association for Computing Machinery,2021,"There has been an increase in the number of video conferencing tools used during the COVID19 outbreak. Video Conferencing has become a more prevalent and reliable tool when traveling, and face-to-face meetings are not an option. Evaluating and choosing the best conferencing tool that meets the requirement of the organization is difficult. A reliable conferencing tool provides the dual benefit of allowing users to communicate effectively with their teams and clients while also making the Information Technology (IT) department's job easier. Choosing an inappropriate conferencing tool may critically affect the functioning and processes of the business. Our study's core purpose is to evaluate and select the video conferencing platform, which is based upon the methodology called Multi-criteria decision making (MCDM). Numerous proceedings were considered during the evaluation process, and the best software packages selected were based upon a set of matrix outcomes using Fuzzy AHP. The experimental observation demonstrates that two software packages, zoom, and blue jeans, can give better results based upon their ranking scores among the selected alternatives.",10.1145/3457682.3457754,"Multi-criteria decision making (MCDM), Open source software, Video Conferencing, Fuzzy Analytical Hierarchy Process (AHP)",,
Granger Causality Driven AHP for Feature Weighted KNN,"Bhattacharya, Gautam and Ghosh, Koushik and Chowdhury, Ananda S.",Elsevier Science Inc.,2017,"The kNN algorithm remains a popular choice for pattern classification till date due to its non-parametric nature, easy implementation and the fact that its classification error is bounded by twice the Bayes error. In this paper, we show that the performance of the kNN classifier improves significantly from the use of (training) class-wise group-statistics based two criteria during pairwise comparison of features in a given dataset. Granger causality is employed to assign preferences to each criteria. Analytic Hierarchy Process (AHP) is applied to obtain weights for different features from the two criteria and their preferences. Finally, these weights are used to build a weighted distance function for the kNN classification. Comprehensive experimentation on fifteen benchmark datasets of the UCI Machine Learning Repository clearly reveals the supremacy of the proposed Granger causality driven AHP induced kNN algorithm over the kNN method with many different distance metrics, and, with various feature selection strategies. In addition, the proposed method is also shown to perform well on high-dimensional face and hand-writing recognition datasets. HighlightsFeature weighting for kNN by a multi-criteria based decision analysis tool called AHP.Automated weight assignment in criteria matrix of AHP using group-statistics.Criteria preference selection in AHP with Granger Causality.Superior classification performance over kNN with many other feature weighting/selection methods.",10.1016/j.patcog.2017.01.018,"KNN Classification, Feature Weighting, Analytic Hierarchy Process, Granger causality",,
Less is More: Exploiting the Standard Compiler Optimization Levels for Better Performance and Energy Consumption,"Georgiou, Kyriakos and Blackmore, Craig and Xavier-de-Souza, Samuel and Eder, Kerstin",Association for Computing Machinery,2018,"This paper presents the interesting observation that by performing fewer of the optimizations available in a standard compiler optimization level such as -02, while preserving their original ordering, significant savings can be achieved in both execution time and energy consumption. This observation has been validated on two embedded processors, namely the ARM Cortex-M0 and the ARM Cortex-M3, using two different versions of the LLVM compilation framework; v3.8 and v5.0. Experimental evaluation with 71 embedded benchmarks demonstrated performance gains for at least half of the benchmarks for both processors. An average execution time reduction of 2.4% and 5.3% was achieved across all the benchmarks for the Cortex-M0 and Cortex-M3 processors, respectively, with execution time improvements ranging from 1% up to 90% over the -02. The savings that can be achieved are in the same range as what can be achieved by the state-of-the-art compilation approaches that use iterative compilation or machine learning to select flags or to determine phase orderings that result in more efficient code. In contrast to these time consuming and expensive to apply techniques, our approach only needs to test a limited number of optimization configurations, less than 64, to obtain similar or even better savings. Furthermore, our approach can support multi-criteria optimization as it targets execution time, energy consumption and code size at the same time.",10.1145/3207719.3207727,"phase-ordering, execution time, Autotuning, embedded systems, compiler optimizations, energy consumption",,
Using Emulation to Engineer and Understand Simulations of Biological Systems,"Alden, Kieran and Cosgrove, Jason and Coles, Mark and Timmis, Jon",IEEE Computer Society Press,2020,"Modeling and simulation techniques have demonstrated success in studying biological systems. As the drive to better capture biological complexity leads to more sophisticated simulators, it becomes challenging to perform statistical analyses that help translate predictions into increased understanding. These analyses may require repeated executions and extensive sampling of high-dimensional parameter spaces: analyses that may become intractable due to time and resource limitations. Significant reduction in these requirements can be obtained using surrogate models, or emulators, that can rapidly and accurately predict the output of an existing simulator. We apply emulation to evaluate and enrich understanding of a previously published agent-based simulator of lymphoid tissue organogenesis, showing an ensemble of machine learning techniques can reproduce results obtained using a suite of statistical analyses within seconds. This performance improvement permits incorporation of previously intractable analyses, including multi-objective optimization to obtain parameter sets that yield a desired response, and Approximate Bayesian Computation to assess parametric uncertainty. To facilitate exploitation of emulation in simulation-focused studies, we extend our open source statistical package, <italic>spartan</italic>, to provide a suite of tools for emulator development, validation, and application. Overcoming resource limitations permits enriched evaluation and refinement, easing translation of simulator insights into increased biological understanding.",10.1109/TCBB.2018.2843339,,,
Classifiers Selection Based on Analytic Hierarchy Process and Similarity Score for Spam Identification,"Mekouar, Soufiana",Elsevier Science Publishers B. V.,2021,,10.1016/j.asoc.2021.108022,"Images spam, Multi-layer perceptron, Analytic hierarchy process, Machine learning classifiers, Corpus spam",,
Integrating Radiologist Feedback with Computer Aided Diagnostic Systems for Breast Cancer Risk Prediction in Ultrasonic Images,"Singh, Bikesh Kumar and Verma, Kesari and Panigrahi, Lipismita and Thoke, A.S.","Pergamon Press, Inc.",2017,"New hybrid classification approach by integrating BPANN and SVM is developed.Radiologist opinion is incorporated in CAD system.Integrating radiologists opinion in CAD systems improves its overall performance.Proposed method outperforms existing ones. With advancements in machine learning algorithms and computer aided diagnostic (CAD) systems, the performance of automated analysis of radiological images has improved substantially in recent times. However, the lack of integration between the radiologist and CAD systems restrains the rate of progress as well as the reach of such advancements in clinical use. This article aims to improve the clinical efficiency of ultrasound based CAD systems for classification of breast lesions by integrating back-propagation artificial neural network (BPANN), support vector machine (SVM) and radiologist feedback. The acquired breast ultrasound images were subjected to wavelet based filtering in order to reduce speckle noise followed by feature extraction, feature selection and classification. Experiments on a database of 178 ultrasound images of breast anomalies (88 benign and 90 malignant) show that the proposed methodology achieves classification accuracy of 98.621% and 98.276%, respectively, when all 457 and 19 most relevant features selected by multi-criteria feature selection method were used for classification. The accuracy achieved is significantly higher than that using conventional classifiers based on BPANN and SVM. Further, it is found that integrating expert opinion in CAD systems improves its overall performance. The quantitative results obtained are discussed in light of some recently reported studies.",10.1016/j.eswa.2017.08.020,"Ultrasound, Support vector machine, Radiologist opinion, Neural network, Machine learning, Breast tumor classification",,
Factorial Design Analysis Applied to the Performance of SMS Anti-Spam Filtering Systems,"Arag\~{a}o, Marcelo V.C. and Frigieri, Edielson Prevato and Ynoguti, Carlos A. and Paiva, Anderson P.","Pergamon Press, Inc.",2016,"An anti-spam filtering system for SMS is proposed.Different system parameters are evaluated through factorial design analysis.The optimal configuration has reached over 98% classification accuracy.The number of features increases system performance but adds computational cost.Linguistic techniques do not represent significant performance boost for some setups. Over the last few decades, the advent of telecommunication systems has allowed a growing exchange of electronic messages around the world. Unfortunately, irrelevant and/or unsolicited content corresponds to the majority of this volume of data, and to decide whether to keep or discard each message is a known challenge in the context of machine learning. This paper proposes an anti-spam filtering approach base on linguistic techniques. The real effect of each system parameter is evaluated through design factorial analysis using two different classifiers: first using Support Vector Machine (SVM) and second applying Naive Bayesian (NB) classification. This analysis is detailed and discussed providing a step-by-step guide for developers and users of anti-spam filters. Based on different system metrics, multi-objective optimization is applied in order to obtain the optimal filter setup. Evaluation of anti-spam filter under optimal configuration showed that SVM-based system achieved an accuracy performance above 98% whereas the NB-based system reached 87%. Results also reveal that linguistic techniques are relevant for the NB classifier but do not contribute to improve the SVM-based system performance.",10.1016/j.eswa.2016.08.038,"Factorial design, Naive bayesian (NB), Short message service (SMS), Spam filtering, Support vector machine (SVM)",,
Autonomous Cooperative Decision-Making in Massively Distributed IoT via Heterogenous Networks,"Rahmani, Rahim and Kanter, Theo",Association for Computing Machinery,2017,"This paper presents a disruptive approach ""Immersive Networking"" enabling massively distributed IoT nodes to participate in autonomous and cooperative decision-making. The approach is mandated by perceived limitations in 5G networking architecture maintaining control in the edge gateway. In our approach, control may be delegated to clusters of IoT nodes beyond the edge gateway. The communication is event-based involving publish-subscribe between related nodes. Clusters are identified in an autonomic fashion based on multi-criteria proximity. Local decisions can combine global and local context information to establish network slices in a decentralized fashion based on application demands. Moreover, such decisions may be part of a collaborative effort (map-reduce) based on either local or global context. Application demands expressed as such are modeled compatible with Open Data initiatives. We demonstrated feasibility of the approach and evaluate its advantages over the 5G architecture involving an edge gateway.",10.1145/3109761.3109786,"autonomic edge gateway, distributed IoT, IoT-middleware, wireless sensor networks, fog computing",,
A Novel Fuzzy Mechanism for Risk Assessment in Software Projects,"Suresh, K. and Dillibabu, R.",Springer-Verlag,2020,"Risk management is a vital factor for ensuring better quality software development processes. Moreover, risks are the events that could adversely affect the organization activities or the development of projects. Effective prioritization of software project risks play a significant role in determining whether the project will be successful in terms of performance characteristics or not. In this work, we develop a new hybrid fuzzy-based machine learning mechanism for performing risk assessment in software projects. This newly developed hybridized risk assessment scheme can be used to determine and rank the significant software project risks that support the decision making during the software project lifecycle. For better assessment of the software project risks, we have incorporated fuzzy decision making trial and evaluation laboratory, adaptive neuro-fuzzy inference system-based multi-criteria decision making (ANFIS MCDM) and intuitionistic fuzzy-based TODIM (IF-TODIM) approaches. More significantly, for the newly introduced ANFIS MCDM approach, the parameters of ANFIS are adjusted using a traditional crow search algorithm (CSA) which applies only a reasonable as well as small changes in variables. The main activity of CSA in ANFIS is to find the best parameter to achieve most accurate software risk estimate. Experimental validation was conducted on NASA 93 dataset having 93 software project values. The result of this method exhibits a vivid picture that provides software risk factors that are key determinant for achievement of the project performance. Experimental outcomes reveal that our proposed integrated fuzzy approaches can exhibit better and accurate performance in the assessment of software project risks compared to other existing approaches.",10.1007/s00500-019-03997-2,"MCDM, Performance, Adaptive neuro-fuzzy inference system, Crow search algorithm, IF-TODIM, Fuzzy DEMATEL, Project risk",,
Understanding Effects of Cognitive Rehabilitation under a Knowledge Discovery Approach,"Garc\'{\i}a-Rudolph, Alejandro and Gibert, Karina","Pergamon Press, Inc.",2016,"Traumatic brain injury (TBI) is the leading cause of death and disability in children and young adults worldwide. Cognitive rehabilitation (CR) plans consist of a sequence of CR tasks targeting main cognitive functions. There is not enough on-field experience yet regarding which specific intervention (tasks or exercise assignment) is more appropriate to help therapists to design plans with significant effectiveness on patient improvement. The selection of specific tasks to be prescribed to the patient and the order in which they might be executed is currently decided by the therapists based on their experience.In this paper a new data mining methodology is proposed, combining several tools from Artificial Intelligence, clustering and post-processing analysis to identify regularities in the sequences of tasks in such a way that treatment profiles (classes) can be discovered. Due to the cumulative effect of rehabilitation tasks, small variations within the sequence of tasks performed by the patient do not significantly change the final outcomes in rehabilitation and makes it difficult to find discriminant rules by using the traditional machine learning inductive methods. However, by relaxing the formalization of the problem to find patterns that might include small variations, and introducing motif discovery techniques in the proposed methodology, the complexity of the neurorehabilitation phenomenon can be better captured and a global structure of successful treatment task sequences can be devised.Following this, the relationship between the discovered patterns and the CR treatment response are analyzed, offering a richer perspective than that provided by the single task focus traditionally used in the CR field.The paper provides a definition of the whole methodological approach proposed from a formal point of view, and its application to a real dataset. Comparisons with traditional AI approaches are also presented and the contribution of the proposed methodology to the AI field discussed. SAIMAP is an innovative combination of pre-processing, clustering, motif discovery and post-processing in a hybrid methodological frame,SAIMAP associates sequential patterns of a predefined set of events including high order interactions and cumulative effects.Effects of treatment are measured through multi-criteria improvement indicators set, referring tot a predefined set of areas.SAIMAP provides the possibility to understand underlying structure in Cognitive Rehabilitation PatternsSAIMAP analyses characteristics of different types of treatments. Length of treatment is associated with different treatment schemesDecision criteria to understand when long or short treatment is required can be also learned.Shorter treaments work for patients with short-term memory mild impaired; high impairment in recognition memory require longer treatment.SAIMAP overcomes classical machine learning approaches for this complex scenarios.SAIMAP is suitable for other research fields out of neuropsychology, provided they fit the formal structure of problem described in the paper.",10.1016/j.engappai.2016.06.007,"Knowledge discovery, Cognitive rehabilitation, Clustering-based on rules, Motifs, Traumatic brain injury, Post-processing",,
IntelliHealth,"Bashir, Saba and Qamar, Usman and Khan, Farhan Hassan",Elsevier Science,2016,"Display Omitted Extensive research has been conducted on disease prediction.An optimal combination of classifiers is presented with multi-layer classification.The ensemble approach uses bagging with multi-objective optimized weighted.Comparison with existing techniques show superiority of our ensemble.An application named ""IntelliHealth"" has been developed. Accuracy plays a vital role in the medical field as it concerns with the life of an individual. Extensive research has been conducted on disease classification and prediction using machine learning techniques. However, there is no agreement on which classifier produces the best results. A specific classifier may be better than others for a specific dataset, but another classifier could perform better for some other dataset. Ensemble of classifiers has been proved to be an effective way to improve classification accuracy. In this research we present an ensemble framework with multi-layer classification using enhanced bagging and optimized weighting. The proposed model called ""HM-BagMoov"" overcomes the limitations of conventional performance bottlenecks by utilizing an ensemble of seven heterogeneous classifiers. The framework is evaluated on five different heart disease datasets, four breast cancer datasets, two diabetes datasets, two liver disease datasets and one hepatitis dataset obtained from public repositories. The analysis of the results show that ensemble framework achieved the highest accuracy, sensitivity and F-Measure when compared with individual classifiers for all the diseases. In addition to this, the ensemble framework also achieved the highest accuracy when compared with the state of the art techniques. An application named ""IntelliHealth"" is also developed based on proposed model that may be used by hospitals/doctors for diagnostic advice.",10.1016/j.jbi.2015.12.001,"Classification, Machine learning, Multi-layer, Disease prediction, Bagging, Ensemble technique",,
Every Drop Counts: Unleashing the Prospective Locations for Water Harvesting Using Geospatial Analytics,"Gupta, Aparana and Garg, Anshul and Rawat, Namrata and Chigurupati, Sandeep and Kumar, U Dinesh",Association for Computing Machinery,2017,"Water is at the heart of 'Sustainable Development Goals (SDGs)' set by United Nations - with an objective to balance the three dimensions of sustainable development: Environment, Social and Economic - and is indirectly associated with the success of all the other Goals. But, with changing climatic patterns, untimely rains, prolonged dry spells, depleting ground water and drought making every drop of water extremely precious, the need of the hour is to gauge and work towards the major aspects of water harvesting - 'Catchment'. Water Harvesting must be a key element of any strategy to bring an end to India's perennial swings between drought and flood and to meet the following SDGs for sustained development. This study presents a structured and meticulous approach, wielding 'Geospatial Analytics' to identify the prospective locations for Water Harvesting in arid and semi-arid parts of the country for sustainable development. This paper is structured as follows. Section 1 describes the background and motivation for this idea. Section 2 details out the objective. In section 3 we present the 'Literature Survey' on the work that has already been carried out in this field. While section 4 discerns our area of study, Section 5 provides process flow starting from Data gathering, Data extraction, Data pre-processing, Model selection and Multi Criteria Decision Making (Model Application). In Section 6, we present and validate our experimental results achieved using the proposed methodology. Section 7 concludes our study followed by Section 8 on Recommendations for future enhancements and next steps.",10.1145/3109761.3158394,"image processing, flood fill model, water tanks, sliding window algorithm, geospatial analytics, AHP, LAND-SAT-8, RWH optimum location selection, rain water harvesting, analytical hierarchy process, smart water, digital elevation model, GIS",,
Textual Entailment--Based Figure Summarization for Biomedical Articles,"Saini, Naveen and Saha, Sriparna and Bhattacharyya, Pushpak and Tuteja, Himanshu",Association for Computing Machinery,2020,"This article proposes a novel unsupervised approach (FigSum++) for automatic figure summarization in biomedical scientific articles using a multi-objective evolutionary algorithm. The problem is treated as an optimization problem where relevant sentences in the summary for a given figure are selected based on various sentence scoring features (or objective functions), such as the textual entailment score between sentences in the summary and a figure’s caption, the number of sentences referring to that figure, semantic similarity between sentences and a figure’s caption, and the number of overlapping words between sentences and a figure’s caption. These objective functions are optimized simultaneously using multi-objective binary differential evolution (MBDE). MBDE consists of a set of solutions, and each solution represents a subset of sentences to be selected in the summary. MBDE generally uses a single differential evolution variant, but in the current study, an ensemble of two different differential evolution variants measuring diversity among solutions and convergence toward global optimal solution, respectively, is employed for efficient search. Usually, in any summarization system, diversity among sentences (called anti-redundancy) in the summary is a very critical feature, and it is calculated in terms of similarity (like cosine similarity) among sentences. In this article, a new way of measuring diversity in terms of textual entailment is proposed. To represent the sentences of the article in the form of numeric vectors, the recently proposed BioBERT pre-trained language model in biomedical text mining is utilized. An ablation study has also been presented to determine the importance of different objective functions. For evaluation of the proposed technique, two benchmark biomedical datasets containing 91 and 84 figures are considered. Our proposed system obtains 5% and 11% improvements in terms of the F-measure metric over two datasets, compared to the state-of-the-art unsupervised methods.",10.1145/3357334,"evolutionary computing, textual entailment, multi-objective optimization (MOO), Figure-assisted text summarization",,
Embedding Temporal Convolutional Networks for Energy-Efficient PPG-Based Heart Rate Monitoring,"Burrello, Alessio and Pagliari, Daniele Jahier and Rapa, Pierangelo Maria and Semilia, Matilde and Risso, Matteo and Polonelli, Tommaso and Poncino, Massimo and Benini, Luca and Benatti, Simone",Association for Computing Machinery,2022,"Photoplethysmography (PPG) sensors allow for non-invasive and comfortable heart rate (HR) monitoring, suitable for compact wrist-worn devices. Unfortunately, motion artifacts (MAs) severely impact the monitoring accuracy, causing high variability in the skin-to-sensor interface. Several data fusion techniques have been introduced to cope with this problem, based on combining PPG signals with inertial sensor data. Until now, both commercial and reasearch solutions are computationally efficient but not very robust, or strongly dependent on hand-tuned parameters, which leads to poor generalization performance. In this work, we tackle these limitations by proposing a computationally lightweight yet robust deep learning-based approach for PPG-based HR estimation. Specifically, we derive a diverse set of Temporal Convolutional Networks for HR estimation, leveraging Neural Architecture Search. Moreover, we also introduce ActPPG, an adaptive algorithm that selects among multiple HR estimators depending on the amount of MAs, to improve energy efficiency. We validate our approaches on two benchmark datasets, achieving as low as 3.84 beats per minute of Mean Absolute Error on PPG-Dalia, which outperforms the previous state of the art. Moreover, we deploy our models on a low-power commercial microcontroller (STM32L4), obtaining a rich set of Pareto optimal solutions in the complexity vs. accuracy space.",10.1145/3487910,"wearable devices, medical IoT, heart rate monitoring, deep learning, Temporal convolutional networks",,
A Probabilistic Graphical Model-Based Approach for Minimizing Energy Under Performance Constraints,"Mishra, Nikita and Zhang, Huazhe and Lafferty, John D. and Hoffmann, Henry",Association for Computing Machinery,2015,"In many deployments, computer systems are underutilized -- meaning that applications have performance requirements that demand less than full system capacity. Ideally, we would take advantage of this under-utilization by allocating system resources so that the performance requirements are met and energy is minimized. This optimization problem is complicated by the fact that the performance and power consumption of various system configurations are often application -- or even input -- dependent. Thus, practically, minimizing energy for a performance constraint requires fast, accurate estimations of application-dependent performance and power tradeoffs. This paper investigates machine learning techniques that enable energy savings by learning Pareto-optimal power and performance tradeoffs. Specifically, we propose LEO, a probabilistic graphical model-based learning system that provides accurate online estimates of an application's power and performance as a function of system configuration. We compare LEO to (1) offline learning, (2) online learning, (3) a heuristic approach, and (4) the true optimal solution. We find that LEO produces the most accurate estimates and near optimal energy savings.",10.1145/2786763.2694373,probabilistic graphical models,,
Deep Room Recognition Using Inaudible Echos,"Song, Qun and Gu, Chaojie and Tan, Rui",Association for Computing Machinery,2018,"Recent years have seen the increasing need of location awareness by mobile applications. This paper presents a room-level indoor localization approach based on the measured room's echos in response to a two-millisecond single-tone inaudible chirp emitted by a smartphone's loudspeaker. Different from other acoustics-based room recognition systems that record full-spectrum audio for up to ten seconds, our approach records audio in a narrow inaudible band for 0.1 seconds only to preserve the user's privacy. However, the short-time and narrowband audio signal carries limited information about the room's characteristics, presenting challenges to accurate room recognition. This paper applies deep learning to effectively capture the subtle fingerprints in the rooms' acoustic responses. Our extensive experiments show that a two-layer convolutional neural network fed with the spectrogram of the inaudible echos achieve the best performance, compared with alternative designs using other raw data formats and deep models. Based on this result, we design a RoomRecognize cloud service and its mobile client library that enable the mobile application developers to readily implement the room recognition functionality without resorting to any existing infrastructures and add-on hardware. Extensive evaluation shows that RoomRecognize achieves 99.7%, 97.7%, 99%, and 89% accuracy in differentiating 22 and 50 residential/office rooms, 19 spots in a quiet museum, and 15 spots in a crowded museum, respectively. Compared with the state-of-the-art approaches based on support vector machine, RoomRecognize significantly improves the Pareto frontier of recognition accuracy versus robustness against interfering sounds (e.g., ambient music).",10.1145/3264945,"smartphone, Room recognition, inaudible sound",,
Online Feature Elicitation in Interactive Optimization,"Boutilier, Craig and Regan, Kevin and Viappiani, Paolo",Association for Computing Machinery,2009,"Most models of utility elicitation in decision support and interactive optimization assume a predefined set of ""catalog"" features over which user preferences are expressed. However, users may differ in the features over which they are most comfortable expressing their preferences. In this work we consider the problem of feature elicitation: a user's utility function is expressed using features whose definitions (in terms of ""catalog"" features) are unknown. We cast this as a problem of concept learning, but whose goal is to identify only enough about the concept to enable a good decision to be recommended. We describe computational procedures for identifying optimal alternatives w.r.t. minimax regret in the presence of concept uncertainty; and describe several heuristic query strategies that focus on reduction of relevant concept uncertainty.",10.1145/1553374.1553384,,,
"Hardware Optimizations of Dense Binary Hyperdimensional Computing: Rematerialization of Hypervectors, Binarized Bundling, and Combinational Associative Memory","Schmuck, Manuel and Benini, Luca and Rahimi, Abbas",Association for Computing Machinery,2019,"Brain-inspired hyperdimensional (HD) computing models neural activity patterns of the very size of the brain’s circuits with points of a hyperdimensional space, that is, with hypervectors. Hypervectors are D-dimensional (pseudo)random vectors with independent and identically distributed (i.i.d.) components constituting ultra-wide holographic words: D=10,000 bits, for instance. At its very core, HD computing manipulates a set of seed hypervectors to build composite hypervectors representing objects of interest. It demands memory optimizations with simple operations for an efficient hardware realization. In this article, we propose hardware techniques for optimizations of HD computing, in a synthesizable open-source VHDL library, to enable co-located implementation of both learning and classification tasks on only a small portion of Xilinx UltraScale FPGAs: (1) We propose simple logical operations to rematerialize the hypervectors on the fly rather than loading them from memory. These operations massively reduce the memory footprint by directly computing the composite hypervectors whose individual seed hypervectors do not need to be stored in memory. (2) Bundling a series of hypervectors over time requires a multibit counter per every hypervector component. We instead propose a binarized back-to-back bundling without requiring any counters. This truly enables on-chip learning with minimal resources as every hypervector component remains binary over the course of training to avoid otherwise multibit components. (3) For every classification event, an associative memory is in charge of finding the closest match between a set of learned hypervectors and a query hypervector by using a distance metric. This operator is proportional to hypervector dimension (D), and hence may take O(D) cycles per classification event. Accordingly, we significantly improve the throughput of classification by proposing associative memories that steadily reduce the latency of classification to the extreme of a single cycle. (4) We perform a design space exploration incorporating the proposed techniques on FPGAs for a wearable biosignal processing application as a case study. Our techniques achieve up to 2.39\texttimes{} area saving, or 2,337\texttimes{} throughput improvement. The Pareto optimal HD architecture is mapped on only 18,340 configurable logic blocks (CLBs) to learn and classify five hand gestures using four electromyography sensors.",10.1145/3314326,"binarized temporal bundling, biosignals, rematerialization, on-chip learning, single-cycle associative memory, electromyography, Hyperdimensional computing, FPGA",,
