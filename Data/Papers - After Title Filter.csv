Title,Authors,Publisher,Year,Abstract,DOI,Keywords,Citations,Remove - Title,Second Filter
Single-Objective/Multiobjective Cat Swarm Optimization Clustering Analysis for Data Partition,D. Yan; H. Cao; Y. Yu; Y. Wang; X. Yu,IEEE Transactions on Automation Science and Engineering,2020.0,"This article proposes single-objective/multiobjective cat swarm optimization clustering algorithms for data partition. The proposed methods use the cat swarm to search the optimal. The position of the cat tightly associates with the clustering centers and is updated by two submodes: the seeking mode and the tracing mode. The seeking mode uses the simulated annealing strategy to update the cat position at a probability. Inspired by the quantum theories, the tracing mode adopts the quantum model to update the cat position in the whole solution space. First, the single-objective method is proposed and adopts the cohesion of clustering as the objective function, in which the kernel method is applied. For considering more objective functions to reveal diverse aspects of data, the multiobjective method is proposed and adopts both the cohesion and the connectivity as the objective functions. The Pareto optimization method is applied to balance the objectives. In the experiments, three kinds of data sets are used to examine the effectiveness of the proposed methods, which are three synthetic data sets, four data sets from the UCI Machine Learning Repository, and a field data set. Experimental results verified that the proposed methods perform better than the traditional clustering algorithms, and the proposed multiobjective method has the highest accuracy. Note to Practitioners-This article presents single-objective/multiobjective cat swarm optimization clustering analysis methods for data partition. Through automatically extracting meaningful or useful classes, clustering analysis could help the practitioners or the intelligent devices find the specific meanings of data, natural data structure, the data relationships, or other characteristics. The proposed methods use the cat swarm to search the optimal clustering result. One or more criterion functions could be selected as the optimization objectives. The time complexity of the multiobjective type is higher than that of the single-objective type. Therefore, in the industrial field, engineers should choose the number of the optimization objectives based on the actual requirements. The proposed methods could be widely used into industrial applications to deal with complex data sets. Future research could consider some more progressive optimization schemes to improve the effectiveness.",10.1109/TASE.2020.2969485,Clustering analysis;data partition;quantum model;single-objective/multiobjective optimization,9.0,0.0,1.0
Rotation effects of objective functions in parallel distributed multiobjective fuzzy genetics-based machine learning,Y. Takahashi; Y. Nojima; H. Ishibuchi,2015 10th Asian Control Conference (ASCC),2015.0,"Fuzzy genetics-based machine learning (FGBML) is one of data mining techniques using evolutionary computation. It can obtain fuzzy rule-based classifiers that are accurate and linguistically interpretable for human users. However, there are two major problems. One is that it is impossible to design the best classifier with respect to both accuracy and interpretability due to their tradeoff. To solve this problem, we proposed multiobjective FGBML (MoFGBML) where an evolutionary multiobjective optimization algorithm is used to obtain a number of classifiers with different tradeoffs between accuracy and complexity. The other is the heavy computational load of FGBML for large data sets. In the previous study, we applied parallel distributed implementation to our MoFGBML to overcome this problem. We examined the effects of the parallel distributed implementation on the search ability. Although the computational time became much shorter, the number of the obtained non-dominated classifiers became small. As a result, accurate classifiers were not obtained for some data sets. In this paper, we propose a simple idea to bias the search direction of our MoFGBML. We rotate one or two objective functions. This rotation changes the dominance relation in multiobjective optimization. Through computational experiments, we examine the effects of the rotated objective functions on the search ability of our MoFGBML for large data sets.",10.1109/ASCC.2015.7244890,Fuzzy genetics-based machine learning;parallel distributed implementation;multiobjective optimization,1.0,0.0,1.0
Multiple Reference Points-Based Decomposition for Multiobjective Feature Selection in Classification: Static and Dynamic Mechanisms,B. H. Nguyen; B. Xue; P. Andreae; H. Ishibuchi; M. Zhang,IEEE Transactions on Evolutionary Computation,2020.0,"Feature selection is an important task in machine learning that has two main objectives: 1) reducing dimensionality and 2) improving learning performance. Feature selection can be considered a multiobjective problem. However, it has its problematic characteristics, such as a highly discontinuous Pareto front, imbalance preferences, and partially conflicting objectives. These characteristics are not easy for existing evolutionary multiobjective optimization (EMO) algorithms. We propose a new decomposition approach with two mechanisms (static and dynamic) based on multiple reference points under the multiobjective evolutionary algorithm based on decomposition (MOEA/D) framework to address the above-mentioned difficulties of feature selection. The static mechanism alleviates the dependence of the decomposition on the Pareto front shape and the effect of the discontinuity. The dynamic one is able to detect regions in which the objectives are mostly conflicting, and allocates more computational resources to the detected regions. In comparison with other EMO algorithms on 12 different classification datasets, the proposed decomposition approach finds more diverse feature subsets with better performance in terms of hypervolume and inverted generational distance. The dynamic mechanism successfully identifies conflicting regions and further improves the approximation quality for the Pareto fronts.",10.1109/TEVC.2019.2913831,Classification;feature selection;multiobjective evolutionary algorithm based on decomposition (MOEA/D);multiobjective optimization;partially conflicting,23.0,0.0,1.0
Multiobjective Neural Network Ensembles Based on Regularized Negative Correlation Learning,H. Chen; X. Yao,IEEE Transactions on Knowledge and Data Engineering,2010.0,"Negative Correlation Learning (NCL) [CHECK END OF SENTENCE], [CHECK END OF SENTENCE] is a neural network ensemble learning algorithm which introduces a correlation penalty term to the cost function of each individual network so that each neural network minimizes its mean-square-error (MSE) together with the correlation. This paper describes NCL in detail and observes that the NCL corresponds to training the entire ensemble as a single learning machine that only minimizes the MSE without regularization. This insight explains that NCL is prone to overfitting the noise in the training set. The paper analyzes this problem and proposes the multiobjective regularized negative correlation learning (MRNCL) algorithm which incorporates an additional regularization term for the ensemble and uses the evolutionary multiobjective algorithm to design ensembles. In MRNCL, we define the crossover and mutation operators and adopt nondominated sorting algorithm with fitness sharing and rank-based fitness assignment. The experiments on synthetic data as well as real-world data sets demonstrate that MRNCL achieves better performance than NCL, especially when the noise level is nontrivial in the data set. In the experimental discussion, we give three reasons why our algorithm outperforms others.",10.1109/TKDE.2010.26,Multiobjective algorithm;multiobjective learning;neural network ensembles;neural networks;negative correlation learning;regularization.,83.0,0.0,1.0
Feature Selection Using Multiobjective Optimization for Named Entity Recognition,A. Ekbal; S. Saha; C. S. Garbe,2010 20th International Conference on Pattern Recognition,2010.0,"Appropriate feature selection is a very crucial issue in any machine learning framework, specially in Maximum Entropy (ME). In this paper, the selection of appropriate features for constructing a ME based Named Entity Recognition (NER) system is posed as a multiobjective optimization (MOO) problem. Two classification quality measures, namely recall and precision are simultaneously optimized using the search capability of a popular evolutionary MOO technique, NSGA-II. The proposed technique is evaluated to determine suitable feature combinations for NER in two languages, namely Bengali and English that have significantly different characteristics. Evaluation results yield the recall, precision and F-measure values of 70.76%, 81.88% and 75.91%, respectively for Bengali, and 78.38%, 81.27% and 79.80%, respectively for English. Comparison with an existing ME based NER system shows that our proposed feature selection technique is more efficient than the heuristic based feature selection.",10.1109/ICPR.2010.477,Multiobjective Optimization;Feature Selection;Maximum Entropy;Named Entity Recognition,17.0,0.0,1.0
Search ability of evolutionary multiobjective optimization algorithms for multiobjective fuzzy genetics-based machine learning,H. Ishibuchi; Y. Nakashima; Y. Nojima,2009 IEEE International Conference on Fuzzy Systems,2009.0,"Recently evolutionary multiobjective optimization (EMO) algorithms have been actively used for the design of accurate and interpretable fuzzy rule-based systems. This research area is often referred to as multiobjective genetic fuzzy systems where EMO algorithms are used to search for a number of non-dominated fuzzy rule-based systems with respect to their accuracy and interpretability. The main advantage of the use of EMO algorithms for fuzzy system design over single-objective optimizers is that multiple alternative fuzzy rule-based systems with different accuracy-interpretability tradeoffs are obtained by their single run. The decision maker can choose a single fuzzy rule-based system according to their preference. There still exist several important issues to be discussed in this research area such as the definition of interpretability, the formulation of interpretability measures, the visualization of tradeoff relations, and the interpretability of the explanation of fuzzy reasoning results. In this paper, we discuss the ability of EMO algorithms as multiobjective optimizers to search for Pareto optimal or near Pareto optimal fuzzy rule-based systems. More specifically, we examine whether EMO algorithms can find non-dominated fuzzy rule-based systems that approximate the entire Pareto fronts of multiobjective fuzzy system design problems.",10.1109/FUZZY.2009.5277370,,6.0,0.0,1.0
A Selective Ensemble Classifier Using Multiobjective Optimization Based Extreme Learning Machine Algorithm,L. Bai; H. Li; W. Gao,2021 17th International Conference on Computational Intelligence and Security (CIS),2021.0,"In a single hidden layer feedforward neural network (SLFN), acquiring optimal values for the number of hidden neurons and connection parameters simultaneously is regarded as one of challenges, which has attracted extensive attention. This is because changing the number of hidden neurons and connection parameters greatly affect overall performance of the SLFN and increase the training complexity. In this article, the training error, validation error, and network complexity are treated as three conflicting objectives of multiobjective model for getting a compact network with good generalization ability. For solving the multiobjective model, a hybrid coding scheme is designed for network structure and connection parameters of a SLFN, and then a multiobjective optimization based extreme learning machine (MOELM) is proposed for structure learning and parameter optimization simultaneously. To improve recognition accuracy, a selective ensemble classifier with three base classifiers according to the selection strategy is utilized to make final decision. Experimental results and comparison with other classifiers on several benchmark classification problems indicate the effectiveness and superiority of the proposed MOELM.",10.1109/CIS54983.2021.00017,feedforward neural network;extreme learning machine;multiobjective optimization;ensemble learning;classification,,0.0,1.0
Multiobjective fuzzy genetics-based machine learning with a reject option,Y. Nojima; H. Ishibuchi,2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),2016.0,"Classifier design for a classification problem with M classes can be viewed as finding an optimal partition of its pattern space into M disjoint subspaces. However, this is not always a good strategy especially when training patterns from different classes are heavily overlapping in the pattern space. A simple but practically useful idea is the use of a reject option. In this case, the pattern space is partitioned into (M+1) disjoint subspace where the classification of new patterns is rejected in the (M+1)th subspace. In this paper, we discuss the design of fuzzy rule-based classifiers with a reject option. The rejection subspace is specified by a threshold value for the difference of a kind of matching degrees between the best matching class and the second best matching class. The important research question is how to specify the threshold value. We examine the following two approaches: One is manual specification after designing a fuzzy rule-based classifier, and the other is simultaneous multiobjective optimization of a threshold value and a fuzzy rule-based classifier. In the latter approach, we use three objectives: maximization of the correct classification, and minimization of the rejection and the complexity of the classifier.",10.1109/FUZZ-IEEE.2016.7737854,Fuzzy genetics-based machine learning;reject option;evolutionary multiobjective optimization,1.0,0.0,1.0
Determination of Weights for Multiobjective Decision Making or Machine Learning,P. Wang; H. Zhu; M. Wilamowska-Korsak; Z. Bi; L. Li,IEEE Systems Journal,2014.0,"Decision-making processes in complex systems generally require the mechanisms to make the tradeoff among contradicting design criteria. When multiple objectives are involved in decision making or machine learning, a crucial step is to determine the weights of individual objectives to the system-level performance. Determining the weights of multiobjectives is an evaluation process, and it has been often treated as an optimization problem. However, our preliminary investigation has shown that existing methodologies in dealing with the weights of multiobjectives have some obvious limitations in the sense that the determination of weights is tackled as a single optimization problem, a result based on such an optimization is incomprehensive, and it can even be unreliable when the information about multiple objectives is incomplete such as an incompleteness caused by poor data. The constraints of weights are also discussed. Variable weights are natural in decision-making processes. Therefore, we are motivated to develop a systematic methodology in determining variable weights of multiobjectives. The roles of weights in an original multiobjective decision-making or machine-learning problem are analyzed, and the weights are determined with the aid of a modular neural network. The inconsistency issue of weights is particularly discussed.",10.1109/JSYST.2013.2265663,Consistency;multidisciplinary design optimization (MDO);multifunctional machine learning (MFML);multiobjective decision making (MODM);neural network;tradeoff;variable weights,12.0,0.0,1.0
An Entropy Driven Multiobjective Particle Swarm Optimization Algorithm for Feature Selection,J. Luo; D. Zhou; L. Jiang; H. Ma,2021 IEEE Congress on Evolutionary Computation (CEC),2021.0,"Feature selection is an important research field in machine learning since high-dimensionality is a common characteristic of real-world data. It has two main objectives, which are to maximize the classification accuracy while minimizing the number of selected features. As the two objectives are usually in conflict with each other, it makes feature selection a multi-objective problem. However, the large search space and discrete Pareto front makes it not easy for existing evolutionary multi-objective algorithms. In order to deal with the above mentioned difficulties in feature selection, an entropy driven multiobjective particle swarm optimization algorithm is proposed to remove redundant feature and decrease computational complexity. First, its basic idea is to model feature selection as a multiobjective optimization problem by optimizing the number of features and the classification accuracy in supervised condition simultaneously. Second, a particle initialization strategy based on information entropy is designed to improve the quality of initial solutions, and an adaptive velocity update rule is used to swap between local search and global search. Besides, a specified discrete nondominated sorting is designed. These strategies enable the proposed algorithm to gain better performance on both the quality and size of feature subset. The experimental results show that the proposed algorithm can maintain or improve the quality of Pareto fronts evolved by the state-of-the-art algorithms for feature selection.",10.1109/CEC45853.2021.9504837,feature selection;multiobjective optimization;particle swarm optimization,1.0,0.0,1.0
Model-based multiobjective fuzzy control using a new multiobjective dynamic programming approach,Dong-Oh Kang; Zeungnam Bien,Proceedings Joint 9th IFSA World Congress and 20th NAFIPS International Conference (Cat. No. 01TH8569),2001.0,"The authors propose a model-based multiobjective fuzzy control method which is optimized online via a novel multiobjective dynamic programming. The new multiobjective dynamic programming is guaranteed to derive a Pareto optimal solution. To estimate the effect of each candidate for control input in the dynamic programming procedure, we use state-value predictors of multiple objectives based on the plant model. Temporal difference learning and supervised learning are used for update of the predictors and the plant model. As the learning proceeds, the proposed method derives the compromised solution among multiple objectives. To show the effectiveness of the proposed method, some simulation results are given.",10.1109/NAFIPS.2001.943752,,1.0,0.0,1.0
Effects of the Use of Multiple Fuzzy Partitions on the Search Ability of Multiobjective Fuzzy Genetics-Based Machine Learning,Y. Nojima; Y. Nakashima; H. Ishibuchi,2009 International Conference of Soft Computing and Pattern Recognition,2009.0,"An important issue in the design of fuzzy rule-based systems is to find a good accuracy-complexity tradeoff. While simple fuzzy systems with high interpretability are usually not accurate, complicated fuzzy systems with high accuracy are usually not interpretable. Recently evolutionary multiobjective optimization (EMO) algorithms have been used to search for simple and accurate fuzzy systems. The main advantage of EMO-based approaches over single-objective techniques is that a number of alternative fuzzy systems with different accuracy-complexity tradeoffs can be obtained by their single run. We have already proposed a multiobjective fuzzy genetics-based machine learning (GBML) algorithm for pattern classification problems. In our GBML algorithm, multiple fuzzy partitions with different granularities are simultaneously used. This is because we usually do not know an appropriate fuzzy partition for each input variable. However, the use of multiple fuzzy partitions significantly increases the size of the search space. In this paper, we examine the effect of the use of multiple fuzzy partitions on the search ability of our multiobjective fuzzy GBML algorithms through computational experiments.",10.1109/SoCPaR.2009.74,Fuzzy genetics-based machine learning;Evolutionary multiobjective optimization;Fuzzy rules;Fuzzy partitions;Pattern classification problems,,0.0,1.0
Multiobjective Fuzzy Genetics-Based Machine Learning for Multi-Label Classification,Y. Omozaki; N. Masuyama; Y. Nojima; H. Ishibuchi,2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),2020.0,"In multi-label classification problems, multiple class labels are assigned to each instance. Two approaches have been studied in the literature. One is a data transformation approach, which transforms a multi-label dataset into a number of singlelabel datasets. However, this approach often loses the correlation information among classes in the multi-class assignment. The other is a method adaptation approach where a conventional classification method is extended to multi-label classification. Recently, some explainable classification models for multi-label classification have been proposed. Their high interpretability has also been discussed with respect to the transparency of the classification process. Although the explainability is a well-known advantage of fuzzy systems, their applications to multi-label classification have not been well studied. Since multi-label classification problems often have vague class boundaries, fuzzy systems seem to be a promising approach to multi-label classification. In this paper, we propose a new multiobjective evolutionary fuzzy system, which can be categorized as a method adaptation approach. The proposed algorithm produces nondominated classifiers with different tradeoffs between accuracy and complexity. We examine the behavior of the proposed algorithm using synthetic multi-label datasets. We also compare the proposed algorithm with five representative algorithms. Our experimental results on real-world datasets show that the obtained fuzzy classifiers with a small number of fuzzy rules have high transparency and comparable generalization ability to the other examined multi-label classification algorithms.",10.1109/FUZZ48607.2020.9177804,multi-label classification;multiobjective fuzzy genetics-based machine learning;fuzzy rule-based classification system;method adaptation approach,1.0,0.0,1.0
Effects of heuristic rule generation from multiple patterns in multiobjective fuzzy genetics-based machine learning,Y. Nojima; K. Watanabe; H. Ishibuchi,2015 IEEE Congress on Evolutionary Computation (CEC),2015.0,"Fuzzy genetics-based machine learning (FGBML) has frequently been used for fuzzy classifier design. It is one of the promising evolutionary machine learning (EML) techniques from the viewpoint of data mining. This is because FGBML can generate accurate classifiers with linguistically interpretable fuzzy if-then rules. Of course, a classifier with tens of thousands of if-then rules is not linguistically understandable. Thus, the complexity minimization of fuzzy classifiers should be considered together with the accuracy maximization. In previous studies, we proposed hybrid FGBML and its multiobjective formulation (MoFGBML) to handle both the accuracy maximization and the complexity minimization simultaneously. MoFGBML can obtain a number of non-dominated classifiers with different tradeoffs between accuracy and complexity. In this paper, we focus on heuristic rule generation in MoFGBML to improve the search performance. In the original heuristic rule generation, each if-then rule is generated from a randomly-selected training pattern in a heuristic manner. This operation is performed at population initialization and during evolution. To generate more generalized rules according to the training data, we propose new heuristic rule generation where each rule is generated from multiple training patterns. Through computational experiments using some benchmark data sets, we discuss the effects of the proposed operation on the search performance of our MoFGBML.",10.1109/CEC.2015.7257262,Fuzzy genetics-based machine learning;heuristic rule generation;evolutionary multiobjective optimization,5.0,0.0,1.0
Multiobjective Particle Swarm Optimization for Feature Selection With Fuzzy Cost,Y. Hu; Y. Zhang; D. Gong,IEEE Transactions on Cybernetics,2021.0,"Feature selection (FS) is an important data processing technique in the field of machine learning. There have been various FS methods, but all assume that the cost associated with a feature is precise, which restricts their real applications. Focusing on the FS problem with fuzzy cost, a fuzzy multiobjective FS method with particle swarm optimization, called PSOMOFS, is studied in this article. The proposed method develops a fuzzy dominance relationship to compare the goodness of candidate particles and defines a fuzzy crowding distance measure to prune the elitist archive and determine the global leader of particles. Also, a tolerance coefficient is introduced into the proposed method to ensure that the Pareto-optimal solutions obtained satisfy decision makers' preferences. The developed method is used to tackle a series of the UCI datasets and is compared with three fuzzy multiobjective evolutionary methods and three typical multiobjective FS methods. Experimental results show that the proposed method can achieve feature sets with superior performances in approximation, diversity, and feature cost.",10.1109/TCYB.2020.3015756,Feature selection (FS);fuzzy cost;multiobjective optimization;particle swarm optimization (PSO),37.0,0.0,1.0
Multiobjective Semisupervised Classifier Ensemble,Z. Yu; Y. Zhang; C. L. P. Chen; J. You; H. Wong; D. Dai; S. Wu; J. Zhang,IEEE Transactions on Cybernetics,2019.0,"Classification of high-dimensional data with very limited labels is a challenging task in the field of data mining and machine learning. In this paper, we propose the multiobjective semisupervised classifier ensemble (MOSSCE) approach to address this challenge. Specifically, a multiobjective subspace selection process (MOSSP) in MOSSCE is first designed to generate the optimal combination of feature subspaces. Three objective functions are then proposed for MOSSP, which include the relevance of features, the redundancy between features, and the data reconstruction error. Then, MOSSCE generates an auxiliary training set based on the sample confidence to improve the performance of the classifier ensemble. Finally, the training set, combined with the auxiliary training set, is used to select the optimal combination of basic classifiers in the ensemble, train the classifier ensemble, and generate the final result. In addition, diversity analysis of the ensemble learning process is applied, and a set of nonparametric statistical tests is adopted for the comparison of semisupervised classification approaches on multiple datasets. The experiments on 12 gene expression datasets and two large image datasets show that MOSSCE has a better performance than other state-of-the-art semisupervised classifiers on high-dimensional data.",10.1109/TCYB.2018.2824299,Ensemble learning;feature selection;multiobjective optimization;semisupervised learning,13.0,0.0,1.0
Convex Hull-Based Multiobjective Genetic Programming for Maximizing Receiver Operating Characteristic Performance,P. Wang; M. Emmerich; R. Li; K. Tang; T. Bäck; X. Yao,IEEE Transactions on Evolutionary Computation,2015.0,"The receiver operating characteristic (ROC) is commonly used to analyze the performance of classifiers in data mining. An important topic in ROC analysis is the ROC convex hull (ROCCH), which is the least convex majorant (LCM) of the empirical ROC curve and covers potential optima for a given set of classifiers. ROCCH maximization problems have been taken as multiobjective optimization problem (MOPs) in some previous work. However, the special characteristics of ROCCH maximization problem makes it different from traditional MOPs. In this paper, the difference will be discussed in detail and a new convex hull-based multiobjective genetic programming (CH-MOGP) is proposed to solve ROCCH maximization problems. Specifically, convex hull-based without redundancy sorting (CWR-sorting) is introduced, which is an indicator-based selection scheme that aims to maximize the area under the convex hull. A novel selection procedure is also proposed based on the proposed sorting scheme. It is hypothesized that by using a tailored indicator-based selection, CH-MOGP becomes more efficient for ROC convex hull approximation than algorithms that compute all Pareto optimal points. Empirical studies are conducted to compare CH-MOGP to both existing machine learning approaches and multiobjective genetic programming (MOGP) methods with classical selection schemes. Experimental results show that CH-MOGP outperforms the other approaches significantly.",10.1109/TEVC.2014.2305671,Classification;evolutionary multiobjective algorithm;genetic programming;memetic algorithm;receiver operating characteristic (ROC) convex hull,26.0,0.0,1.0
Learning From a Stream of Nonstationary and Dependent Data in Multiobjective Evolutionary Optimization,J. Sun; H. Zhang; A. Zhou; Q. Zhang; K. Zhang; Z. Tu; K. Ye,IEEE Transactions on Evolutionary Computation,2019.0,"Combining machine learning techniques has shown great potentials in evolutionary optimization since the domain knowledge of an optimization problem, if well learned, can be a great help for creating high-quality solutions. However, existing learning-based multiobjective evolutionary algorithms (MOEAs) spend too much computational overhead on learning. To address this problem, we propose a learning-based MOEA where an online learning algorithm is embedded within the evolutionary search procedure. The online learning algorithm takes the stream of sequentially generated solutions along the evolution as its training data. It is noted that the stream of solutions are temporal, dependent, nonstationary, and nonstatic. These data characteristics make existing online learning algorithm not suitable for the evolution data. We hence modify an existing online agglomerative clustering algorithm to accommodate these characteristics. The modified online clustering algorithm is applied to adaptively discover the structure of the Pareto optimal set; and the learned structure is used to guide new solution creation. Experimental results have shown significant improvement over four state-of-the-art MOEAs on a variety of benchmark problems.",10.1109/TEVC.2018.2865495,Evolutionary algorithms (EAs);machine learning (ML);multiobjective optimization;online agglomerative clustering,12.0,0.0,1.0
Semi-supervised clustering using multiobjective optimization,S. Saha; A. Ekbal; A. K. Alok,2012 12th International Conference on Hybrid Intelligent Systems (HIS),2012.0,"Semi-supervised clustering uses the information of unsupervised and supervised learning to overcome the problems associated with them. Extracted information are given in the form of class labels and data distribution during clustering process. In this paper the problem of semi-supervised clustering is formulated under the framework of multiobjective optimization (MOO). Thereafter, a multiobjective based clustering technique is extended to solve the semi-supervised clustering problem. The newly developed semi-supervised multiobjective clustering algorithm (Semi-GenClustMOO), is used for appropriate partitioning of data into appropriate number of clusters. Four objective functions are optimized, out of which first three use some unsupervised information and the last one uses supervised information. These four objective functions represent, respectively, the, total compactness of the partitioning, total symmetry present in the clusters, cluster connectedness and Adjust Rand Index. These four objective functions are optimized simultaneously using AMOSA, a newly developed simulated annealing based multiobjective optimization method. Results show that it can easily detect the appropriate number of clusters as well as the appropriate partitioning from data sets having either well-separated clusters of any shape or symmetrical clusters with or without overlaps. Seven artificial and four real-life data sets have been used for evaluation to show the effectiveness of the Semi-GenClustMOO technique. In each case class information of 10% randomly chosen data point is known to us <sup>1</sup>.",10.1109/HIS.2012.6421361,Semi-supervised clustering;Multiobjective optimization;Cluster validity index;Adjusted Rand Index (ARI);Sym-index;Con-index;I-Index;AMOSA,11.0,0.0,1.0
On a Multiobjective Training Algorithm for RBF Networks Using Particle Swarm Optimization,G. R. L. Silva; D. A. G. Vieira; A. C. Lisboa; V. Palade,2010 22nd IEEE International Conference on Tools with Artificial Intelligence,2010.0,"This paper presents a novel algorithm for multiobjective training of Radial Basis Function (RBF) networks based on least-squares and Particle Swarm Optimization methods. The formulation is based on the fundamental concept that supervised learning is a bi-objective optimization problem, in which two conflicting objectives should be minimized. The objectives are related to the empirical training error and the machine complexity. The training is done in three steps: i) a conventional minimization of the training error, ii) multiobjective least-squares optimization for the linear parameters and, iii) particle swarm optimization for the nonlinear parameters. Some results are presented and they show the effectiveness of the proposed approach.",10.1109/ICTAI.2010.112,radial basis network;multiobjective least squares;particle swarm optimization,,0.0,1.0
Classification rule mining approach based on multiobjective optimization,T. Sağ; H. Kahramanli,2017 International Artificial Intelligence and Data Processing Symposium (IDAP),2017.0,"In this paper, a novel approach for classification rule mining is presented. The remarkable relationship between the rule extraction procedure and the concept of multiobjective optimization is emphasized. The range values of features composing the rules are handled as decision variables in the modelled multiobjective optimization problem. The proposed method is applied to three well-known datasets in literature. These are Iris, Haberman's Survival Data and Pima Indians Diabetes Datasets obtained from machine learning repository of University of California at Irvine (UCI). The classification rules are extracted with 100% accuracy for all datasets. These experimental results are the best outcomes found in literature so far.",10.1109/IDAP.2017.8090264,Rule extraction;multiobjective optimization;genetic algorithms,,0.0,1.0
A Robo-Advisor Design using Multiobjective RankNets with Gated Neural Network Structure,P. Wang; C. Liu; Y. Yang; S. Huang,2019 IEEE International Conference on Agents (ICA),2019.0,"With rapid developments in deep learning and financial technology, a customized robo-advisory service based on novel artificial intelligence techniques has been widely adopted to realize financial inclusion. This study proposes a novel robo-advisor system that integrates trend prediction, portfolio management, and a recommendation mechanism. A gated neural network structure combining three multiobjective RankNet kernels could rank target financial products and recommend the top-n securities to investors. The gated neural network learns to choose or weigh each RankNet for incorporating the most important partial network inputs, such as earnings per share, market index, and hidden information from the time series. Experimental results indicate that the recommendation results of our proposed robo-advisor based on a gated neural network and multiobjective RankNets can outperform existing models.",10.1109/AGENTS.2019.8929188,learning preferences;rankings;deep learning,2.0,0.0,1.0
Design and Development of a Benchmark for Dynamic Multi-objective Optimisation Problem in the Context of Deep Reinforcement Learning,M. M. Hasan; K. Lwin; A. Shabut; M. A. Hossain,2019 22nd International Conference on Computer and Information Technology (ICCIT),2019.0,"Different benchmarks have played an important role in analysing algorithms for dynamic multi-objective optimisation problems. According to the literature, there are several benchmarks to deal with the dynamic multi-objective optimisation problems, especially in the evolutionary approaches. In this study, a comprehensive review has been done regarding the existing benchmarks in the single objective and multi-objective reinforcement learning (MORL) settings. To the best of our knowledge, there is no benchmark in the context of dynamic multiobjective reinforcement learning (DMORL). Therefore, this study has addressed this gap by applying the existing knowledge to propose a benchmark which may help to investigate the performance of different algorithms. It can also support to understand the dynamics while objectives are conflicting with each other and deal with the constraints and problem parameters that change over time. The proposed benchmark is the modified version of the deep-sea treasure hunt problem where several features such as changing parameters and objectives have been integrated to support the dynamics in a multi-objective environment. This paper highlights the methodology part of designing and developing a benchmark.",10.1109/ICCIT48885.2019.9038529,machine learning;deep reinforcement learning;multiobjective reinforcement learning;artificial intelligence;Markov decision process;dynamic multi-objective optimisation,,0.0,1.0
Designing Fuzzy Ensemble Classifiers by Evolutionary Multiobjective Optimization with an Entropy-Based Diversity Criterion,Y. Nojima; H. Ishibuchi,2006 Sixth International Conference on Hybrid Intelligent Systems (HIS'06),2006.0,"In this paper, we propose a multi-classifier coding scheme and an entropy-based diversity criterion in evolutionary multiobjective optimization algorithms for the design of fuzzy ensemble classifiers. In a multi-classifier coding scheme, an ensemble classifier is coded as an integer string. Each string is evaluated by using its accuracy and diversity. We use two accuracy criteria. One is the overall classification rate of the string as an ensemble classifier. The other is the average classification rate of component classifiers in the ensemble classifier. As a diversity criterion, we use the entropy of outputs from component classifiers in the ensemble classifier. We examine four formulations based on the above criteria through computational experiments on benchmark data sets in the UCI machine learning repository. The experimental results show the effectiveness of the multi-classifier coding scheme and the entropy-based diversity criterion.",10.1109/HIS.2006.264942,,9.0,0.0,1.0
An Adaptive Multiobjective Approach to Evolving ART Architectures,A. Kaylani; M. Georgiopoulos; M. Mollaghasemi; G. C. Anagnostopoulos; C. Sentelle; M. Zhong,IEEE Transactions on Neural Networks,2010.0,"In this paper, we present the evolution of adaptive resonance theory (ART) neural network architectures (classifiers) using a multiobjective optimization approach. In particular, we propose the use of a multiobjective evolutionary approach to simultaneously evolve the weights and the topology of three well-known ART architectures; fuzzy ARTMAP (FAM), ellipsoidal ARTMAP (EAM), and Gaussian ARTMAP (GAM). We refer to the resulting architectures as MO-GFAM, MO-GEAM, and MO-GGAM, and collectively as MO-GART. The major advantage of MO-GART is that it produces a number of solutions for the classification problem at hand that have different levels of merit [accuracy on unseen data (generalization) and size (number of categories created)]. MO-GART is shown to be more elegant (does not require user intervention to define the network parameters), more effective (of better accuracy and smaller size), and more efficient (faster to produce the solution networks) than other ART neural network architectures that have appeared in the literature. Furthermore, MO-GART is shown to be competitive with other popular classifiers, such as classification and regression tree (CART) and support vector machines (SVMs).",10.1109/TNN.2009.2037813,ARTMAP;category proliferation;classification;genetic algorithms (GAs);genetic operators;machine learning,31.0,0.0,1.0
Design Space Exploration based on multiobjective genetic algorithms and clustering-based high-level estimation,L. G. A. Martins; E. Marques,2013 23rd International Conference on Field programmable Logic and Applications,2013.0,"A desirable characteristic in high-level synthesis (HLS) is fast search and analysis of implementation alternatives with low or none intervention. This process is known as Design Space Exploration (DSE) and it requires an efficient search method. The employment of intelligent techniques like evolutionary algorithms has been investigated as an alternative to DSE. They turn possible to reduce the search time through selection of higher potential regions of the solution space. We propose here the development of a DSE approach based on a multiobjective evolutionary algorithm (MOEA) and machine learning techniques. It must be employed to indicate the code transformations and architectural parameters adopted in design solution. Furthermore, DSE will use a high-level estimator model to evaluate candidate solutions. Such model must be able to provide a good estimation of energy consumption and execution time at early stages of design.",10.1109/FPL.2013.6645608,,1.0,0.0,1.0
Multicriteria approaches based on a new discrimination criterions for feature selection,H. Chamlal; T. Ouaderhman; B. E. Mourtji,2021 Fifth International Conference On Intelligent Computing in Data Sciences (ICDS),2021.0,"In machine learning, the presence of a large number of explanatory features leads to a greater complexity of the algorithms and to a strong degradation in the performance of the prediction models. For this, a selection of an optimal discriminating subset is necessary. Feature selection can be viewed as a multi-objective optimization problem, since, in the simplest case, it involves feature subset size minimization and performance maximization. In our work, we introduced approaches based on the theory of preordering that use association techniques between two heterogeneous features, introduced by S.Chah, H. Chamla1 [14], and between several ones, the first approach consists in selecting at each step a variable by maximizing two criteria, eliminating the effect of the variable in question, and moving on to the next step, the procedure stops when the second criterion is no longer significant. In the second approach, we adapted SFFS (Sequential Floating Forward Selection) and SFBS (Sequential Floating Backward Selection) to our problem. These methods consist of using the SFS (Sequential Forward Selection) algorithm once so as to add 1 features, then using the SBS (Sequential Backward Selection) algorithm for r times in order to remove r features. These steps are then repeated until the stop criterion is obtained. As the optimal values of these parameters cannot be determined theoretically, we proposed to leave them floating during the selection process in order to get as close as possible to the optimal solution. The proposed approaches were tested on several datasets and the experimental results were very satisfactory.",10.1109/ICDS53782.2021.9626744,Heterogeneous variables;preordonnance;concordance measures;feature selection;discrimination;TOPSIS,,0.0,1.0
Multiobjective fuzzy genetics-based machine learning based on MOEA/D with its modifications,Y. Nojima; K. Arahari; S. Takemura; H. Ishibuchi,2017 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),2017.0,"Various evolutionary multiobjective optimization (EMO) algorithms have been used in the field of evolutionary fuzzy systems (EFS), because EMO algorithms can easily handle multiple objective functions such as the accuracy maximization and complexity minimization for fuzzy system design. Most EMO algorithms used in EFS are Pareto dominance-based algorithms such as NSGA-II, SPEA2, and PAES. There are a few studies where other types of EMO algorithms are used in EFS. In this paper, we apply a multiobjective evolutionary algorithm based on decomposition called MOEA/D to EFS for fuzzy classifier design. MOEA/D is one of the most well-known decomposition-based EMO algorithms. The key idea is to divide a multiobjective optimization problem into a number of single-objective problems using a set of uniformly distributed weight vectors in a scalarizing function. We propose a new scalarizing function called an accuracy-oriented function (AOF) which is specialized for classifier design. We examine the effects of using AOF in MOEA/D on the search ability of our multiobjective fuzzy genetics-based machine learning (GBML). We also examine the synergy effect of MOEA/D with AOF and parallel distributed implementation of fuzzy GBML on the generalization ability.",10.1109/FUZZ-IEEE.2017.8015749,Fuzzy classifier design;evolutionary fuzzy systems;MOEA/D;accuracy-oriented scalarizingfunction,3.0,0.0,1.0
Multiobjective Optimization for Model Selection in Kernel Methods in Regression,D. You; C. F. Benitez-Quiroz; A. M. Martinez,IEEE Transactions on Neural Networks and Learning Systems,2014.0,"Regression plays a major role in many scientific and engineering problems. The goal of regression is to learn the unknown underlying function from a set of sample vectors with known outcomes. In recent years, kernel methods in regression have facilitated the estimation of nonlinear functions. However, two major (interconnected) problems remain open. The first problem is given by the bias-versus-variance tradeoff. If the model used to estimate the underlying function is too flexible (i.e., high model complexity), the variance will be very large. If the model is fixed (i.e., low complexity), the bias will be large. The second problem is to define an approach for selecting the appropriate parameters of the kernel function. To address these two problems, this paper derives a new smoothing kernel criterion, which measures the roughness of the estimated function as a measure of model complexity. Then, we use multiobjective optimization to derive a criterion for selecting the parameters of that kernel. The goal of this criterion is to find a tradeoff between the bias and the variance of the learned function. That is, the goal is to increase the model fit while keeping the model complexity in check. We provide extensive experimental evaluations using a variety of problems in machine learning, pattern recognition, and computer vision. The results demonstrate that the proposed approach yields smaller estimation errors as compared with methods in the state of the art.",10.1109/TNNLS.2013.2297686,Kernel methods;kernel optimization;optimization;Pareto optimality;regression.;Kernel methods;kernel optimization;optimization;Pareto optimality;regression,12.0,0.0,1.0
Evolutionary Multiobjective Ensemble Learning Based on Bayesian Feature Selection,Huanhuan Chen; Xin Yao,2006 IEEE International Conference on Evolutionary Computation,2006.0,"This paper proposes to incorporate evolutionary multiobjective algorithm and Bayesian Automatic Relevance Determination (ARD) to automatically design and train ensemble. The algorithm determines almost all the parameters of ensemble automatically. Our algorithm adopts different feature subsets, selected by Bayesian ARD, to maintain accuracy and promote diversity among individual NNs in an ensemble. The multiobjective evaluation of the fitness of the networks encourages the networks with lower error rate and fewer features. The proposed algorithm is applied to several real-world classification problems and in all cases the performance of the method is better than the performance of other ensemble construction algorithms.",10.1109/CEC.2006.1688318,,4.0,0.0,1.0
Difficulties in choosing a single final classifier from non-dominated solutions in multiobjective fuzzy genetics-based machine learning,H. Ishibuchi; Y. Nojima,2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS),2013.0,"A large number of non-dominated fuzzy rule-based classifiers are often obtained by applying a multiobjective fuzzy genetics-based machine learning (MoFGBML) algorithm to a pattern classification problem. The obtained set of non-dominated classifiers can be used to analyze their accuracy-interpretability tradeoff relation. One important issue, which has not been discussed in many studies on MoFGBML, is the choice of a single final classifier from a large number of non-dominated classifiers. The selected classifier is used for the classification of new input patterns. In this paper, we focus on this important research issue: classifier selection from a large number of non-dominated fuzzy rule-based classifiers. In general, it is not easy to choose a single final solution from non-dominated solutions in multiobjective optimization. This is because further information on the decision maker's preference is needed to choose the single final solution. In addition to this general difficulty in multiobjective optimization, MoFGBML has its own difficulty in classifier selection, which is the difference between training data accuracy and test data accuracy. While our true objective is to maximize the test data accuracy (i.e., classifier's generalization ability), only the training data accuracy is available for fitness evaluation and classifier selection. In this paper, we discuss why classifier selection is difficult in MoFGBML.",10.1109/IFSA-NAFIPS.2013.6608572,,1.0,0.0,1.0
Ranking Machine Learning Classifiers Using Multicriteria Approach,F. de Moura Rezende dos Santos; F. Guedes de Oliveira Almeida; A. C. Pereira Rocha Martins; A. C. Bittencourt Reis; M. Holanda,2018 11th International Conference on the Quality of Information and Communications Technology (QUATIC),2018.0,"Classification algorithms are widely used as data mining tools for knowledge extraction. The literature presents several classifiers, but none of them applies to all problems. encountered in the various context in which they are used. Faced with this situation, the present article proposes a multicriteria approach to help practitioners to select the classifiers that will generate the best quality results by observing their performance measures. An empirical study was performed using a baseline of fetal examination from an UCI database using five classification algorithms (C4.5, Naive Bayes, SMO, KNN and Bayesnet), and each classifier was measured using five performance indicators (accuracy, true positive rate, precision, ROC curve and f-measure). Once implemented, a classifier ranking was conducted based on MCDA PROMETHEE II method, and the results show that SMO, C4.5 and Naive Bayes achieved the highest overall ranking.",10.1109/QUATIC.2018.00034,MCDA;Promethee;Machine Learning;Algorithms Selection;Classification Algorithms,,0.0,1.0
Sensitivity Versus Accuracy in Multiclass Problems Using Memetic Pareto Evolutionary Neural Networks,J. C. Fernandez Caballero; F. J. Martinez; C. Hervas; P. A. Gutierrez,IEEE Transactions on Neural Networks,2010.0,"This paper proposes a multiclassification algorithm using multilayer perceptron neural network models. It tries to boost two conflicting main objectives of multiclassifiers: a high correct classification rate level and a high classification rate for each class. This last objective is not usually optimized in classification, but is considered here given the need to obtain high precision in each class in real problems. To solve this machine learning problem, we use a Pareto-based multiobjective optimization methodology based on a memetic evolutionary algorithm. We consider a memetic Pareto evolutionary approach based on the NSGA2 evolutionary algorithm (MPENSGA2). Once the Pareto front is built, two strategies or automatic individual selection are used: the best model in accuracy and the best model in sensitivity (extremes in the Pareto front). These methodologies are applied to solve 17 classification benchmark problems obtained from the University of California at Irvine (UCI) repository and one complex real classification problem. The models obtained show high accuracy and a high classification rate for each class.",10.1109/TNN.2010.2041468,Accuracy;local search;multiclassification;multiobjective evolutionary algorithms;neural networks;sensitivity,113.0,0.0,1.0
Semi-supervised training of Least Squares Support Vector Machine using a multiobjective evolutionary algorithm,C. Silva; J. S. Santos; E. F. Wanner; E. G. Carrano; R. H. C. Takahashi,2009 IEEE Congress on Evolutionary Computation,2009.0,"Support Vector Machines (SVMs) are considered state-of-the-art learning machines techniques for classification problems. This paper studies the training of SVMs in the special case of problems in which the raw data to be used for training purposes is composed of both labeled and unlabeled data - the semi-supervised learning problem. This paper proposes the definition of an intermediate problem of attributing labels to the unlabeled data as a multiobjective optimization problem, with the conflicting objectives of minimizing the classification error over the training data set and maximizing the regularity of the resulting classifier. This intermediate problem is solved using an evolutionary multiobjective algorithm, the SPEA2. Simulation results are presented in order to illustrate the suitability of the proposed technique.",10.1109/CEC.2009.4983321,,1.0,0.0,1.0
Ensemble of Classifiers Based on Multiobjective Genetic Sampling for Imbalanced Data,E. R. Q. Fernandes; A. C. P. L. F. de Carvalho; X. Yao,IEEE Transactions on Knowledge and Data Engineering,2020.0,"Imbalanced datasets may negatively impact the predictive performance of most classical classification algorithms. This problem, commonly found in real-world, is known in machine learning domain as imbalanced learning. Most techniques proposed to deal with imbalanced learning have been proposed and applied only to binary classification. When applied to multiclass tasks, their efficiency usually decreases and negative side effects may appear. This paper addresses these limitations by presenting a novel adaptive approach, E-MOSAIC (Ensemble of Classifiers based on MultiObjective Genetic Sampling for Imbalanced Classification). E-MOSAIC evolves a selection of samples extracted from training dataset, which are treated as individuals of a MOEA. The multiobjective process looks for the best combinations of instances capable of producing classifiers with high predictive accuracy in all classes. E-MOSAIC also incorporates two mechanisms to promote the diversity of these classifiers, which are combined into an ensemble specifically designed for imbalanced learning. Experiments using twenty imbalanced multi-class datasets were carried out. In these experiments, the predictive performance of E-MOSAIC is compared with state-of-the-art methods, including methods based on presampling, active-learning, cost-sensitive, and boosting. According to the experimental results, the proposed method obtained the best predictive performance for the multiclass accuracy measures mAUC and G-mean.",10.1109/TKDE.2019.2898861,Imbalanced datasets;ensemble of classifiers;evolutionary algorithm,18.0,0.0,1.0
Pareto-Optimal Adaptive Loss Residual Shrinkage Network for Imbalanced Fault Diagnostics of Machines,Y. Yu; L. Guo; H. Gao; Y. Liu; T. Feng,IEEE Transactions on Industrial Informatics,2022.0,"In the industrial applications of mechanical fault diagnosis, machines work in normal condition at most time. In other words, most of the collected datasets are highly imbalanced. Although deep learning has been widely applied in intelligent diagnosis, it is unsuitable for such imbalanced situation. In addition, few studies attempted to determine the parameters in the diagnosis models. For solving such problems, Pareto-optimal adaptive loss residual shrinkage network (PALRSN) is proposed. First, a fixed length-based encoding method is implemented to represent the candidate architectures of PALRSN. Then, multiply accumulate operations and Gmean value representing the model complexity and identification performance, respectively, on imbalanced datasets are selected as the optimization targets to search for the optimal PALRSN architecture. In the training process, an adaptive loss function assigns different misclassification costs on all categories according to their number discrepancy to highlight the minority samples. The proposed method is validated by bearing data and milling cutter data with different imbalanced ratio. The experimental results demonstrate that such approach outperforms the state-of-the-art methods in imbalanced classification.",10.1109/TII.2021.3094186,Adaptive loss function;imbalanced data;multiobjective genetic algorithm (GA);Pareto front;residual shrinkage network,,0.0,1.0
Multicriteria Classifier Ensemble Learning for Imbalanced Data,W. Węgier; M. Koziarski; M. Woźniak,IEEE Access,2022.0,"One of the vital problems with the imbalanced data classifier training is the definition of an optimization criterion. Typically, since the exact cost of misclassification of the individual classes is unknown, combined metrics and loss functions that roughly balance the cost for each class are used. However, this approach can lead to a loss of information, since different trade-offs between class misclassification rates can produce similar combined metric values. To address this issue, this paper discusses a multi-criteria ensemble training method for the imbalanced data. The proposed method jointly optimizes <italic>precision</italic> and <italic>recall</italic>, and provides the end-user with a set of Pareto optimal solutions, from which the final one can be chosen according to the user’s preference. The proposed approach was evaluated on a number of benchmark datasets and compared with the single-criterion approach (where the selected criterion was one of the chosen metrics). The results of the experiments confirmed the usefulness of the obtained method, which on the one hand guarantees good quality, i.e., not worse than the one obtained with the use of single-criterion optimization, and on the other hand, offers the user the opportunity to choose the solution that best meets their expectations regarding the trade-off between errors on the minority and the majority class.",10.1109/ACCESS.2022.3149914,Classifier ensemble;imbalanced data;multi-objective optimization;pattern classification,,0.0,1.0
Feature Selection with Dynamic Classifier Ensembles,H. E. Kiziloz; A. Deniz,"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",2020.0,"With the advance in technology, the volume of available data grows massively. Therefore, feature selection has become an essential preprocessing step to extract valuable information. Feature selection is the task of reducing the number of features by removing redundant features from data while preserving the classification accuracy. It is a multiobjective problem as there are two objectives. In general, multiobjective selection algorithms with machine learning techniques are utilized to find the most promising feature subsets; however, classification performances of these machine learning techniques are analyzed separately. In this study, we propose a new multiobjective selection model that dynamically searches for the best ensemble of five classifiers to extract the best representative feature subsets. We present the experiment results on 12 well-known datasets. The results show that the proposed method performs significantly better than all the machine learning techniques when they are executed separately. Moreover, the proposed method outperforms two existing ensemble algorithms, namely AdaBoost and Gradient Boosting.",10.1109/SMC42975.2020.9282969,feature selection;multiobjective optimization;machine learning;classifier ensemble,1.0,0.0,1.0
Multicriteria approaches for predictive model generation: A comparative experimental study,B. Al-Jubouri; B. Gabrys,2014 IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making (MCDM),2014.0,"This study investigates the evaluation of machine learning models based on multiple criteria. The criteria included are: predictive model accuracy, model complexity, and algorithmic complexity (related to the learning/adaptation algorithm and prediction delivery) captured by monitoring the execution time. Furthermore, it compares the models generated from optimising the criteria using two approaches. The first approach is a scalarized multi objective optimisation, where the models are generated from optimising a single cost function that combines the criteria. On the other hand the second approach uses a Pareto-based multi objective optimisation to trade-off the three criteria and to generate a set of non-dominated models. This study shows that defining universal measures for the three criteria is not always feasible. Furthermore, it was shown that, the models generated from Pareto-based multi objective optimisation approach can be more accurate and more diverse than the models generated from scalarized multi objective optimisation approach.",10.1109/MCDM.2014.7007189,,3.0,0.0,1.0
Cooperative coevolution of artificial neural network ensembles for pattern classification,N. Garcia-Pedrajas; C. Hervas-Martinez; D. Ortiz-Boyer,IEEE Transactions on Evolutionary Computation,2005.0,"This paper presents a cooperative coevolutive approach for designing neural network ensembles. Cooperative coevolution is a recent paradigm in evolutionary computation that allows the effective modeling of cooperative environments. Although theoretically, a single neural network with a sufficient number of neurons in the hidden layer would suffice to solve any problem, in practice many real-world problems are too hard to construct the appropriate network that solve them. In such problems, neural network ensembles are a successful alternative. Nevertheless, the design of neural network ensembles is a complex task. In this paper, we propose a general framework for designing neural network ensembles by means of cooperative coevolution. The proposed model has two main objectives: first, the improvement of the combination of the trained individual networks; second, the cooperative evolution of such networks, encouraging collaboration among them, instead of a separate training of each network. In order to favor the cooperation of the networks, each network is evaluated throughout the evolutionary process using a multiobjective method. For each network, different objectives are defined, considering not only its performance in the given problem, but also its cooperation with the rest of the networks. In addition, a population of ensembles is evolved, improving the combination of networks and obtaining subsets of networks to form ensembles that perform better than the combination of all the evolved networks. The proposed model is applied to ten real-world classification problems of a very different nature from the UCI machine learning repository and proben1 benchmark set. In all of them the performance of the model is better than the performance of standard ensembles in terms of generalization error. Moreover, the size of the obtained ensembles is also smaller.",10.1109/TEVC.2005.844158,Classification;cooperative coevolution;multiobjective optimization;neural network ensembles,150.0,0.0,1.0
Plausible Counterfactuals: Auditing Deep Learning Classifiers with Realistic Adversarial Examples,A. Barredo-Arrieta; J. Del Ser,2020 International Joint Conference on Neural Networks (IJCNN),2020.0,"The last decade has witnessed the proliferation of Deep Learning models in many applications, achieving unrivaled levels of predictive performance. Unfortunately, the black-box nature of Deep Learning models has posed unanswered questions about what they learn from data. Certain application scenarios have highlighted the importance of assessing the bounds under which Deep Learning models operate, a problem addressed by using assorted approaches aimed at audiences from different domains. However, as the focus of the application is placed more on non-expert users, it results mandatory to provide the means for him/her to trust the model, just like a human gets familiar with a system or process: by understanding the hypothetical circumstances under which it fails. This is indeed the angular stone for this research work: to undertake an adversarial analysis of a Deep Learning model. The proposed framework constructs counterfactual examples by ensuring their plausibility, e.g. there is a reasonable probability that a human could generate them without resorting to a computer program. Therefore, this work must be regarded as valuable auditing exercise of the usable bounds a certain model is constrained within, thereby allowing for a much greater understanding of the capabilities and pitfalls of a model used in a real application. To this end, a Generative Adversarial Network (GAN) and multi-objective heuristics are used to furnish a plausible attack to the audited model, efficiently trading between the confusion of this model, the intensity and plausibility of the generated counterfactual. Its utility is showcased within a human face classification task, unveiling the enormous potential of the proposed framework.",10.1109/IJCNN48605.2020.9206728,Explainable Artificial Intelligence;Deep Learning;Counterfactuals;Generative Adversarial Networks;Multiobjective Optimization;Meta-heuristics,2.0,0.0,1.0
A Novel Deep Learning Approach: Stacked Evolutionary Auto-encoder,Y. Cai; Z. Cai; M. Zeng; X. Liu; J. Wu; G. Wang,2018 International Joint Conference on Neural Networks (IJCNN),2018.0,"Deep neural networks have been successfully applied to many data mining problems in recent works. The training of deep neural networks relies heavily upon gradient descent methods, however, which may lead to the failure of training due to the vanishing gradient (or exploding gradient) and local optima problems. In this paper, we present SEvoAE method based on using Evolutionary Multiobjective optimization (EMO) algorithm to train single layer auto-encoder, and sequentially learning deeper representation in a stacking way. SEvoAE is able to achieve accurate feature representation with good sparseness by globally simultaneously optimizing two conflicting objective functions and allows users to flexibly design objective functions and evolutionary optimizers. We compare results of the proposed method with existing architectures for seven classification problems, showing that the proposed method is able to outperform existing methods with a reduced risk of overfitting the training data.",10.1109/IJCNN.2018.8489138,Deep learning;Auto-encoder;Evolutionary Multiobjective Optimization,4.0,0.0,1.0
Transfer Clustering Ensemble Selection,Y. Shi; Z. Yu; C. L. P. Chen; J. You; H. -S. Wong; Y. Wang; J. Zhang,IEEE Transactions on Cybernetics,2020.0,"Clustering ensemble (CE) takes multiple clustering solutions into consideration in order to effectively improve the accuracy and robustness of the final result. To reduce redundancy as well as noise, a CE selection (CES) step is added to further enhance performance. Quality and diversity are two important metrics of CES. However, most of the CES strategies adopt heuristic selection methods or a threshold parameter setting to achieve tradeoff between quality and diversity. In this paper, we propose a transfer CES (TCES) algorithm which makes use of the relationship between quality and diversity in a source dataset, and transfers it into a target dataset based on three objective functions. Furthermore, a multiobjective self-evolutionary process is designed to optimize these three objective functions. Finally, we construct a transfer CE framework (TCE-TCES) based on TCES to obtain better clustering results. The experimental results on 12 transfer clustering tasks obtained from the 20newsgroups dataset show that TCE-TCES can find a better tradeoff between quality and diversity, as well as obtaining more desirable clustering results.",10.1109/TCYB.2018.2885585,Clustering ensemble selection (CES);machine learning;multiobjective;transfer learning,7.0,0.0,1.0
Pareto-Path Multitask Multiple Kernel Learning,C. Li; M. Georgiopoulos; G. C. Anagnostopoulos,IEEE Transactions on Neural Networks and Learning Systems,2015.0,"A traditional and intuitively appealing Multitask Multiple Kernel Learning (MT-MKL) method is to optimize the sum (thus, the average) of objective functions with (partially) shared kernel function, which allows information sharing among the tasks. We point out that the obtained solution corresponds to a single point on the Pareto Front (PF) of a multiobjective optimization problem, which considers the concurrent optimization of all task objectives involved in the Multitask Learning (MTL) problem. Motivated by this last observation and arguing that the former approach is heuristic, we propose a novel support vector machine MT-MKL framework that considers an implicitly defined set of conic combinations of task objectives. We show that solving our framework produces solutions along a path on the aforementioned PF and that it subsumes the optimization of the average of objective functions as a special case. Using the algorithms we derived, we demonstrate through a series of experimental results that the framework is capable of achieving a better classification performance, when compared with other similar MTL approaches.",10.1109/TNNLS.2014.2309939,Machine learning;optimization methods;pattern recognition;supervised learning;support vector machines (SVM).;Machine learning;optimization methods;pattern recognition;supervised learning;support vector machines (SVM),14.0,0.0,1.0
Evolving Diverse Ensembles Using Genetic Programming for Classification With Unbalanced Data,U. Bhowan; M. Johnston; M. Zhang; X. Yao,IEEE Transactions on Evolutionary Computation,2013.0,"In classification, machine learning algorithms can suffer a performance bias when data sets are unbalanced. Data sets are unbalanced when at least one class is represented by only a small number of training examples (called the minority class), while the other class(es) make up the majority. In this scenario, classifiers can have good accuracy on the majority class, but very poor accuracy on the minority class(es). This paper proposes a multiobjective genetic programming (MOGP) approach to evolving accurate and diverse ensembles of genetic program classifiers with good performance on both the minority and majority of classes. The evolved ensembles comprise of nondominated solutions in the population where individual members vote on class membership. This paper evaluates the effectiveness of two popular Pareto-based fitness strategies in the MOGP algorithm (SPEA2 and NSGAII), and investigates techniques to encourage diversity between solutions in the evolved ensembles. Experimental results on six (binary) class imbalance problems show that the evolved ensembles outperform their individual members, as well as single-predictor methods such as canonical GP, naive Bayes, and support vector machines, on highly unbalanced tasks. This highlights the importance of developing an effective fitness evaluation strategy in the underlying MOGP algorithm to evolve good ensemble members.",10.1109/TEVC.2012.2199119,Classification;class imbalance learning;genetic programming (GP);multiobjective machine learning (ML),140.0,0.0,1.0
A multi-objective competitive co-evolutionary approach for classification problems,V. T. VU; L. T. BUI; T. T. NGUYEN,2019 6th NAFOSTED Conference on Information and Computer Science (NICS),2019.0,"This paper proposes a multi-objective competitive co-evolutionary algorithm (MOCPCEA) based on the PreyPredator model to solve classification problems. In the MOCPCEA, a data population acts as preys. To be specific, each prey represents a selected subset of the training dataset. Another population is ANN classifiers which play as Predators. The task of the Predators is to try to classify the data sets as correctly as possible, whereas the Preys try to find the data sets that are difficult to be classified. Through this interaction process, MOCPCEA generates a set of classifiers that are able to classify difficult data sets. The final classification result is given by the ensemble voting mechanism among these sets of classifiers. The performance of the proposed algorithm is performed on seven benchmark problems. Through comparison with other algorithms, the proposed algorithm indicates that it could create an ensemble of ANN networks that give high and stable classification results.",10.1109/NICS48868.2019.9023887,competitive co-evolutionary;Prey-Predator;multiobjective optimization;classification;ensemble learning.,,0.0,1.0
Pareto-based Multi-Objective Machine Learning,Y. Jin,7th International Conference on Hybrid Intelligent Systems (HIS 2007),2007.0,"Machine learning is inherently a multi-objective task. Traditionally, however, either only one of the objectives is adopted as the cost function or multiple objectives are aggregated to a scalar cost function. This can mainly attributed to the fact that most conventional learning algorithms can only deal with a scalar cost function. Over the last decade, efforts on solving machine learning problems using the Pareto-based multi-objective optimization methodology have gained increasing impetus, particularly thanks to the great success of multi-objective optimization using evolutionary algorithms and other population-based stochastic search methods. It has been shown that Pareto-based multi-objective learning approaches are more powerful compared to learning algorithms with a scalar cost functions in addressing various topics of machine learning, such as clustering, feature selection, improvement of generalization ability, knowledge extraction, and ensemble generation. This talk provides first a brief overview of Pareto-based multi-objective machine learning techniques. In addition, a number of case studies are provided to illustrate the major benefits of the Pareto-based approach to machine learning, e.g., how to identify interpretable models and models that can generalize on unseen data from the obtained Pareto-optimal solutions. Three approaches to Pareto-based multi-objective ensemble generation are compared and discussed in detail. Most recent results on multi-objective optimization of spiking neural networks will be presented.",10.1109/HIS.2007.73,,2.0,0.0,1.0
Enhancing utility and privacy with noisy minimax filters,J. Hamm,"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",2017.0,"Preserving privacy of continuous and/or high-dimensional data such as images, videos and audios is challenging. Syntactic anonymization methods were proposed typically for discrete data types and can be unsuitable. Differential privacy, which provides a stricter type of privacy, has shown more success in sanitizing continuous data. However, both syntactic and differential privacy are susceptible to inference attacks, i.e., an adversary can accurately guess sensitive attributes from insensitive attributes. On the other hand, minimax filters were proposed previously to minimize the accuracy of inference while maximizing utility at the same time. The paper presents noisy minimax filter that combines minimax filter and differentially private mechanism, which can attain high average utility and protection against inference attacks and a formal worst-case privacy guarantee. The proposed algorithm is demonstrated with real databases of faces, voices, and motion data.",10.1109/ICASSP.2017.7953386,syntactic anonymity;differential privacy;minimax optimization;postprocessing;machine learning,8.0,0.0,1.0
"Pareto-optimality is everywhere: From engineering design, machine learning, to biological systems",Yaochu Jin,2008 3rd International Workshop on Genetic and Evolving Systems,2008.0,"This talk attempts to argue that almost all adaptive systems have multiple objectives to achieve. Very often, there is no single solution that can optimize all objectives, in which case, the concept of Pareto-optimization plays an important rule. Examples will be given ranging from engineering design, machine learning, to biological systems to show how Pareto-optimality can make a difference in analyzing these systems. The first example we will discuss is the aerodynamic design optimization of turbine blades, where energy efficiency in terms of pressure loss as well as the variation of pressure distribution must be minimized. One additional difficulty in aerodynamic design optimization is that the quality of candidate designs must be assessed by performing computational fluid dynamics analysis, which is very time consuming. To reduce computation time, computational techniques like parallel computation, and machine learning techniques, such as meta-modeling can be employed. Surprisingly interesting results will also be achieved when the concept of Pareto-optimality is applied to machine learning. Two cases will be provided to illustrate this idea. In the first case, we show how Pareto-based approach can address neural network regularization more elegantly, through which deeper insights into the problem can be gained. In the second case, we show that analysis of the Pareto-optimal solutions will help determine the optimal number of clusters in data clustering, which again shown how the Pareto front can disclose additional knowledge about the problem at hand. The final example is concerned with tradeoffs in simulated evolution of genetic representation. It has been argued that robustness is critical for biological evolution, because without certain degree of robustness to mutations, it is impossible for evolution to create new functionalities. Therefore, evolution must find representations that are sufficiently robust yet have the potential to innovate. Examples will be given to show that such tradeoff does exist in evolving both a stationary genotype-phenotype mapping, and also a gene regulatory network described by a random Boolean network.",10.1109/GEFS.2008.4484555,,2.0,0.0,1.0
Resource-Aware Pareto-Optimal Automated Machine Learning Platform,Y. Yang; A. Nam; M. Nasr-Azadani; T. Tung,2020 3rd International Seminar on Research of Information Technology and Intelligent Systems (ISRITI),2020.0,"In this study, we introduce a novel platform Resource-Aware AutoML (RA-AutoML) which enables flexible and generalized algorithms to build machine learning models subjected to multiple objectives, as well as resource and hardware constraints. RA-AutoML intelligently conducts Hyper-Parameter Search (HPS) as well as Neural Architecture Search (NAS) to build models optimizing predefined objectives. RA-AutoML is a versatile framework that allows user to prescribe many resource/hardware constraints along with objectives demanded by the problem or even business requirements. At its core, RA-AutoML relies on our in-house search-engine algorithm, MOBOGA, which combines a modified constraint-aware Bayesian Osptimization and Genetic Algorithm to construct Pareto optimal candidates. Our experiments on CIFAR-10 dataset shows very good accuracy compared to results obtained by state-of-art neural network models, while subjected to resource constraints in the form of model size.",10.1109/ISRITI51436.2020.9315336,Automatic Machine Learning;Resource-aware optimization;Hardware-aware Machine;Learning Resource constraints;Bayesian optimization;Pareto optimal;Constraint-aware AutoML Platform,,0.0,1.0
Supervised learning for feed-forward neural networks: a new minimax approach for fast convergence,A. Chella; A. Gentile; F. Sorbello; A. Tarantino,IEEE International Conference on Neural Networks,1993.0,"An approach to the problem of the learning process for feedforward neural networks, based on an optimization point of view, is proposed. The developed algorithm is a minimax method based on a configuration of the quasi-Newton and steepest-descent methods. The optimum point is reached by minimizing the maximum of the error functions of the network without requiring any tuning of internal parameters. The algorithm is tested on several widespread benchmarks and shows superior convergence properties when compared with other algorithms available in the literature. Significant experimental results are included.<<ETX>></ETX>",10.1109/ICNN.1993.298626,,3.0,0.0,1.0
Feature Learning in Feature-Sample Networks Using Multi-Objective Optimization,F. A. Neto Verri; R. Tinós; L. Zhao,2018 IEEE Congress on Evolutionary Computation (CEC),2018.0,"Data and knowledge representation are fundamental concepts in machine learning. The quality of the representation impacts the performance of a learning model directly. Feature learning transforms or enhances raw data to structures that are effectively exploited by those methods. In recent years, several works have been using complex networks for data representation and analysis. However, no feature learning method has been proposed to enhance such category of representation. Here, we present an unsupervised feature learning mechanism that works on datasets with binary features. First, the dataset is mapped into a feature-sample network. Then, a multi-objective optimization process selects a set of new vertices to produce an enhanced version of the network. The new features depend on a nonlinear function of a combination of preexisting features. Effectively, the process projects the input data into a higher-dimensional space. To solve the optimization problem, we design two metaheuristics based on the lexicographic genetic algorithm and the improved strength Pareto evolutionary algorithm (SPEA2). We show that the enhanced network contains more useful information and can be exploited to improve the performance of machine learning methods. The advantages and disadvantages of each optimization strategy are discussed.",10.1109/CEC.2018.8477891,Feature learning;complex networks;multiobjective optimization;genetic algorithm,,0.0,1.0
Selective ensemble learning method for belief-rule-base classification system based on PAES,W. Liu; W. Wu; Y. Wang; Y. Fu; Y. Lin,Big Data Mining and Analytics,2019.0,"Traditional Belief-Rule-Based (BRB) ensemble learning methods integrate all of the trained sub-BRB systems to obtain better results than a single belief-rule-based system. However, as the number of BRB systems participating in ensemble learning increases, a large amount of redundant sub-BRB systems are generated because of the diminishing difference between subsystems. This drastically decreases the prediction speed and increases the storage requirements for BRB systems. In order to solve these problems, this paper proposes BRBCS-PAES: a selective ensemble learning approach for BRB Classification Systems (BRBCS) based on Pareto-Archived Evolutionary Strategy (PAES) multi-objective optimization. This system employs the improved Bagging algorithm to train the base classifier. For the purpose of increasing the degree of difference in the integration of the base classifier, the training set is constructed by the repeated sampling of data. In the base classifier selection stage, the trained base classifier is binary coded, and the number of base classifiers participating in integration and generalization error of the base classifier is used as the objective function for multi-objective optimization. Finally, the elite retention strategy and the adaptive mesh algorithm are adopted to produce the PAES optimal solution set. Three experimental studies on classification problems are performed to verify the effectiveness of the proposed method. The comparison results demonstrate that the proposed method can effectively reduce the number of base classifiers participating in the integration and improve the accuracy of BRBCS.",10.26599/BDMA.2019.9020008,belief-rule-base;pareto-archived evolutionary strategy;selective ensemble;classification,1.0,0.0,1.0
Classification as Clustering: A Pareto Cooperative-Competitive GP Approach,A. R. McIntyre; M. I. Heywood,Evolutionary Computation,2011.0,"Intuitively population based algorithms such as genetic programming provide a natural environment for supporting solutions that learn to decompose the overall task between multiple individuals, or a team. This work presents a framework for evolving teams without recourse to prespecifying the number of cooperating individuals. To do so, each individual evolves a mapping to a distribution of outcomes that, following clustering, establishes the parameterization of a (Gaussian) local membership function. This gives individuals the opportunity to represent <italic>subsets</italic> of tasks, where the overall task is that of classification under the supervised learning domain. Thus, rather than each team member representing an entire class, individuals are free to identify unique subsets of the overall classification task. The framework is supported by techniques from evolutionary multiobjective optimization (EMO) and Pareto competitive coevolution. EMO establishes the basis for encouraging individuals to provide accurate yet nonoverlaping behaviors; whereas competitive coevolution provides the mechanism for scaling to potentially large unbalanced datasets. Benchmarking is performed against recent examples of nonlinear SVM classifiers over 12 UCI datasets with between 150 and 200,000 training instances. Solutions from the proposed coevolutionary multiobjective GP framework appear to provide a good balance between classification performance and model complexity, especially as the dataset instance count increases.",10.1162/EVCO_a_00016,Genetic programming;Pareto multi-objective optimization;coevolution;problem decomposition;classification,5.0,0.0,1.0
The Blessing of Dimensionality in Many-Objective Search: An Inverse Machine Learning Insight,A. Gupta; Y. -S. Ong; M. Shakeri; X. Chi; A. Z. NengSheng,2019 IEEE International Conference on Big Data (Big Data),2019.0,"Sample-based evolutionary algorithms (EAs) are widely used for optimizing problems with multi (greater than one but less than four) or even many (greater than or equal to four) objectives of interest. In general, the difficulty of a problem exponentially increases with the number of objectives, serving as a clear example of the curse of dimensionality. The exploratory approach an EA takes in these cases has led to it being thought of as a big data generator, progressively sampling and evaluating solutions in high performing regions of a decision space to guide the search towards optimal solutions. Notably, in both multi- and many-objective EAs, the sampled data can be further utilized for building inverse generative models, mapping points in objective space back to solutions in the decision space. Such models offer immense flexibility to a decision maker in generating new target solutions on the fly, thereby facilitating real-time a posteriori preference incorporation into the search. In this paper, we show that the data distribution resulting from a many-objective formulation is in fact more conducive to building accurate inverse models than its multiobjective counterpart. Given the potential utility of these models, we in turn shed light on a rare blessing of dimensionality that is yet to be explored in the context of optimization. We first present simple theoretical arguments supporting our claim. Thereafter, experimental studies of Gaussian process-based inverse modeling for a synthetic and a real-world example are carried out to further confirm the theory.",10.1109/BigData47090.2019.9005525,Blessing of dimensionality;inverse modeling;many-objective optimization;Gaussian processes,,0.0,1.0
"Local Minimax Learning of Functions With Best Finite Sample Estimation Error Bounds: Applications to Ridge and Lasso Regression, Boosting, Tree Learning, Kernel Machines, and Inverse Problems",L. K. Jones,IEEE Transactions on Information Theory,2009.0,"Optimal local estimation is formulated in the minimax sense for inverse problems and nonlinear regression. This theory provides best mean squared finite sample error bounds for some popular statistical learning algorithms and also for several optimal improvements of other existing learning algorithms such as smoothing splines and kernel regularization. The bounds and improved algorithms are not based on asymptotics or Bayesian assumptions and are truly local for each query, not depending on cross validating estimates at other queries to optimize modeling parameters. Results are given for optimal local learning of approximately linear functions with side information (context) using real algebraic geometry. In particular, finite sample error bounds are given for ridge regression and for a local version of lasso regression. The new regression methods require only quadratic programming with linear or quadratic inequality constraints for implementation. Greedy additive expansions are then combined with local minimax learning via a change in metric. An optimal strategy is presented for fusing the local minimax estimators of a class of experts-providing optimal finite sample prediction error bounds from (random) forests. Local minimax learning is extended to kernel machines. Best local prediction error bounds for finite samples are given for Tikhonov regularization. The geometry of reproducing kernel Hilbert space is used to derive improved estimators with finite sample mean squared error (MSE) bounds for class membership probability in two class pattern classification problems. A purely local, cross validation free algorithm is proposed which uses Fisher information with these bounds to determine best local kernel shape in vector machine learning. Finally, a locally quadratic solution to the finite Fourier moments problem is presented. After reading the first three sections the reader may proceed directly to any of the subsequent applications sections.",10.1109/TIT.2009.2027479,Fusion;inverse problem;minimax;reproducing kernel;ridge regression,6.0,0.0,1.0
Minimax Learning for Distributed Inference,C. T. Li; X. Wu; A. Özgür; A. El Gamal,IEEE Transactions on Information Theory,2020.0,"The classical problem of supervised learning is to infer an accurate estimate of a target variable Y from a measured variable X using a set of labeled training samples. Motivated by the increasingly distributed nature of data and decision making, this paper considers a variation of this classical problem in which the inference is distributed between two nodes, e.g., a mobile device and a cloud, with a rate constraint on the communication between them. The mobile device observes X and sends a description M of X to the cloud, which computes an estimate Y̑ of Y. We follow the recent minimax learning approach to study this inference problem and show that it corresponds to a one-shot minimax noisy lossy source coding problem. We then establish information theoretic bounds on the risk-rate Lagrangian cost, leading to a general method for designing a near-optimal descriptor-estimator pair. A key ingredient in the proof of our result is a refined version of the strong functional representation lemma previously used to establish several one-shot source coding theorems. Our results show that a naive estimate-compress scheme for rate-constrained inference is not optimal in general. When the distribution of (X, Y) is known and the error is measured by the logarithmic loss, our bounds on the risk-rate Lagrangian cost provide a new one-shot operational interpretation of the information bottleneck. We also demonstrate a way to bound the excess risk of the descriptor-estimator pair obtained by our method.",10.1109/TIT.2020.3029182,Minimax learning;distributionally robust learning;information bottleneck;one-shot source coding;functional representation,1.0,0.0,1.0
On generalisation of machine learning with neural-evolutionary computations,R. Kumar,Proceedings Third International Conference on Computational Intelligence and Multimedia Applications. ICCIMA'99 (Cat. No.PR00300),1999.0,"Generalisation is a non-trivial problem in machine learning and more so with neural networks which have the capabilities of inducing varying degrees of freedom. It is influenced by many factors in network design, such as network size, initial conditions, learning rate, weight decay factor, pruning algorithms, and many more. In spite of continuous research efforts, we could not arrive at a practical solution which can offer a superior generalisation. We present a novel approach for handling complex problems of machine learning. A multiobjective genetic algorithm is used for identifying (near-) optimal subspaces for hierarchical learning. This strategy of explicitly partitioning the data for subsequent mapping onto a hierarchical classifier is found both to reduce the learning complexity and the classification time. The classification performance of various algorithms is compared and it is argued that the neural modules are superior for learning the localised decision surfaces of such partitions and offer better generalisation.",10.1109/ICCIMA.1999.798512,,,0.0,1.0
Design-Space Exploration of Pareto-Optimal Architectures for Deep Learning with DVFS,G. Santoro; M. R. Casu; V. Peluso; A. Calimera; M. Alioto,2018 IEEE International Symposium on Circuits and Systems (ISCAS),2018.0,"Specialized computing engines are required to accelerate the execution of Deep Learning (DL) algorithms in an energy-efficient way. To adapt the processing throughput of these accelerators to the workload requirements while saving power, Dynamic Voltage and Frequency Scaling (DVFS) seems the natural solution. However, DL workloads need to frequently access the off-chip memory, which tends to make the performance of these accelerators memory-bound rather than computation-bound, hence reducing the effectiveness of DVFS. In this work we use a performance-power analytical model fitted on a parametrized implementation of a DL accelerator in a 28-nm FDSOI technology to explore a large design space and to obtain the Pareto points that maximize the effectiveness of DVFS in the sub-space of throughput and energy efficiency. In our model we consider the impact on performance and power of the off-chip memory using real data of a commercial low-power DRAM.",10.1109/ISCAS.2018.8351685,,4.0,0.0,1.0
SPIRIT: Spectral-Aware Pareto Iterative Refinement Optimization for Supervised High-Level Synthesis,S. Xydis; G. Palermo; V. Zaccaria; C. Silvano,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,2015.0,"Supervised high-level synthesis (HLS) is a new class of design problems where exploration strategies play the role of supervisor for tuning an HLS engine. The complexity of the problem is increased due to the large set of tunable parameters exposed by the “new wave” of HLS tools that include not only architectural alternatives but also compiler transformations. In this paper, we developed a novel exploration approach, called spectral-aware Pareto iterative refinement, that exploits response surface models (RSMs) and spectral analysis for predicting the quality of the design points without resorting to costly architectural synthesis procedures. We show that the target solution space can be accurately modeled through RSMs, thus enabling a speedup of the overall exploration without compromising the quality of results. Furthermore, we introduce the usage of spectral techniques to find high variance regions of the design space that require analysis for improving the RSMs prediction accuracy.",10.1109/TCAD.2014.2363392,system level design;high level synthesis;design space exploration;machine learning;spectral analysis;Design space exploration (DSE);high-level synthesis (HLS);machine learning;spectral analysis;system level design,29.0,0.0,1.0
Scalable Pareto Front Approximation for Deep Multi-Objective Learning,M. Ruchte; J. Grabocka,2021 IEEE International Conference on Data Mining (ICDM),2021.0,"Multi-objective optimization is important for various Deep Learning applications, however, no prior multi-objective method suits very deep networks. Existing approaches either require training a new network for every solution on the Pareto front or add a considerable overhead to the number of parameters by introducing hyper-networks conditioned on modifiable preferences. In this paper, we present a novel method that contextualizes the network directly on the preferences by adding them to the input space. In addition, we ensure a well-spread Pareto front by forcing the solutions to preserve a small angle to the preference vector. Through extensive experiments, we demonstrate that our Pareto fronts achieve state-of-the-art quality despite being computed significantly faster. Furthermore, we demonstrate the scalability as our method approximates the full Pareto front on the CelebA dataset with an EfficientNet network at a marginal training time overhead of 7% compared to a single-objective optimization. We make the code publicly available at https://github.com/ruchtem/cosmos.",10.1109/ICDM51629.2021.00162,Multi-objective optimization;Deep Learning;Fairness,,0.0,1.0
A study of Pareto-based methods for ensemble pool generation and aggregation,V. H. Alves Ribeiro; G. Reynoso-Meza,2019 IEEE Congress on Evolutionary Computation (CEC),2019.0,"In the field of machine learning, the application of ensemble methods is one of the most successful techniques in order to achieve a good performance in classification tasks. The combination of multiple classifiers is able to achieve better results than a single model, and much effort has been put into applying multi-objective optimisation for improving results with diverse ensemble generation and classifier aggregation. Most recently, dynamic classifier selection and weighting has acquired relevance in the field of multiple-classifier systems. However, to the authors knowledge, there has not yet been a comparison study of Pareto based techniques and dynamic ensemble methods. Thus, this paper proposes a comparison of two ensemble member generation techniques (Pareto-based diverse ensemble generation and bootstrap aggregating) and five aggregation methods (selection of the best classifier, majority voting with all members, majority voting with members selected with multi-objective optimisation, dynamic classifier selection and dynamic classifier weighting), performed on six binary classification benchmark data sets. Results indicate that the combination of bootstrap aggregating and majority voting with multi-objective ensemble member selection achieves the best performance.",10.1109/CEC.2019.8790291,Ensemble methods;multi-objective optimisation;supervised learning.,,0.0,1.0
Study of the approximation of the fitness landscape and the ranking process of scalarizing functions for many-objective problems,G. Toscano; K. Deb,2016 IEEE Congress on Evolutionary Computation (CEC),2016.0,"Although surrogate models have been successfully adopted by evolutionary algorithms to solve time-consuming multiobjective problems, their use has been confined to solving problems with a low number of objectives. On the other hand, scalarizing functions have proved to work well with many-objective problems. This paper presents a novel study on many-objective optimization concerning the use of surrogate models to approximate both (1) the fitness landscape of traditional multiobjective approaches and (2) the ranking relation imposed by such approaches. Our methodology involves a thorough comparison of four popular surrogate modeling techniques in order to approximate the fitness landscape and the ranking relations of three different scalarizing functions. Additionally, we explored the interactions of these methods through four well-known scalable test problems with four, six, eight, and ten objectives. Besides finding that Tchebycheff scalarizing function and Gaussian processes for machine learning are accurate methods to handle many-objective problems, one of our most important findings involves the capabilities of metamodeling techniques to approximate the ranking procedure from the information gathered from the parameter space. Such a capability can be effectively used for pre-screening purposes on MOEAs.",10.1109/CEC.2016.7744344,,3.0,0.0,1.0
Minimax Modifications of Linear Discriminant Analysis for Classification with Rare Classes,K. Bratanova; I. Kareev; R. Salimov,2020 IEEE East-West Design & Test Symposium (EWDTS),2020.0,"We consider the problem of classification for imbalanced samples with rare classes. A common problem for machine learning methods in such setting is that a rare class would have extremely high classification error compared to more widespread classes. In general, this problem could be mitigated with re-sampling or fitting additional weights to control the classification errors in classes, though those methods are computationally expensive for large datasets and sometimes fail to attain appropriate results. It this paper we present cost-efficient modifications of Linear Discriminant Analysis allowing to mitigate the problem by minimizing maximal classification error among the classes. For example, this allows achieving more robust machinery malfunction detection algorithms where our expectations on recall would be more consistent among different malfunction types.",10.1109/EWDTS50664.2020.9224895,classification;imbalanced sample dataset;rare class;linear discriminant analysis;minimax error,,0.0,1.0
Constrained Multi-Objective Optimization for Automated Machine Learning,S. Gardner; O. Golovidov; J. Griffin; P. Koch; W. Thompson; B. Wujek; Y. Xu,2019 IEEE International Conference on Data Science and Advanced Analytics (DSAA),2019.0,"Automated machine learning has gained a lot of attention recently. Building and selecting the right machine learning models is often a multi-objective optimization problem. General purpose machine learning software that simultaneously supports multiple objectives and constraints is scant, though the potential benefits are great. In this work, we present a framework called Autotune that effectively handles multiple objectives and constraints that arise in machine learning problems. Autotune is built on a suite of derivative-free optimization methods, and utilizes multi-level parallelism in a distributed computing environment for automatically training, scoring, and selecting good models. Incorporation of multiple objectives and constraints in the model exploration and selection process provides the flexibility needed to satisfy trade-offs necessary in practical machine learning applications. Experimental results from standard multi-objective optimization benchmark problems show that Autotune is very efficient in capturing Pareto fronts. These benchmark results also show how adding constraints can guide the search to more promising regions of the solution space, ultimately producing more desirable Pareto fronts. Results from two real-world case studies demonstrate the effectiveness of the constrained multi-objective optimization capability offered by Autotune.",10.1109/DSAA.2019.00051,Multi-objective Optimization;Automated Machine Learning;Distributed Computing System,4.0,0.0,1.0
HyperASPO: Fusion of Model and Hyper Parameter Optimization for Multi-objective Machine Learning,A. Kannan; A. Roy Choudhury; V. Saxena; S. Raje; P. Ram; A. Verma; Y. Sabharwal,2021 IEEE International Conference on Big Data (Big Data),2021.0,"Current state of the art methods for generating Pareto-optimal solutions for multi-objective optimization problems mostly rely on optimizing the hyper-parameters of the models (HPO - hyper-parameter Optimization). Few recent, less studied methods focus on optimizing over the space of model parameters, leveraging the problem specific knowledge. We present a generic first-of-a-kind method, referred to as HyperASPO, that combines optimization over the spaces of both hyper-parameters and model parameters for multi-objective optimization of learning problems. HyperASPO consists of two stages. First, we perform a coarse HPO to determine a set of favorable hyper-parameter configurations. In the second step, for each of these configurations, we solve a sequence of weighted single objective optimization problems for estimating Pareto-optimal solutions. We generate the weights in the second step using an adaptive mesh constructed iteratively based on the metrics of interest, resulting in further refinement of Pareto frontier efficiently. We consider the widely used XGBoost (Gradient Boosted Trees) model and validate our method on multiple classification datasets. Our proposed method shows up to 20% improvement over the hypervolumes of Pareto fronts obtained through state of the art HPO based methods with up to 2× reduction in computational time.",10.1109/BigData52589.2021.9671604,Hyperparameter optimization;Model parameters;XGBoost;HyperASPO;Pareto Optimization,,0.0,1.0
Pareto-Optimal Bit Allocation for Collaborative Intelligence,S. R. Alvar; I. V. Bajić,IEEE Transactions on Image Processing,2021.0,"In recent studies, collaborative intelligence (CI) has emerged as a promising framework for deployment of Artificial Intelligence (AI)-based services on mobile/edge devices. In CI, the AI model (a deep neural network) is split between the edge and the cloud, and intermediate features are sent from the edge sub-model to the cloud sub-model. In this article, we study bit allocation for feature coding in multi-stream CI systems. We model task distortion as a function of rate using convex surfaces similar to those found in distortion-rate theory. Using such models, we are able to provide closed-form bit allocation solutions for single-task systems and scalarized multi-task systems. Moreover, we provide analytical characterization of the full Pareto set for 2-stream k-task systems, and bounds on the Pareto set for 3-stream 2-task systems. Analytical results are examined on a variety of DNN models from the literature to demonstrate wide applicability of the results.",10.1109/TIP.2021.3060875,Bit allocation;rate distortion optimization;collaborative intelligence;multi objective optimization;deep learning;multi-task learning,2.0,0.0,1.0
A Multiple Gradient Descent Design for Multi-Task Learning on Edge Computing: Multi-Objective Machine Learning Approach,X. Zhou; Y. Gao; C. Li; Z. Huang,IEEE Transactions on Network Science and Engineering,2022.0,"Multi-task learning technique is widely utilized in machine learning modeling where commonalities and differences across multiple tasks are exploited. However, multiple conflicting objectives often occur in multi-task learning. Conventionally, a common compromise is to minimize the weighted sum of multiple objectives which may be invalid if the objectives are competing. In this paper, a novel multi-objective machine learning approach is proposed to solve this challenging issue, which reformulates the multi-task learning as multi-objective optimization. To address the issues contributed by existing multi-objective optimization algorithms, a multi-gradient descent algorithm is introduced for the multi-objective machine learning problem by which an innovative gradient-based optimization is leveraged to converge to an optimal solution of the Pareto set. Moreover, the gradient surgery for the multi-gradient descent algorithm is proposed to obtain a stable Pareto optimal solution. As most of the edge computing devices are computational resource-constrained, the proposed method is implemented for optimizing the edge device's memory, computation and communication demands. The proposed method is applied to the multiple license plate recognition problem. The experimental results show that the proposed method outperforms state-of-the-art learning methods and can successfully find solutions that balance multiple objectives of the learning task over different datasets.",10.1109/TNSE.2021.3067454,Deep neural network;edge computing;multi-objective machine learning;multi-task learning;multiple gradient descent,1.0,0.0,1.0
Wrapper Framework for Test-Cost-Sensitive Feature Selection,L. Jiang; G. Kong; C. Li,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",2021.0,"Feature selection is an optional preprocessing procedure and is frequently used to improve the classification accuracy of a machine learning algorithm by removing irrelevant and/or redundant features. However, in many real-world applications, the test cost is also required for making optimal decisions, in addition to the classification accuracy. To the best of our knowledge, thus far, few studies have been conducted on test-cost-sensitive feature selection (TCSFS). In TCSFS, the objectives are twofold: 1) to improve the classification accuracy and 2) to decrease the test cost. Therefore, in fact, it constitutes a multiobjective optimization problem. In this paper, we transformed this multiobjective optimization problem into a single-objective optimization problem by utilizing a new evaluation function and in this paper, we propose a new general wrapper framework for TCSFS. Specifically, in our proposed framework, we add a new term to the evaluation function of a wrapper feature selection method so that the test cost of measuring features is taken into account. We experimentally tested our proposed framework, using 36 classification problems from the University of California at Irvine (UCI) repository, and compared it to some other state-of-the-art feature selection frameworks. The experimental results showed that our framework allows users to select an optimal feature subset with the minimal test cost, while simultaneously maintaining a high classification accuracy.",10.1109/TSMC.2019.2904662,Classification accuracy;decision making;feature selection;test cost;test-cost-sensitive learning,18.0,0.0,1.0
Training Confidence-Calibrated Classifier via Distributionally Robust Learning,H. Wu; M. D. Wang,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",2020.0,"Supervised learning via empirical risk minimization, despite its solid theoretical foundations, faces a major challenge in generalization capability, which limits its application in real-world data science problems. In particular, current models fail to distinguish in-distribution and out-of-distribution and give over confident predictions for out-of-distribution samples. In this paper, we propose an distributionally robust learning method to train classifiers via solving an unconstrained minimax game between an adversary test distribution and a hypothesis. We showed the theoretical generalization performance guarantees, and empirically, our learned classifier when coupled with thresholded detectors, can efficiently detect out-of-distribution samples.",10.1109/COMPSAC48688.2020.0-230,supervised learning;adversarial machine learning;robust machine learning;distributionally robust optimization,,0.0,1.0
Minimax Learning for Remote Prediction,C. T. Li; X. Wu; A. Ozgur; A. El Gamal,2018 IEEE International Symposium on Information Theory (ISIT),2018.0,"The classical problem of supervised learning is to infer an accurate predictor of a target variable Y from a measured variable X by using a finite number of labeled training samples. Motivated by the increasingly distributed nature of data and decision making, in this paper we consider a variation of this classical problem in which the prediction is performed remotely based on a rate-constrained description M of X. Upon receiving M, the remote node computes an estimate Y of Y. We follow the recent minimax approach to study this learning problem and show that it corresponds to a one-shot minimax noisy source coding problem. We then establish information theoretic bounds on the risk-rate Lagrangian cost and a general method to design a near-optimal descriptor-estimator pair, which can be viewed as a rate-constrained analog to the maximum conditional entropy principle used in the classical minimax learning problem. Our results show that a naive estimate-compress scheme for rate-constrained prediction is not in general optimal.",10.1109/ISIT.2018.8437318,,4.0,0.0,1.0
A New Multi-Layer Classification Method Based on Logistic Regression,K. Kang; F. Gao; J. Feng,2018 13th International Conference on Computer Science & Education (ICCSE),2018.0,"To improve the effect of logistic regression in multiobjective classification and explore its greatest potential, a set of training and classification algorithms is constructed, by using the high accuracy of two-class classification. Multi-layer predictions are made under the premise of ensuring clear structure of the model. The method of outlier detection is introduced to choose a proper number of two-class classifiers for categories that are prone to be confused. Then further predictions are made with these two-class classifiers. The evaluation on MNIST dataset show that this method can effectively improve the classification accuracy of multi-class datasets with limited increase of running time.",10.1109/ICCSE.2018.8468725,logistic regression;multi-objective classification;outlier;machine learning;MNIST,5.0,0.0,1.0
Minimax strategies for training classifiers under unknown priors,R. Alaiz-Rodriguez; J. Cid-Sueiro,Proceedings of the 12th IEEE Workshop on Neural Networks for Signal Processing,2002.0,"Most supervised learning algorithms are based on the assumption that the training data set reflects the underlying statistical model of the real data. However, this stationarity assumption is not always satisfied in practice: quite frequently, class prior probabilities are not in accordance with the class proportions in the training data set. The minimax approach is based on selecting the classifier that minimize the error probability under the worst case conditions. We propose a two-step learning algorithm to train a neural network in order to estimate the minimax classifier that is robust to changes in the class priors. During the first step, posterior probabilities based on training data priors are estimated. During the second step, class priors are modified in order to minimize a cost function that is asymptotically equivalent to the worst-case error rate. This procedure is illustrated on a softmax-based neural network. Several experimental results show the advantages of the proposed method with respect to other approaches.",10.1109/NNSP.2002.1030036,,,0.0,1.0
Reusing Genetic Programming for Ensemble Selection in Classification of Unbalanced Data,U. Bhowan; M. Johnston; M. Zhang; X. Yao,IEEE Transactions on Evolutionary Computation,2014.0,"Classification algorithms can suffer from performance degradation when the class distribution is unbalanced. This paper develops a two-step approach to evolving ensembles using genetic programming (GP) for unbalanced data. The first step uses multiobjective (MO) GP to evolve a Pareto-approximated front of GP classifiers to form the ensemble by trading-off the minority and the majority class against each other during learning. The MO component alleviates the reliance on sampling to artificially rebalance the data. The second step, which is the focus this paper, proposes a novel ensemble selection approach using GP to automatically find/choose the best individuals for the ensemble. This new GP approach combines multiple Pareto-approximated front members into a single composite genetic program solution to represent the (optimized) ensemble. This ensemble representation has two main advantages/novelties over traditional genetic algorithm (GA) approaches. First, by limiting the depth of the composite solution trees, we use selection pressure during evolution to find small highly-cooperative groups of individuals for the ensemble. This means that ensemble sizes are not fixed a priori (as in GA), but vary depending on the strength of the base learners. Second, we compare different function set operators in the composite solution trees to explore new ways to aggregate the member outputs and thus, control how the ensemble computes its output. We show that the proposed GP approach evolves smaller more diverse ensembles compared to an established ensemble selection algorithm, while still performing as well as, or better than the established approach. The evolved GP ensembles also perform well compared to other bagging and boosting approaches, particularly on tasks with high levels of class imbalance.",10.1109/TEVC.2013.2293393,Classification;ensemble machine learning;genetic programming;unbalanced data,57.0,0.0,1.0
Pareto-Optimal Model Selection via SPRINT-Race,T. Zhang; M. Georgiopoulos; G. C. Anagnostopoulos,IEEE Transactions on Cybernetics,2018.0,"In machine learning, the notion of multi-objective model selection (MOMS) refers to the problem of identifying the set of Pareto-optimal models that optimize by compromising more than one predefined objectives simultaneously. This paper introduces SPRINT-Race, the first multi-objective racing algorithm in a fixed-confidence setting, which is based on the sequential probability ratio with indifference zone test. SPRINT-Race addresses the problem of MOMS with multiple stochastic optimization objectives in the proper Pareto-optimality sense. In SPRINT-Race, a pairwise dominance or non-dominance relationship is statistically inferred via a non-parametric, ternary-decision, dual-sequential probability ratio test. The overall probability of falsely eliminating any Pareto-optimal models or mistakenly returning any clearly dominated models is strictly controlled by a sequential Holm's step-down family-wise error rate control method. As a fixed-confidence model selection algorithm, the objective of SPRINT-Race is to minimize the computational effort required to achieve a prescribed confidence level about the quality of the returned models. The performance of SPRINT-Race is first examined via an artificially constructed MOMS problem with known ground truth. Subsequently, SPRINT-Race is applied on two real-world applications: 1) hybrid recommender system design and 2) multi-criteria stock selection. The experimental results verify that SPRINT-Race is an effective and efficient tool for such MOMS problems.<sup>11</sup>MATLAB code of SPRINT-Race is available at https://github.com/watera427/SPRINT-Race.",10.1109/TCYB.2017.2647821,Model selection (MS);multi-objective optimization;racing algorithm;sequential probability ratio test (SPRT),3.0,0.0,1.0
Design of Artificial Neural Networks Using a Memetic Pareto Evolutionary Algorithm Using as Objectives Entropy versus Variation Coefficient,J. C. Fernández; C. Hervás; F. J. Martínez; M. Cruz,2009 Ninth International Conference on Intelligent Systems Design and Applications,2009.0,"This paper proposes a multi-classification pattern algorithm using multilayer perceptron neural network models which try to boost two conflicting main objectives of a classifier, a high correct classification rate and a high classification rate for each class. To solve this machine learning problem, we consider a Memetic Pareto Evolutionary approach based on the NSGA2 algorithm (MPENSGA2), where we defined two objectives for determining the goodness of a classifier: the cross-entropy error function and the variation coefficient of its sensitivities, because both measures are continuous functions, making the convergence more robust. Once the Pareto front is built, we use an automatic selection methodology of individuals: the best model in accuracy (upper extreme in the Pareto front). This methodology is applied to solve six benchmark classification problems, obtaining promising results and achieving a high classification rate in the generalization set with an acceptable level of accuracy for each class.",10.1109/ISDA.2009.153,Classification;Neural Networks;Multi-objective;Entropy;Variation Coefficient,,0.0,1.0
Smart Multi-Objective Evolutionary GAN,M. Baioletti; G. D. Bari; V. Poggioni; C. A. C. Coello,2021 IEEE Congress on Evolutionary Computation (CEC),2021.0,"Generative Adversarial Network (GAN) is a family of machine learning algorithms designed to train neural networks able to imitate real data distributions. Unfortunately, GAN suffers from problems such as gradient vanishing and mode collapse. In Multi-Objective Evolutionary Generative Adversarial Network (MO-EGAN) these problems were addressed using an evolutionary technique combined with Multi-Objective selection, obtaining better results on synthetic datasets at the expense of larger computation times. In this works, we present the Smart MultiObjective Evolutionary Generative Adversarial Network (SMO-EGAN) algorithm, which reduces the computational cost of MO-EGAN and achieves better results on real data distributions.",10.1109/CEC45853.2021.9504858,,,0.0,1.0
Pareto Self-Supervised Training for Few-Shot Learning,Z. Chen; J. Ge; H. Zhan; S. Huang; D. Wang,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2021.0,"While few-shot learning (FSL) aims for rapid generalization to new concepts with little supervision, self-supervised learning (SSL) constructs supervisory signals directly computed from unlabeled data. Exploiting the complementarity of these two manners, few-shot auxiliary learning has recently drawn much attention to deal with few labeled data. Previous works benefit from sharing inductive bias between the main task (FSL) and auxiliary tasks (SSL), where the shared parameters of tasks are optimized by minimizing a linear combination of task losses. However, it is challenging to select a proper weight to balance tasks and reduce task conflict. To handle the problem as a whole, we propose a novel approach named as Pareto self-supervised training (PSST) for FSL. PSST explicitly decomposes the few-shot auxiliary problem into multiple constrained multi-objective subproblems with different trade-off preferences, and here a preference region in which the main task achieves the best performance is identified. Then, an effective preferred Pareto exploration is proposed to find a set of optimal solutions in such a preference region. Extensive experiments on several public benchmark datasets validate the effectiveness of our approach by achieving state-of-the-art performance.",10.1109/CVPR46437.2021.01345,,2.0,0.0,1.0
Learning classifiers from imbalanced data based on biased minimax probability machine,Kaizhu Huang; Haiqin Yang; I. King; M. R. Lyu,"Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.",2004.0,"We consider the problem of the binary classification on imbalanced data, in which nearly all the instances are labelled as one class, while far fewer instances are labelled as the other class, usually the more important class. Traditional machine learning methods seeking an accurate performance over a full range of instances are not suitable to deal with this problem, since they tend to classify all the data into the majority, usually the less important class. Moreover, some current methods have tried to utilize some intermediate factors, e.g., the distribution of the training set, the decision thresholds or the cost matrices, to influence the bias of the classification. However, it remains uncertain whether these methods can improve the performance in a systematic way. In this paper, we propose a novel model named biased minimax probability machine. Different from previous methods, this model directly controls the worst-case real accuracy of classification of the future data to build up biased classifier;. Hence, it provides a rigorous treatment on imbalanced data. The experimental results on the novel model comparing with those of three competitive methods, i.e., the naive Bayesian classifier, the k-nearest neighbor method, and the decision tree method C4.5, demonstrate the superiority of our novel model.",10.1109/CVPR.2004.1315213,,12.0,0.0,1.0
Investigating the Robustness and Stability to Noisy Data of a Dynamic Feature Selection Method,J. Jesus; A. Canuto; D. Araújo,2019 8th Brazilian Conference on Intelligent Systems (BRACIS),2019.0,"The curse of dimensionality is one of the major problems faced by machine learning researchers. If we consider the fast growing of complex data in real world scenarios, feature selection (FS) becomes a imperative step for many application domains to reduce both data complexity and computing time. Based on that, several studies have been developed in order to create efficient FS methods that performs this task. However, a bad selection of one single criterion to evaluate the attribute importance and the arbitrary choice of the number of features usually leads to a poor analysis. On the other hand, recent studies have successfully created models to select features considering the particularities of the data, known as dynamic feature selection. In this paper, we evaluate one of this successful methods, called pareto front based dynamic feature selection (PF-DFS), to test its stability and robustness in noisy data. We used 15 artificial and real world data with additional noise data. Results shown that the PF-DFS is more stable to noisy scenarios than existing feature selection methods.",10.1109/BRACIS.2019.00040,feature selection;dynamic feature selection;data analysis;pareto front;supervised learning,,0.0,1.0
A Pareto Corner Search Evolutionary Algorithm and Principal Component Analysis for Objective Dimensionality Reduction,X. H. Nguyen; L. Thu Bui; C. T. Tran,2019 11th International Conference on Knowledge and Systems Engineering (KSE),2019.0,"Many-objective optimisation problems (MaOPs) cause serious difficulties for existing multi-objective evolutionary algorithms (MOEAs). One common way to alleviate these difficulties is to use objective dimensionality reduction. Most existing objective reduction methods are time-consuming because they require MOEAs to run numerous generations. Pareto corner search evolutionary algorithm (PCSEA) was proposed in [18] to speed up objective reduction methods by only seeking corner solutions instead of whole solutions. However, the PCSEA-based objective reduction method in [18] needs to predefine a threshold to select objectives which strongly depends on problems and is not straightforward to obtain. This paper proposes a new objective dimensionality reduction method by integrating PCSEA and principal component analysis (PCA). Thanks to combining advantages of PCSEA and PCA, the proposed method not only can be efficient to eliminate redundant objectives, but also not require to define any parameter in advanced. The experimental results also show that the proposed method can perform objective reduction more successfully than the PCSEA-based objective reduction method. The results further strengthen the links between evolutionary computation and machine learning to address optimization problems.",10.1109/KSE.2019.8919438,many-objective optimisation;objective dimensionality reduction;feature selection;evolutionary computation,2.0,0.0,1.0
Dynamic Feature Selection Based on Pareto Front Optimization,J. Jesus; A. Canuto; D. Araújo,2018 International Joint Conference on Neural Networks (IJCNN),2018.0,"One of the main issues of machine learning algorithms is the curse of dimensionality. With the fast growing of complex data in real world scenarios, the feature selection becomes a mandatory preprocessing step in any application to reduce both the complexity of the data and the computing time. Based on that, several works have been produced in order to develop efficient methods to perform this task. Most feature selection methods select the best attributes based on some specific criteria. Additionally, recent studies have successfully constructed models to select features considering the particularities of the data, assuming that similar samples should be treated separately. Although some advance has been made, a bad choice of one single criteria to evaluate the importance of the attributes and the arbitrary choice of the number of features made by the user can lead to a poor analysis. In order to overcome some of these issues, this work brings an improvement of a dynamic feature selection algorithm (DFS) by using the idea of pareto front multi-objective optimization, which allow us to both consider distinct perspectives of the features relevance and automatically set the number of attributes to select. We tested our approach using 15 artificial and real world data and results have shown that when compared to the original DFS method, the performance of the proposed method is remarkable superior. In fact, the results are very promising since the proposed method also achieved better performance than well-established dimensionality reduction methods and when using the original datasets, showing that the reduction of noisy and/or redundant attributes can have a positive effect in the performance of a classification task.",10.1109/IJCNN.2018.8489680,,2.0,0.0,1.0
Synthesizing Pareto-Optimal Interpretations for Black-Box Models,H. Torfah; S. Shah; S. Chakraborty; S. Akshay; S. A. Seshia,2021 Formal Methods in Computer Aided Design (FMCAD),2021.0,"We present a new multi-objective optimization approach for synthesizing interpretations that “explain” the behavior of black-box machine learning models. Constructing human-understandable interpretations for black-box models often requires balancing conflicting objectives. A simple interpretation may be easier to understand for humans while being less precise in its predictions vis-a-vis a complex interpretation. Existing methods for synthesizing interpretations use a single objective function and are often optimized for a single class of interpretations. In contrast, we provide a more general and multi-objective synthesis framework that allows users to choose (1) the class of syntactic templates from which an interpretation should be synthesized, and (2) quantitative measures on both the correctness and explainability of an interpretation. For a given black-box, our approach yields a set of Pareto-optimal interpretations with respect to the correctness and explainability measures. We show that the underlying multi-objective optimization problem can be solved via a reduction to quantitative constraint solving, such as weighted maximum satisfiability. To demonstrate the benefits of our approach, we have applied it to synthesize interpretations for black-box neural-network classifiers. Our experiments show that there often exists a rich and varied set of choices for interpretations that are missed by existing approaches.",10.34727/2021/isbn.978-3-85448-046-4_24,,,0.0,1.0
An Approach to Large Margin Design of Prototype-Based Pattern Classifiers,T. He; Y. Hu; Q. Huo,"2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07",2007.0,"In this paper, we propose a maximum separation margin (MSM) training method for multiple-prototype (MP)-based pattern classifiers in which a sample separation margin defined as the distance from the training sample to the classification boundary can be calculated precisely. Similar to support vector machine (SVM) methodology, MSM training is formulated as a multicriteria optimization problem which aims at maximizing the separation margin and minimizing the empirical error rate on training data simultaneously. By making certain relaxation assumptions, MSM training can be reformulated as a semidefinite programming (SDP) problem that can be solved efficiently by some standard optimization algorithms designed for SDP. Evaluation experiments are conducted on the task of the recognition of most confusable Kanji character pairs identified from popular Nakayosi and Kuchibue handwritten Japanese character databases. It is observed that the MSM-trained MP-based classifier achieves a similar character recognition accuracy as that of the state-of-the-art SVM-based classifier, yet requires much fewer classifier parameters.",10.1109/ICASSP.2007.366313,large margin;pattern classification;support vector machine;machine;machine learning;semidefinite programming,2.0,0.0,1.0
Multi-objective parameter configuration of machine learning algorithms using model-based optimization,D. Horn; B. Bischl,2016 IEEE Symposium Series on Computational Intelligence (SSCI),2016.0,"The performance of many machine learning algorithms heavily depends on the setting of their respective hyperparameters. Many different tuning approaches exist, from simple grid or random search approaches to evolutionary algorithms and Bayesian optimization. Often, these algorithms are used to optimize a single performance criterion. But in practical applications, a single criterion may not be sufficient to adequately characterize the behavior of the machine learning method under consideration and the Pareto front of multiple criteria has to be considered. We propose to use model-based multi-objective optimization to efficiently approximate such Pareto fronts.",10.1109/SSCI.2016.7850221,,11.0,0.0,1.0
Machine learning with incomplete datasets using multi-objective optimization models,H. A. Khorshidi; M. Kirley; U. Aickelin,2020 International Joint Conference on Neural Networks (IJCNN),2020.0,"Machine learning techniques have been developed to learn from complete data. When missing values exist in a dataset, the incomplete data should be preprocessed separately by removing data points with missing values or imputation. In this paper, we propose an online approach to handle missing values while a classification model is learnt. To reach this goal, we develop a multi-objective optimization model with two objective functions for imputation and model selection. We also propose three formulations for imputation objective function. We use an evolutionary algorithm based on NSGA II to find the optimal solutions as the Pareto solutions. We investigate the reliability and robustness of the proposed model using experiments by defining several scenarios in dealing with missing values and classification. We also describe how the proposed model can contribute to medical informatics. We compare the performance of three different formulations via experimental results. The proposed model results get validated by comparing with a comparable literature.",10.1109/IJCNN48605.2020.9206742,incomplete data;multi-objective model;uncertainty;model selection;classification,,0.0,1.0
An Evolutionary Machine Learning Approach Towards Less Conservative Robust Optimization,P. D. Pantula; K. Mitra,2019 IEEE Congress on Evolutionary Computation (CEC),2019.0,"In the recent era, multi-criteria decision making under uncertainty is gaining importance due to its wide range of applicability. Among several types of uncertainty handling techniques, Robust Optimization (RO) is considered as an efficient and tractable approach provided one has accessibility to data in uncertain regions. However, solutions of RO may actually deviate from actual results in real scenarios, due to conservative sampling. This paper proposes a methodology to amalgamate unsupervised machine learning algorithms with RO which thereby makes it data-driven. A novel evolutionary fuzzy clustering mechanism is implemented to transcript the uncertain space such that the exact regions of uncertainty are identified. Subsequently, density based boundary point detection and Delaunay triangulation based boundary construction enables intelligent Sobol based sampling in these regions for use in RO. Results of two test cases with varying dimensions are presented along with a comprehensive comparison between conventional RO approach using box uncertainty set and proposed methodology. Considered case studies include highly nonlinear real life model for continuous casting from steelmaking industries, where a time expensive multi-objective optimization problem under uncertainty is formulated to resolve the conflict in productivity and energy consumption. Optimal Artificial Neural Network (ANN) surrogate assisted optimization under uncertainty for casting model is performed to obtain solutions in realistic time. The resulting RO problem being multi-objective in nature, the Pareto solutions are obtained by NSGA II.",10.1109/CEC.2019.8790094,Data Driven Robust Optimization;Evolutionary Algorithms;Fuzzy Clustering;ANN surrogate models;Multi objective Optimization,1.0,0.0,1.0
A new approach on multi-agent Multi-Objective Reinforcement Learning based on agents' preferences,Z. Daavarani Asl; V. Derhami; M. Yazdian-Dehkordi,2017 Artificial Intelligence and Signal Processing Conference (AISP),2017.0,"Reinforcement Learning (RL) is a powerful machine learning paradigm for solving Markov Decision Process (MDP). Traditional RL algorithms aim to solve one-objective problems, but many real-world problems have more than one objective which conflict each other. In recent years, Multi-Objective Reinforcement Learning (MORL) algorithms, which employ a reward vector instead of a scalar reward signal, have been proposed to solve multi-objective problems. In MORL, because of conflicting objectives, there is no one optimal solution and a set of solutions named Pareto Front will be learned. In this paper, we proposed a new multi-agent method, which uses a shared Q-table for all agents to solve bi-objective problems. However, each agent selects actions based on its preference. These preferences are different with each other and the agents reach to Pareto Front solutions based on this preferences. The proposed method is simple in understanding and its computational cost is very low. Moreover, after finding the Pareto Front set, we can easily track the policy. Simulation results show that our proposed method outperforms the available methods in the term of learning speed.",10.1109/AISP.2017.8324111,reinforcement learning;multi-agent systems;multi-objective;Pareto Front,2.0,0.0,1.0
The Trade-off Between Privacy and Utility in Local Differential Privacy,M. Li; Y. Tian; J. Zhang; D. Fan; D. Zhao,2021 International Conference on Networking and Network Applications (NaNA),2021.0,"In statistical queries work, such as frequency estimation, the untrusted data collector could as an honest-but-curious (HbC) or malicious adversary to learn true values. Local differential privacy(LDP) protocols have been applied against the untrusted third party in data collecting. Nevertheless, excessive noise of LDP will reduce data utility, thus affecting the results of statistical queries. Therefore, it is significant to research the trade-off between privacy and utility. In this paper, we first measure the privacy loss by observing the maximum posterior confidence of the adversary (data collector). Then, through theoretical analysis and comparison we obtain the most suitable utility measure that is Wasserstein distance. Based on these, we introduce an originality framework for privacy-utility tradeoff framework, finding that this system conforms to the Pareto optimality state and formalizing a payoff function to find optimal equilibrium point under Pareto efficiency. Finally, we illustrate the efficacy of our system model by the Adult dataset from the UCI machine learning repository.",10.1109/NaNA53684.2021.00071,data collecting;local differential privacy;privacy metric;utility metric;Pareto optimality,,0.0,1.0
A Characterization of Stochastic Mirror Descent Algorithms and Their Convergence Properties,N. Azizan; B. Hassibi,"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",2019.0,"Stochastic mirror descent (SMD) algorithms have recently garnered a great deal of attention in optimization, signal processing, and machine learning. They are similar to stochastic gradient descent (SGD), in that they perform updates along the negative gradient of an instantaneous (or stochastically chosen) loss function. However, rather than update the parameter (or weight) vector directly, they update it in a ""mirrored"" domain whose transformation is given by the gradient of a strictly convex differentiable potential function. SMD was originally conceived to take advantage of the underlying geometry of the problem as a way to improve the convergence rate over SGD. In this paper, we study SMD, for linear models and convex loss functions, through the lens of H<sup>∞</sup> estimation theory and come up with a minimax interpretation of the SMD algorithm which is the counterpart of the H<sup>∞</sup>-optimality of the SGD algorithm for linear models and quadratic loss. In doing so, we identify a fundamental conservation law that SMD satisfies and use it to study the convergence properties of the algorithm. For constant step size SMD, when the linear model is over-parameterized, we give a deterministic proof of convergence for SMD and show that from any initial point, it converges to the closest point in the space of all parameter vectors that interpolate the data, where closest is in the sense of the Bregman divergence of the potential function. This property is referred to as implicit regularization: with an appropriate choice of the potential function one can guarantee convergence to the minimizer of any desired convex regularizer. For vanishing step size SMD, and in the standard stochastic optimization setting, we give a direct and elementary proof of convergence for SMD to the ""true"" parameter vector which avoids ergodic averaging or appealing to stochastic differential equations.",10.1109/ICASSP.2019.8682271,Stochastic gradient descent;mirror descent;minimax optimality;convergence;implicit regularization,2.0,0.0,1.0
Adversarially Robust Classification Based on GLRT,B. Puranik; U. Madhow; R. Pedarsani,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",2021.0,"Machine learning models are vulnerable to adversarial attacks that can often cause misclassification by introducing small but well designed perturbations. In this paper, we explore, in the setting of classical composite hypothesis testing, a defense strategy based on the generalized likelihood ratio test (GLRT), which jointly estimates the class of interest and the adversarial perturbation. We evaluate the GLRT approach for the special case of binary hypothesis testing in white Gaussian noise under ℓ<inf>∞</inf> norm-bounded adversarial perturbations, a setting for which a minimax strategy optimizing for the worst-case attack is known. We show that the GLRT approach yields performance competitive with that of the minimax approach under the worst-case attack, while yielding a better robustness-accuracy trade-off under weaker attacks. The GLRT defense is applicable in multi-class settings and generalizes naturally to more complex models for which optimal minimax classifiers are not known.",10.1109/ICASSP39728.2021.9413587,Adversarial machine learning;hypothesis testing;robust classification,,0.0,1.0
Improving Robustness of DNNs against Common Corruptions via Gaussian Adversarial Training,C. Yi; H. Li; R. Wan; A. C. Kot,2020 IEEE International Conference on Visual Communications and Image Processing (VCIP),2020.0,"Deep neural networks have demonstrated tremendous success in image classification, but their performance sharply degrades when evaluated on slightly different test data (e.g., data with corruptions). To address these issues, we propose a minimax approach to improve common corruption robustness of deep neural networks via Gaussian Adversarial Training. To be specific, we propose to train neural networks with adversarial examples where the perturbations are Gaussian-distributed. Our experiments show that our proposed GAT can improve neural networks' robustness to noise corruptions more than other baseline methods. It also outperforms the state-of-the-art method in improving the overall robustness to common corruptions.",10.1109/VCIP49819.2020.9301856,Deep Learning;Robustness to Common Corruptions;Adversarial Training;Data Augmentation,1.0,0.0,1.0
Signal Recovery on Graphs: Fundamental Limits of Sampling Strategies,S. Chen; R. Varma; A. Singh; J. Kovačević,IEEE Transactions on Signal and Information Processing over Networks,2016.0,"This paper builds theoretical foundations for the recovery of a newly proposed class of smooth graph signals, approximately bandlimited graph signals, under three sampling strategies: uniform sampling, experimentally designed sampling, and active sampling. We then state minimax lower bounds on the maximum risk for the approximately bandlimited class under these three sampling strategies and show that active sampling cannot fundamentally outperform experimentally designed sampling. We propose a recovery strategy to compare uniform sampling with experimentally designed sampling. As the proposed recovery strategy lends itself well to statistical analysis, we derive the exact mean square error for each sampling strategy. To study convergence rates, we introduce two types of graphs and find that 1) the proposed recovery strategy achieves the optimal rates; and 2) the experimentally designed sampling fundamentally outperforms uniform sampling for Type-2 class of graphs. To validate our proposed recovery strategy, we test it on five specific graphs: a ring graph with k nearest neighbors, an Erdos-Rényi graph, a random geometric graph, a small-world graph, and a power-law graph and find that experimental results match the proposed theory well. This paper also presents a comprehensive explanation for when and why sampling for semi-supervised learning with graphs works.",10.1109/TSIPN.2016.2614903,Active sampling;experimentally designed sampling;semi-supervised learning;signal processing on graphs;signal recovery,53.0,0.0,1.0
Neural Signature of Efficiency Relations,S. Basterrech; K. Ohnishi; M. Köppen,"2015 IEEE International Conference on Systems, Man, and Cybernetics",2015.0,"In last years -- especially due to the development of telecommunications -- fairness modelling has received a strong attention. This article presents an approach for categorizing unknown relations according to their ""closeness"" to known relations. We consider as reference relations, the well-known: Pareto dominance, Leximin and Proportional fairness relation. We simulate each relation generating a learning dataset that is used for learning Neural Networks. The learning performance evaluation is based in several metrics, which are used as a ""signature"" of each relation. Besides, we develop a new function that gives an estimation about the ""closeness"" between relations. This concept permits us to categorise a new dataset (generated by an unknown relation) according its ""closeness"" with the Pareto dominance, Leximin and Proportional fairness relations know relations. Our experimental results are coherent with the alpha fairness concept.",10.1109/SMC.2015.365,Fairness;Maxmin Fairness;Priority Fairness;Neural Networks;Supervised Learning,,0.0,1.0
Multi-Objective Optimization for Size and Resilience of Spiking Neural Networks,M. Dimovska; T. Johnston; C. D. Schuman; J. P. Mitchell; T. E. Potok,"2019 IEEE 10th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",2019.0,"Inspired by the connectivity mechanisms in the brain, neuromorphic computing architectures model Spiking Neural Networks (SNNs) in silicon. As such, neuromorphic architectures are designed and developed with the goal of having small, low power chips that can perform control and machine learning tasks. However, the power consumption of the developed hardware can greatly depend on the size of the network that is being evaluated on the chip. Furthermore, the accuracy of a trained SNN that is evaluated on chip can change due to voltage and current variations in the hardware that perturb the learned weights of the network. While efforts are made on the hardware side to minimize those perturbations, a software based strategy to make the deployed networks more resilient can help further alleviate that issue. In this work, we study Spiking Neural Networks in two neuromorphic architecture implementations with the goal of decreasing their size, while at the same time increasing their resiliency to hardware faults. We leverage an evolutionary algorithm to train the SNNs and propose a multiobjective fitness function to optimize the size and resiliency of the SNN. We demonstrate that this strategy leads to well-performing, small-sized networks that are more resilient to hardware faults.",10.1109/UEMCON47517.2019.8992983,Neuromorphic Computing;Spiking Neural Networks;Multi-objective;Fault Tolerance;Evolutionary Optimization,5.0,0.0,1.0
APENAS: An Asynchronous Parallel Evolution Based Multi-objective Neural Architecture Search,M. Hu; L. Liu; W. Wang; Y. Liu,"2020 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)",2020.0,"Machine learning is widely used in pattern classification, image processing and speech recognition. Neural architecture search (NAS) could reduce the dependence of human experts on machine learning effectively. Due to the high complexity of NAS, the tradeoff between time consumption and classification accuracy is vital. This paper presents APENAS, an asynchronous parallel evolution based multi-objective neural architecture search, using the classification accuracy and the number of parameters as objectives, encoding the network architectures as individuals. To make full use of computing resource, we propose a multi-generation undifferentiated fusion scheme to achieve asynchronous parallel evolution on multiple GPUs or CPUs, which speeds up the process of NAS. Accordingly, we propose an election pool and a buffer pool for two-layer filtration of individuals. The individuals are sorted in the election pool by non-dominated sorting and filtered in the buffer pool by the roulette algorithm to improve the elitism of the Pareto front. APENAS is evaluated on the CIFAR-10 and CIFAR-100 datasets [25]. The experimental results demonstrate that APENAS achieves 90.05% accuracy on CIFAR-10 with only 0.07 million parameters, which is comparable to state of the art. Especially, APENAS has high parallel scalability, achieving 92.5% parallel efficiency on 64 nodes.",10.1109/ISPA-BDCloud-SocialCom-SustainCom51426.2020.00045,automated machine learning;neural architecture search;multi-objective;asynchronous parallel evolution,1.0,0.0,1.0
Clustering by multi objective genetic algorithm,D. Dutta; P. Dutta; J. Sil,2012 1st International Conference on Recent Advances in Information Technology (RAIT),2012.0,"The aim of the paper is to study a real coded multi objective genetic algorithm based K-clustering, where K represents the number of clusters, may be known or unknown. If the value of K is known, it is called K-clustering algorithm. The searching power of Genetic Algorithm (GA) is exploited to get for proper clusters and centers of clusters in the feature space to optimize simultaneously intra-cluster distance (Homogeneity) (H) and inter-cluster distances (Separation) (S). Maximization of 1/H and S are the twin objectives of Multi Objective Genetic Algorithm (MOGA) achieved by measuring H and S using Euclidean distance metric, suitable for continuous features (attributes). We have selected 10 data sets from the UCI machine learning repository containing continuous features only to validate the proposed algorithms. All-important steps of algorithms are shown here. At the end, classification accuracies obtained by best chromosomes are shown.",10.1109/RAIT.2012.6194619,Clustering;homogeneity and separation;real coded multi objective genetic algorithm;Pareto optimal front,10.0,0.0,1.0
REPAIR: Removing Representation Bias by Dataset Resampling,Y. Li; N. Vasconcelos,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2019.0,"Modern machine learning datasets can have biases for certain representations that are leveraged by algorithms to achieve high performance without learning to solve the underlying task. This problem is referred to as “representation bias”. The question of how to reduce the representation biases of a dataset is investigated and a new dataset REPresentAtion bIas Removal (REPAIR) procedure is proposed. This formulates bias minimization as an optimization problem, seeking a weight distribution that penalizes examples easy for a classifier built on a given feature representation. Bias reduction is then equated to maximizing the ratio between the classification loss on the reweighted dataset and the uncertainty of the ground-truth class labels. This is a minimax problem that REPAIR solves by alternatingly updating classifier parameters and dataset resampling weights, using stochastic gradient descent. An experimental set-up is also introduced to measure the bias of any dataset for a given representation, and the impact of this bias on the performance of recognition models. Experiments with synthetic and action recognition data show that dataset REPAIR can significantly reduce representation bias, and lead to improved generalization of models trained on REPAIRed datasets. The tools used for characterizing representation bias, and the proposed dataset REPAIR algorithm, are available at https://github.com/JerryYLi/Dataset-REPAIR/.",10.1109/CVPR.2019.00980,Datasets and Evaluation;Action Recognition ; Deep Learning ; Representation Learning; Video Analytics,29.0,0.0,1.0
Variational Bound of Mutual Information for Fairness in Classification,Z. Alsulaimawi,2020 IEEE 22nd International Workshop on Multimedia Signal Processing (MMSP),2020.0,"Machine learning applications have emerged in many aspects of our lives, such as for credit lending, insurance rates, and employment applications. Consequently, it is required that such systems be nondiscriminatory and fair in sensitive features user, e.g., race, sexual orientation, and religion. To address this issue, this paper develops a minimax adversarial framework, called features protector (FP) framework, to achieve the information-theoretical trade-off between minimizing distortion of target data and ensuring that sensitive features have similar distributions. We evaluate the performance of the proposed framework on two real-world datasets. Preliminary empirical evaluation shows that our framework provides both accurate and fair decisions.",10.1109/MMSP48831.2020.9287139,Fairness;privacy-preserving;adversarial learning;variational mutual information;big data security;deep learning,,0.0,1.0
Pruning In Time (PIT): A Lightweight Network Architecture Optimizer for Temporal Convolutional Networks,M. Risso; A. Burrello; D. J. Pagliari; F. Conti; L. Lamberti; E. Macii; L. Benini; M. Poncino,2021 58th ACM/IEEE Design Automation Conference (DAC),2021.0,"Temporal Convolutional Networks (TCNs) are promising Deep Learning models for time-series processing tasks. One key feature of TCNs is time-dilated convolution, whose optimization requires extensive experimentation. We propose an automatic dilation optimizer, which tackles the problem as a weight pruning on the time-axis, and learns dilation factors together with weights, in a single training. Our method reduces the model size and inference latency on a real SoC hardware target by up to 7.4× and 3×, respectively with no accuracy drop compared to a network without dilation. It also yields a rich set of Pareto-optimal TCNs starting from a single model, outperforming hand-designed solutions in both size and accuracy.",10.1109/DAC18074.2021.9586187,Neural Architecture Search;Temporal Convolutional Networks;Edge Computing;Deep Learning,1.0,0.0,1.0
Transfer Learning for Design-Space Exploration with High-Level Synthesis,J. Kwon; L. P. Carloni,2020 ACM/IEEE 2nd Workshop on Machine Learning for CAD (MLCAD),2020.0,"High-level synthesis (HLS) raises the level of design abstraction, expedites the process of hardware design, and enriches the set of final designs by automatically translating a behavioral specification into a hardware implementation. To obtain different implementations, HLS users can apply a variety of knobs, such as loop unrolling or function inlining, to particular code regions of the specification. The applied knob configuration significantly affects the synthesized design's performance and cost, e.g., application latency and area utilization. Hence, HLS users face the design-space exploration (DSE) problem, i.e. determine which knob configurations result in Pareto-optimal implementations in this multi-objective space. Whereas it can be costly in time and resources to run HLS flows with an enormous number of knob configurations, machine learning approaches can be employed to predict the performance and cost. Still, they require a sufficient number of sample HLS runs. To enhance the training performance and reduce the sample complexity, we propose a transfer learning approach that reuses the knowledge obtained from previously explored design spaces in exploring a new target design space. We develop a novel neural network model for mixed-sharing multi-domain transfer learning. Experimental results demonstrate that the proposed model outperforms both single-domain and hard-sharing models in predicting the performance and cost at early stages of HLS-driven DSE.",10.1145/3380446.3430636,high-level synthesis;design space exploration;neural networks;machine learning;transfer learning;multi-task learning,4.0,0.0,1.0
Discovering Knowledge Rules with Multi-Objective Evolutionary Computing,R. Giusti; G. E. A. P. A. Batista,2010 Ninth International Conference on Machine Learning and Applications,2010.0,"Most Machine Learning systems target into inducing classifiers with optimal coverage and precision measures. Although this constitutes a good approach for prediction, it might not provide good results when the user is more interested in description. In this case, the induced models should present other properties such as novelty, interestingness and so forth. In this paper we present a research work based in Multi-Objective Evolutionary Computing to construct individual knowledge rules targeting arbitrary user-defined criteria via objective quality measures such as precision, support, novelty etc. This paper also presents a comparison among multi-objective and ranking composition techniques. It is shown that multi-objective-based methods attain better results than ranking-based methods, both in terms of solution dominance and diversity of solutions in the Pareto front.",10.1109/ICMLA.2010.25,Multi-Objective Machine Learning;Knowledge Discovery in Databases;Evolutionary Computing,,0.0,1.0
Autonomous Virulence Adaptation Improves Coevolutionary Optimization,J. Cartlidge; D. Ait-Boudaoud,IEEE Transactions on Evolutionary Computation,2011.0,"A novel approach for the autonomous virulence adaptation (AVA) of competing populations in a coevolutionary optimization framework is presented. Previous work has demonstrated that setting an appropriate virulence, v, of populations accelerates coevolutionary optimization by avoiding detrimental periods of disengagement. However, since the likelihood of disengagement varies both between systems and over time, choosing the ideal value of v is problematic. The AVA technique presented here uses a machine learning approach to continuously tune v as system engagement varies. In a simple, abstract domain, AVA is shown to successfully adapt to the most productive values of v. Further experiments, in more complex domains of sorting networks and maze navigation, demonstrate AVA's efficiency over reduced virulence and the layered Pareto coevolutionary archive.",10.1109/TEVC.2010.2073471,Autonomous virulence adaptation;coevolution;disengagement;genetic algorithms;machine learning;maze navigation;optimization methods;reduced virulence;sorting networks,6.0,0.0,1.0
Heuristically-Accelerated Multiagent Reinforcement Learning,R. A. C. Bianchi; M. F. Martins; C. H. C. Ribeiro; A. H. R. Costa,IEEE Transactions on Cybernetics,2014.0,"This paper presents a novel class of algorithms, called Heuristically-Accelerated Multiagent Reinforcement Learning (HAMRL), which allows the use of heuristics to speed up well-known multiagent reinforcement learning (RL) algorithms such as the Minimax-Q. Such HAMRL algorithms are characterized by a heuristic function, which suggests the selection of particular actions over others. This function represents an initial action selection policy, which can be handcrafted, extracted from previous experience in distinct domains, or learnt from observation. To validate the proposal, a thorough theoretical analysis proving the convergence of four algorithms from the HAMRL class (HAMMQ, HAMQ(λ), HAMQS, and HAMS) is presented. In addition, a comprehensive systematical evaluation was conducted in two distinct adversarial domains. The results show that even the most straightforward heuristics can produce virtually optimal action selection policies in much fewer episodes, significantly improving the performance of the HAMRL over vanilla RL algorithms.",10.1109/TCYB.2013.2253094,Artificial intelligence;heuristic algorithms;machine learning;multiagent systems,53.0,0.0,1.0
On Stabilizing Generative Adversarial Training With Noise,S. Jenni; P. Favaro,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2019.0,"We present a novel method and analysis to train generative adversarial networks (GAN) in a stable manner. As shown in recent analysis, training is often undermined by the probability distribution of the data being zero on neighborhoods of the data space. We notice that the distributions of real and generated data should match even when they undergo the same filtering. Therefore, to address the limited support problem we propose to train GANs by using different filtered versions of the real and generated data distributions. In this way, filtering does not prevent the exact matching of the data distribution, while helping training by extending the support of both distributions. As filtering we consider adding samples from an arbitrary distribution to the data, which corresponds to a convolution of the data distribution with the arbitrary one. We also propose to learn the generation of these samples so as to challenge the discriminator in the adversarial training. We show that our approach results in a stable and well-behaved training of even the original minimax GAN formulation. Moreover, our technique can be incorporated in most modern GAN formulations and leads to a consistent improvement on several common datasets.",10.1109/CVPR.2019.01242,Image and Video Synthesis;Deep Learning,6.0,0.0,1.0
Partially Adversarial Learning and Adaptation,J. -T. Chien; Y. -Y. Lyu,2019 27th European Signal Processing Conference (EUSIPCO),2019.0,"An image classification system for a specific target domain is usually trained with initialization from a source domain given with a large number of classes, particularly in an application of image recognition. The classes in target domain are usually seen as a subset in source domain. Partial domain adaptation aims to tackle this generalization issue where no labeled data are provided in target domain. This paper presents an adversarial learning for partial domain adaptation where a symmetric metric based on the Wasserstein distance is adopted in an adversarial learning objective. We build a Wasserstein partial transfer network where the Wasserstein adversarial objective is jointly optimized to partially transfer the relevance knowledge from source to target domains. The geometric property for optimal transport is assured to mitigate the gradient vanishing problem in adversarial training. The neural network components for feature extraction, relevance transfer, domain matching and task classification are jointly trained by solving a minimax optimization over multiple objectives. Experiments on image classification show the merits of the proposed partially adversarial domain adaptation over different tasks.",10.23919/EUSIPCO.2019.8903147,image classification;domain adaptation;deep learning;adversarial learning;partial transfer,5.0,0.0,1.0
Transferable Representation Learning with Deep Adaptation Networks,M. Long; Y. Cao; Z. Cao; J. Wang; M. I. Jordan,IEEE Transactions on Pattern Analysis and Machine Intelligence,2019.0,"Domain adaptation studies learning algorithms that generalize across source domains and target domains that exhibit different distributions. Recent studies reveal that deep neural networks can learn transferable features that generalize well to similar novel tasks. However, as deep features eventually transition from general to specific along the network, feature transferability drops significantly in higher task-specific layers with increasing domain discrepancy. To formally reduce the effects of this discrepancy and enhance feature transferability in task-specific layers, we develop a novel framework for deep adaptation networks that extends deep convolutional neural networks to domain adaptation problems. The framework embeds the deep features of all task-specific layers into reproducing kernel Hilbert spaces (RKHSs) and optimally matches different domain distributions. The deep features are made more transferable by exploiting low-density separation of target-unlabeled data in very deep architectures, while the domain discrepancy is further reduced via the use of multiple kernel learning that enhances the statistical power of kernel embedding matching. The overall framework is cast in a minimax game setting. Extensive empirical evidence shows that the proposed networks yield state-of-the-art results on standard visual domain-adaptation benchmarks.",10.1109/TPAMI.2018.2868685,Domain adaptation;deep learning;convolutional neural network;two-sample test;multiple kernel learning,120.0,0.0,1.0
Unsupervised Visual Domain Adaptation: A Deep Max-Margin Gaussian Process Approach,M. Kim; P. Sahu; B. Gholami; V. Pavlovic,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2019.0,"For unsupervised domain adaptation, the target domain error can be provably reduced by having a shared input representation that makes the source and target domains indistinguishable from each other. Very recently it has been shown that it is not only critical to match the marginal input distributions, but also align the output class distributions. The latter can be achieved by minimizing the maximum discrepancy of predictors. In this paper, we take this principle further by proposing a more systematic and effective way to achieve hypothesis consistency using Gaussian processes (GP). The GP allows us to induce a hypothesis space of classifiers from the posterior distribution of the latent random functions, turning the learning into a large-margin posterior separation problem, significantly easier to solve than previous approaches based on adversarial minimax optimization. We formulate a learning objective that effectively influences the posterior to minimize the maximum discrepancy. This is shown to be equivalent to maximizing margins and minimizing uncertainty of the class predictions in the target domain. Empirical results demonstrate that our approach leads to state-to-the-art performance superior to existing methods on several challenging benchmarks for domain adaptation.",10.1109/CVPR.2019.00451,Deep Learning;Recognition: Detection;Categorization;Retrieval; Statistical Learning,16.0,0.0,1.0
Leveraging the Invariant Side of Generative Zero-Shot Learning,J. Li; M. Jing; K. Lu; Z. Ding; L. Zhu; Z. Huang,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2019.0,"Conventional zero-shot learning (ZSL) methods generally learn an embedding, e.g., visual-semantic mapping, to handle the unseen visual samples via an indirect manner. In this paper, we take the advantage of generative adversarial networks (GANs) and propose a novel method, named leveraging invariant side GAN (LisGAN), which can directly generate the unseen features from random noises which are conditioned by the semantic descriptions. Specifically, we train a conditional Wasserstein GANs in which the generator synthesizes fake unseen features from noises and the discriminator distinguishes the fake from real via a minimax game. Considering that one semantic description can correspond to various synthesized visual samples, and the semantic description, figuratively, is the soul of the generated features, we introduce soul samples as the invariant side of generative zero-shot learning in this paper. A soul sample is the meta-representation of one class. It visualizes the most semantically-meaningful aspects of each sample in the same category. We regularize that each generated sample (the varying side of generative ZSL) should be close to at least one soul sample (the invariant side) which has the same class label with it. At the zero-shot recognition stage, we propose to use two classifiers, which are deployed in a cascade way, to achieve a coarse-to-fine result. Experiments on five popular benchmarks verify that our proposed approach can outperform state-of-the-art methods with significant improvements.",10.1109/CVPR.2019.00758,Recognition: Detection;Categorization;Retrieval;Deep Learning ; Image and Video Synthesis,62.0,0.0,1.0
Automatic Decision Making for Parameters in Kernel Method,Y. Pei,2019 IEEE Symposium Series on Computational Intelligence (SSCI),2019.0,"We propose to use the relationship between the parameter of kernel function and its decisional angle or distance metrics for selecting the optimal setting of the parameter of kernel functions in kernel method-based algorithms. Kernel method is established in the reproducing kernel Hilbert space, the angle and distance are two metrics in such space. We analyse and investigate the relationship between the parameter of kernel function and the metrics (distance or angle) in the reproducing kernel Hilbert space. We design a target function of optimization to model the relationship between these two variables, and found that (1) the landscape shapes of parameter and the metrics are the same in Gaussian kernel function because the norm of all the vectors are equal to one in reproducing kernel Hilbert space; (2) the landscape monotonicity of that are opposite in polynomial kernel function from that of Gaussian kernel. The monotonicity of designed target functions of optimization using Gaussian kernel and polynomial kernel is different as well. The distance metric and angle metric have different distribution characteristics for the decision of parameter setting in kernel function. It needs to balance these two metrics when selecting a proper parameter of the kernel function in kernel-based algorithms. We use evolutionary multi-objective optimization algorithms to obtain the Pareto solutions for optimal selection of the parameter in kernel functions. We found that evolutionary multi-objective optimization algorithms are useful tools to balance the distance metric and angle metric in the decision of parameter setting in kernel method-based algorithms.",10.1109/SSCI44817.2019.9002691,kernel method;kernel function;reproducing kernel Hilbert space;machine learning;decision making;evolutionary multi-objective optimization,2.0,0.0,1.0
Fast and Accurate PPA Modeling with Transfer Learning,W. R. Davis; P. Franzon; L. Francisco; B. Huggins; R. Jain,2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD),2021.0,"The power, performance and area (PPA) of digital blocks can vary 10:1 based on their synthesis, place, and route tool recipes. With rapid increase in number of PVT corners and complexity of logic functions approaching 10M gates, industry has an acute need to minimize the human resources, compute servers, and EDA licenses needed to achieve a Pareto optimal recipe. We first present models for fast accurate PPA prediction that can reduce the manual optimization iterations with EDA tools. Secondly we investigate techniques to automate the PPA optimization using evolutionary algorithms. For PPA prediction, a baseline model is trained on a known design using Latin hypercube sample runs of the EDA tool, and transfer learning is then used to train the model for an unseen design. For a known design the baseline needed 150 training runs to achieve a 95% accuracy. With transfer learning the same accuracy was achieved on a different (unseen) design in only 15 runs indicating the viability of transfer learning to generalize PPA models. The PPA optimization technique, based on evolutionary algorithms, effectively combines the PPA modeling and optimization. Our approach reached the same PPA solution as human designers in the same or fewer runs for a CORTEX-M0 system design. This shows potential for automating the recipe optimization without needing more runs than a human designer would need.",10.1109/ICCAD51958.2021.9643533,PPA;Machine Learning;Power;Performance;Area;Gradient Boost;Neural Network;Transfer Learning;Surrogate Modeling,,0.0,1.0
Multi-Objective Learning of Multi-Dimensional Bayesian Classifiers,J. D. Rodríguez; J. A. Lozano,2008 Eighth International Conference on Hybrid Intelligent Systems,2008.0,"Multi-dimensional classification is a generalization of supervised classification that considers more than one class variable to classify. In this paper we review the existing multi-dimensional Bayesian classifiers and introduce a new one: the KDB multi-dimensional classifier. Then we define different classification rules for multi-dimensional scope. Finally, we introduce a structural learning approach of a multi-dimensional Bayesian classifier based on the multi-objective evolutionary algorithm NSGA-II. The solution of the learning approach is a Pareto front representing different multi-dimensional classifiers and their accuracy values for the different classes, so a decision maker can easily choose the classifier which is more interesting for the particular problem and domain.",10.1109/HIS.2008.143,Machine Learning;multi-dimensional classification;Bayesian classifiers;multi-objective;NSGA-II,16.0,0.0,1.0
Distilling Optimal Neural Networks: Rapid Search in Diverse Spaces,B. Moons; P. Noorzad; A. Skliar; G. Mariani; D. Mehta; C. Lott; T. Blankevoort,2021 IEEE/CVF International Conference on Computer Vision (ICCV),2021.0,"Current state-of-the-art Neural Architecture Search (NAS) methods neither efficiently scale to multiple hardware platforms, nor handle diverse architectural search-spaces. To remedy this, we present DONNA (Distilling Optimal Neural Network Architectures), a novel pipeline for rapid, scalable and diverse NAS, that scales to many user scenarios. DONNA consists of three phases. First, an accuracy predictor is built using blockwise knowledge distillation from a reference model. This predictor enables searching across diverse networks with varying macro-architectural parameters such as layer types and attention mechanisms, as well as across micro-architectural parameters such as block repeats and expansion rates. Second, a rapid evolutionary search finds a set of pareto-optimal architectures for any scenario using the accuracy predictor and on-device measurements. Third, optimal models are quickly fine-tuned to training-from-scratch accuracy. DONNA is up to 100× faster than MNasNet in finding state-of-the-art architectures on-device. Classifying ImageNet, DONNA architectures are 20% faster than EfficientNet-B0 and Mo-bileNetV2 on a Nvidia V100 GPU and 10% faster with 0.5% higher accuracy than MobileNetV2-1.4x on a Samsung S20 smartphone. In addition to NAS, DONNA is used for search-space extension and exploration, as well as hardware-aware model compression.",10.1109/ICCV48922.2021.01201,Machine learning architectures and formulations; Efficient training and inference methods; Optimization and learning methods; Recognition and classification; Segmentation;grouping and shape,,0.0,1.0
Probabilistic Sequential Multi-Objective Optimization of Convolutional Neural Networks,Z. Yin; W. Gross; B. H. Meyer,"2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)",2020.0,"With the advent of deeper, larger and more complex convolutional neural networks (CNN), manual design has become a daunting task, especially when hardware performance must be optimized. Sequential model-based optimization (SMBO) is an efficient method for hyperparameter optimization on highly parameterized machine learning (ML) algorithms, able to find good configurations with a limited number of evaluations by predicting the performance of candidates before evaluation. A case study on MNIST shows that SMBO regression model prediction error significantly impedes search performance in multi-objective optimization. To address this issue, we propose probabilistic SMBO, which selects candidates based on probabilistic estimation of their Pareto efficiency. With a formulation that incorporates error in accuracy prediction and uncertainty in latency measurement, probabilistic Pareto efficiency quantifies a candidate's quality in two ways: its likelihood of being Pareto optimal, and the expected number of current Pareto optimal solutions that it will dominate. We evaluate our proposed method on four image classification problems. Compared to a deterministic approach, probabilistic SMBO consistently generates Pareto optimal solutions that perform better, and that are competitive with state-of-the-art efficient CNN models, offering tremendous speedup in inference latency while maintaining comparable accuracy.",10.23919/DATE48585.2020.9116535,,2.0,0.0,1.0
Learning with Privileged Tasks,Y. Song; Z. Lou; S. You; E. Yang; F. Wang; C. Qian; C. Zhang; X. Wang,2021 IEEE/CVF International Conference on Computer Vision (ICCV),2021.0,"Multi-objective multi-task learning aims to boost the performance of all tasks by leveraging their correlation and conflict appropriately. Nevertheless, in real practice, users may have preference for certain tasks, and other tasks simply serve as privileged or auxiliary tasks to assist the training of target tasks. The privileged tasks thus possess less or even no priority in the final task assessment by users. Motivated by this, we propose a privileged multiple descent algorithm to arbitrate the learning of target tasks and privileged tasks. Concretely, we introduce a privileged parameter so that the optimization direction does not necessarily follow the gradient from the privileged tasks, but concentrates more on the target tasks. Besides, we also encourage a priority parameter for the target tasks to control the potential distraction of optimization direction from the privileged tasks. In this way, the optimization direction can be more aggressively determined by weighting the gradients among target and privileged tasks, and thus highlight more the performance of target tasks under the unified multi-task learning context. Extensive experiments on synthetic and real-world datasets indicate that our method can achieve versatile Pareto solutions under varying preference for the target tasks.",10.1109/ICCV48922.2021.01051,Machine learning architectures and formulations; Recognition and classification,,0.0,1.0
Substituting Convolutions for Neural Network Compression,E. J. Crowley; G. Gray; J. Turner; A. Storkey,IEEE Access,2021.0,"Many practitioners would like to deploy deep, convolutional neural networks in memory-limited scenarios, e.g., on an embedded device. However, with an abundance of compression techniques available it is not obvious how to proceed; many bring with them additional hyperparameter tuning, and are specific to particular network types. In this paper, we propose a simple compression technique that is general, easy to apply, and requires minimal tuning. Given a large, trained network, we propose (i) substituting its expensive convolutions with cheap alternatives, leaving the overall architecture unchanged; (ii) treating this new network as a student and training it with the original as a teacher through distillation. We demonstrate this approach separately for (i) networks predominantly consisting of full 3 ×3 convolutions and (ii) 1 ×1 or pointwise convolutions which together make up the vast majority of contemporary networks. We are able to leverage a number of methods that have been developed as efficient alternatives to fully-connected layers for pointwise substitution, allowing us provide Pareto-optimal benefits in efficiency/accuracy.",10.1109/ACCESS.2021.3086321,Machine learning;deep neural networks;computer vision;DNN compression,,0.0,1.0
FasTrCaps: An Integrated Framework for Fast yet Accurate Training of Capsule Networks,A. Marchisio; B. Bussolino; A. Colucci; M. A. Hanif; M. Martina; G. Masera; M. Shafique,2020 International Joint Conference on Neural Networks (IJCNN),2020.0,"Recently, Capsule Networks (CapsNets) have shown improved performance compared to the traditional Convolutional Neural Networks (CNNs), by encoding and preserving spatial relationships between the detected features in a better way. This is achieved through the so-called Capsules (i.e., groups of neurons) that encode both the instantiation probability and the spatial information. However, one of the major hurdles in the wide adoption of CapsNets is their gigantic training time, which is primarily due to the relatively higher complexity of their new constituting elements that are different from CNNs.In this paper, we implement different optimizations in the training loop of the CapsNets, and investigate how these optimizations affect their training speed and the accuracy. Towards this, we propose a novel framework FasTrCaps that integrates multiple lightweight optimizations and a novel learning rate policy called WarmAdaBatch (that jointly performs warm restarts and adaptive batch size), and steers them in an appropriate way to provide high training-loop speedup at minimal accuracy loss. We also propose weight sharing for capsule layers. The goal is to reduce the hardware requirements of CapsNets by removing unused/redundant connections and capsules, while keeping high accuracy through tests of different learning rate policies and batch sizes. We demonstrate that one of the solutions generated by the FasTrCaps framework can achieve 58.6% reduction in the training time, while preserving the accuracy (even 0.12% accuracy improvement for the MNIST dataset), compared to the CapsNet by Google Brain [25]. Moreover, the Pareto-optimal solutions generated by FasTrCaps can be leveraged to realize trade-offs between training time and achieved accuracy. We have open-sourced our framework on GitHub<sup>1</sup>.",10.1109/IJCNN48605.2020.9207533,Machine Learning;Capsule Networks;Training;Accuracy;Efficiency;Performance;Weight Sharing;Decoder;Batch Sizing;Adaptivity,4.0,0.0,1.0
Statistical Relational Learning for Game Theory,M. Lippi,IEEE Transactions on Computational Intelligence and AI in Games,2016.0,"In this paper, we motivate the use of models and algorithms from the area of Statistical Relational Learning (SRL) as a framework for the description and the analysis of games. SRL combines the powerful formalism of first-order logic with the capability of probabilistic graphical models in handling uncertainty in data and representing dependencies between random variables: for this reason, SRL models can be effectively used to represent several categories of games, including games with partial information, graphical games and stochastic games. Inference algorithms can be used to approach the opponent modeling problem, as well as to find Nash equilibria or Pareto optimal solutions. Structure learning algorithms can be applied, in order to automatically extract probabilistic logic clauses describing the strategies of an opponent with a high-level, human-interpretable formalism. Experiments conducted using Markov logic networks, one of the most used SRL frameworks, show the potential of the approach.",10.1109/TCIAIG.2015.2490279,Machine learning;probabilistic logic,2.0,0.0,1.0
Automated Test Input Generation for Convolutional Neural Networks by Implementing Multi-objective Evolutionary Algorithms,L. ZHANG; H. SATO,2020 Eighth International Symposium on Computing and Networking Workshops (CANDARW),2020.0,"Deep Neural Networks (DNNs) have been widely applied in safety- and security-critical aspects, where the robustness of the system is of great significance, especially for corner case inputs. Traditionally, a DNN is tested with manually labeled data, which is not only labor-consuming, but also unable to contain statistically rare case inputs.In our work, we design, implement and evaluate the test input generation framework guided by multi-objective functions. The multi-objective functions are formed from neuron coverage, behavioral divergence and perturbation degree. We leverage evolutionary algorithms (EAs) to resolve such optimization problem by generating approximation to Pareto-optimal solutions. By implementing our framework, we successfully generated more than 6,000 test inputs for a convolutional neural network. And the generated test inputs help to improve the system’s accuracy by up to 4.4%.",10.1109/CANDARW51189.2020.00040,Deep learning testing;Automated test input generation;Evolutionary algorithms,,0.0,1.0
Optimal v-SVM parameter estimation using multi objective evolutionary algorithms,J. Ethridge; G. Ditzler; R. Polikar,IEEE Congress on Evolutionary Computation,2010.0,"Using a machine learning algorithm for a given application often requires tuning design parameters of the classifier to obtain optimal classification performance without overfitting. In this contribution, we present an evolutionary algorithm based approach for multi-objective optimization of the sensitivity and specificity of a v-SVM. The v-SVM is often preferred over the standard C-SVM due to smaller dynamic range of the v parameter compared to the unlimited dynamic range of the C parameter. Instead of looking for a single optimization result, we look for a set of optimal solutions that lie along the Pareto optimality front. The traditional advantage of using the Pareto optimality is of course the flexibility to choose any of the solutions that lies on the Pareto optimality front. However, we show that simply maximizing sensitivity and specificity over the Pareto front leads to parameters that appear to be mathematically optimal yet still cause overfitting. We propose a multiple objective optimization approach with three objective functions to find additional parameter values that do not cause overfitting.",10.1109/CEC.2010.5586029,multi-objective optimization;v-SVM;evolutionary algorithms,3.0,0.0,1.0
cSmartML: A Meta Learning-Based Framework for Automated Selection and Hyperparameter Tuning for Clustering,R. ElShawi; H. Lekunze; S. Sakr,2021 IEEE International Conference on Big Data (Big Data),2021.0,"Novel technologies in automated machine learning ease the complexity of algorithm selection and hyper-parameter optimization. However, these are usually restricted to supervised learning tasks such as classification and regression, while unsupervised learning remains a largely unexplored problem. In this paper, we offer a solution for automating machine learning specifically for the case of unsupervised learning with clustering, in a domain-agnostic manner. This is achieved through a combination of state-of-the-art processes based on meta-learning for algorithm and evaluation criteria selection, and evolutionary algorithm for hyper-parameter tuning. We introduce a robust and scalable interactive tool, named cSmartML, built on scikit-learn with 8 clustering algorithms. In order to capture more than a single measure of goodness of the output clustering solution, cSmartML optimizes multiple objective functions. A pareto-approach evaluates each objective simultaneously for each clustering solution. On each of the 27 real and synthetic benchmark datasets, we show that the performance of cSmartML is often much better than using standard selection and hyper-parameter optimization methods. In addition, experimentation reveals that cSmartML takes advantage of the defined objective functions on multi-objective functions framework.",10.1109/BigData52589.2021.9671542,clustering;meta-learning;hyper-parameter optimization,,0.0,1.0
Robust Model Predictive Control of Nonlinear Systems With Unmodeled Dynamics and Bounded Uncertainties Based on Neural Networks,Z. Yan; J. Wang,IEEE Transactions on Neural Networks and Learning Systems,2014.0,"This paper presents a neural network approach to robust model predictive control (MPC) for constrained discrete-time nonlinear systems with unmodeled dynamics affected by bounded uncertainties. The exact nonlinear model of underlying process is not precisely known, but a partially known nominal model is available. This partially known nonlinear model is first decomposed to an affine term plus an unknown high-order term via Jacobian linearization. The linearization residue combined with unmodeled dynamics is then modeled using an extreme learning machine via supervised learning. The minimax methodology is exploited to deal with bounded uncertainties. The minimax optimization problem is reformulated as a convex minimization problem and is iteratively solved by a two-layer recurrent neural network. The proposed neurodynamic approach to nonlinear MPC improves the computational efficiency and sheds a light for real-time implementability of MPC technology. Simulation results are provided to substantiate the effectiveness and characteristics of the proposed approach.",10.1109/TNNLS.2013.2275948,Extreme learning machine (ELM);real-time optimization;recurrent neural networks (RNNs);robust model predictive control (MPC);unmodeled dynamics,103.0,0.0,1.0
Pattern Classification by Evolutionary RBF Networks Ensemble Based on Multi-objective Optimization,N. Kondo; T. Hatanaka; K. Uosaki,The 2006 IEEE International Joint Conference on Neural Network Proceedings,2006.0,"In this paper, evolutionary multi-objective selection method of RBF networks structure and its application to the ensemble learning is considered. The candidates of RBF network structure are encoded into the chromosomes in GAs. Then, they evolve toward Pareto-optimal front defined by several objective functions concerning with model accuracy, model complexity and model smoothness. RBF network ensemble is constructed of the obtained Pareto-optimal models since such models are diverse. This method is applied to the pattern classification problem. Experiments on the benchmark problem demonstrate that the proposed method has comparable generalization ability to conventional ensemble methods.",10.1109/IJCNN.2006.247224,,3.0,0.0,1.0
Deconstructing Generative Adversarial Networks,B. Zhu; J. Jiao; D. Tse,IEEE Transactions on Information Theory,2020.0,"Generative Adversarial Networks (GANs) are a thriving unsupervised machine learning technique that has led to significant advances in various fields such as computer vision, natural language processing, among others. However, GANs are known to be difficult to train and usually suffer from mode collapse and the discriminator winning problem. To interpret the empirical observations of GANs and design better ones, we deconstruct the study of GANs into three components and make the following contributions. Formulation: we propose a perturbation view of the population target of GANs. Building on this interpretation, we show that GANs can be connected to the robust statistics framework, and propose a novel GAN architecture, termed as Cascade GANs, to provably recover meaningful low-dimensional generator approximations when the real distribution is high-dimensional and corrupted by outliers. Generalization: given a population target of GANs, we design a systematic principle, projection under admissible distance, to design GANs to meet the population requirement using only finite samples. We implement our principle in three cases to achieve polynomial and sometimes near-optimal sample complexities: (1) learning an arbitrary generator under an arbitrary pseudonorm; (2) learning a Gaussian location family under total variation distance, where we utilize our principle to provide a new proof for the near-optimality of the Tukey median viewed as GANs; (3) learning a low-dimensional Gaussian approximation of a high-dimensional arbitrary distribution under Wasserstein distance. We demonstrate a fundamental trade-off in the approximation error and statistical error in GANs, and demonstrate how to apply our principle in practice with only empirical samples to predict how many samples would be sufficient for GANs in order not to suffer from the discriminator winning problem. Optimization: we demonstrate alternating gradient descent is provably not locally asymptotically stable in optimizing the GAN formulation of PCA. We found that the minimax duality gap being non-zero might be one of the causes, and propose a new GAN architecture whose duality gap is zero, where the value of the game is equal to the previous minimax value (not the maximin value). We prove the new GAN architecture is globally asymptotically stable in solving PCA under alternating gradient descent.",10.1109/TIT.2020.2983698,Generative Adversarial Networks (GANs);wasserstein distance;optimal transport;generalization error;information-theoretic limit;robust statistics,3.0,0.0,1.0
Surrogate-based Multi-Objective Particle Swarm Optimization,L. V. Santana-Quintero; C. A. Coello Coello; A. G. Hernandez-Diaz; J. M. O. Velazquez,2008 IEEE Swarm Intelligence Symposium,2008.0,"This paper presents a new algorithm that approximates real function evaluations using supervised learning with a surrogate method called support vector machine (SVM). We perform a comparative study among different leader selection schemes in a Multi-Objective Particle Swarm Optimizer (MOPSO), in order to determine the most appropriate approach to be adopted for solving the sort of problems of our interest. The resulting hybrid presents a poor spread of solutions, which motivates the introduction of a second phase to our algorithm, in which an approach called rough sets is adopted in order to improve the spread of solutions along the Pareto front. Rough sets are used as a local search engine, which is able to generate solutions in the neighborhood of the nondominated solutions previously generated by the surrogate-based algorithm. The resulting approach is able to generate reasonably good approximations of the Pareto front of problems of up to 30 decision variables with only 2,000 fitness function evaluations. Our results are compared with respect to the NSGA-II, which is a multi-objective evolutionary algorithm representative of the state-of-the-art in the area.",10.1109/SIS.2008.4668300,Multi-objective optimization;surrogates;support vector machines;PSO;rough sets;hybrid algorithms,5.0,0.0,1.0
Supervised tensor learning,Dacheng Tao; Xuelong Li; Weiming Hu; S. Maybank; Xindong Wu,Fifth IEEE International Conference on Data Mining (ICDM'05),2005.0,"This paper aims to take general tensors as inputs for supervised learning. A supervised tensor learning (STL) framework is established for convex optimization based learning techniques such as support vector machines (SVM) and minimax probability machines (MPM). Within the STL framework, many conventional learning machines can be generalized to take n/sup th/-order tensors as inputs. We also study the applications of tensors to learning machine design and feature extraction by linear discriminant analysis (LDA). Our method for tensor based feature extraction is named the tenor rank-one discriminant analysis (TR1DA). These generalized algorithms have several advantages: 1) reduce the curse of dimension problem in machine learning and data mining; 2) avoid the failure to converge; and 3) achieve better separation between the different categories of samples. As an example, we generalize MPM to its STL version, which is named the tensor MPM (TMPM). TMPM learns a series of tensor projections iteratively. It is then evaluated against the original MPM. Our experiments on a binary classification problem show that TMPM significantly outperforms the original MPM.",10.1109/ICDM.2005.139,,4.0,0.0,1.0
A Design Flow for Mapping Spiking Neural Networks to Many-Core Neuromorphic Hardware,S. Song; M. L. Varshika; A. Das; N. Kandasamy,2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD),2021.0,"The design of many-core neuromorphic hardware is becoming increasingly complex as these systems are now expected to execute large machine-learning models. A predictable design flow is needed to guarantee real-time performance such as latency and throughput without significantly increasing the buffer requirement of computing cores. Synchronous Data Flow Graphs (SDFGs) have been previously used for predictable mapping of streaming applications to multiprocessor systems. We propose an SDFG-based design flow to map spiking neural networks (SNNs) to many-core neuromorphic hardware with the objective of exploring the tradeoff between throughput and buffer-size requirements. The proposed design flow integrates an iterative partitioning approach based on Kernighan-Lin graph partitioning heuristic to create SNN clusters such that each cluster can be mapped to a core of the hardware. The partitioning approach minimizes inter-cluster spike communication, which improves latency on the shared interconnect of the hardware. Next, the design flow maps clusters to cores using Particle Swarm Optimization (PSO), an evolutionary algorithm, while exploring the design space of throughput and buffer size. Pareto-optimal mappings are retained from the design flow, allowing system designers to select a Pareto mapping that satisfies throughput and buffer-size requirements of the design. We evaluated the developed design flow using five large-scale convolutional neural network (CNN) models. Results demonstrate 63% higher maximum throughput and 10% lower buffer-size requirement compared to state-of-the-art dataflow-based mapping solutions.",10.1109/ICCAD51958.2021.9643500,neuromorphic computing;spiking neural network (SNN);design-space exploration (DSE);oxide-based resistive random access memory (OxRRAM);dataflow,2.0,0.0,1.0
Multi-Objective Model Selection via Racing,T. Zhang; M. Georgiopoulos; G. C. Anagnostopoulos,IEEE Transactions on Cybernetics,2016.0,"Model selection is a core aspect in machine learning and is, occasionally, multi-objective in nature. For instance, hyper-parameter selection in a multi-task learning context is of multi-objective nature, since all the tasks' objectives must be optimized simultaneously. In this paper, a novel multi-objective racing algorithm (RA), namely S-Race, is put forward. Given an ensemble of models, our task is to reliably identify Pareto optimal models evaluated against multiple objectives, while minimizing the total computational cost. As a RA, S-Race attempts to eliminate non-promising models with confidence as early as possible, so as to concentrate computational resources on promising ones. Simultaneously, it addresses the problem of multi-objective model selection (MOMS) in the sense of Pareto optimality. In S-Race, the nonparametric sign test is utilized for pair-wise dominance relationship identification. Moreover, a discrete Holm's step-down procedure is adopted to control the family-wise error rate of the set of hypotheses made simultaneously. The significance level assigned to each family is adjusted adaptively during the race. In order to illustrate its merits, S-Race is applied on three MOMS problems: (1) selecting support vector machines for classification; (2) tuning the parameters of artificial bee colony algorithms for numerical optimization; and (3) constructing optimal hybrid recommendation systems for movie recommendation. The experimental results confirm that S-Race is an efficient and effective MOMS algorithm compared to a brute-force approach.",10.1109/TCYB.2015.2456187,Discrete Holm’s step-down procedure;model selection;multi-objective optimization;racing algorithm (RA);sign test (ST),6.0,0.0,1.0
Simultaneous feature selection and clustering for categorical features using multi objective genetic algorithm,D. Dutta; P. Dutta; J. Sil,2012 12th International Conference on Hybrid Intelligent Systems (HIS),2012.0,"Clustering is unsupervised learning where ideally class levels and number of clusters (K) are not known. K-clustering can be categorized as semi-supervised learning where K is known. Here we have considered K-Clustering with simultaneous feature selection. Feature subset selection helps to identify relevant features for clustering, increase understandability, better scalability and improve accuracy. Here we have used two measures, intra-cluster distance (Homogeneity, H) and inter-cluster distances (Separation, S) for clustering. Measures are using mod distance per feature suitable for categorical features (attributes). Rather than combining H and S to frame the problem as single objective optimization problem, we use multi objective genetic algorithm (MOGA) to find out diverse solutions near to Pareto optimal front in the two-dimensional objective space. Each evolved solution represents a set of cluster modes (CMs) build by selected feature subset. Here, K-modes is hybridized with MOGA. We have used hybridized GA to combine global searching powers of GA with local searching powers of K-modes. Considering context sensitivity, we have used a special crossover operator called “pairwise crossover” and “substitution”. The main contribution of this paper is simultaneous dimensionality reduction and optimization of objectives using MOGA. Results on 3 benchmark data sets from UCI Machine Learning Repository containing categorical features shows the superiority of the algorithm.",10.1109/HIS.2012.6421332,,6.0,0.0,1.0
Improving generalization of radial basis function network with adaptive multi-objective particle swarm optimization,S. N. Qasem; S. M. H. Shamsuddin,"2009 IEEE International Conference on Systems, Man and Cybernetics",2009.0,"In this paper, an adaptive evolutionary multi-objective selection method of RBF Networks structure is discussed. The candidates of RBF Network structures are encoded into particles in Particle Swarm Optimization (PSO). These particles evolve toward Pareto-optimal front defined by several objective functions with model accuracy and complexity. The problem of unsupervised and supervised learning is discussed with Adaptive Multi-Objective PSO (AMOPSO). This study suggests an approach of RBF Network training through simultaneous optimization of architectures and weights with Adaptive PSO-based multi-objective algorithm. Our goal is to determine whether Adaptive Multi-objective PSO can train RBF Networks, and the performance is validated on accuracy and complexity. The experiments are conducted on two benchmark datasets obtained from the machine learning repository. The results show that our proposed method provides an effective means for training RBF Networks that is competitive with PSO-based multi-objective algorithm.",10.1109/ICSMC.2009.5346876,Radial basis function network;Adaptive Multi-objective particle swarm optimization;Multi-Objective particle swarm optimization,3.0,0.0,1.0
Radial basis function Network based on multi-objective particle swarm optimization,S. N. Qasem; S. M. H. Shamsuddin,2009 6th International Symposium on Mechatronics and its Applications,2009.0,"The problem of unsupervised and supervised learning is discussed within the context of multi-objective optimization. In this paper, an evolutionary multi-objective selection method of RBF networks structure is discussed. The candidates of RBF network structure are encoded into the particles in PSO. Then, they evolve toward Pareto-optimal front defined by several objective functions concerning with model accuracy and model complexity. This study suggests an approach of RBF network training through simultaneous optimization of architectures and weights with PSO-based multi-objective algorithm. Our goal is to determine whether multi-objective PSO can train RBF networks, and the performance is validated on accuracy and complexity. The experiments are conducted on benchmark datasets obtained from the UCI machine learning repository. The results show that our proposed method provides an effective means for training RBF networks that is competitive with other evolutionary computational-based methods.",10.1109/ISMA.2009.5164833,,11.0,0.0,1.0
Characterising Across-Stack Optimisations for Deep Convolutional Neural Networks,J. Turner; J. Cano; V. Radu; E. J. Crowley; M. O’Boyle; A. Storkey,2018 IEEE International Symposium on Workload Characterization (IISWC),2018.0,"Convolutional Neural Networks (CNNs) are extremely computationally demanding, presenting a large barrier to their deployment on resource-constrained devices. Since such systems are where some of their most useful applications lie (e.g. obstacle detection for mobile robots, vision-based medical assistive technology), significant bodies of work from both machine learning and systems communities have attempted to provide optimisations that will make CNNs available to edge devices. In this paper we unify the two viewpoints in a Deep Learning Inference Stack and take an across-stack approach by implementing and evaluating the most common neural network compression techniques (weight pruning, channel pruning, and quantisation) and optimising their parallel execution with a range of programming approaches (OpenMP, OpenCL) and hardware architectures (CPU, GPU). We provide comprehensive Pareto curves to instruct trade-offs under constraints of accuracy, execution time, and memory space.",10.1109/IISWC.2018.8573503,,13.0,0.0,1.0
Learning variable importance to guide recombination,M. Sagawa; H. Aguirre; F. Daolio; A. Liefooghe; B. Derbel; S. Verel; K. Tanaka,2016 IEEE Symposium Series on Computational Intelligence (SSCI),2016.0,"In evolutionary multi-objective optimization, variation operators are crucially important to produce improving solutions, hence leading the search towards the most promising regions of the solution space. In this paper, we propose to use a machine learning modeling technique, namely random forest, in order to estimate, at each iteration in the course of the search process, the importance of decision variables with respect to convergence to the Pareto front. Accordingly, we are able to propose an adaptive mechanism guiding the recombination step with the aim of stressing the convergence of the so-obtained offspring. By conducting an experimental analysis using some of the WFG and DTLZ benchmark test problems, we are able to elicit the behavior of the proposed approach, and to demonstrate the benefits of incorporating machine learning techniques in order to design new efficient adaptive variation mechanisms.",10.1109/SSCI.2016.7850229,,3.0,0.0,1.0
Neural networks designing neural networks: Multi-objective hyper-parameter optimization,S. C. Smithson; Guang Yang; W. J. Gross; B. H. Meyer,2016 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),2016.0,"Artificial neural networks have gone through a recent rise in popularity, achieving state-of-the-art results in various fields, including image classification, speech recognition, and automated control. Both the performance and computational complexity of such models are heavily dependant on the design of characteristic hyper-parameters (e.g., number of hidden layers, nodes per layer, or choice of activation functions), which have traditionally been optimized manually. With machine learning penetrating low-power mobile and embedded areas, the need to optimize not only for performance (accuracy), but also for implementation complexity, becomes paramount. In this work, we present a multi-objective design space exploration method that reduces the number of solution networks trained and evaluated through response surface modelling. Given spaces which can easily exceed 10<sup>20</sup> solutions, manually designing a near-optimal architecture is unlikely as opportunities to reduce network complexity, while maintaining performance, may be overlooked. This problem is exacerbated by the fact that hyper-parameters which perform well on specific datasets may yield sub-par results on others, and must therefore be designed on a per-application basis. In our work, machine learning is leveraged by training an artificial neural network to predict the performance of future candidate networks. The method is evaluated on the MNIST and CIFAR-10 image datasets, optimizing for both recognition accuracy and computational complexity. Experimental results demonstrate that the proposed method can closely approximate the Pareto-optimal front, while only exploring a small fraction of the design space.",10.1145/2966986.2967058,,12.0,0.0,1.0
Universal Learning of Individual Data,Y. Fogel; M. Feder,2019 IEEE International Symposium on Information Theory (ISIT),2019.0,"Universal supervised learning of individual data is considered from an information theoretic point of view in the standard supervised “batch” learning where prediction is done on a test sample once the entire training data is observed. In this individual setting the features and labels, both in the training and the test, are specific individual, deterministic quantities. Prediction loss is naturally measured by the log-loss. The presented results provide a minimax universal learning scheme, termed the Predictive Normalized Maximum Likelihood (pNML) that competes with a “genie” (or reference) that knows the true test label. In addition, a pointwise learnability measure associated with the pNML, for the specific training and test, is provided. This measure may also indicate the performance of the commonly used Empirical Risk Minimizer (ERM) learner.",10.1109/ISIT.2019.8849222,,1.0,0.0,1.0
A Multi-objective Learning Algorithm for RBF Neural Network,I. Kokshenev; A. P. Braga,2008 10th Brazilian Symposium on Neural Networks,2008.0,"In this paper, the problem of multi-objective supervised learning is discussed within the non-evolutionary optimization framework. The proposed MOBJ learning algorithm performs the search of Pareto-optimal models determining weights,width, prototype vectors, and the quantity of basis functions of the RBF network. In combination with the Akaike information criterion, the algorithm provides high quality solutions.",10.1109/SBRN.2008.39,multi-objective learning;radial basis functions;generalization;regularization;LASSO,3.0,0.0,1.0
ADA: Adaptive Deep Log Anomaly Detector,Y. Yuan; S. Srikant Adhatarao; M. Lin; Y. Yuan; Z. Liu; X. Fu,IEEE INFOCOM 2020 - IEEE Conference on Computer Communications,2020.0,"Large private and government networks are often subjected to attacks like data extrusion and service disruption. Existing anomaly detection systems use offline supervised learning and employ experts for labeling. Hence they cannot detect anomalies in real-time. Even though unsupervised algorithms are increasingly used nowadays, they cannot readily adapt to newer threats. Moreover, many such systems also suffer from high cost of storage and require extensive computational resources. In this paper, we propose ADA: Adaptive Deep Log Anomaly Detector, an unsupervised online deep neural network framework that leverages LSTM networks and regularly adapts to newer log patterns to ensure accurate anomaly detection. In ADA, an adaptive model selection strategy is designed to choose pareto-optimal configurations and thereby utilize resources efficiently. Further, a dynamic threshold algorithm is proposed to dictate the optimal threshold based on recently detected events to improve the detection accuracy. We also use the predictions to guide storage of abnormal data and effectively reduce the overall storage cost. We compare ADA with state-of-the-art approaches through leveraging the Los Alamos National Laboratory cyber security dataset and show that ADA accurately detects anomalies with high F1-score ~95% and it is 97 times faster than existing approaches and incurs very low storage cost.",10.1109/INFOCOM41043.2020.9155487,Anomaly detection;deep neural networks;logs;online training;unsupervised;log-normal;threshold,7.0,0.0,1.0
Adversarial Feature Augmentation for Unsupervised Domain Adaptation,R. Volpi; P. Morerio; S. Savarese; V. Murino,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,2018.0,"Recent works showed that Generative Adversarial Networks (GANs) can be successfully applied in unsupervised domain adaptation, where, given a labeled source dataset and an unlabeled target dataset, the goal is to train powerful classifiers for the target samples. In particular, it was shown that a GAN objective function can be used to learn target features indistinguishable from the source ones. In this work, we extend this framework by (i) forcing the learned feature extractor to be domain-invariant, and (ii) training it through data augmentation in the feature space, namely performing feature augmentation. While data augmentation in the image space is a well established technique in deep learning, feature augmentation has not yet received the same level of attention. We accomplish it by means of a feature generator trained by playing the GAN minimax game against source features. Results show that both enforcing domain-invariance and performing feature augmentation lead to superior or comparable performance to state-of-the-art results in several unsupervised domain adaptation benchmarks.",10.1109/CVPR.2018.00576,,81.0,0.0,1.0
Feature-Level Frankenstein: Eliminating Variations for Discriminative Recognition,X. Liu; S. Li; L. Kong; W. Xie; P. Jia; J. You; B. V. K. Kumar,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2019.0,"Recent successes of deep learning-based recognition rely on maintaining the content related to the main-task label. However, how to explicitly dispel the noisy signals for better generalization remains an open issue. We systematically summarize the detrimental factors as task-relevant/irrelevant semantic variations and unspecified latent variation. In this paper, we cast these problems as an adversarial minimax game in the latent space. Specifically, we propose equipping an end-to-end conditional adversarial network with the ability to decompose an input sample into three complementary parts. The discriminative representation inherits the desired invariance property guided by prior knowledge of the task, which is marginally independent to the task-relevant/irrelevant semantic and latent variations. Our proposed framework achieves top performance on a serial of tasks, including digits recognition, lighting, makeup, disguise-tolerant face recognition, and facial attributes recognition.",10.1109/CVPR.2019.00073,Recognition: Detection;Categorization;Retrieval;Biometrics,20.0,0.0,1.0
NAS-OoD: Neural Architecture Search for Out-of-Distribution Generalization,H. Bai; F. Zhou; L. Hong; N. Ye; S. . -H. G. Chan; Z. Li,2021 IEEE/CVF International Conference on Computer Vision (ICCV),2021.0,"Recent advances on Out-of-Distribution (OoD) generalization reveal the robustness of deep learning models against distribution shifts. However, existing works focus on OoD algorithms, such as invariant risk minimization, domain generalization, or stable learning, without considering the influence of deep model architectures on OoD generalization, which may lead to sub-optimal performance. Neural Architecture Search (NAS) methods search for architecture based on its performance on the training data, which may result in poor generalization for OoD tasks. In this work, we propose robust Neural Architecture Search for OoD generalization (NAS-OoD), which optimizes the architecture with respect to its performance on generated OoD data by gradient descent. Specifically, a data generator is learned to synthesize OoD data by maximizing losses computed by different neural architectures, while the goal for architecture search is to find the optimal architecture parameters that minimize the synthetic OoD data losses. The data generator and the neural architecture are jointly optimized in an end-to-end manner, and the minimax training process effectively discovers robust architectures that generalize well for different distribution shifts. Extensive experimental results show that NAS-OoD achieves superior performance on various OoD generalization benchmarks with deep models having a much fewer number of parameters. In addition, on a real industry dataset, the proposed NAS-OoD method reduces the error rate by more than 70% compared with the state-of-the-art method, demonstrating the proposed method’s practicality for real applications.",10.1109/ICCV48922.2021.00821,Transfer/Low-shot/Semi/Unsupervised Learning; Visual reasoning and logical representation,,0.0,1.0
A Unified Classification Model Based on Robust Optimization,A. Takeda; H. Mitsugi; T. Kanamori,Neural Computation,2013.0,"A wide variety of machine learning algorithms such as the support vector machine (SVM), minimax probability machine (MPM), and Fisher discriminant analysis (FDA) exist for binary classification. The purpose of this letter is to provide a unified classification model that includes these models through a robust optimization approach. This unified model has several benefits. One is that the extensions and improvements intended for SVMs become applicable to MPM and FDA, and vice versa. For example, we can obtain nonconvex variants of MPM and FDA by mimicking Perez-Cruz, Weston, Hermann, and Schölkopf's (<xref ref-type=""bibr"" rid=""B19"">2003</xref>) extension from convex ν-SVM to nonconvex Eν-SVM. Another benefit is to provide theoretical results concerning these learning methods at once by dealing with the unified model. We give a statistical interpretation of the unified classification model and prove that the model is a good approximation for the worst-case minimization of an expected loss with respect to the uncertain probability distribution. We also propose a nonconvex optimization algorithm that can be applied to nonconvex variants of existing learning methods and show promising numerical results.",10.1162/NECO_a_00412,,,0.0,1.0
NetCut: Real-Time DNN Inference Using Layer Removal,M. Zandigohar; D. Erdoğmuş; G. Schirner,"2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)",2021.0,"Deep Learning plays a significant role in assisting humans in many aspects of their lives. As these networks tend to get deeper over time, they extract more features to increase accuracy at the cost of additional inference latency. This accuracy-performance trade-off makes it more challenging for Embedded Systems, as resource-constrained processors with strict deadlines, to deploy them efficiently. This can lead to selection of networks that can prematurely meet a specified deadline with excess slack time that could have potentially contributed to increased accuracy. In this work, we propose: (i) the concept of layer removal as a means of constructing TRimmed Networks (TRNs) that are based on removing problem-specific features of a pretrained network used in transfer learning, and (ii) NetCut, a methodology based on an empirical or an analytical latency estimator, which only proposes and retrains TRNs that can meet the application's deadline, hence reducing the exploration time significantly. We demonstrate that TRNs can expand the Pareto frontier that trades off latency and accuracy to provide networks that can meet arbitrary deadlines with potential accuracy improvement over off-the-shelf networks. Our experimental results show that such utilization of TRNs, while transferring to a simpler dataset, in combination with NetCut, can lead to the proposal of networks that can achieve relative accuracy improvement of up to 10.43% among existing off-the-shelf neural architectures while meeting a specific deadline, and 27x speedup in exploration time.",10.23919/DATE51398.2021.9474052,,1.0,0.0,1.0
On Resource-Efficient Bayesian Network Classifiers and Deep Neural Networks,W. Roth; F. Pernkopf; G. Schindler; H. Fröning,2020 25th International Conference on Pattern Recognition (ICPR),2021.0,"We present two methods to reduce the complexity of Bayesian network (BN) classifiers. First, we introduce quantization-aware training using the straight-through gradient estimator to quantize the parameters of BNs to few bits. Second, we extend a recently proposed differentiable tree-augmented naive Bayes (TAN) structure learning approach by also considering the model size. Both methods are motivated by recent developments in the deep learning community, and they provide effective means to trade off between model size and prediction accuracy, which is demonstrated in extensive experiments. Furthermore, we contrast quantized BN classifiers with quantized deep neural networks (DNNs) for small-scale scenarios which have hardly been investigated in the literature. We show Pareto optimal models with respect to model size, number of operations, and test error and find that both model classes are viable options.",10.1109/ICPR48806.2021.9413156,,,0.0,1.0
Multi-Task Learning for Multi-Objective Evolutionary Neural Architecture Search,R. Cai; J. Luo,2021 IEEE Congress on Evolutionary Computation (CEC),2021.0,"Neural architecture search (NAS) is an exciting new field in automating machine learning. It can automatically search for the architecture of neural networks. But the current NAS has extremely high requirements for hardware equipment and time costs. In this work, we propose a predictor based on Radial basis function neural network (RBFNN) as a surrogate model of Bayesian optimization to predict the performance of neural architecture. The existing work does not consider the difficulty of directly searching for neural architectures that meet the performance requirements of NAS in real-world applications. Meanwhile, NAS needs to execute multiple times independently when facing multiple similar tasks. Therefore, we further propose a multi-task learning surrogate model with multiple RBFNNs. The model not only functions as a predictor, but also learns knowledge of similar tasks jointly. The performance of NAS is improved by processing multiple tasks simultaneously. Also, the current NAS is committed to searching for very high-performance networks and does not take into account that neural architectures are limited by device memory during actual deployment. The scale of architecture also needs to be considered. We use a multi-objective optimization algorithm to simultaneously balance the performance and the scale, and build a multi-objective evolutionary search framework to find the Pareto optimal front. Once the NAS is completed, decision-makers can choose the appropriate architecture for deployment according to different performance requirements and hardware conditions. Compared with existing NAS work, our proposed MT-ENAS algorithm is able to find a neural architecture with competitive performance and smaller scale in a shorter time.",10.1109/CEC45853.2021.9504721,neural architecture search;multi-task learning;surrogate model;multi-objective optimization,,0.0,1.0
Alleviating Catastrophic Forgetting via Multi-Objective Learning,Yaochu Jin; B. Sendhoff,The 2006 IEEE International Joint Conference on Neural Network Proceedings,2006.0,"Handling catastrophic forgetting is an interesting and challenging topic in modeling the memory mechanisms of the human brain using machine learning models. From a more general point of view, catastrophic forgetting reflects the stability-plasticity dilemma, which is one of the several dilemmas to be addressed in learning systems: to retain the stored memory while learning new information. Different to the existing approaches, we introduce a Pareto-optimality based multi-objective learning framework for alleviating catastrophic learning. Compared to the single-objective learning methods, multi-objective evolutionary learning with the help of pseudo-rehearsal is shown to be more promising in dealing with the stability-plasticity dilemma.",10.1109/IJCNN.2006.247332,,2.0,0.0,1.0
A Novel Feature Selection with Many-Objective Optimization and Learning Mechanism,L. Shu; F. He; X. Hu; H. Li,2021 IEEE 24th International Conference on Computer Supported Cooperative Work in Design (CSCWD),2021.0,"Feature selection is extremely important in machine learning and data mining. Typical two-objective feature selection methods aim to minimize the number of features and maximize classification performance. However, they overlook the fact that there may be multiple subsets with similar information content for a given cardinality. The paper presents a many-objective feature selection approach to address this problem. Firstly, we establish a five-objective optimization model, which consists of classification accuracy, the number of features, feature relevance, feature redundancy, and feature complementarity. Therefore, the proposed model can enlarge the search space with more Pareto solutions. Secondly, we propose a wrapper structure for many-objective feature selection, which integrates a learning algorithm. Thirdly, in order to reduce the computional overhead, we propose a filter structure, which separates the learning algorithm. For implementation, we adopt NSGA-III multi-objective evolutionary algorithm and extreme learning machine. The experiments on mainstream datasets confirm the superiority of the proposed method.",10.1109/CSCWD49262.2021.9437707,optimization driven design;intelligent cloud manufacturing;collaborative processing of big data;feature selection;classification;many-objective optimization;extreme learning machine,,0.0,1.0
Guest Editorial Evolutionary Computation Meets Deep Learning,W. Ding; W. Pedrycz; G. G. Yen; B. Xue,IEEE Transactions on Evolutionary Computation,2021.0,"Deep learning is a timely research direction in machine learning, where breakthrough progress has been made in both academe and industries, bringing promising results in speech recognition, computer vision, industrial control and automation, etc. The motivation of deep learning is primarily to establish a model to simulate the neural connection structure of the human brain. While dealing with complex tasks, deep learning adopts a number of transformation stages to deliver the in-depth description and interpretation of the data. Deep learning achieves exceptional power and flexibility by learning to represent the task through a nested hierarchy of layers, with more abstract representations formed successively in terms of less abstract ones. One of the key issues of existing deep learning approaches is that the meaningful representations can be learned only when their hyperparameter settings are properly specified beforehand, and general parameters are learned during the training process. Until now, not much research has been dedicated to automatically set the hyperparameters, and accurately find the globally optimal general parameters. However, this problem can be formulated as optimization problems, including discrete optimization, constrained optimization, large-scale global optimization, and multiobjective optimization, by engaging mechanisms of evolutionary computation.",10.1109/TEVC.2021.3096336,,,0.0,1.0
Label propagation through minimax paths for scalable semi-supervised learning,"Kim, KH; Choi, S",PATTERN RECOGNITION LETTERS,2014.0,"Semi-supervised learning (SSL) is attractive for labeling a large amount of data. Motivated from cluster assumption, we present a path-based SSL framework for efficient large-scale SSL, propagating labels through only a few important paths between labeled nodes and unlabeled nodes. From the framework, minimax paths emerge as a minimal set of important paths in a graph, leading us to a novel algorithm, minimax label propagation. With an appropriate stopping criterion, learning time is (1) linear with respect to the number of nodes in a graph and (2) independent of the number of classes. Experimental results show the superiority of our method over existing SSL methods, especially on large-scale data with many classes. (C) 2014 Elsevier B.V. All rights reserved.",10.1016/j.patrec.2014.02.020,Label propagation; Minimax path; Semi-supervised learning,,0.0,1.0
On the minimax optimality and superiority of deep neural network learning over sparse parameter spaces,"Hayakawa, S; Suzuki, T",NEURAL NETWORKS,2020.0,"Deep learning has been applied to various tasks in the field of machine learning and has shown superiority to other common procedures such as kernel methods. To provide a better theoretical understanding of the reasons for its success, we discuss the performance of deep learning and other methods on a nonparametric regression problem with a Gaussian noise. Whereas existing theoretical studies of deep learning have been based mainly on mathematical theories of well-known function classes such as Holder and Besov classes, we focus on function classes with discontinuity and sparsity, which are those naturally assumed in practice. To highlight the effectiveness of deep learning, we compare deep learning with a class of linear estimators representative of a class of shallow estimators. It is shown that the minimax risk of a linear estimator on the convex hull of a target function class does not differ from that of the original target function class. This results in the suboptimality of linear methods over a simple but non-convex function class, on which deep learning can attain nearly the minimax-optimal rate. In addition to this extreme case, we consider function classes with sparse wavelet coefficients. On these function classes, deep learning also attains the minimax rate up to log factors of the sample size, and linear methods are still suboptimal if the assumed sparsity is strong. We also point out that the parameter sharing of deep neural networks can remarkably reduce the complexity of the model in our setting. (C) 2019 The Author(s). Published by Elsevier Ltd.",10.1016/j.neunet.2019.12.014,Neural network; Deep learning; Linear estimator; Nonparametric regression; Minimax optimality,,0.0,1.0
An alternative approach to dimension reduction for pareto distributed data: a case study,"Roccetti, M; Delnevo, G; Casini, L; Mirri, S",JOURNAL OF BIG DATA,2021.0,"Deep learning models are tools for data analysis suitable for approximating (non-linear) relationships among variables for the best prediction of an outcome. While these models can be used to answer many important questions, their utility is still harshly criticized, being extremely challenging to identify which data descriptors are the most adequate to represent a given specific phenomenon of interest. With a recent experience in the development of a deep learning model designed to detect failures in mechanical water meter devices, we have learnt that a sensible deterioration of the prediction accuracy can occur if one tries to train a deep learning model by adding specific device descriptors, based on categorical data. This can happen because of an excessive increase in the dimensions of the data, with a correspondent loss of statistical significance. After several unsuccessful experiments conducted with alternative methodologies that either permit to reduce the data space dimensionality or employ more traditional machine learning algorithms, we changed the training strategy, reconsidering that categorical data, in the light of a Pareto analysis. In essence, we used those categorical descriptors, not as an input on which to train our deep learning model, but as a tool to give a new shape to the dataset, based on the Pareto rule. With this data adjustment, we trained a more performative deep learning model able to detect defective water meter devices with a prediction accuracy in the range 87-90%, even in the presence of categorical descriptors.",10.1186/s40537-021-00428-8,Deep learning models; Categorical data; Learning space dimensions; Pareto analysis; Imbalanced datasets; Dataset coherence analysis; Principal component analysis; Binning; Machine learning,,0.0,1.0
Pareto Inspired Multi-objective Rule Fitness for Adaptive Rule-based Machine Learning,"Urbanowicz, RJ; Olson, RS; Moore, JH",PROCEEDINGS OF THE 2016 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE (GECCO'16 COMPANION),2016.0,"Learning classifier systems (LCSs) are rule-based evolutionary algorithms uniquely suited to classification and data mining in complex, multi-factorial, and heterogeneous problems. LCS rule fitness is commonly based on accuracy, but this metric alone is not ideal for assessing global rule 'value' in noisy problem domains, and thus impedes effective knowledge extraction. Multi-objective fitness functions are promising but rely on knowledge of how to weigh objective importance. Prior knowledge would be unavailable in most real-world problems. The Pareto-front concept offers a strategy for multi-objective machine learning that is agnostic to objective importance. We propose a Pareto-inspired multi objective rule fitness (PIMORF) for LCS, and combine it with a complimentary rule-compaction approach (SRC). We implemented these strategies in ExSTraCS, a successful supervised LCS and evaluated performance over an array of complex simulated noisy and clean problems (i.e. genetic and multiplexer) that each concurrently model pure interaction effects and heterogeneity. While evaluation over multiple performance metrics yielded mixed results, this work represents an important first step towards efficiently learning complex problem spaces without the advantage of prior problem knowledge. Overall the results suggest that PIMORF paired with SRC improved rule set interpretability, particularly with regard to heterogeneous patterns.",10.1145/2908961.2931736,data mining; classifier systems; fitness evaluation; multi-objective optimization; machine learning,,0.0,1.0
Ensemble learning by means of a multi-objective optimization design approach for dealing with imbalanced data sets,"Ribeiro, VHA; Reynoso-Meza, G",EXPERT SYSTEMS WITH APPLICATIONS,2020.0,"Ensemble learning methods have already shown to be powerful techniques for creating classifiers. However, when dealing with real-world engineering problems, class imbalance is usually found. In such scenario, canonical machine learning algorithms may not present desirable solutions, and techniques for overcoming this problem must be used. In addition to using learning algorithms that alleviate the imbalance between classes, multi-objective optimization design (MOOD) approaches can be used to improve the prediction performance of ensembles of classifiers. This paper proposes a study of different MOOD approaches for ensemble learning. First, a taxonomy on multi-objective ensemble learning (MOEL) is proposed. In it, four types of existing approaches are defined: multi-objective ensemble member generation, multi-objective ensemble member selection, multi-objective ensemble member combination, and multiobjective ensemble member selection and combination. Additionally, new approaches can be derived by combining the previous ones, such as multi-objective ensemble member generation and selection, multiobjective ensemble member generation and combination and multi-objective ensemble member generation, selection and combination. With the given taxonomy, two experiments are conducted for comparing (1) the performance of the MOEL techniques for generating and aggregating base models on several imbalanced benchmark problems and (2) the performance of MOEL techniques against other machine learning techniques in a real-world imbalanced drinking-water quality anomaly detection problem. Finally, results indicate that MOOD is able to improve the predictive performance of existing ensemble learning techniques. (C) 2020 Elsevier Ltd. All rights reserved.",10.1016/j.eswa.2020.113232,Ensemble learning; Multi-objective optimization; Imbalanced data sets,,0.0,1.0
The fairness-accuracy Pareto front,"Wei, SS; Niethammer, M",STATISTICAL ANALYSIS AND DATA MINING,,"Algorithmic fairness seeks to identify and correct sources of bias in machine learning algorithms. Confoundingly, ensuring fairness often comes at the cost of accuracy. We provide formal tools in this work for reconciling this fundamental tension in algorithm fairness. Specifically, we put to use the concept of Pareto optimality from multiobjective optimization and seek the fairness-accuracy Pareto front of a neural network classifier. We demonstrate that many existing algorithmic fairness methods are performing the so-called linear scalarization scheme, which has severe limitations in recovering Pareto optimal solutions. We instead apply the Chebyshev scalarization scheme which is provably superior theoretically and no more computationally burdensome at recovering Pareto optimal solutions compared to the linear scheme.",10.1002/sam.11560,fairness; multiobjective optimization; neural network; Pareto front; Pareto optimality,,0.0,1.0
A selective ensemble learning based two-sided cross-domain collaborative filtering algorithm,"Yu, X; Peng, QL; Xu, LW; Jiang, F; Du, JW; Gong, DW",INFORMATION PROCESSING & MANAGEMENT,2021.0,"ABS T R A C T Recently, various Cross-Domain Collaborative Filtering (CDCF) algorithms are presented to address the sparsity problem, leveraging ratings of auxiliary domains to improve target domain's recommendation performance. Therein, two-sided CDCF algorithms have shown better performance, given the fact that they can extract both user and item information. However, as the auxiliary domains are not all related to the target domain, utilizing information from all the auxiliary domains may not be optimal and would lead to low efficiency. A Two-Sided CDCF model based on Selective Ensemble learning considering both Accuracy and Efficiency (TSSEAE) is proposed to balance recommendation accuracy and efficiency. In TSSEAE, user-sided and item-sided auxiliary domains are firstly combined to improve performance of target domain. Then, CDCF problems are converted to ensemble learning problems, with each combination corresponding to a classifier. In this way, the problem of selecting combinations can be converted to that of selecting classifiers, which is a selective ensemble learning problem. Finally, a bi-objective optimization problem is solved to obtain Pareto optimal solutions for the selective ensemble learning problem. The experimental result on Amazon dataset shows the effectiveness of TSSEAE.",10.1016/j.ipm.2021.102691,Cross-domain collaborative filtering; Selective ensemble; Ensemble learning; Pareto optimal solutions; Bi-objective optimization&nbsp; problem,,0.0,1.0
An asymptotically minimax kernel machine,"Ghosh, D",STATISTICS & PROBABILITY LETTERS,2014.0,"Recently, a class of machine learning-inspired procedures, termed kernel machine methods, has been extensively developed in the statistical literature. In this note, we construct a so-called 'adaptively minimax' kernel machine. Such a construction highlights the limits on the interpretability of such kernel machines. (C) 2014 Elsevier B.V. All rights reserved.",10.1016/j.spl.2014.08.005,Data mining; Decision theory; Hard-thresholding; Nonparametric regression; Support vector machines,,0.0,1.0
An imprecise extension of SVM-based machine learning models,"Utkin, LV",NEUROCOMPUTING,2019.0,"A general approach for incorporating imprecise prior knowledge and for robustifying the machine learning SVM-based models is proposed in the paper. The main idea underlying the approach is to use a double duality representation in the framework of the minimax strategy of decision making. This idea allows us to get simple extensions of SVMs including additional constraints for optimization variables (the Lagrange multipliers) formalizing the incorporated imprecise information. The approach is applied to regression, binary classification and one-class classification SVM-based problems. Moreover, it is adopted to set-valued or interval-valued training data. For every problem, numerical examples are provided which illustrate how imprecise information may improve the machine learning algorithm performance. (C) 2018 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2018.11.053,Machine learning; Support vector machine; Duality; Classification; Regression; Interval-valued data; Imprecise model,,0.0,1.0
Constructing accuracy and diversity ensemble using Pareto-based multi-objective learning for evolving data streams,"Sun, YG; Dai, HH",NEURAL COMPUTING & APPLICATIONS,2021.0,"Ensemble learning is one of the most frequently used techniques for handling concept drift, which is the greatest challenge for learning high-performance models from big evolving data streams. In this paper, a Pareto-based multi-objective optimization technique is introduced to learn high-performance base classifiers. Based on this technique, a multi-objective evolutionary ensemble learning scheme, named Pareto-optimal ensemble for a better accuracy and diversity (PAD), is proposed. The approach aims to enhance the generalization ability of ensemble in evolving data stream environment by balancing the accuracy and diversity of ensemble members. In addition, an adaptive window change detection mechanism is designed for tracking different kinds of drifts constantly. Extensive experiments show that PAD is capable of adapting to dynamic change environments effectively and efficiently in achieving better performance.",10.1007/s00521-020-05386-5,Data streams; Concept drift; Ensemble learning; Diversity; Classifier selection; Multi-objective optimization,,0.0,1.0
LOCAL NEAREST NEIGHBOUR CLASSIFICATION WITH APPLICATIONS TO SEMI-SUPERVISED LEARNING,"Cannings, TI; Berrett, TB; Samworth, RJ",ANNALS OF STATISTICS,2020.0,"We derive a new asymptotic expansion for the global excess risk of a local-k-nearest neighbour classifier, where the choice of k may depend upon the test point. This expansion elucidates conditions under which the dominant contribution to the excess risk comes from the decision boundary of the optimal Bayes classifier, but we also show that if these conditions are not satisfied, then the dominant contribution may arise from the tails of the marginal distribution of the features. Moreover, we prove that, provided the d-dimensional marginal distribution of the features has a finite rho th moment for some rho > 4 (as well as other regularity conditions), a local choice of k , can yield a rate of convergence of the excess risk of O(n(-4/(d+4))), where n is the sample size, whereas for the standard k-nearest neighbour classifier, our theory would require d >= 5 and rho > 4d/(d - 4) finite moments to achieve this rate. These results motivate a new k-nearest neighbour classifier for semi-supervised learning problems, where the unlabelled data are used to obtain an estimate of the marginal feature density, and fewer neighbours are used for classification when this density estimate is small. Our worst-case rates are complemented by a minimax lower bound, which reveals that the local, semi-supervised k-nearest neighbour classifier attains the minimax optimal rate over our classes for the excess risk, up to a subpolynomial factor in n. These theoretical improvements over the standard k-nearest neighbour classifier are also illustrated through a simulation study.",10.1214/19-AOS1868,Classification problems; nearest neighbours; nonparametric classification; semi- supervised learning,,0.0,1.0
Fairer Machine Learning Through Multi-objective Evolutionary Learning,"Zhang, QQ; Liu, JL; Zhang, ZQ; Wen, JY; Mao, BF; Yao, X","ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2021, PT IV",2021.0,"Dilemma between model accuracy and fairness in machine learning models has been shown theoretically and empirically. So far, dozens of fairness measures have been proposed, among which incompatibility and complementarity exist. However, no fairness measure has been universally accepted as the single fairest measure. No one has considered multiple fairness measures simultaneously. In this paper, we propose a multi-objective evolutionary learning framework for mitigating unfairness caused by considering a single measure only, in which a multi-objective evolutionary algorithm is used during training to balance accuracy and multiple fairness measures simultaneously. In our case study, besides the model accuracy, two fairness measures that are conflicting to each other are selected. Empirical results show that our proposed multi-objective evolutionary learning framework is able to find Pareto-front models efficiently and provide fairer machine learning models that consider multiple fairness measures.",10.1007/978-3-030-86380-7_10,Fairness in machine learning; Discrimination in machine learning; AI ethics; Fairness measures; Multi-objective learning,,0.0,1.0
A new ensemble learning methodology based on hybridization of classifier ensemble selection approaches,"Mousavi, R; Eftekhari, M",APPLIED SOFT COMPUTING,2015.0,"Ensemble learning is a system that improves the performance and robustness of the classification problems. How to combine the outputs of base classifiers is one of the fundamental challenges in ensemble learning systems. In this paper, an optimized Static Ensemble Selection (SES) approach is first proposed on the basis of NSGA-II multi-objective genetic algorithm (called SES-NSGAII), which selects the best classifiers along with their combiner, by simultaneous optimization of error and diversity objectives. In the second phase, the Dynamic Ensemble Selection-Performance (DES-P) is improved by utilizing the first proposed method. The second proposed method is a hybrid methodology that exploits the abilities of both SES and DES approaches and is named Improved DES-P (IDES-P). Accordingly, combining static and dynamic ensemble strategies as well as utilizing NSGA-II are the main contributions of this research. Findings of the present study confirm that the proposed methods outperform the other ensemble approaches over 14 datasets in terms of classification accuracy. Furthermore, the experimental results are described from the view point of Pareto front with the aim of illustrating the relationship between diversity and the over-fitting problem. (C) 2015 Elsevier B.V. All rights reserved.",10.1016/j.asoc.2015.09.009,Ensemble learning system; Static ensemble selection; Dynamic ensemble selection; Classifier combination; Classifier diversity; Multi-objective optimization,,0.0,1.0
Multiobjective sparse ensemble learning by means of evolutionary algorithms,"Zhao, JQ; Jiao, LC; Xia, SX; Fernandes, VB; Yevseyeva, I; Zhou, Y; Emmerich, MTM",DECISION SUPPORT SYSTEMS,2018.0,"Ensemble learning can improve the performance of individual classifiers by combining their decisions. The sparseness of ensemble learning has attracted much attention in recent years. In this paper, a novel multi objective sparse ensemble learning (MOSEL) model is proposed. Firstly, to describe the ensemble classifiers more precisely the detection error trade-off (DET) curve is taken into consideration. The sparsity ratio (sr) is treated as the third objective to be minimized, in addition to false positive rate (fpr) and false negative rate (fnr) minimization. The MOSEL turns out to be augmented DET (ADET) convex hull maximization problem. Secondly, several evolutionary multiobjective algorithms are exploited to find sparse ensemble classifiers with strong performance. The relationship between the sparsity and the performance of ensemble classifiers on the ADET space is explained. Thirdly, an adaptive MOSEL classifiers selection method is designed to select the most suitable ensemble classifiers for a given dataset. The proposed MOSEL method is applied to well-known MNIST datasets and a real-world remote sensing image change detection problem, and several datasets are used to test the performance of the method on this problem. Experimental results based on both MNIST datasets and remote sensing image change detection show that MOSEL performs significantly better than conventional ensemble learning methods.",10.1016/j.dss.2018.05.003,Ensemble learning; Sparse representation; Classification; Multiobjective optimization; Change detection,,0.0,1.0
Using financial risk measures for analyzing generalization performance of machine learning models,"Takeda, A; Kanamori, T",NEURAL NETWORKS,2014.0,"We propose a unified machine learning model (UMLM) for two-class classification, regression and outlier (or novelty) detection via a robust optimization approach. The model embraces various machine learning models such as support vector machine-based and minimax probability machine-based classification and regression models. The unified framework makes it possible to compare and contrast existing learning models and to explain their differences and similarities. In this paper, after relating existing learning models to UMLM, we show some theoretical properties for UMLM. Concretely, we show an interpretation of UMLM as minimizing a well-known financial risk measure (worst-case value-at risk (VaR) or conditional VaR), derive generalization bounds for UMLM using such a risk measure, and prove that solving problems of UMLM leads to estimators with the minimized generalization bounds. Those theoretical properties are applicable to related existing learning models. (C) 2014 Elsevier Ltd. All rights reserved.",10.1016/j.neunet.2014.05.006,Support vector machine; Minimax probability machine; Financial risk measure; Generalization performance,,0.0,1.0
How fair can we go in machine learning? Assessing the boundaries of accuracy and fairness,"Valdivia, A; Sanchez-Monedero, J; Casillas, J",INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS,2021.0,"Fair machine learning has been focusing on the development of equitable algorithms that address discrimination. Yet, many of these fairness-aware approaches aim to obtain a unique solution to the problem, which leads to a poor understanding of the statistical limits of bias mitigation interventions. In this study, a novel methodology is presented to explore the tradeoff in terms of a Pareto front between accuracy and fairness. To this end, we propose a multiobjective framework that seeks to optimize both measures. The experimental framework is focused on logistiregression and decision tree classifiers since they are well-known by the machine learning community. We conclude experimentally that our method can optimize classifiers by being fairer with a small cost on the classification accuracy. We believe that our contribution will help stakeholders of sociotechnical systems to assess how far they can go being fair and accurate, thus serving in the support of enhanced decision making where machine learning is used.",10.1002/int.22354,algorithmic fairness; group fairness; multiobjective optimization,,0.0,1.0
Horseshoe Regularisation for Machine Learning in Complex and Deep Models,"Bhadra, A; Datta, J; Li, YF; Polson, NG",INTERNATIONAL STATISTICAL REVIEW,2020.0,"Since the advent of the horseshoe priors for regularisation, global-local shrinkage methods have proved to be a fertile ground for the development of Bayesian methodology in machine learning, specifically for high-dimensional regression and classification problems. They have achieved remarkable success in computation and enjoy strong theoretical support. Most of the existing literature has focused on the linear Gaussian case; for which systematic surveys are available. The purpose of the current article is to demonstrate that the horseshoe regularisation is useful far more broadly, by reviewing both methodological and computational developments in complex models that are more relevant to machine learning applications. Specifically, we focus on methodological challenges in horseshoe regularisation in non-linear and non-Gaussian models, multivariate models and deep neural networks. We also outline the recent computational developments in horseshoe shrinkage for complex models along with a list of available software implementations that allows one to venture out beyond the comfort zone of the canonical linear regression problems.",10.1111/insr.12360,complex data; deep learning; large scale machine learning; non-linear; non-Gaussian; shrinkage,,0.0,1.0
StressGAN: A Generative Deep Learning Model for Two-Dimensional Stress Distribution Prediction,"Jiang, HL; Nie, ZG; Yeo, R; Farimani, AB; Kara, LB",JOURNAL OF APPLIED MECHANICS-TRANSACTIONS OF THE ASME,2021.0,"Using deep learning to analyze mechanical stress distributions is gaining interest with the demand for fast stress analysis. Deep learning approaches have achieved excellent outcomes when utilized to speed up stress computation and learn the physical nature without prior knowledge of underlying equations. However, most studies restrict the variation of geometry or boundary conditions, making it difficult to generalize the methods to unseen configurations. We propose a conditional generative adversarial network (cGAN) model called StressGAN for predicting 2D von Mises stress distributions in solid structures. The StressGAN model learns to generate stress distributions conditioned by geometries, loads, and boundary conditions through a two-player minimax game between two neural networks with no prior knowledge. By evaluating the generative network on two stress distribution datasets under multiple metrics, we demonstrate that our model can predict more accurate stress distributions than a baseline convolutional neural-network model, given various and complex cases of geometries, loads, and boundary conditions.",10.1115/1.4049805,StressGAN; stress; conditional generative adversarial network; deep learning,,0.0,1.0
An Augmented Lagrangian Deep Learning Method for Variational Problems with Essential Boundary Conditions,"Huang, JG; Wang, HQ; Zhou, T",COMMUNICATIONS IN COMPUTATIONAL PHYSICS,2022.0,"This paper is concerned with a novel deep learning method for variational problems with essential boundary conditions. To this end, we first reformulate the original problem into a minimax problem corresponding to a feasible augmented Lagrangian, which can be solved by the augmented Lagrangian method in an infinite dimensional setting. Based on this, by expressing the primal and dual variables with two individual deep neural network functions, we present an augmented Lagrangian deep learning method for which the parameters are trained by the stochastic optimization method together with a projection technique. Compared to the traditional penalty method, the new method admits two main advantages: i) the choice of the penalty parameter is flexible and robust, and ii) the numerical solution is more accurate in the same magnitude of computational cost. As typical applications, we apply the new approach to solve elliptic problems and (nonlinear) eigenvalue problems with essential boundary conditions, and numerical experiments are presented to show the effectiveness of the new method.",10.4208/cicp.OA-2021-0176,The augmented Lagrangian method; deep learning; variational problems; saddle point problems; essential boundary conditions,,0.0,1.0
Pareto analysis of evolutionary and learning systems,"Jin, YC; Gruna, R; Sendhoff, B",FRONTIERS OF COMPUTER SCIENCE IN CHINA,2009.0,"This paper attempts to argue that most adaptive systems, such as evolutionary or learning systems, have inherently multiple objectives to deal with. Very often, there is no single solution that can optimize all the objectives. In this case, the concept of Pareto optimality is key to analyzing these systems. To support this argument, we first present an example that considers the robustness and evolvability trade-off in a redundant genetic representation for simulated evolution. It is well known that robustness is critical for biological evolution, since without a sufficient degree of mutational robustness, it is impossible for evolution to create new functionalities. On the other hand, the genetic representation should also provide the chance to find new phenotypes, i.e., the ability to innovate. This example shows quantitatively that a trade-off between robustness and innovation does exist in the studied redundant representation. Interesting results will also be given to show that new insights into learning problems can be gained when the concept of Pareto optimality is applied to machine learning. In the first example, a Pareto-based multi-objective approach is employed to alleviate catastrophic forgetting in neural network learning. We show that learning new information and memorizing learned knowledge are two conflicting objectives, and a major part of both information can be memorized when the multi-objective learning approach is adopted. In the second example, we demonstrate that a Pareto-based approach can address neural network regularization more elegantly. By analyzing the Pareto-optimal solutions, it is possible to identifying interesting solutions on the Pareto front.",10.1007/s11704-009-0004-8,Pareto analysis; multi-objective optimization; evolution; evolvability; robustness; learning; accuracy; complexity,,0.0,1.0
A novel twin minimax probability machine for classification and regression,"Ma, J; Shen, JM",KNOWLEDGE-BASED SYSTEMS,2020.0,"As an excellent machine learning tool, the minimax probability machine (MPM) has been widely used in many fields. However, MPM does not include a regularization term for the construction of the separating hyperplane, and it needs to solve a large-scale second-order cone programming problem (SOCP) in the solution process, which greatly limits it development and application. In this paper, to improve the performance of MPM, we propose a novel binary classification method called twin minimax probability machine classification (TMPMC). The TMPMC constructs two non-parallel hyperplanes for final classification by solving two smaller SOCPs to improve the performance of the MPM. For each hyperplane, TMPMC attempts to minimize the worst case (maximum) probability that a class of samples is misclassified while being as far away as possible from the other class. Additionally, we extend TMPMC to the regression problem and propose a new regularized twin minimax probability machine regression (TMPMR). Intuitively, the idea of TMPMR is to convert the regression problem into a classification problem to solve. Both TMPMC and TMPMR avoid the assumption of distribution of conditional density. Finally, we extend the linear models of TMPMC and TMPMR to nonlinear case. Experimental results on several datasets show that TMPMC and TMPMR are competitive in terms of generalization performance compared to other algorithms. (C) 2020 Elsevier B.V. All rights reserved.",10.1016/j.knosys.2020.105703,Minimax probability machine; Classification; Regression; Non-parallel hyperplane; Second-order cone programming,,0.0,1.0
Performance evaluation of evolutionary multiobjective optimization algorithms for multiobjective fuzzy genetics-based machine learning,"Ishibuchi, H; Nakashima, Y; Nojima, Y",SOFT COMPUTING,2011.0,"Recently, evolutionary multiobjective optimization (EMO) algorithms have been utilized for the design of accurate and interpretable fuzzy rule-based systems. This research area is often referred to as multiobjective genetic fuzzy systems (MoGFS), where EMO algorithms are used to search for non-dominated fuzzy rule-based systems with respect to their accuracy and interpretability. In this paper, we examine the ability of EMO algorithms to efficiently search for Pareto optimal or near Pareto optimal fuzzy rule-based systems for classification problems. We use NSGA-II (elitist non-dominated sorting genetic algorithm), its variants, and MOEA/D (multiobjective evolutionary algorithm based on decomposition) in our multiobjective fuzzy genetics-based machine learning (MoFGBML) algorithm. Classification performance of obtained fuzzy rule-based systems by each EMO algorithm is evaluated for training data and test data under various settings of the available computation load and the granularity of fuzzy partitions. Experimental results in this paper suggest that reported classification performance of MoGFS in the literature can be further improved using more computation load, more efficient EMO algorithms, and/or more antecedent fuzzy sets from finer fuzzy partitions.",10.1007/s00500-010-0669-9,Fuzzy rule-based classification; Genetic algorithms; Genetics-based machine learning; Multiobjective machine learning; Evolutionary multiobjective optimization,,0.0,1.0
"A MINIMAX THEOREM WITH APPLICATIONS TO MACHINE LEARNING, SIGNAL PROCESSING, AND FINANCE","Kim, SJ; Boyd, S",SIAM JOURNAL ON OPTIMIZATION,2008.0,"This paper concerns a fractional function of the form x(T)a/root x(T)Bx, where B is positive definite. We consider the game of choosing x from a convex set, to maximize the function, and choosing (a, B) from a convex set, to minimize it. We prove the existence of a saddle point and describe an efficient method, based on convex optimization, for computing it. We describe applications in machine learning (robust Fisher linear discriminant analysis), signal processing (robust beamforming and robust matched filtering), and finance (robust portfolio selection). In these applications, x corresponds to some design variables to be chosen, and the pair (a, B) corresponds to the statistical model, which is uncertain.",10.1137/060677586,convex optimization; minimax theorem; robust optimization,,0.0,1.0
An efficient multi-objective learning algorithm for RBF neural network,"Kokshenev, I; Braga, AP",NEUROCOMPUTING,2010.0,"Most of modern multi-objective machine learning methods are based on evolutionary optimization algorithms. They are known to be global convergent, however, usually deliver nondeterministic results. In this work we propose the deterministic global solution to a multi-objective problem of supervised learning with the methodology of nonlinear programming. As the result, the proposed multi-objective algorithm performs a global search of Pareto-optimal hypotheses in the space of RBF networks, determining their weights and basis functions. In combination with the Akaike and Bayesian information criteria, the algorithm demonstrates a high generalization efficiency on several synthetic and real-world benchmark problems. (C) 2010 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2010.06.022,Multi-objective learning; Radial-basis functions; Pareto-optimality; Model selection; Regularization,,0.0,1.0
The stochastic multi-gradient algorithm for multi-objective optimization and its application to supervised machine learning,"Liu, S; Vicente, LN",ANNALS OF OPERATIONS RESEARCH,,"Optimization of conflicting functions is of paramount importance in decision making, and real world applications frequently involve data that is uncertain or unknown, resulting in multi-objective optimization (MOO) problems of stochastic type. We study the stochastic multi-gradient (SMG) method, seen as an extension of the classical stochastic gradient method for single-objective optimization. At each iteration of the SMG method, a stochastic multi-gradient direction is calculated by solving a quadratic subproblem, and it is shown that this direction is biased even when all individual gradient estimators are unbiased. We establish rates to compute a point in the Pareto front, of order similar to what is known for stochastic gradient in both convex and strongly convex cases. The analysis handles the bias in the multi-gradient and the unknown a priori weights of the limiting Pareto point. The SMG method is framed into a Pareto-front type algorithm for calculating an approximation of the entire Pareto front. The Pareto-front SMG algorithm is capable of robustly determining Pareto fronts for a number of synthetic test problems. One can apply it to any stochastic MOO problem arising from supervised machine learning, and we report results for logistic binary classification where multiple objectives correspond to distinct-sources data groups.",10.1007/s10479-021-04033-z,Multi-objective optimization; Pareto front; Stochastic gradient descent; Supervised machine learning,,0.0,1.0
Orthogonalized Kernel Debiased Machine Learning for Multimodal Data Analysis,"Dai, XW; Li, LX",JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,,"Multimodal imaging has transformed neuroscience research. While it presents unprecedented opportunities, it also imposes serious challenges. Particularly, it is difficult to combine the merits of the interpretability attributed to a simple association model with the flexibility achieved by a highly adaptive nonlinear model. In this article, we propose an orthogonalized kernel debiased machine learning approach, which is built upon the Neyman orthogonality and a form of decomposition orthogonality, for multimodal data analysis. We target the setting that naturally arises in almost all multimodal studies, where there is a primary modality of interest, plus additional auxiliary modalities. We establish the root-N-consistency and asymptotic normality of the estimated primary parameter, the semi-parametric estimation efficiency, and the asymptotic validity of the confidence band of the predicted primary modality effect. Our proposal enjoys, to a good extent, both model interpretability and model flexibility. It is also considerably different from the existing statistical methods for multimodal data integration, as well as the orthogonality-based methods for high-dimensional inferences. We demonstrate the efficacy of our method through both simulations and an application to a multimodal neuroimaging study of Alzheimer's disease. Supplementary materials for this article are available online.",10.1080/01621459.2021.2013851,Basis expansion; High-dimensional inference; Multimodal data integration; Neuroimaging analysis; Neyman orthogonality; Reproducing kernel Hilbert space,,0.0,1.0
MODE: multiobjective differential evolution for feature selection and classifier ensemble,"Sikdar, UK; Ekbal, A; Saha, S",SOFT COMPUTING,2015.0,"In this paper, we propose a multiobjective differential evolution (MODE)-based feature selection and ensemble learning approaches for entity extraction in biomedical texts. The first step of the algorithm concerns with the problem of automatic feature selection in a machine learning framework, namely conditional random field. The final Pareto optimal front which is obtained as an output of the feature selection module contains a set of solutions, each of which represents a particular feature representation. In the second step of our algorithm, we combine a subset of these classifiers using a MODE-based ensemble technique. Our experiments on three benchmark datasets namely GENIA, GENETAG and AIMed show the F-measure values of 76.75, 94.15 and 91.91 %, respectively. Comparisons with the existing systems show that our proposed algorithm achieves the performance levels which are at par with the state of the art. These results also exhibit that our method is general in nature and because of this it performs well across the several domain of datasets. The key contribution of this work is the development of MODE-based generalized feature selection and ensemble learning techniques with the aim of extracting entities from the biomedical texts of several domains.",10.1007/s00500-014-1565-5,,,0.0,1.0
A novel feature selection approach with Pareto optimality for multi-label data,"Li, GH; Li, Y; Zheng, YF; Li, Y; Hong, YF; Zhou, XM",APPLIED INTELLIGENCE,2021.0,"Multi-label learning has widely applied in machine learning and data mining. The purpose of feature selection is to select an approximately optimal feature subset to characterize the original feature space. Similar to single-label data, feature selection is an import preprocessing step to enhance the performance of multi-label classification model. In this paper, we propose a multi-label feature selection approach with Pareto optimality for continuous data, called MLFSPO. It maps multi-label features to high-dimensional space to evaluate the correlation between features and labels by utilizing the Hilbert-Schmidt Independence Criterion (HSIC). Then, the feature subset obtains by combining the Pareto optimization with feature ordering criteria and label weighting. Eventually, extensive experimental results on publicly available data sets show the effectiveness of the proposed algorithm in multi-label tasks.",10.1007/s10489-021-02228-2,Feature selection; Multi-label learning; Pareto optimality; Hilbert-Schmidt independence criterion,,0.0,1.0
Pareto Inspired Multi-objective Rule Fitness for Noise-Adaptive Rule-Based Machine Learning,"Urbanowicz, RJ; Olson, RS; Moore, JH",PARALLEL PROBLEM SOLVING FROM NATURE - PPSN XIV,2016.0,"Learning classifier systems (LCSs) are rule-based evolutionary algorithms uniquely suited to classification and data mining in complex, multi-factorial, and heterogeneous problems. The fitness of individual LCS rules is commonly based on accuracy, but this metric alone is not ideal for assessing global rule 'value' in noisy problem domains and thus impedes effective knowledge extraction. Multi-objective fitness functions are promising but rely on prior knowledge of how to weigh objective importance (typically unavailable in real world problems). The Pareto-front concept offers a multi-objective strategy that is agnostic to objective importance. We propose a Pareto-inspired multi-objective rule fitness (PIMORF) for LCS, and combine it with a complimentary rule-compaction approach (SRC). We implemented these strategies in ExSTraCS, a successful supervised LCS and evaluated performance over an array of complex simulated noisy and clean problems (i.e. genetic and multiplexer) that each concurrently model pure interaction effects and heterogeneity. While evaluation over multiple performance metrics yielded mixed results, this work represents an important first step towards efficiently learning complex problem spaces without the advantage of prior problem knowledge. Overall the results suggest that PIMORF paired with SRC improved rule set interpretability, particularly with regard to heterogeneous patterns.",10.1007/978-3-319-45823-6_48,Data mining; Classifier systems; Fitness evaluation; Multi-objective optimization; Machine learning,,0.0,1.0
A Pareto-smoothing method for causal inference using generalized Pareto distribution,"Zhu, FJ; Lu, J; Lin, A; Zhang, GQ",NEUROCOMPUTING,2020.0,"Causal inference aims to estimate the treatment effect of an intervention on the target outcome variable and has received great attention across fields ranging from economics and statistics to machine learning. Observational causal inference is challenging because the pre-treatment variables may influence both the treatment and the outcome, resulting in confounding bias. The classic inverse propensity weighting (IPW) estimator is theoretically able to eliminate the confounding bias. However, in observational studies, the propensity scores used in the IPW estimator must be estimated from finite observational data and may be subject to extreme values, leading to the problem of highly variable importance weights, which consequently makes the estimated causal effect unstable or even misleading. In this paper, by reframing the IPW estimator in the importance sampling framework, we propose a Pareto-smoothing method to tackle this problem. The generalized Pareto distribution (GPD) from extreme value theory is used to fit the upper tail of the estimated importance weights and to replace them using the order statistics of the fitted GPD. To validate the performance of the new method, we conducted extensive experiments on simulated and semi-simulated datasets. Compared with two existing methods for importance weight stabilization, i.e., weight truncation and self-normalization, the proposed method generally achieves better performance in settings with a small sample size and high-dimensional covariates. Its application on a real-world heath dataset indicates its utility in estimating causal effects for program evaluation. (C) 2019 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2019.09.095,Causality; Causal inference; Machine learning; Treatment effect; Importance sampling,,0.0,1.0
Identifying Pareto-based solutions for regression subset selection via a feasible solution algorithm,"Lambert, JW; Hawk, GS",INTERNATIONAL JOURNAL OF DATA SCIENCE AND ANALYTICS,2020.0,"The concept of Pareto optimality has been utilized in fields such as engineering and economics to understand fluid dynamics and consumer behavior. In machine learning contexts, Pareto-optimality has been used to identify tuning parameters that best optimize a set ofmcriteria (multi-objective optimization). During the process of regression model selection, data scientists are often concerned with choosing a model which has the best single criterion (e.g., Akaike information criterion (AIC) orR-squared (R-2)) before continuing to check a number of other regression model characteristics (e.g., model size, form, diagnostics, and interpretability). This strategy is multi-objective in nature but single objective in its numeric execution. This paper will first introduce a feasible solution algorithm (FSA) and explain how it can be applied to multi-objective problems for regression subset selection. Then we introduce the general framework of Pareto optimality within the regression setting. We then apply the algorithm in a simulation setting where we seek to estimate the first four Pareto boundaries for regression models using two model fit criteria. Finally, we present an application where we use a US communities and crime dataset.",10.1007/s41060-020-00218-0,Pareto; Optimal; Feasible solution; Multiple; Objective; Subset selection; Regression,,0.0,1.0
Adversarial Machine Learning in Recommender Systems (AML-RecSys),"Deldjoo, Y; Di Noia, T; Merra, FA",PROCEEDINGS OF THE 13TH INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM '20),2020.0,"Recommender systems (RS) are an integral part of many online services aiming to provide an enhanced user-oriented experience. Machine learning (ML) models are nowadays broadly adopted in modern state-of-the-art approaches to recommendation, which are typically trained to maximize a user-centred utility (e.g., user satisfaction) or a business-oriented one (e.g., profitability or sales increase). They work under the main assumption that users' historical feedback can serve as proper ground-truth for model training and evaluation. However, driven by the success in the ML community, recent advances show that state-of-the-art recommendation approaches such as matrix factorization (MF) models or the ones based on deep neural networks can be vulnerable to adversarial perturbations applied on the input data. These adversarial samples can impede the ability for training high-quality MF models and can put the driven success of these approaches at high risk. As a result, there is a new paradigm of secure training for RS that takes into account the presence of adversarial samples into the recommendation process. We present adversarial machine learning in Recommender Systems (AML-RecSys), which concerns the study of effective ML techniques in RS to fight against an adversarial component. AML-RecSys has been proposed in two main fashions within the RS literature: (i) adversarial regularization, which attempts to combat against adversarial perturbation added to input data or model parameters of a RS and, (ii) generative adversarial network (GAN)-based models, which adopt a generative process to train powerful ML models. We discuss a theoretical framework to unify the two above models, which is performed via a minimax game between an adversarial component and a discriminator. Furthermore, we explore various examples illustrating the successful application of AML to solve various RS tasks. Finally, we present a global taxonomy/overview of the academic literature based on several identified dimensions, namely (i) research goals and challenges, (ii) application domains and (iii) technical overview.",10.1145/3336191.3371877,,,0.0,1.0
A two-stage minimax concave penalty based method in pruned AdaBoost ensemble,"Jiang, H; Zheng, WH; Luo, LQ; Dong, Y",APPLIED SOFT COMPUTING,2019.0,"AdaBoost is a highly effective ensemble learning method that combines several weak learners to produce a strong committee with higher accuracy. However, similar to other ensemble methods, AdaBoost uses a large number of base learners to produce the final outcome while addressing high-dimensional data. Thus, it poses a critical challenge in the form of high memory-space consumption. Feature selection methods can significantly reduce dimensionality in regression and have been established to be applicable in ensemble pruning. By pruning the ensemble, it is possible to generate a simpler ensemble with fewer base learners but a higher accuracy. In this article, we propose the minimax concave penalty (MCP) function to prune an AdaBoost ensemble to simplify the model and improve its accuracy simultaneously. The MCP penalty function is compared with LASSO and SCAD in terms of performance in pruning the ensemble. Experiments performed on real datasets demonstrate that MCP-pruning outperforms the other two methods. It can reduce the ensemble size effectively, and generate marginally more accurate predictions than the unpruned AdaBoost model. (C) 2019 Elsevier B.V. All rights reserved.",10.1016/j.asoc.2019.105674,Ensemble pruning; Feature selection; Minimax concave penalty; AdaBoost,,0.0,1.0
MultiETSC: automated machine learning for early time series classification,"Ottervanger, G; Baratchi, M; Hoos, HH",DATA MINING AND KNOWLEDGE DISCOVERY,2021.0,"Early time series classification (EarlyTSC) involves the prediction of a class label based on partial observation of a given time series. Most EarlyTSC algorithms consider the trade-off between accuracy and earliness as two competing objectives, using a single dedicated hyperparameter. To obtain insights into this trade-off requires finding a set of non-dominated (Pareto efficient) classifiers. So far, this has been approached through manual hyperparameter tuning. Since the trade-off hyperparameters only provide indirect control over the earliness-accuracy trade-off, manual tuning is tedious and tends to result in many sub-optimal hyperparameter settings. This complicates the search for optimal hyperparameter settings and forms a hurdle for the application of EarlyTSC to real-world problems. To address these issues, we propose an automated approach to hyperparameter tuning and algorithm selection for EarlyTSC, building on developments in the fast-moving research area known as automated machine learning (AutoML). To deal with the challenging task of optimising two conflicting objectives in early time series classification, we propose MultiETSC, a system for multi-objective algorithm selection and hyperparameter optimisation (MO-CASH) for EarlyTSC. MultiETSC can potentially leverage any existing or future EarlyTSC algorithm and produces a set of Pareto optimal algorithm configurations from which a user can choose a posteriori. As an additional benefit, our proposed framework can incorporate and leverage time-series classification algorithms not originally designed for EarlyTSC for improving performance on EarlyTSC; we demonstrate this property using a newly defined, naive fixed-time algorithm. In an extensive empirical evaluation of our new approach on a benchmark of 115 data sets, we show that MultiETSC performs substantially better than baseline methods, ranking highest (avg. rank 1.98) compared to conceptually simpler single-algorithm (2.98) and single-objective alternatives (4.36).",10.1007/s10618-021-00781-5,Early classification; Time series classification; Automated machine learning,,0.0,1.0
The explanation game: a formal framework for interpretable machine learning,"Watson, DS; Floridi, L",SYNTHESE,2021.0,"We propose a formal framework for interpretable machine learning. Combining elements from statistical learning, causal interventionism, and decision theory, we design an idealised explanation game in which players collaborate to find the best explanation(s) for a given algorithmic prediction. Through an iterative procedure of questions and answers, the players establish a three-dimensional Pareto frontier that describes the optimal trade-offs between explanatory accuracy, simplicity, and relevance. Multiple rounds are played at different levels of abstraction, allowing the players to explore overlapping causal patterns of variable granularity and scope. We characterise the conditions under which such a game is almost surely guaranteed to converge on a (conditionally) optimal explanation surface in polynomial time, and highlight obstacles that will tend to prevent the players from advancing beyond certain explanatory thresholds. The game serves a descriptive and a normative function, establishing a conceptual space in which to analyse and compare existing proposals, as well as design new and improved solutions.",10.1007/s11229-020-02629-9,Algorithmic explainability; Explanation game; Interpretable machine learning; Pareto frontier; Relevance,,0.0,1.0
Minimax optimal goodness-of-fit testing for densities and multinomials under a local differential privacy constraint,"Lam-Weil, J; Laurent, B; Loubes, JM",BERNOULLI,2022.0,"Finding anonymization mechanisms to protect personal data is at the heart of recent machine learning research. Here, we consider the consequences of local differential privacy constraints on goodness-of-fit testing, that is, the statistical problem assessing whether sample points are generated from a fixed density f(0), or not. The observations are kept hidden and replaced by a stochastic transformation satisfying the local differential privacy constraint. In this setting, we propose a testing procedure which is based on an estimation of the quadratic distance between the density f of the unobserved samples and f(0). We establish an upper bound on the separation distance associated with this test, and a matching lower bound on the minimax separation rates of testing under non-interactive privacy in the case that f(0) is uniform, in discrete and continuous settings. To the best of our knowledge, we provide the first minimax optimal test and associated private transformation under a local differential privacy constraint over Besov balls in the continuous setting, quantifying the price to pay for data privacy. We also present a test that is adaptive to the smoothness parameter of the unknown density and remains minimax optimal up to a logarithmic factor. Finally, we note that our results can be translated to the discrete case, where the treatment of probability vectors is shown to be equivalent to that of piecewise constant densities in our setting. That is why we work with a unified setting for both the continuous and the discrete cases.",10.3150/21-BEJ1358,Local differential privacy; non-interactive privacy; goodness-of-fit testing; minimax separation rates; continuous and discrete distributions,,0.0,1.0
An exploratory analysis of data noisy scenarios in a Pareto-front based dynamic feature selection method,"Jesus, J; Canuto, A; Araujo, D",APPLIED SOFT COMPUTING,2021.0,"Feature selection has become a mandatory step in several data exploration and Machine Learning applications since data quality can have a strong impact in the performance of machine learning models. Many feature selection strategies have been developed in the past decades, using different criteria to select the most relevant features. The use of dynamic feature selection, however, has showed that the use of multiple simultaneously criteria to determine the best attribute subset for similar instances can deliver encouraging results. In this context, this paper proposes to analyze the performance of a pareto-front based dynamic feature selection (PF-DFS) method under data noise scenarios. In order to do this, we intentionally added noise in 15 datasets and evaluated the PF-DFS performance in order to measure its stability under two different data noise scenarios. The obtained results are compared to some state-of-the-art algorithms and show that, in terms of accuracy, the PF-DFS method is more robust to the other methods for the majority of the analyzed scenarios.",10.1016/j.asoc.2020.106951,Feature selection; Clustering algorithms; Pareto-front selection; Noisy data,,0.0,1.0
Pareto Multi-task Deep Learning,"Riccio, SD; Dyankov, D; Jansen, G; Di Fatta, G; Nicosia, G","ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING, ICANN 2020, PT II",2020.0,"Neuroevolution has been used to train Deep Neural Networks on reinforcement learning problems. A few attempts have been made to extend it to address either multi-task or multi-objective optimization problems. This research work presents the Multi-Task Multi-Objective Deep Neuroevolution method, a highly parallelizable algorithm that can be adopted for tackling both multi-task and multi-objective problems. In this method prior knowledge on the tasks is used to explicitly define multiple utility functions, which are optimized simultaneously. Experimental results on some Atari 2600 games, a challenging testbed for deep reinforcement learning algorithms, show that a single neural network with a single set of parameters can outperform previous state of the art techniques. In addition to the standard analysis, all results are also evaluated using the Hypervolume indicator and the Kallback-Leibler divergence to get better insights on the underlying training dynamics. The experimental results show that a neural network trained with the proposed evolution strategy can outperform networks individually trained respectively on each of the tasks.",10.1007/978-3-030-61616-8_11,Multi-task learning; Multi-objective learning; Deep Neuroevolution; Hypervolume; Kullback-Leibler divergence; Pareto front; Evolution strategy; Atari 2600 games,,0.0,1.0
Ensemble Learning in Non-Gaussian Data Assimilation,"Seybold, H; Ravela, S; Tagade, P","DYNAMIC DATA-DRIVEN ENVIRONMENTAL SYSTEMS SCIENCE, DYDESS 2014",2015.0,"The demand for tractable non-Gaussian Bayesian estimation has increased the popularity of kernel and mixture density representations. Here, using Gaussian Mixture Models (GMM), we posit that the reduction of total variance also remains an important objective in nonlinear filtering, particularly in the presence of bias. We propose multiobjective estimation as an essential ingredient in data assimilation. Using Ensemble Learning, two relatively weak estimators, namely the EnKF and Mixture Ensemble Filter (MEnF), are combined to produce a strong one. The Boosted-MEnF (B-MEnF) stacks MEnF and EnKF to mitigate bias and uses cascade generalization to reduce variance. In the Lorenz-63 model, it lowers mixture complexity without resampling and reduces posterior variance without increasing estimation error. Our MEnF is a purely ensemble-based GMM filter with a reduced dimensionality burden and without ad-hoc ensemble-mixture member associations. It is expressed as a compact ensemble transform which enables efficient fixed-interval and fixed-lag smoothers (MEnS) as well as the B-MEnF/S.",10.1007/978-3-319-25138-7_21,Data assimilation; Gaussian mixture models; Ensemble learning; Multi-objective assimilation; Non-linear filtering and smoothing; Non-Gaussian estimation,,0.0,1.0
Margin-Based Pareto Ensemble Pruning: An Ensemble Pruning Algorithm That Learns to Search Optimized Ensembles,"Hu, RH; Zhou, SB; Liu, YS; Tang, ZR",COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE,2019.0,"The ensemble pruning system is an effective machine learning framework that combines several learners as experts to classify a test set. Generally, ensemble pruning systems aim to define a region of competence based on the validation set to select the most competent ensembles from the ensemble pool with respect to the test set. However, the size of the ensemble pool is usually fixed, and the performance of an ensemble pool heavily depends on the definition of the region of competence. In this paper, a dynamic pruning framework called margin-based Pareto ensemble pruning is proposed for ensemble pruning systems. The framework explores the optimized ensemble pool size during the overproduction stage and finetunes the experts during the pruning stage. The Pareto optimization algorithm is used to explore the size of the overproduction ensemble pool that can result in better performance. Considering the information entropy of the learners in the indecision region, the marginal criterion for each learner in the ensemble pool is calculated using margin criterion pruning, which prunes the experts with respect to the test set. The effectiveness of the proposed method for classification tasks is assessed using datasets. The results show that margin-based Pareto ensemble pruning can achieve smaller ensemble sizes and better classification performance in most datasets when compared with state-of-the-art models.",10.1155/2019/7560872,,,0.0,1.0
Semi-supervised learning with summary statistics,"Qin, HH; Guo, X",ANALYSIS AND APPLICATIONS,2019.0,"Nowadays, the extensive collection and analyzing of data is stimulating widespread privacy concerns, and therefore is increasing tensions between the potential sources of data and researchers. A privacy-friendly learning framework can help to ease the tensions, and to free up more data for research. We propose a new algorithm, LESS (Learning with Empirical feature-based Summary statistics from Semi-supervised data), which uses only summary statistics instead of raw data for regression learning. The selection of empirical features serves as a trade-off between prediction precision and the protection of privacy. We show that LESS achieves the minimax optimal rate of convergence in terms of the size of the labeled sample. LESS extends naturally to the applications where data are separately held by different sources. Compared with the existing literature on distributed learning, LESS removes the restriction of minimum sample size on single data sources.",10.1142/S0219530519400037,Distributed learning; semi-supervised learning; empirical features; summary statistics; privacy protection,,0.0,1.0
Multi-objective Decision in Machine Learning,"de Medeiros, TH; Rocha, HP; Torres, FS; Takahashi, RHC; Braga, AP",JOURNAL OF CONTROL AUTOMATION AND ELECTRICAL SYSTEMS,2017.0,"This work presents a novel approach for decisionmaking for multi-objective binary classification problems. The purpose of the decision process is to select within a set of Pareto-optimal solutions, one model that minimizes the structural risk (generalization error). This new approach utilizes a kind of prior knowledge that, if available, allows the selection of a model that better represents the problem in question. Prior knowledge about the imprecisions of the collected data enables the identification of the region of equivalent solutions within the set of Pareto-optimal solutions. Results for binary classification problems with sets of synthetic and real data indicate equal or better performance in terms of decision efficiency compared to similar approaches.",10.1007/s40313-016-0295-6,Machine learning; Multi-objective optimization; Decision-making; Classification,,0.0,1.0
An evolutionary generation method of deep neural network sets combined with Gaussian random field,"Zhang, C; Dai, ZF; Liang, XL; Xu, GH; Zhang, CS; Zhang, B",WIRELESS NETWORKS,,"As a research hotspot in the field of machine learning, ensemble learning improved the prediction accuracy of the final model by constructing and combining multiple basic models. In recent years, many experts and scholars are committed to combining deep networks with ensemble learning to improve the accuracy of neural network models in various scenarios and tasks. But not all neural networks are suitable for participating in the construction of ensemble models. Deep networks with ensemble learning require that the single neural network involved in the integration has high accuracy and great discrepancy with other networks. In the initial stage of deep networks with ensemble learning, the process of generating sets of candidate deep networks is first required. After studying an existing multiobjective deep belief networks ensemble (MODBNE) method, the Gaussian random field model is used as a pre-screening strategy in the process of generating the candidate deep network sets. Individuals with great potential for improvement are selected for fitness function evaluation so that a large number of neural network models with higher accuracy and the larger discrepancy between networks can be easily obtained, which effectively improves the quality of the solution and reduces the time consumed in training the neural networks.",10.1007/s11276-021-02677-0,Gaussian random field; Deep neural network; Differential evolution algorithm; Ensemble learning,,0.0,1.0
Training feedforward neural network via multiobjective optimization model using non-smooth L-1/2 regularization,"Senhaji, K; Ramchoun, H; Ettaouil, M",NEUROCOMPUTING,2020.0,"The paper presents a new approach to optimize the Multilayer Perceptron Neural Network (MLPNN), to deal with the generalization problem. As known, most supervised learning algorithms aim to minimize the training error. However, the mentioned methods, based only on error minimizing, may generate a solution with an insufficient generalization performance. This present work proposes a multiobjective modelling problem involving two objectives: accuracy and complexity since the learning problem is multiobjective by nature. The learning task is carried on by minimizing both objectives simultaneously, according to Pareto domination concept, using NSGAII (Non-dominated Sorting Genetic Algorithm II) as a solver. This method leads us to a set of solutions called Pareto front, being the optimal solutions set, the adequate MLPNN need to be extracted. We show empirically that the proposed method is capable of reducing the neural networks topology and improved generalization performance, in addition to a good classification rate compared to different methods. (C) 2020 Published by Elsevier B.V.",10.1016/j.neucom.2020.05.066,Multiobjective optimization; NSGAII; Learning algorithm; L-1/2 regularization; Neural network,,0.0,1.0
Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science,"Olson, RS; Bartley, N; Urbanowicz, RJ; Moore, JH",GECCO'16: PROCEEDINGS OF THE 2016 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE,2016.0,"As the field of data science continues to grow, there will be an ever-increasing demand for tools that make machine learning accessible to non-experts. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning-pipeline design. We implement an open source Tree-based Pipeline Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a series of simulated and real-world benchmark data sets. In particular, we show that TPOT can design machine learning pipelines that provide a significant improvement over a basic machine learning analysis while requiring little to no input nor prior knowledge from the user. We also address the tendency for TPOT to design overly complex pipelines by integrating Pareto optimization, which produces compact pipelines without sacrificing classification accuracy. As such, this work represents an important step toward fully automating machine learning pipeline design.",10.1145/2908812.2908918,pipeline optimization; hyperparameter optimization; data science; machine learning; genetic programming; Pareto optimization; Python,,0.0,1.0
Solving Test Case Based Problems With Fuzzy Dominance,"Zutty, J; Rohling, G",PROCEEDINGS OF THE 2017 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE (GECCO'17),2017.0,"Genetic algorithms and genetic programming lend themselves well to the field of machine learning, which involves solving test case based problems. However, most traditional multi-objective selection methods work with scalar objectives, such as minimizing false negative and false positive rates, that are computed from underlying test cases. In this paper, we propose a new fuzzy selection operator that takes into account the statistical nature of machine learning problems based on test cases. Rather than use a Pareto rank or strength computed from scalar objectives, such as with NSGA2 or SPEA2, we will compute a probability of Pareto optimality. This will be accomplished through covariance estimation and Markov chain Monte Carlo simulation in order to generate probabilistic objective scores for each individual. We then compute a probability that each individual will generate a Pareto optimal solution. This probability is directly used with a roulette wheel selection technique. Our method's performance is evaluated on the evolution of a feature selection vector for a binary classification on each of eight different activities. Fuzzy selection performance varies, outperforming both NSGA2 and SPEA2 in both speed (measured in generations) and solution quality (measured by area under the curve) in some cases, while underperforming in others.",10.1145/3071178.3071234,Genetic Algorithms; Machine Learning; Markov Chain Monte Carlo; Pareto Dominance,,0.0,1.0
Efficient multi-criteria optimization on noisy machine learning problems,"Koch, P; Wagner, T; Emmerich, MTM; Back, T; Konen, W",APPLIED SOFT COMPUTING,2015.0,"Recent research revealed that model-assisted parameter tuning can improve the quality of supervised machine learning (ML) models. The tuned models were especially found to generalize better and to be more robust compared to other optimization approaches. However, the advantages of the tuning often came along with high computation times, meaning a real burden for employing tuning algorithms. While the training with a reduced number of patterns can be a solution to this, it is often connected with decreasing model accuracies and increasing instabilities and noise. Hence, we propose a novel approach defined by a two criteria optimization task, where both the runtime and the quality of ML models are optimized. Because the budgets for this optimization task are usually very restricted in ML, the surrogate-assisted Efficient Global Optimization (EGO) algorithm is adapted. In order to cope with noisy experiments, we apply two hypervolume indicator based EGO algorithms with smoothing and reinterpolation of the surrogate models. The techniques do not need replicates. We find that these EGO techniques can outperform traditional approaches such as latin hypercube sampling (LHS), as well as EGO variants with replicates. (C) 2015 Elsevier B.V. All rights reserved.",10.1016/j.asoc.2015.01.005,Machine learning; Multi-criteria optimization; Efficient Global Optimization; Kriging; Hypervolume indicator,,0.0,1.0
The MBPEP: a deep ensemble pruning algorithm providing high quality uncertainty prediction,"Hu, RH; Huang, QJ; Chang, S; Wang, H; He, J",APPLIED INTELLIGENCE,2019.0,"Machine learning algorithms have been effectively applied into various real world tasks. However, it is difficult to provide high-quality machine learning solutions to accommodate an unknown distribution of input datasets; this difficulty is called the uncertainty prediction problems. In this paper, a margin-based Pareto deep ensemble pruning (MBPEP) model is proposed. It achieves the high-quality uncertainty estimation with a small value of the prediction interval width (MPIW) and a high confidence of prediction interval coverage probability (PICP) by using deep ensemble networks. In addition to these networks, unique loss functions are proposed, and these functions make the sub-learners available for standard gradient descent learning. Furthermore, the margin criterion fine-tuning-based Pareto pruning method is introduced to optimize the ensembles. Several experiments including predicting uncertainties of classification and regression are conducted to analyze the performance of MBPEP. The experimental results show that MBPEP achieves a small interval width and a low learning error with an optimal number of ensembles. For the real-world problems, MBPEP performs well on input datasets with unknown distributions datasets incomings and improves learning performance on a multi task problem when compared to that of each single model.",10.1007/s10489-019-01421-8,Uncertainty prediction; Ensemble pruning; Loss function; Margin criterion tuning,,0.0,1.0
DEEP NEURAL NETWORKS FOR ESTIMATION AND INFERENCE,"Farrell, MH; Liang, TY; Misra, S",ECONOMETRICA,2021.0,"We study deep neural networks and their use in semiparametric inference. We establish novel nonasymptotic high probability bounds for deep feedforward neural nets. These deliver rates of convergence that are sufficiently fast (in some cases minimax optimal) to allow us to establish valid second-step inference after first-step estimation with deep learning, a result also new to the literature. Our nonasymptotic high probability bounds, and the subsequent semiparametric inference, treat the current standard architecture: fully connected feedforward neural networks (multilayer perceptrons), with the now-common rectified linear unit activation function, unbounded weights, and a depth explicitly diverging with the sample size. We discuss other architectures as well, including fixed-width, very deep networks. We establish the nonasymptotic bounds for these deep nets for a general class of nonparametric regression-type loss functions, which includes as special cases least squares, logistic regression, and other generalized linear models. We then apply our theory to develop semiparametric inference, focusing on causal parameters for concreteness, and demonstrate the effectiveness of deep learning with an empirical application to direct mail marketing.",10.3982/ECTA16901,Deep learning; neural networks; rectified linear unit; nonasymptotic bounds; convergence rates; semiparametric inference; treatment effects; program evaluation,,0.0,1.0
A multiobjective optimization-based sparse extreme learning machine algorithm,"Wu, Y; Zhang, YS; Liu, XB; Cai, ZH; Cai, YM",NEUROCOMPUTING,2018.0,"Extreme Learning Machine (ELM) is a popular machine learning method and has been widely applied to real-world problems due to its fast training speed and good generalization performance. However, in ELM, the randomly assigned input weights and hidden biases usually degrade the generalization performance. Furthermore, ELM is considered as an empirical risk minimization model and easily leads to overfitting when dataset exists some outliers. In this paper, we proposed a novel algorithm named Multiobjective Optimization-based Sparse Extreme Learning Machine (MO-SELM), where parameter optimization and structure learning are integrated into the learning process to simultaneously enhance the generalization performance and alleviate the overfitting problem. In MO-SELM, the training error and the connecting sparsity are taken as two conflicting objectives of the multiobjective model, which aims to find sparse connecting structures with optimal weights and biases. Then, a hybrid encoding-based MOEA/D is used to optimize the multiobjective model. In addition, ensemble learning is embedded into this algorithm to make decisions after multiobjective optimization. Experimental results of several classification and regression applications demonstrate the effectiveness of the proposed MO-SELM. (C) 2018 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2018.07.060,Extreme learning machine; Sparse connecting structure; Parameter optimization; Structure learning; Multiobjective optimization,,0.0,1.0
Maxi-min margin machine: Learning large margin classifiers locally and globally,"Huang, KZ; Yang, HQ; King, I; Lyu, MR",IEEE TRANSACTIONS ON NEURAL NETWORKS,2008.0,"In this paper, we propose a novel large margin classifier, called the maxi-min margin machine (M-4). This model learns the decision boundary both locally and globally. In comparison, other large margin classifiers construct separating hyperplanes only either locally or globally. For example, a state-of-the-art large margin classifier, the support vector machine (SVM), considers data only locally, while another significant model, the minimax probability machine (MPM), focuses on building the decision hyperplane exclusively based on the global information. As a major contribution, we show that SVM yields the same solution as M-4 when data satisfy certain conditions, and MPM can be regarded as a relaxation model of M-4. Moreover, based on our proposed local and global view of data, another popular model, the linear discriminant analysis, can easily be interpreted and extended as well. We describe the M-4 model definition, provide a geometrical interpretation, present theoretical justifications, and propose a practical sequential conic programming method to solve the optimization problem. We also show how to exploit Mercer kernels to extend M-4 for nonlinear classifications. Furthermore, we perform a series of evaluations on both synthetic data sets and real-world benchmark data sets. Comparison with SVM and MPM demonstrates the advantages of our new model.",10.1109/TNN.2007.905855,classification; kernel methods; large margin; learning locally and globally; second-order cone programming,,0.0,1.0
Multi-objective learning of hybrid classifiers,"Piltaver, R; Lustrek, M; Zupancic, J; Dzeroski, S; Gams, M",21ST EUROPEAN CONFERENCE ON ARTIFICIAL INTELLIGENCE (ECAI 2014),2014.0,"We propose a multi-objective machine learning approach guaranteed to find the Pareto optimal set of hybrid classification models consisting of comprehensible and incomprehensible submodels. The algorithm run-times are below 1 s for typical applications despite the exponential worst-case time complexity. The user chooses the model with the best comprehensibility-accuracy trade-off from the Pareto front which enables a well informed decision or repeats finding new Pareto fronts with modified seeds. For a classification trees as the comprehensible seed, the hybrids include single black-box model, invoked in hybrid leaves. The comprehensibility of such hybrid classifiers is measured with the proportion of examples classified by the regular leaves. We propose one simple and one computationally efficient algorithm for finding the Pareto optimal hybrid trees, starting from an initial classification tree and a black-box classifier. We evaluate the proposed algorithms empirically, comparing them to the baseline solution set, showing that they often provide valuable improvements. Furthermore, we show that the efficient algorithm outperforms the NSGA-II algorithm in terms of quality of the result set and efficiency (for this optimisation problem). Finally we show that the algorithm returns hybrid classifiers that reflect the expert's knowledge on activity recognition problem well.",10.3233/978-1-61499-419-0-717,,,0.0,1.0
Robust multiobjective evolutionary feature subset selection algorithm for binary classification using machine learning techniques,"Deniz, A; Kiziloz, HE; Dokeroglu, T; Cosar, A",NEUROCOMPUTING,2017.0,"This study investigates the success of a multiobjective genetic algorithm (GA) combined with state-of-the-art machine learning (ML) techniques for the feature subset selection (FSS) in binary classification problem (BCP). Recent studies have focused on improving the accuracy of BCP by including all of the features, neglecting to determine the best performing subset of features. However, for some problems, the number of features may reach thousands, which will cause too much computation power to be consumed during the feature evaluation and classification phases, also possibly reducing the accuracy of the results. Therefore, selecting the minimum number of features while preserving and/or increasing the accuracy of the results at a high level becomes an important issue for achieving fast and accurate binary classification. Our multiobjective evolutionary algorithm includes two phases, FSS using a GA and applying ML techniques for the BCP. Since exhaustively investigating all of the feature subsets is intractable, a GA is preferred for the first phase of the algorithm for intelligently detecting the most appropriate feature subset. The GA uses multiobjective crossover and mutation operators to improve a population of individuals (each representing a selected feature subset) and obtain (near-) optimal solutions through generations. In the second phase of the algorithms, the fitness of the selected subset is decided by using state-of-the-art ML techniques; Logistic Regression, Support Vector Machines, Extreme Learning Machine, K-means, and Affinity Propagation. The performance of the multiobjective evolutionary algorithm (and the ML techniques) is evaluated with comprehensive experiments and compared with state-of-the-art algorithms, Greedy Search, Particle Swarm Optimization, Tabu Search, and Scatter Search. The proposed algorithm was observed to be robust and it performed better than the existing methods on most of the datasets. (C) 2017 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2017.02.033,Multiobjective feature selection; Evolutionary algorithm; Binary classification; Supervised/unsupervised machine learning,,0.0,1.0
Artificial chicken swarm algorithm for multi-objective optimization with deep learning,"Wei, QZ; Huang, DR; Zhang, Y",JOURNAL OF SUPERCOMPUTING,2021.0,"With the rapid development of computer hardware in the past three decades, various classic algorithms such as neural computing and bionic optimization computing have been widely used in practical problems. This paper extended the new bionic algorithm-flock algorithm proposed in 2014 and obtained a multi-objective flock algorithm to solve the multi-objective problem. This study used aggregate functions to define social ranks, and simulated the foraging behavior of chickens in the process of searching for food in the objective space and found the balance between diversity and convergence when looking for the best Pareto solution. The algorithm took five types of bi-objective functions and four types of three-objective functions as objects and compared it with four more widely used algorithms in multi-objective problems. The results demonstrate that the MOCSO (multi-objective chicken swarm optimization) algorithm shows better results in the optimization of multi-objective problems.",10.1007/s11227-021-03770-z,Neural computing; Bionic optimization calculation; Multi-objective optimization; Chicken swarm optimization algorithm; Pareto solution set; Deep learning,,0.0,1.0
LEARNING MODELS WITH UNIFORM PERFORMANCE VIA DISTRIBUTIONALLY ROBUST OPTIMIZATION,"Duchi, JC; Namkoong, H",ANNALS OF STATISTICS,2021.0,"A common goal in statistics and machine learning is to learn models that can perform well against distributional shifts, such as latent heterogeneous subpopulations, unknown covariate shifts or unmodeled temporal effects. We develop and analyze a distributionally robust stochastic optimization (DRO) framework that learns a model providing good performance against perturbations to the data-generating distribution. We give a convex formulation for the problem, providing several convergence guarantees. We prove finite-sample minimax upper and lower bounds, showing that distributional robustness sometimes comes at a cost in convergence rates. We give limit theorems for the learned parameters, where we fully specify the limiting distribution so that confidence intervals can be computed. On real tasks including generalizing to unknown subpopulations, fine-grained recognition and providing good tail performance, the distributionally robust approach often exhibits improved performance.",10.1214/20-AOS2004,Robust optimization; minimax optimality; risk-averse learning,,0.0,1.0
Robust randomized optimization with k nearest neighbors,"Reeve, HWJ; Kaban, A",ANALYSIS AND APPLICATIONS,2019.0,"Modern applications of machine learning typically require the tuning of a multitude of hyperparameters. With this motivation in mind, we consider the problem of optimization given a set of noisy function evaluations. We focus on robust optimization in which the goal is to find a point in the input space such that the function remains high when perturbed by an adversary within a given radius. Here we identify the minimax optimal rate for this problem, which turns out to be of order O(n(-lambda/(2 lambda+ 1))), where n is the sample size and lambda quantifies the smoothness of the function for a broad class of problems, including situations where the metric space is unbounded. The optimal rate is achieved (up to logarithmic factors) by a conceptually simple algorithm based on k-nearest neighbor regression.",10.1142/S0219530519400086,Optimization for machine learning; metric spaces; non-parametric methods,,0.0,1.0
A multi-objective evolutionary approach to training set selection for support vector machine,"Acampora, G; Herrera, F; Tortora, G; Vitiello, A",KNOWLEDGE-BASED SYSTEMS,2018.0,"The Support Vector Machine (SVM) is one of the most powerful algorithms for machine learning and data mining in numerous and heterogenous application domains. However, in spite of its competitiveness, SVM suffers from scalability problems which drastically worsens its performance in terms of memory requirements and execution time. As a consequence, there is a strong emergence of approaches for supporting SVM in efficiently addressing the aforementioned problems without affecting its classification capabilities. In this scenario, methods for Training Set Selection (TSS) represent a suitable and consolidated pre-processing technique to compute a reduced but representative training dataset, and improve SVM's scalability without deprecating its classification accuracy. Recently, TSS has been formulated as an optimization problem characterized by two objectives (the classification accuracy and the reduction rate) and solved through the application of evolutionary algorithms. However, so far, all the evolutionary approaches for TSS have been based on a so-called multi-objective a priori technique, where multiple objectives are aggregated together into a single objective through a weighted combination. This paper proposes to apply, for the first time, a Pareto-based multi-objective optimization approach to the TSS problem in order to explicitly deal with both its objectives and offer a better trade-off between SVM's classification and reduction performance. The benefits of the proposed approach are validated by a set of experiments involving well-known datasets taken from the UCI Machine Learning Database Repository. As shown by statistical tests, the application of a Pareto-based multi-objective optimization approach improves on state-of-the-art TSS techniques and enhances SVM efficiency. (c) 2018 Elsevier B.V. All rights reserved.",10.1016/j.knosys.2018.02.022,Training set selection; Multi-objective optimization; Support vector machine,,0.0,1.0
MCR SVM classifier with group sparsity,"Liu, JW; Cui, LP; Luo, XL",OPTIK,2016.0,"Classification and dimensionality reduction of high-dimensional data are two important topics in bioinformatics, data mining and machine learning. We propose a novel sparse minimax concave ridge support vector machine (MCR SVM) classifier that simultaneously performs classification and dimensionality reduction. The MCR SVM classifier proposed in this study combines the advantages of the unbiasedness of the estimators of the SCAD SVM and the ability of feature group selection of HHSVM to overcome the disadvantages. We also provide a theoretical justification for the group sparsity of the selected features. The experiments on artificial highly correlated data and high-dimensional real-world data with a small sample size show that the MCR SVM classifier is a attractive technique of classification and dimensionality reduction and its performance is better than the other sparse SVMs. (C) 2016 Elsevier GmbH. All rights reserved.",10.1016/j.ijleo.2016.03.060,Sparsity; Feature selection; Group feature selection; MCR penalty; MCR SVM,,0.0,1.0
Learning comprehensible and accurate hybrid trees,"Piltaver, R; Lustrek, M; Dzeroski, S; Gjoreski, M; Gams, M",EXPERT SYSTEMS WITH APPLICATIONS,2021.0,"Finding the best classifiers according to different criteria is often performed by a multi-objective machine learning algorithm. This study considers two criteria that are usually treated as the most important when deciding which classifier to apply in practice: comprehensibility and accuracy. A model that offers a broad range of trade-offs between the two criteria is introduced because they conflict; i.e., increasing one decreases the other. The choice of the model is motivated by the fact that domain experts often formalize decisions based on knowledge that can be represented by comprehensible rules and some tacit knowledge. This approach is mimicked by a hybrid tree that consists of comprehensible parts that originate from a regular classification tree and incomprehensible parts that originate from an accurate black-box classifier. An empirical evaluation on 23 UCI datasets shows that the hybrid trees provide trade-offs between the accuracy and comprehensibility that are not possible using traditional machine learning models. A corresponding hybrid-tree comprehensibility metric is also proposed. Furthermore, the paper presents a novel algorithm for learning MAchine LeArning Classifiers with HybrId TrEes (MALACHITE), and it proves that the algorithm finds a complete set of nondominated hybrid trees with regard to their accuracy and comprehensibility. The algorithm is shown to be faster than the well-known multi-objective evolutionary optimization algorithm NSGA-II for trees with moderate size, which is a prerequisite for comprehensibility. On the other hand, the MALACHITE algorithm can generate considerably larger hybrid-trees than a naive exhaustive search algorithm in a reasonable amount of time. In addition, an interactive iterative data mining process based on the algorithm is proposed that enables inspection of the Pareto set of hybrid trees. In each iteration, the domain expert analyzes the current set of nondominated hybrid trees, infers domain relations, and sets the parameters for the next machine learning step accordingly.",10.1016/j.eswa.2020.113980,Hybrid tree; Multi-objective learning; Comprehensibility; Accuracy; Classification,,0.0,1.0
High Dimensional Restrictive Federated Model Selection with Multi-objective Bayesian Optimization over Shifted Distributions,"Sun, XD; Bommert, A; Pfisterer, F; Rahenfurher, J; Lang, M; Bischl, B","INTELLIGENT SYSTEMS AND APPLICATIONS, VOL 1",2020.0,"A novel machine learning optimization process coined Restrictive Federated Model Selection (RFMS) is proposed under the scenario, for example, when data from healthcare units can not leave the site it is situated on and it is forbidden to carry out training algorithms on remote data sites due to either technical or privacy and trust concerns. To carry out a clinical research in this scenario, an analyst could train a machine learning model only on local data site, but it is still possible to execute a statistical query at a certain cost in the form of sending a machine learning model to some of the remote data sites and get the performance measures as feedback, maybe due to prediction being usually much cheaper. Compared to federated learning, which is optimizing the model parameters directly by carrying out training across all data sites, RFMS trains model parameters only on one local data site but optimizes hyper parameters across other data sites jointly since hyper-parameters play an important role in machine learning performance. The aim is to get a Pareto optimal model with respective to both local and remote unseen prediction losses, which could generalize well across data sites. In this work, we specifically consider high dimensional data with different distributions over data sites. As an initial investigation, Bayesian Optimization especially multi-objective Bayesian Optimization is used to guide an adaptive hyper-parameter optimization process to select models under the RFMS scenario. Empirical results shows that solely using the local data site to tune hyper-parameters generalizes poorly across data sites, compared to methods that utilize the local and remote performances. Furthermore, in terms of hypervolumes, multi-objective Bayesian Optimization algorithms show increased performance across multiple data sites among other candidates.",10.1007/978-3-030-29516-5_48,Federated learning; Multi-objective Bayesian Optimization; High dimensional data; Differential privacy; Distribution shift; Model selection,,0.0,1.0
SPRINT Multi-Objective Model Racing,"Zhang, TT; Georgiopoulos, M; Anagnostopoulos, GC",GECCO'15: PROCEEDINGS OF THE 2015 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE,2015.0,"Multi-objective model selection, which is an important aspect of Machine Learning, refers to the problem of identifying a set of Pareto optimal models from a given ensemble of models. This paper proposes SPRINT-Race, a multi-objective racing algorithm based on the Sequential Probability Ratio Test with an Indifference Zone. In SPRINT-Race, a non-parametric ternary-decision sequential analogue of the sign test is adopted to identify pair-wise dominance and non-dominance relationship. In addition, a Bonferroni approach is employed to control the overall probability of any erroneous decisions. In the fixed confidence setting, SPRINT-Race tries to minimize the computational effort needed to achieve a predefined confidence about the quality of the returned models. The efficiency of SPRTNT-Race is analyzed on artificially-constructed multi-objective model selection problems with known ground-truth. Moreover, SPRINT-Race is applied to identifying the Pareto optimal parameter settings of Ant Colony Optimization algorithms in the context of solving Traveling Salesman Problems. The experimental results confirm the advantages of SPRINT-Race for multi-objective model selection.",10.1145/2739480.2754791,Racing Algorithm; Model Selection; Multi-objective Optimization; Sequential Probability Ratio Test,,0.0,1.0
A Method for Entity Resolution in High Dimensional Data Using Ensemble Classifiers,"Liu, Y; Diao, XC; Cao, JJ; Zhou, X; Shang, YL",MATHEMATICAL PROBLEMS IN ENGINEERING,2017.0,"In order to improve utilization rate of high dimensional data features, an ensemble learning method based on feature selection for entity resolution is developed. Entity resolution is regarded as a binary classification problem, an optimization model is designed to maximize each classifier's classification accuracy and dissimilarity between classifiers and minimize cardinality of features. A modified multiobjective ant colony optimization algorithm is employed to solve the model for each base classifier, two pheromone matrices are set up, weighted product method is applied to aggregate values of two pheromone matrices, and feature's Fisher discriminant rate of records' similarity vector is calculated as heuristic information. A solution which is called complementary subset is selected from Pareto archive according to the descending order of three objectives to train the given base classifier. After training all base classifiers, their classification outputs are aggregated by max-wins voting method to obtain the ensemble classifiers' final result. A simulation experiment is carried out on three classical datasets. The results show the effectiveness of our method, as well as a better performance compared with the other two methods.",10.1155/2017/4953280,,,0.0,1.0
A projection multi-objective SVM method for multi-class classification,"Liu, L; Martin-Barragan, B; Prieto, FJ",COMPUTERS & INDUSTRIAL ENGINEERING,2021.0,"Support Vector Machines (SVMs), originally proposed for classifications of two classes, have become a very popular technique in the machine learning field. For multi-class classifications, various single-objective models and multi-objective ones have been proposed. However,in most single-objective models, neither the different costs of different misclassifications nor the users' preferences were considered. This drawback has been taken into account in multi-objective models. In these models, large and hard second-order cone programs(SOCPs) were constructed ane weakly Pareto-optimal solutions were offered. In this paper, we propose a Projected Multi-objective SVM (PM), which is a multi-objective technique that works in a higher dimensional space than the object space. For PM, we can characterize the associated Pareto-optimal solutions. Additionally, it significantly alleviates the computational bottlenecks for classifications with large numbers of classes. From our experimental results, we can see PM outperforms the single-objective multi-class SVMs (based on an all-together method, one-against-all method and one-against-one method) and other multi-objective SVMs. Compared to the single-objective multi-class SVMs, PM provides a wider set of options designed for different misclassifications, without sacrificing training time. Compared to other multi-objective methods, PM promises the out-of-sample quality of the approximation of the Pareto frontier, with a considerable reduction of the computational burden.",10.1016/j.cie.2021.107425,Multiple objective programming; Support vector machine; Multi-class multi-objective SVM; Pareto-optimal solution,,0.0,1.0
GGADN: Guided generative adversarial dehazing network,"Zhang, J; Dong, QQ; Song, WJ",SOFT COMPUTING,,"Image dehazing has always been a challenging topic in image processing. The development of deep learning methods, especially the generative adversarial networks (GAN), provides a new way for image dehazing. In recent years, many deep learning methods based on GAN have been applied to image dehazing. However, GAN has two problems in image dehazing. Firstly, For haze image, haze not only reduces the quality of the image but also blurs the details of the image. For GAN network, it is difficult for the generator to restore the details of the whole image while removing the haze. Secondly, GAN model is defined as a minimax problem, which weakens the loss function. It is difficult to distinguish whether GAN is making progress in the training process. Therefore, we propose a guided generative adversarial dehazing network (GGADN). Different from other generation adversarial networks, GGADN adds a guided module on the generator. The guided module verifies the network of each layer of the generator. At the same time, the details of the map generated by each layer are strengthened. Network training is based on the pre-trained VGG feature model and L1-regularized gradient prior which is developed by new loss function parameters. From the dehazing results of synthetic images and real images, the proposed method is better than the state-of-the-art dehazing methods.",10.1007/s00500-021-06049-w,Dehazing; Generative adversarial networks; Guidance; Loss function,,0.0,1.0
Optimizing logistic regression coefficients for discrimination and calibration using estimation of distribution algorithms,"Robles, V; Bielza, C; Larranaga, P; Gonzalez, S; Ohno-Machado, L",TOP,2008.0,"Logistic regression is a simple and efficient supervised learning algorithm for estimating the probability of an outcome or class variable. In spite of its simplicity, logistic regression has shown very good performance in a range of fields. It is widely accepted in a range of fields because its results are easy to interpret. Fitting the logistic regression model usually involves using the principle of maximum likelihood. The Newton-Raphson algorithm is the most common numerical approach for obtaining the coefficients maximizing the likelihood of the data. This work presents a novel approach for fitting the logistic regression model based on estimation of distribution algorithms (EDAs), a tool for evolutionary computation. EDAs are suitable not only for maximizing the likelihood, but also for maximizing the area under the receiver operating characteristic curve (AUC). Thus, we tackle the logistic regression problem from a double perspective: likelihood-based to calibrate the model and AUC-based to discriminate between the different classes. Under these two objectives of calibration and discrimination, the Pareto front can be obtained in our EDA framework. These fronts are compared with those yielded by a multiobjective EDA recently introduced in the literature.",10.1007/s11750-008-0054-3,Logistic regression; Evolutionary algorithms; Estimation of distribution algorithms; Calibration and discrimination; 62J12; 90C59; 90C29,,0.0,1.0
Understanding and Improving Fairness-Accuracy Trade-offs in Multi-Task Learning,"Wang, YY; Wang, XZ; Beutel, A; Prost, F; Chen, JL; Chi, EH",KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING,2021.0,"As multi-task models gain popularity in a wider range of machine learning applications, it is becoming increasingly important for practitioners to understand the fairness implications associated with those models. Most existing fairness literature focuses on learning a single task more fairly, while how ML fairness interacts with multiple tasks in the joint learning setting is largely under-explored. In this paper, we are concerned with how group fairness (e.g., equal opportunity, equalized odds) as an ML fairness concept plays out in the multi-task scenario. In multi-task learning, several tasks are learned jointly to exploit task correlations for a more efficient inductive transfer. This presents a multi-dimensional Pareto frontier on (1) the trade-off between group fairness and accuracy with respect to each task, as well as (2) the trade-offs across multiple tasks. We aim to provide a deeper understanding on how group fairness interacts with accuracy in multi-task learning, and we show that traditional approaches that mainly focus on optimizing the Pareto frontier of multi-task accuracy might not perform well on fairness goals. We propose a new set of metrics to better capture the multi-dimensional Pareto frontier of fairness-accuracy tradeoffs uniquely presented in a multi-task learning setting. We further propose a Multi-Task-Aware Fairness (MTA-F) approach to improve fairness in multi-task learning. Experiments on several real-world datasets demonstrate the effectiveness of our proposed approach.",10.1145/3447548.3467326,fairness; multi-task learning; Pareto frontier; multi-task-aware fairness treatment,,0.0,1.0
"Assessing the frontier: Active learning, model accuracy, and multi-objective candidate discovery and optimization","del Rosario, Z; Rupp, M; Kim, Y; Antono, E; Ling, J",JOURNAL OF CHEMICAL PHYSICS,2020.0,"Discovering novel chemicals and materials can be greatly accelerated by iterative machine learning-informed proposal of candidates-active learning. However, standard global error metrics for model quality are not predictive of discovery performance and can be misleading. We introduce the notion of Pareto shell error to help judge the suitability of a model for proposing candidates. Furthermore, through synthetic cases, an experimental thermoelectric dataset and a computational organic molecule dataset, we probe the relation between acquisition function fidelity and active learning performance. Results suggest novel diagnostic tools, as well as new insights for the acquisition function design. (c) 2020 Author(s). All article content, except where otherwise noted, is licensed under a Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).",10.1063/5.0006124,,,0.0,1.0
A dual-objective evolutionary algorithm for rules extraction in data mining,"Tan, KC; Yu, Q; Ang, JH",COMPUTATIONAL OPTIMIZATION AND APPLICATIONS,2006.0,"This paper presents a dual-objective evolutionary algorithm (DOEA) for extracting multiple decision rule lists in data mining, which aims at satisfying the classification criteria of high accuracy and ease of user comprehension. Unlike existing approaches, the algorithm incorporates the concept of Pareto dominance to evolve a set of non-dominated decision rule lists each having different classification accuracy and number of rules over a specified range. The classification results of DOEA are analyzed and compared with existing rule-based and non-rule based classifiers based upon 8 test problems obtained from UCI Machine Learning Repository. It is shown that the DOEA produces comprehensible rules with competitive classification accuracy as compared to many methods in literature. Results obtained from box plots and t-tests further examine its invariance to random partition of datasets.",10.1007/s10589-005-3907-9,data mining; evolutionary algorithm; classification; rules extraction,,0.0,1.0
Ensemble of heterogeneous flexible neural trees using multiobjective genetic programming,"Ojha, VK; Abraham, A; Snasel, V",APPLIED SOFT COMPUTING,2017.0,"Machine learning algorithms are inherently multiobjective in nature, where approximation error minimization and models complexity simplification are two conflicting objectives. We proposed a multiobjective genetic programming (MOGP) for creating a heterogeneous flexible neural tree (HFNT), tree-like flexible feedforward neural network model. The functional heterogeneity in neural tree nodes was introduced to capture a better insight of data during learning because each input in a dataset possess different features. MOGP guided an initial HFNT population towards Pareto-optimal solutions, where the final population was used for making an ensemble system. A diversity index measure along with approximation error and complexity was introduced to maintain diversity among the candidates in the population. Hence, the ensemble was created by using accurate, structurally simple, and diverse candidates from MOGP final population. Differential evolution algorithm was applied to fine-tune the underlying parameters of the selected candidates. A comprehensive test over classification, regression, and time-series datasets proved the efficiency of the proposed algorithm over other available prediction methods. Moreover, the heterogeneous creation of HFNT proved to be efficient in making ensemble system from the final population. (C) 2016 Elsevier B.V. All rights reserved.",10.1016/j.asoc.2016.09.035,Pareto-based multiobjectives; Flexible neural tree; Ensemble; Approximation; Feature selection,,0.0,1.0
Feature weighting and selection with a Pareto-optimal trade-off between relevancy and redundancy,"Das, A; Das, S",PATTERN RECOGNITION LETTERS,2017.0,"Feature Selection (FS) is an important pre-processing step in machine learning and it reduces the number of features/variables used to describe each member of a dataset. Such reduction occurs by eliminating some of the non-discriminating and redundant features and selecting a subset of the existing features with higher discriminating power among various classes in the data. In this paper, we formulate the feature selection as a bi-objective optimization problem of some real-valued weights corresponding to each feature. A subset of the weighted features is thus selected as the best subset for subsequent classification of the data. Two information theoretic measures, known as 'relevancy' and 'redundancy' are chosen for designing the objective functions for a very competitive Multi-Objective Optimization (MOO) algorithm called 'Multi-Objective Evolutionary Algorithm based on Decomposition (MOEA/D)'. We experimentally determine the best possible constraints on the weights to be optimized. We evaluate the proposed bi-objective feature selection and weighting framework on a set of 15 standard datasets by using the popular k-Nearest Neighbor (k-NN) classifier. As is evident from the experimental results, our method appears to be quite competitive to some of the state-of-the-art FS methods of current interest. We further demonstrate the effectiveness of our framework by changing the choices of the optimization scheme and the classifier to Non-dominated Sorting Genetic Algorithm (NSGA)-II and Support Vector Machines (SVMs) respectively. (C) 2017 Elsevier B.V. All rights reserved.",10.1016/j.patrec.2017.01.004,Feature selection; Feature weighting; Multi-objective optimization; Information measure; Classification,,0.0,1.0
Boosting with structural sparsity: A differential inclusion approach,"Huang, CD; Sun, XW; Xiong, JC; Yao, Y",APPLIED AND COMPUTATIONAL HARMONIC ANALYSIS,2020.0,"Boosting as gradient descent algorithms is one popular method in machine learning. In this paper a novel Boosting-type algorithm is proposed based on restricted gradient descent with structural sparsity control whose underlying dynamics are governed by differential inclusions. In particular, we present an iterative regularization path with structural sparsity where the parameter is sparse under some linear transforms, based on variable splitting and the Linearized Bregman Iteration. Hence it is called Split LBI. Despite its simplicity, Split LBI outperforms the popular generalized Lasso in both theory and experiments. A theory of path consistency is presented that equipped with a proper early stopping, Split LBI may achieve model selection consistency under a family of Irrepresentable Conditions which can be weaker than the necessary and sufficient condition for generalized Lasso. Furthermore, some l(2) error bounds are also given at the minimax optimal rates. The utility and benefit of the algorithm are illustrated by several applications including image denoising, partial order ranking of sport teams, and world university grouping with crowdsourced ranking data. (C) 2018 Elsevier Inc. All rights reserved.",10.1016/j.acha.2017.12.004,Boosting; Differential inclusions; Structural sparsity; Linearized Bregman iteration; Variable splitting; Generalized Lasso; Model selection; Consistency,,0.0,1.0
CHIME: CLUSTERING OF HIGH-DIMENSIONAL GAUSSIAN MIXTURES WITH EM ALGORITHM AND ITS OPTIMALITY,"Cai, TT; Ma, J; Zhang, LJ",ANNALS OF STATISTICS,2019.0,"Unsupervised learning is an important problem in statistics and machine learning with a wide range of applications. In this paper, we study clustering of high-dimensional Gaussian mixtures and propose a procedure, called CHIME, that is based on the EM algorithm and a direct estimation method for the sparse discriminant vector. Both theoretical and numerical properties of CHIME are investigated. We establish the optimal rate of convergence for the excess misclustering error and show that CHIME is minimax rate optimal. In addition, the optimality of the proposed estimator of the discriminant vector is also established. Simulation studies show that CHIME outperforms the existing methods under a variety of settings. The proposed CHIME procedure is also illustrated in an analysis of a glioblastoma gene expression data set and shown to have superior performance. Clustering of Gaussian mixtures in the conventional low-dimensional setting is also considered. The technical tools developed for the high-dimensional setting are used to establish the optimality of the clustering procedure that is based on the classical EM algorithm.",10.1214/18-AOS1711,High-dimensional data; unsupervised learning; Gaussian mixture model; EM algorithm; misclustering error; Minimax optimality,,0.0,1.0
Maximum relevance minimum common redundancy feature selection for nonlinear data,"Che, JX; Yang, YL; Li, L; Bai, XY; Zhang, SH; Deng, CZ",INFORMATION SCIENCES,2017.0,"In recent years, feature selection based on relevance redundancy trade-off criteria has become a very promising and popular approach in the field of machine learning. However, the existing algorithmic frameworks of mutual information feature selection have certain limitations for the common feature selection problems in practice. To overcome these limitations, the idea of a new framework is developed by introducing a novel maximum relevance and minimum common redundancy criterion and a minimax nonlinear optimization approach. In particular, a novel mutual information feature selection method based on the normalization of the maximum relevance and minimum common redundancy (N-MRMCR-MI) is presented, which produces a normalized value in the range [0, 1] and results in a regression problem. We perform extensive experimental comparisons over numerous state-of-art algorithms using different forecasts (Bayesian Additive Regression tree, treed Gaussian process, k-NN, and SVM) and different data sets (two simulated and five real datasets). The results show that the proposed algorithm outperforms the others in terms of feature selection and forecasting accuracy. (C) 2017 Elsevier Inc. All rights reserved.",10.1016/j.ins.2017.05.013,Feature selection; Mutual information; Normalization; Minimal common redundancy; Maximal relevance,,0.0,1.0
Multi-objective cost-sensitive attribute reduction on data with error ranges,"Fang, Y; Liu, ZH; Min, F",INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS,2016.0,"In current supervised machine learning research spectrum, there are several attribute reduction methodologies to acquire reducts with low test cost. They can deal with symbolic data, or numeric data with error ranges. In many cases, they consider the situation with only one type of cost; therefore the problem is single-objective. This paper addresses the attribute reduction problem on data with multi-type-costs and error ranges. First, we define the multi-objective attribute reduction problem where multi-type-costs are involved. Second, we propose three metrics to evaluate the quality of a reduct set. Third, we design a backtrack algorithm to compute the Pareto optimal set, and a heuristic algorithm to find a sub-optimal reduct set. Finally, we compare these algorithms on seven UCI (University of California-Irvine) datasets. Experimental results indicate that our heuristic algorithm has good capability of tackling the proposed problem.",10.1007/s13042-014-0296-3,Cost-sensitive learning; Attribute reduction; Test cost; Error range,,0.0,1.0
A multi-objective optimization algorithm for feature selection problems,"Abdollahzadeh, B; Gharehchopogh, FS",ENGINEERING WITH COMPUTERS,,"Feature selection (FS) is a critical step in data mining, and machine learning algorithms play a crucial role in algorithms performance. It reduces the processing time and accuracy of the categories. In this paper, three different solutions are proposed to FS. In the first solution, the Harris Hawks Optimization (HHO) algorithm has been multiplied, and in the second solution, the Fruitfly Optimization Algorithm (FOA) has been multiplied, and in the third solution, these two solutions are hydride and are named MOHHOFOA. The results were tested with MOPSO, NSGA-II, BGWOPSOFS and B-MOABC algorithms for FS on 15 standard data sets with mean, best, worst, standard deviation (STD) criteria. The Wilcoxon statistical test was also used with a significance level of 5% and the Bonferroni-Holm method to control the family-wise error rate. The results are shown in the Pareto front charts, indicating that the proposed solutions' performance on the data set is promising.",10.1007/s00366-021-01369-9,Feature selection; Harris hawks optimization; Fruitfly optimization algorithm; Multiobjective; Bonferroni&#8211; Holm; Family-wise error rate,,0.0,1.0
Instance Selection Using Multi-objective CHC Evolutionary Algorithm,"Rathee, S; Ratnoo, S; Ahuja, J",INFORMATION AND COMMUNICATION TECHNOLOGY FOR COMPETITIVE STRATEGIES,2019.0,"Data reduction has always been an important field of research to enhance the performance of data mining algorithms. Instance selection, a data reduction technique, relates to selecting a subset of informative and non-redundant examples from data. This paper deals with the problem of instance selection in a multi-objective perspective and, hence, proposes a multi-objective cross-generational elitist selection, heterogeneous recombination, and cataclysmic mutation (CHC) for discovering a set of Pareto-optimal solutions. The suggested MOCHC algorithm integrates the concept of non-dominating sorting with CHC. The algorithm has been employed to eight datasets available from UCI machine learning repository. The MOCHC has been successful in finding a range of multiple optimal solutions instead of yielding a single solution. These solutions provide a user with several choices of reduced datasets. Further, the solutions may be combined into a single instance subset by exploiting the promising characteristics across the potentially good solutions based on some user-defined criteria.",10.1007/978-981-13-0586-3_48,Multi-objective optimization; CHC algorithm; Instance selection; KNN,,0.0,1.0
Fair Classification and Social Welfare,"Hu, L; Chen, YL","FAT* '20: PROCEEDINGS OF THE 2020 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY",2020.0,"Now that machine learning algorithms lie at the center of many important resource allocation pipelines, computer scientists have been unwittingly cast as partial social planners. Given this state of affairs, important questions follow. How do leading notions of fairness as defined by computer scientists map onto longer-standing notions of social welfare? In this paper, we present a welfare-based analysis of fair classification regimes. Our main findings assess the welfare impact of fairness-constrained empirical risk minimization programs on the individuals and groups who are subject to their outputs. We fully characterize the ranges of Delta epsilon perturbations to a fairness parameter epsilon in a fair Soft Margin SVM problem that yield better, worse, and neutral outcomes in utility for individuals and by extension, groups. Our method of analysis allows for fast and efficient computation of fairness-to-welfare solution paths, thereby allowing practitioners to easily assess whether and which fair learning procedures result in classification outcomes that make groups better-off. Our analyses show that applying stricter fairness criteria codified as parity constraints can worsen welfare outcomes for both groups. More generally, always preferring more fair classifiers does not abide by the Pareto Principle-a fundamental axiom of social choice theory and welfare economics. Recent work in machine learning has rallied around these notions of fairness as critical to ensuring that algorithmic systems do not have disparate negative impact on disadvantaged social groups. By showing that these constraints often fail to translate into improved outcomes for these groups, we cast doubt on their effectiveness as a means to ensure fairness and justice.",10.1145/3351095.3372857,,,0.0,1.0
Multi-objective approach based on grammar-guided genetic programming for solving multiple instance problems,"Zafra, A; Ventura, S",SOFT COMPUTING,2012.0,"Multiple instance learning (MIL) is considered a generalization of traditional supervised learning which deals with uncertainty in the information. Together with the fact that, as in any other learning framework, the classifier performance evaluation maintains a trade-off relationship between different conflicting objectives, this makes the classification task less straightforward. This paper introduces a multi-objective proposal that works in a MIL scenario to obtain well-distributed Pareto solutions to multi-instance problems. The algorithm developed, Multi-Objective Grammar Guided Genetic Programming for Multiple Instances (MOG3P-MI), is based on grammar-guided genetic programming, which is a robust tool for classification. Thus, this proposal combines the advantages of the grammar-guided genetic programming with benefits provided by multi-objective approaches. First, a study of multi-objective optimization for MIL is carried out. To do this, three different extensions of MOG3P-MI are designed and implemented and their performance is compared. This study allows us on the one hand, to check the performance of multi-objective techniques in this learning paradigm and on the other hand, to determine the most appropriate evolutionary process for MOG3P-MI. Then, MOG3P-MI is compared with some of the most significant proposals developed throughout the years in MIL. Computational experiments show that MOG3P-MI often obtains consistently better results than the other algorithms, achieving the most accurate models. Moreover, the classifiers obtained are very comprehensible.",10.1007/s00500-011-0794-0,Multiple instance learning; Multiple objective learning; Grammar guided genetic programming; Evolutionary rule learning,,0.0,1.0
CommunityGAN: Community Detection with Generative Adversarial Nets,"Jia, YT; Zhang, QQ; Zhang, WN; Wang, XB",WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019),2019.0,"Community detection refers to the task of discovering groups of vertices sharing similar properties or functions so as to understand the network data. With the recent development of deep learning, graph representation learning techniques are also utilized for community detection. However, the communities can only be inferred by applying clustering algorithms based on learned vertex embeddings. These general cluster algorithms like K-means and Gaussian Mixture Model cannot output much overlapped communities, which have been proved to be very common in many real-world networks. In this paper, we propose CommunityGAN, a novel community detection framework that jointly solves overlapping community detection and graph representation learning. First, unlike the embedding of conventional graph representation learning algorithms where the vector entry values have no specific meanings, the embedding of CommunityGAN indicates the membership strength of vertices to communities. Second, a specifically designed Generative Adversarial Net (GAN) is adopted to optimize such embedding. Through the minimax competition between the motif-level generator and discriminator, both of them can alternatively and iteratively boost their performance and finally output a better community structure. Extensive experiments on synthetic data and real-world tasks demonstrate that CommunityGAN achieves substantial community detection performance gains over the state-of-the-art methods.",10.1145/3308558.3313564,Community Detection; Graph Representation Learning; Generative Adversarial Nets,,0.0,1.0
A PSO algorithm for multi-objective cost-sensitive attribute reduction on numeric data with error ranges,"Fang, Y; Liu, ZH; Min, F",SOFT COMPUTING,2017.0,"Multi-objective cost-sensitive attribute reduction is an attractive problem in supervised machine learning. Most research has focused on single-objective minimal test cost reduction or dealt with symbolic data. In this paper, we propose a particle swarm optimization algorithm for the attribute reduction problem on numeric data with multiple costs and error ranges and use three metrics with which to evaluate the performance of the algorithm. The proposed algorithm benefits from a fitness function based on the positive region, the selected n types of the test cost, a set of constant weight values , and a designated non-positive exponent . We design a learning strategy by setting dominance principles, which ensures the preservation of Pareto-optimal solutions and the rejection of redundant solutions. With different parameter settings, our PSO algorithm searches for a sub-optimal reduct set. Finally, we test our algorithm on seven UCI (University of California, Irvine) datasets. Comparisons with alternative approaches including the -weighted method and exhaustive calculation method of reduction are analyzed. Experimental results indicate that our heuristic algorithm outperforms existing algorithms.",10.1007/s00500-016-2260-5,Attribute reduction; Cost-sensitive learning; Particle swarm optimization; Rough sets,,0.0,1.0
Multi-objective selection for collecting cluster alternatives,"Kraus, JM; Mussel, C; Palm, G; Kestler, HA",COMPUTATIONAL STATISTICS,2011.0,"Grouping objects into different categories is a basic means of cognition. In the fields of machine learning and statistics, this subject is addressed by cluster analysis. Yet, it is still controversially discussed how to assess the reliability and quality of clusterings. In particular, it is hard to determine the optimal number of clusters inherent in the underlying data. Running different cluster algorithms and cluster validation methods usually yields different optimal clusterings. In fact, several clusterings with different numbers of clusters are plausible in many situations, as different methods are specialized on diverse structural properties. To account for the possibility of multiple plausible clusterings, we employ a multi-objective approach for collecting cluster alternatives (MOCCA) from a combination of cluster algorithms and validation measures. In an application to artificial data as well as microarray data sets, we demonstrate that exploring a Pareto set of optimal partitions rather than a single solution can identify alternative solutions that are overlooked by conventional clustering strategies. Competitive solutions are hereby ranked following an impartial criterion, while the ultimate judgement is left to the investigator.",10.1007/s00180-011-0244-6,Cluster analysis; Multi-objective optimization; Cluster number estimation; Cluster validation,,0.0,1.0
An effective multiobjective approach for hard partitional clustering,"Prakash, J; Singh, PK",MEMETIC COMPUTING,2015.0,"Clustering is an unsupervised classification method in the field of data mining. Many population based evolutionary and swarm intelligence optimization methods are proposed to optimize clustering solutions globally based on a single selected objective function which lead to produce a single best solution. In this sense, optimized solution is biased towards a single objective, hence it is not equally well to the data set having clusters of different geometrical properties. Thus, clustering having multiple objectives should be naturally optimized through multiobjective optimization methods for capturing different properties of the data set. To achieve this clustering goal, many multiobjective population based optimization methods, e.g., multiobjective genetic algorithm, mutiobjective particle swarm optimization (MOPSO), are proposed to obtain diverse tradeoff solutions in the pareto-front. As single directional diversity mechanism in particle swarm optimization converges prematurely to local optima, this paper presents a two-stage diversity mechanism in MOPSO to improve its exploratory capabilities by incorporating crossover operator of the genetic algorithm. External archive is used to store non-dominated solutions, which is further utilized to find one best solution having highest F-measure value at the end of the run. Two conceptually orthogonal internal measures SSE and connectedness are used to estimate the clustering quality. Results demonstrate effectiveness of the proposed method over its competitors MOPSO, non-dominated sorting genetic algorithm, and multiobjective artificial bee colony on seven real data sets from UCI machine learning repository.",10.1007/s12293-014-0147-5,Data clustering; Multiobjective optimization; Evolutionary and swarm intelligence; Mutiobjective particle swarm optimization,,0.0,1.0
A bi-objective hybrid algorithm for the classification of imbalanced noisy and borderline data sets,"Saeed, S; Ong, HC",PATTERN ANALYSIS AND APPLICATIONS,2019.0,"Classification of imbalanced data sets is one of the significant problems of machine learning and data mining. Traditional classifiers usually produced suboptimal results for imbalanced data sets. This study proposed an idea of using a newly proposed bi-objective hybrid algorithm for the given classification task of binary imbalanced noisy and borderline data sets. The bi-objective hybrid algorithm was based on the hybridization of two metaheuristics, namely cuckoo search and covariance matrix adaptation evolution strategy. The validation of this proposed hybrid algorithm was confirmed in terms of the Pareto fronts. Thereafter, this algorithm was used in a methodology proposed for the classification task of the binary imbalanced data sets. The proposed methodology was based on an idea of estimating the probabilities from both classes (majority and minority) of a data set, using normal distribution. Optimization of parameters of the normal distribution was done with the help of the proposed algorithm. Different data sets (simulated, noisy borderline and real) were used. Four well-known classifiers with a preprocessing algorithm were cast-off for the comparison purpose. Performances of all classifiers were evaluated using three evaluation measures, sensitivity, G mean and F measure. A promising performance of proposed methodology was observed.",10.1007/s10044-018-0693-4,Data mining; Classification; Imbalance data sets; Bi-objective hybrid algorithm; Metaheuristics,,0.0,1.0
Constraint Programming for Multi-criteria Conceptual Clustering,"Chabert, M; Solnon, C",PRINCIPLES AND PRACTICE OF CONSTRAINT PROGRAMMING (CP 2017),2017.0,"A conceptual clustering is a set of formal concepts (i.e., closed itemsets) that defines a partition of a set of transactions. Finding a conceptual clustering is an NP-complete problem for which Constraint Programming (CP) and Integer Linear Programming (ILP) approaches have been recently proposed. We introduce new CP models to solve this problem: a pure CP model that uses set constraints, and an hybrid model that uses a data mining tool to extract formal concepts in a preprocessing step and then uses CP to select a subset of formal concepts that defines a partition. We compare our new models with recent CP and ILP approaches on classical machine learning instances. We also introduce a new set of instances coming from a real application case, which aims at extracting setting concepts from an Enterprise Resource Planning (ERP) software. We consider two classic criteria to optimize, i.e., the frequency and the size. We show that these criteria lead to extreme solutions with either very few small formal concepts or many large formal concepts, and that compromise clusterings may be obtained by computing the Pareto front of non dominated clusterings.",10.1007/978-3-319-66158-2_30,,,0.0,1.0
Novel elegant fuzzy genetic algorithms in classification problems,"Venkatanareshbabu, K; Nisheel, S; Sakthivel, R; Muralitharan, K",SOFT COMPUTING,2019.0,"In this paper, we propose three novel algorithms such as Novel genetic algorithm complex-valued backpropagation neural network (GA-CVBNN), Novel elegant fuzzy genetic algorithm (EFGA) and elegant fuzzy genetic algorithm-based complex-valued backpropagation neural network (EFGA-CVBNN) for classification of accuracy in datasets. In GA-CVBNN, classical Genetic Algorithm has been used for selecting appropriate initial weights for CVBNN. The EFGA is developed to resolve the drawback of classical GA by employing fuzzy logic to control parameters and selective pressure of GA. The EFGA uses a Min-Heap data structure and Pareto principle to improve the classical genetic algorithm. The EFGA-CVBNN resolves the drawbacks of classical CVBNN by employing EFGA at the time of initial weight selection. From the simulation result, the GA-CVBNN performs better than existing CVBNN and it is not efficient. To enhance the performance of GA-CVBNN, we have developed EFGA-CVBNN. Experimental results on various synthetic datasets and benchmark datasets taken from UCI machine learning repository shows that EFGA-CVBNN outperforms PSO-CVBNN in terms of classification accuracy and time. Statistical t test has been used to validate the obtained results.",10.1007/s00500-018-3216-8,Classification; Complex number; Fuzzy logic; Genetic algorithm; Neural network; Optimization,,0.0,1.0
Bi-level multi-objective evolution of a Multi-Layered Echo-State Network Autoencoder for data representations,"Chouikhi, N; Ammar, B; Hussain, A; Alimi, AM",NEUROCOMPUTING,2019.0,"The Multi-Layered Echo-State Network (ML-ESN) is a recently developed, highly powerful type of recurrent neural network. It has succeeded in dealing with several non-linear benchmark problems. On account of its rich dynamics, ML-ESN is exploited in this paper, for the first time, as a recurrent Autoencoder (ML-ESNAE) to extract new features from original data representations. Further, the challenging and crucial task of optimally determining the ML-ESNAE architecture and training parameters is addressed, in order to extract more efficient features from the data. Traditionally, in a ML-ESN, the number of parameters (hidden neurons, sparsity rates, weights) are randomly chosen and manually altered to achieve a minimum learning error. On one hand, this random setting may not guarantee best generalization results. On the other, it can increase the network's complexity. In this paper, a novel bi-level evolutionary optimization approach is thus proposed for the ML-ESNAE, to deal with these challenges. The first level offers Pareto multi-objective architecture optimization, providing maximum learning accuracy while maintaining a reduced complexity target. Next, every Pareto optimal solution obtained from the first level undergoes a mono-objective weights optimization at the second level. Particle Swarm Optimization (PSO) is used as an evolutionary tool for both levels 1 and 2. An empirical study shows that the evolved ML-ESNAE produces a noticeable improvement in extracting new, more expressive data features from original ones. A number of application case studies, using a range of benchmark datasets, show that the extracted features produce excellent results in terms of classification accuracy. The effectiveness of the evolved ML-ESNAE is demonstrated for both noisy and noise-free data. In conclusion, the evolutionary ML-ESNAE is proposed as a new benchmark for the evolutionary AI and machine learning research community. (C) 2019 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2019.03.012,Multi-Layered Echo State Network; Autoencoder; Data representation; PSO; Multi-objective optimization; Architecture optimization; Weights optimization,,0.0,1.0
Policy invariance under reward transformations for multi-objective reinforcement learning,"Mannion, P; Devlin, S; Mason, K; Duggan, J; Howley, E",NEUROCOMPUTING,2017.0,"Reinforcement Learning (RL) is a powerful and well-studied Machine Learning paradigm, where an agent learns to improve its performance in an environment by maximising a reward signal. In multi-objective Reinforcement Learning (MORL) the reward signal is a vector, where each component represents the performance on a different objective. Reward shaping is a well-established family of techniques that have been successfully used to improve the performance and learning speed of RL agents in single-objective problems. The basic premise of reward shaping is to add an additional shaping reward to the reward naturally received from the environment, to incorporate domain knowledge and guide an agent's exploration. Potential-Based Reward Shaping (PBRS) is a specific form of reward shaping that offers additional guarantees. In this paper, we extend the theoretical guarantees of PBRS to MORL problems. Specifically, we provide theoretical proof that PBRS does not alter the true Pareto front in both single- and multi-agent MORL. We also contribute the first published empirical studies of the effect of PBRS in single- and multi-agent MORL problems. (C) 2017 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2017.05.090,Reinforcement learning; Multi-objective; Potential-based; Reward shaping; Multi-agent systems,,0.0,1.0
Active Fairness in Algorithmic Decision Making,"Noriega-Campero, A; Bakker, MA; Garcia-Bulle, B; Pentland, A","AIES '19: PROCEEDINGS OF THE 2019 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY",2019.0,"Society increasingly relies on machine learning models for automated decision making. Yet, efficiency gains from automation have come paired with concern for algorithmic discrimination that can systematize inequality. Recent work has proposed optimal post processing methods that randomize classification decisions for a fraction of individuals, in order to achieve fairness measures related to parity in errors and calibration. These methods, however, have raised concern due to the information inefficiency intra-group unfairness, and Pareto sub-optimality they entail. The present work proposes an alternative active framework for fair classification, where, in deployment, a decision-maker adaptively acquires information according to the needs of different groups or individuals, towards balancing disparities in classification performance. We propose two such methods, where information collection is adapted to group- and individual-level needs respectively. We show on real-world datasets that these can achieve: 1) calibration and single error parity (e.g., equal opportunity); and 2) parity in both false positive and false negative rates (i.e., equal odds). Moreover, we show that by leveraging their additional degree of freedom, active approaches can substantially outperform randomization-based classifiers previously considered optimal, while avoiding limitations such as intra-group unfairness.",10.1145/3306618.3314277,algorithmic fairness; adaptive inquiry; active feature acquisition,,0.0,1.0
Reference-point-based multi-objective optimization algorithm with opposition-based voting scheme for multi-label feature selection,"Bidgoli, AA; Ebrahimpour-Komleh, H; Rahnamayan, S",INFORMATION SCIENCES,2021.0,"Multi-label classification is a machine learning task to construct a model for assigning an entity in the dataset to two or more class labels. In order to improve the performance of multi-label classification, a multi-objective feature selection algorithm has been proposed in this paper. Feature selection as a preprocessing task for Multi-label classification problems aims to choose a subset of relevant features. Selecting a small number of high-quality features decreases the computational cost and at the same time maximizes the classification performance. However extreme decreasing the number of features causes the failure of classification. As a result, feature selection has two conflicting objectives, namely, minimizing the classification error and minimizing the number of selected features. This paper proposes a multi-objective optimization algorithm to tackle the multi-label feature selection. The task is to find a set of solutions (a subset of features) in a sophisticated large-scale search space using a reference-based multi-objective optimization method. The proposed algorithm utilizes an opposition-based binary operator to generate more diverse solutions. Injection of extreme point of the Pareto-front is another component of the algorithm which aims to find feature subsets with less classification error. The proposed method is compared with two other existing methods on eight multi-label benchmark datasets. The experimental results show that the proposed method outperforms existing algorithms in terms of various multi-objective evaluation measures, such as Hyper-volume indicator, Pure diversity, Two-set coverage, and Pareto-front proportional contribution. The proposed method leads to get a set of well-distributed trade-off solutions which reach less classification error in comparing with competitors, even with the fewer number of features. (C) 2020 Elsevier Inc. All rights reserved.",10.1016/j.ins.2020.08.004,Multi-label classification; Feature selection; Multi-objective optimization; Evolutionary algorithm; Opposition-based computation,,0.0,1.0
Multiobjective evolutionary RBF networks and its application to ensemble learning,"Kondo, N; Hatanaka, T; Uosaki, K",FRONTIERS OF COMPUTATIONAL SCIENCE,2007.0,"This paper considers a pattern classification by the ensemble of evolutionary RBF networks. Mathematical models generally have a dilemma about model complexity, so the structure determination of RBF network can be considered as the multi-objective optimization problem concerning with accuracy, complexity, and smoothness of the model. The set of RBF network are obtained by multi-objective evolutionary computation, and then RBF network ensemble is constructed of all or some RBF networks at the final generation. Some experiments on the benchmark problem of the pattern classification demonstrate that the RBF network ensemble has comparable generalization ability to conventional ensemble methods.",10.1007/978-3-540-46375-7_50,,,0.0,1.0
Novel multiobjective TLBO algorithms for the feature subset selection problem,"Kiziloz, HE; Deniz, A; Dokeroglu, T; Cosar, A",NEUROCOMPUTING,2018.0,"Teaching Learning Based Optimization (TLBO) is a new metaheuristic that has been successfully applied to several intractable optimization problems in recent years. In this study, we propose a set of novel multiobjective TLBO algorithms combined with supervised machine learning techniques for the solution of Feature Subset Selection (FSS) in Binary Classification Problems (FSS-BCP). Selecting the minimum number of features while not compromising the accuracy of the results in FSS-BCP is a multiobjective optimization problem. We propose TLBO as a FSS mechanism and utilize its algorithm-specific parameterless concept that does not require any parameters to be tuned during the optimization. Most of the classical metaheuristics such as Genetic and Particle Swarm Optimization algorithms need additional efforts for tuning their parameters (crossover ratio, mutation ratio, velocity of particle, inertia weight, etc.), which may have an adverse influence on their performance. Comprehensive experiments are carried out on the well-known machine learning datasets of UCI Machine Learning Repository and significant improvements have been observed when the proposed multiobjective TLBO algorithms are compared with state-of-the-art NSGA-II, Particle Swarm Optimization, Tabu Search, Greedy Search, and Scatter Search algorithms. (C) 2018 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2018.04.020,Teaching learning based optimization; Multiobjective feature selection; Supervised learning,,0.0,1.0
An Effective Metaheuristic for Bi-objective Feature Selection in Two-Class Classification Problem,"Lyubchenko, AA; Pacheco, JA; Casado, S; Nunez, L",XII INTERNATIONAL SCIENTIFIC AND TECHNICAL CONFERENCE APPLIED MECHANICS AND SYSTEMS DYNAMICS,2019.0,"Feature selection is known as a very useful technique in machine learning practice as it may result in the development of more straightforward models with better accuracy. Traditionally, feature selection is considered as a single-objective problem, however, it can be easily formulated in terms of two objectives. The solving of such problems requires the application of appropriate multi-objective optimization methods that do not always offer equally good solutions even under the same conditions. This paper focuses on the development of a metaheuristic optimization approach for bi-objective feature selection problem in two-class classification. We consider the solving of this problem in terms of minimization of both misclassification error and feature subset size. For solving the considered problem, an adaptation of the Multi-Objective Adaptive Memory Programming (MOAMP) metaheuristic based on the tabu search strategy is proposed. Our MOAMP adaption has been utilized to obtain the sets of most relevant features for two real classification problems with two classes. Finally, using popular Pareto front quality indicators, the obtained results have been compared with the sets of non-dominated solutions derived by the well-known NSGA2 algorithm. The conducted research allows concluding about the ability of the MOAMP adaptation to get a better efficient frontier for the same number of objective function calls.",10.1088/1742-6596/1210/1/012086,bi-objective feature selection; classification; tabu search; MOAMP,,0.0,1.0
Unsupervised feature construction for improving data representation and semantics,"Rizoiu, MA; Velcin, J; Lallich, S",JOURNAL OF INTELLIGENT INFORMATION SYSTEMS,2013.0,"Attribute-based format is the main data representation format used by machine learning algorithms. When the attributes do not properly describe the initial data, performance starts to degrade. Some algorithms address this problem by internally changing the representation space, but the newly constructed features rarely have any meaning. We seek to construct, in an unsupervised way, new attributes that are more appropriate for describing a given dataset and, at the same time, comprehensible for a human user. We propose two algorithms that construct the new attributes as conjunctions of the initial primitive attributes or their negations. The generated feature sets have reduced correlations between features and succeed in catching some of the hidden relations between individuals in a dataset. For example, a feature like would be true for non-urban images and is more informative than simple features expressing the presence or the absence of an object. The notion of Pareto optimality is used to evaluate feature sets and to obtain a balance between total correlation and the complexity of the resulted feature set. Statistical hypothesis testing is employed in order to automatically determine the values of the parameters used for constructing a data-dependent feature set. We experimentally show that our approaches achieve the construction of informative feature sets for multiple datasets.",10.1007/s10844-013-0235-x,Unsupervised feature construction; Feature evaluation; Nonparametric statistics; Data mining; Clustering; Representations; Algorithms for data and knowledge management; Heuristic methods; Pattern analysis,,0.0,1.0
Meta-Analysis Based on Nonconvex Regularization,"Zhang, H; Li, SJ; Zhang, H; Yang, ZY; Ren, YQ; Xia, LY; Liang, Y",SCIENTIFIC REPORTS,2020.0,"The widespread applications of high-throughput sequencing technology have produced a large number of publicly available gene expression datasets. However, due to the gene expression datasets have the characteristics of small sample size, high dimensionality and high noise, the application of biostatistics and machine learning methods to analyze gene expression data is a challenging task, such as the low reproducibility of important biomarkers in different studies. Meta-analysis is an effective approach to deal with these problems, but the current methods have some limitations. In this paper, we propose the meta-analysis based on three nonconvex regularization methods, which are L-1/2 regularization (meta-Half), Minimax Concave Penalty regularization (meta-MCP) and Smoothly Clipped Absolute Deviation regularization (meta-SCAD). The three nonconvex regularization methods are effective approaches for variable selection developed in recent years. Through the hierarchical decomposition of coefficients, our methods not only maintain the flexibility of variable selection and improve the efficiency of selecting important biomarkers, but also summarize and synthesize scientific evidence from multiple studies to consider the relationship between different datasets. We give the efficient algorithms and the theoretical property for our methods. Furthermore, we apply our methods to the simulation data and three publicly available lung cancer gene expression datasets, and compare the performance with state-of-the-art methods. Our methods have good performance in simulation studies, and the analysis results on the three publicly available lung cancer gene expression datasets are clinically meaningful. Our methods can also be extended to other areas where datasets are heterogeneous.",10.1038/s41598-020-62473-2,,,0.0,1.0
SA-CGAN: An oversampling method based on single attribute guided conditional GAN for multi-class imbalanced learning,"Dong, YF; Xiao, HX; Dong, Y",NEUROCOMPUTING,2022.0,"Imbalanced data can always be observed in our daily life and various practical tasks. A lot of well constructed machine learning methodologies may produce ineffective performance, when conducted on this kind of data. This originates from the produced high training biases that towards the majority class instances. Among all the solutions of this problem, data generation of the minority class is always considered the most effective approach. However, in all the previous works, data are always processed sample-wisely and the distribution of each single data attribute is never noticed. So, in this paper, to estimate the mechanism of how each attribute contributes to its label, we explore the potential connection between the two items by Conditional Generative Adversarial Networks (CGAN) separately and individually. Then, the constructed new instances are purified by a designed attribute-based minimax filter and the survivors are concatenated to form the eventual generated data. In other words, different from the CGAN based data generation way, the proposed approach improves it by additionally considering all the single attribute patterns of the data that to construct new instances. In addition, we extend the binary class imbalanced learning framework to multiple class one. In the experimental part, the improved model is compared against GAN, CGAN and some other standard multiple-class oversampling algorithms on several widely used datasets. Results, in terms of four common measurements, have shown that the proposed approach can produce comparable and always superior performance when compared with the competitors. (c) 2021 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2021.04.135,Generative adversarial network; Multi-class uneven; imbalanced data; Data generation; Attribute; feature pattern learning,,0.0,1.0
Towards Efficient Robust Optimization using Data based Optimal Segmentation of Uncertain Space,"Pantula, PD; Mitra, K",RELIABILITY ENGINEERING & SYSTEM SAFETY,2020.0,"Performing multi-objective optimization under uncertainty is a common requirement in industries and academia. Robust optimization (RO) is considered as an efficient and tractable approach provided one has access to behavioral data for the uncertain parameters. However, solutions of RO may be far from the real solution and less reliable due to inability to map the uncertain space accurately, especially when the data appears discontinuous and scattered in the uncertain domain. Amalgamating machine learning algorithms with RO, this paper proposes a data-driven methodology, where a novel fuzzy clustering mechanism is implemented along-with boundary construction, to transcript the uncertain space such that the specific regions of uncertainty are identified. Subsequently, using intelligent Sobol sampling, samples are generated in the mapped uncertain regions. Results of two test cases are presented along with a comprehensive comparison study. Considered case-studies include highly nonlinear model for continuous casting process from steelmaking industries, where a multi-objective optimization problem under uncertainty is solved to balance the conflict between productivity and energy consumption. The Pareto-optimal solutions of the resulting RO problem are obtained through Non-Dominated Sorting Genetic Algorithm - II, and similar to 23-29% improvement is observed in the uncertain objective function. Further, the spread and diversity metrics are enhanced by similar to 10-95% as compared to those obtained using other standard uncertainty sets.",10.1016/j.ress.2020.106821,Data-driven robust optimization; Fuzzy clustering; Boundary construction; Sobol sampling; Multi-objective optimization; Pareto-optimal solutions,,0.0,1.0
Enhancing the effectiveness of Ant Colony Decision Tree algorithms by co-learning,"Boryczka, U; Kozak, J",APPLIED SOFT COMPUTING,2015.0,"Data mining and visualization techniques for high-dimensional data provide helpful information to substantially augment decision-making. Optimization techniques provide a way to efficiently search for these solutions. ACO applied to data mining tasks - a decision tree construction - is one of these methods and the focus of this paper. The Ant Colony Decision Tree (ACDT) approach generates solutions efficiently and effectively but scales poorly to large problems. This article merges the methods that have been developed for better construction of decision trees by ants. The ACDT approach is tested in the context of the bi-criteria evaluation function by focusing on two problems: the size of the decision trees and the accuracy of classification obtained during ACDT performance. This approach is tested in co-learning mechanism, it means agents-ants can interact during the construction decision trees via pheromone values. This cooperation is a chance of getting better results. The proposed methodology of analysis of ACDT is tested in a number of well-known benchmark data sets from the UCI Machine Learning Repository. The empirical results clearly show that the ACDT algorithm creates good solutions which are located in the Pareto front. The software that implements the ACDT algorithm used to generate the results of this study can be downloaded freely from http://www.acdtalgorithm.com. (C) 2015 Elsevier B.V. All rights reserved.",10.1016/j.asoc.2014.12.036,Ant Colony Optimization; Ant Colony Decision Trees; Decision trees; Pareto front; Quality of decision trees; Ant-Miner,,0.0,1.0
PRELIMINARY-REPORT ON MACHINE LEARNING VIA MULTIOBJECTIVE OPTIMIZATION,"AHMAD, Z; GUEZ, A","APPLICATIONS OF ARTIFICIAL NEURAL NETWORKS III, PTS 1 AND 2",1992.0,,10.1117/12.140027,,,0.0,1.0
Learning multicriteria classification models from examples: Decision rules in continuous space,"Dombi, J; Zsiros, A",EUROPEAN JOURNAL OF OPERATIONAL RESEARCH,2005.0,"The classification problem statement of multicriteria decision analysis is to model the classification of the alternatives/actions according to the decision maker's preferences. These models are based on outranking relations, utility functions or (linear) discriminant functions. Model parameters can be given explicitly or learnt from a preclassified set of alternatives/actions. In this paper we propose a novel approach, the Continuous Decision (CD) method, to learn parameters of a discriminant function, and we also introduce its extension, the Continuous Decision Tree (CDT) method, which describes the classification more accurately. The proposed methods are results of integration of Machine Learning methods in Decision Analysis. From a Machine Learning point of view, the CDT method can be considered as an extension of the C4.5 decision tree building algorithm that handles only numeric criteria but applies more complex tests in the inner nodes of the tree. For the sake of easier interpretation, the decision trees are transformed to rules. (C) 2003 Elsevier B.V. All rights reserved.",10.1016/j.ejor.2003.10.006,multiple criteria analysis; classification; artificial intelligence; decision trees; fuzzy sets,,0.0,1.0
A Model Falsification Approach to Learning in Non-Stationary Environments for Experimental Design,"Murari, A; Lungaroni, M; Peluso, E; Craciunescu, T; Gelfusa, M",SCIENTIFIC REPORTS,2019.0,"The application of data driven machine learning and advanced statistical tools to complex physics experiments, such as Magnetic Confinement Nuclear Fusion, can be problematic, due the varying conditions of the systems to be studied. In particular, new experiments have to be planned in unexplored regions of the operational space. As a consequence, care must be taken because the input quantities used to train and test the performance of the analysis tools are not necessarily sampled by the same probability distribution as in the final applications. The regressors and dependent variables cannot therefore be assumed to verify the i.i.d. (independent and identical distribution) hypothesis and learning has therefore to take place under non stationary conditions. In the present paper, a new data driven methodology is proposed to guide planning of experiments, to explore the operational space and to optimise performance. The approach is based on the falsification of existing models. The deployment of Symbolic Regression via Genetic Programming to the available data is used to identify a set of candidate models, using the method of the Pareto Frontier. The confidence intervals for the predictions of such models are then used to find the best region of the parameter space for their falsification, where the next set of experiments can be most profitably carried out. Extensive numerical tests and applications to the scaling laws in Tokamaks prove the viability of the proposed methodology.",10.1038/s41598-019-54145-7,,,0.0,1.0
Discretization Techniques and Genetic Algorithm for Learning the Classification Method PROAFTN,"Al-Obeidat, F; Belacel, N; Mahanti, P; Carretero, JA","EIGHTH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS, PROCEEDINGS",2009.0,"This paper introduces new techniques for learning the classification method PROAFTN from data. PROAFTN is a multi-criteria classification method and belongs to the class of supervised learning algorithms. To use PROAFTN for classification, some parameters must be obtained for this purpose. Therefore, an automatic method to extract these parameters from data with minimum classification errors is required. Here, discretization techniques and genetic algorithms are proposed for establishing these parameters and then building the classification model. Based on the obtained results, the newly proposed approach outperforms widely used classification methods.",10.1109/ICMLA.2009.37,,,0.0,1.0
Adversarial Sample Crafting for Time Series Classification with Elastic Similarity Measures,"Oregi, I; Del Ser, J; Perez, A; Lozano, JA",INTELLIGENT DISTRIBUTED COMPUTING XII,2018.0,"Adversarial Machine Learning (AML) refers to the study of the robustness of classification models when processing data samples that have been intelligently manipulated to confuse them. Procedures aimed at furnishing such confusing samples exploit concrete vulnerabilities of the learning algorithm of the model at hand, by which perturbations can make a given data instance to be misclassified. In this context, the literature has so far gravitated on different AML strategies to modify data instances for diverse learning algorithms, in most cases for image classification. This work builds upon this background literature to address AML for distance based time series classifiers (e.g., nearest neighbors), in which attacks (i.e. modifications of the samples to be classified by the model) must be intelligently devised by taking into account the measure of similarity used to compare time series. In particular, we propose different attack strategies relying on guided perturbations of the input time series based on gradient information provided by a smoothed version of the distance based model to be attacked. Furthermore, we formulate the AML sample crafting process as an optimization problem driven by the Pareto trade-off between (1) a measure of distortion of the input sample with respect to its original version; and (2) the probability of the crafted sample to confuse the model. In this case, this formulated problem is efficiently tackled by using multi-objective heuristic solvers. Several experiments are discussed so as to assess whether the crafted adversarial time series succeed when confusing the distance based model under target.",10.1007/978-3-319-99626-4_3,Adversarial Machine Learning; Time series classification; Elastic similarity measures,,0.0,1.0
Learning Optimal Time Series Combination and Pre-Processing by Smart Joins,"Gil, A; Quartulli, M; Olaizola, IG; Sierra, B",APPLIED SCIENCES-BASEL,2020.0,"In industrial applications of data science and machine learning, most of the steps of a typical pipeline focus on optimizing measures of model fitness to the available data. Data preprocessing, instead, is often ad-hoc, and not based on the optimization of quantitative measures. This paper proposes the use of optimization in the preprocessing step, specifically studying a time series joining methodology, and introduces an error function to measure the adequateness of the joining. Experiments show how the method allows monitoring preprocessing errors for different time slices, indicating when a retraining of the preprocessing may be needed. Thus, this contribution helps quantifying the implications of data preprocessing on the result of data analysis and machine learning methods. The methodology is applied to two case studies: synthetic simulation data with controlled distortions, and a real scenario of an industrial process.",10.3390/app10186346,optimization; machine learning; preprocessing,,0.0,1.0
LRP-Based path relevances for global explanation of deep architectures,"Guerrero-Gomez-Olmedo, R; Salmeron, JL; Kuchkovsky, C",NEUROCOMPUTING,2020.0,"Understanding what Machine Learning models are doing is not always trivial. This is especially true for complex models such as Deep Neural Networks (DNN), which are the best-suited algorithms for modeling very complex and nonlinear relationships. But this need to understand has become a must since privacy regulations are hardening the industrial use of these models. There are different techniques to address the interpretability issues that Machine Learning models arises. This paper is focused on opening the so-called Deep Neural architectures black-box. This research extends the technique called Layer-wise Relevant Propagation (LRP) enhancing its properties to compute the most critical paths in different deep neural architectures using multicriteria analysis. We call this technique Ranked-LRP and it was tested on four different datasets and tasks, including classification and regression. The results show the worth of our proposal. (C) 2020 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2019.11.059,Explainable AI; Deep learning; Interpretable machine learning; Layer-wise relevant propagation,,0.0,1.0
A robust multiobjective Harris' Hawks Optimization algorithm for the binary classification problem,"Dokeroglu, T; Deniz, A; Kiziloz, HE",KNOWLEDGE-BASED SYSTEMS,2021.0,"The Harris' Hawks Optimization (HHO) is a recent metaheuristic inspired by the cooperative behavior of the hawks. These avians apply many intelligent techniques like surprise pounce (seven kills) while they are catching their prey according to the escaping patterns of the target. The HHO simulates these hunting patterns of the hawks to obtain the best/optimal solutions to the problems. In this study, we propose a new multiobjective HHO algorithm for the solution of the well-known binary classification problem. In this multiobjective problem, we reduce the number of selected features and try to keep the accuracy prediction as maximum as possible at the same time. We propose new discrete exploration (perching) and exploitation (besiege) operators for the hunting patterns of the hawks. We calculate the prediction accuracy of the selected features with four machine learning techniques, namely, Logistic Regression, Support Vector Machines, Extreme Learning Machines, and Decision Trees. To verify the performance of the proposed algorithm, we conduct comprehensive experiments on many benchmark datasets retrieved from the University of California, Irvine (UCI) Machine Learning Repository. Moreover, we apply it to a recent real-world dataset, i.e., a Coronavirus disease (COVID-19) dataset. Significant improvements are observed during the comparisons with state-of-the-art metaheuristic algorithms. (C) 2021 Elsevier B.V. All rights reserved.",10.1016/j.knosys.2021.107219,Binary classification; Multiobjective optimization; Feature selection; Harris' Hawks optimization,,0.0,1.0
Evolutionary inversion of class distribution in overlapping areas for multi-class imbalanced learning,"Fernandes, ERQ; de Carvalho, ACPLF",INFORMATION SCIENCES,2019.0,"Inductive learning from multi-class and imbalanced datasets is one of the main challenges for machine learning. Most machine learning algorithms have their predictive performance negatively affected by imbalanced data. Although several techniques have been proposed to deal with this difficulty, they are usually restricted to binary classification datasets. Thus, one of the research challenges in this area is how to deal with imbalanced multiclass classification datasets. This challenge become more difficult when classes containing fewer instances are located in overlapping regions of the data attribute space. In fact, several studies have indicated that the degree of class overlapping has a higher effect on predictive performance than the global class imbalance ratio. This paper proposes a novel evolutionary ensemble-based method for multi-class imbalanced learning called the evolutionary inversion of class distribution in overlapping areas for multi-class imbalanced learning (EVINCI). EVINCI uses a multiobjective evolutionary algorithm (MOEA) to evolve a set of samples taken from an imbalanced dataset. It selectively reduces the concentration of less representative instances of the majority classes in the overlapping areas while selecting samples that produce more accurate models. In experiments performed to evaluate its predictive accuracy, EVINCI was superior to state-of-the-art ensemble-based methods for imbalanced learning. (C) 2019 Published by Elsevier Inc.",10.1016/j.ins.2019.04.052,Multi-Class imbalanced learning; Ensemble of classifiers; Evolutionary algorithms,,0.0,1.0
Generalized approach for multi-response machining process optimization using machine learning and evolutionary algorithms,"Ghosh, T; Martinsen, K",ENGINEERING SCIENCE AND TECHNOLOGY-AN INTERNATIONAL JOURNAL-JESTECH,2020.0,"Contemporary manufacturing processes are substantially complex due to the involvement of a sizable number of correlated process variables. Uncovering the correlations among these variables would be the most demanding task in this scenario, which require exclusive tools and techniques. Data-driven surrogate-assisted optimization is an ideal modeling approach, which eliminates the necessity of resource driven mathematical or simulation paradigms for the manufacturing process optimization. In this paper, a data-driven evolutionary algorithm is introduced, which is based on the improved Non-dominated Sorting Genetic Algorithm (NSGA-III). For objective approximation, the Gaussian Kernel Regression is selected. The multi-response manufacturing process data are employed to train this model. The proposed data-driven approach is generic, which could be evaluated for any type of manufacturing process. In order to verify the proposed methodology, a comprehensive number of cases are considered from the past literature. The proposed data-driven NSGA-III is compared with the Multi-Objective Evolutionary Algorithm based on Decomposition (MOEA/D) and shown to attain improved solutions within the imposed boundary conditions. Both the algorithms are shown to perform well using statistical analysis. The obtained results could be utilized to improve the machining conditions and performances. The novelty of this research is twofold, first, the surrogate-assisted NSGA III is implemented and second, the proposed approach is adopted for the multi-response manufacturing process optimization. (C) 2019 Karabuk University. Publishing services by Elsevier B.V.",10.1016/j.jestch.2019.09.003,Machining process optimization; Data-driven surrogate model; NSGA-III; Many-response parametric design; MOEA/D,,0.0,1.0
Balancing Exploration and Exploitation in Multiobjective Batch Bayesian Optimization,"Wang, HY; Xu, H; Yuan, Y; Sun, XM; Deng, JH",PROCEEDINGS OF THE 2019 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE COMPANION (GECCCO'19 COMPANION),2019.0,"Many applications such as hyper-parameter tunning in Machine Learning can be casted to multiobjective black-box problems and it is challenging to optimize them. Bayesian Optimization (130) is an effective method to deal with black-box functions. This paper mainly focuses on balancing exploration and exploitation in multiobjective black-box optimization problems by multiple samplings in BBO. In each iteration, multiple reconunendations are generated via two different trade-off strategies respectively, the expected improvement (El) and a multiobjective framework with the mean and variance function of the GP posterior forming two conflict objectives. We compare our algorithm with ParEGO by running on 12 test functions. Hypervoltune (HV, also known as S-metric) results show that our algoritlun works well in exploration-exploitation trade-off for multiobjective black-box optimization problems.",10.1145/3319619.3321962,expensive multiobjective optimization; batch Bayesian optimization; Gaussian Process; exploration and exploitation; ParEGO,,0.0,1.0
A generic optimising feature extraction method using multiobjective genetic programming,"Zhang, Y; Rockett, PI",APPLIED SOFT COMPUTING,2011.0,"In this paper, we present a generic, optimising feature extraction method using multiobjective genetic programming. We re-examine the feature extraction problem and show that effective feature extraction can significantly enhance the performance of pattern recognition systems with simple classifiers. A framework is presented to evolve optimised feature extractors that transform an input pattern space into a decision space in which maximal class separability is obtained. We have applied this method to real world datasets from the UCI Machine Learning and StatLog databases to verify our approach and compare our proposed method with other reported results. We conclude that our algorithm is able to produce classifiers of superior (or equivalent) performance to the conventional classifiers examined, suggesting removal of the need to exhaustively evaluate a large family of conventional classifiers on any new problem. (C) 2010 Elsevier B.V. All rights reserved.",10.1016/j.asoc.2010.02.008,Feature extraction; Multiobjective optimisation; Genetic programming; Pattern recognition,,0.0,1.0
Feature selection and semi-supervised clustering using multiobjective optimization,"Saha, S; Ekbal, A; Alok, AK; Spandana, R",SPRINGERPLUS,2014.0,"In this paper we have coupled feature selection problem with semi-supervised clustering. Semi-supervised clustering utilizes the information of unsupervised and supervised learning in order to overcome the problems related to them. But in general all the features present in the data set may not be important for clustering purpose. Thus appropriate selection of features from the set of all features is very much relevant from clustering point of view. In this paper we have solved the problem of automatic feature selection and semi-supervised clustering using multiobjective optimization. A recently created simulated annealing based multiobjective optimization technique titled archived multiobjective simulated annealing (AMOSA) is used as the underlying optimization technique. Here features and cluster centers are encoded in the form of a string. We assume that for each data set for 10% data points class level information are known to us. Two internal cluster validity indices reflecting different data properties, an external cluster validity index measuring the similarity between the obtained partitioning and the true labelling for 10% data points and a measure counting the number of features present in a particular string are optimized using the search capability of AMOSA. AMOSA is utilized to detect the appropriate subset of features, appropriate number of clusters as well as the appropriate partitioning from any given data set. The effectiveness of the proposed semi-supervised feature selection technique as compared to the existing techniques is shown for seven real-life data sets of varying complexities.",10.1186/2193-1801-3-465,Clustering; Multiobjective optimization (MOO); Symmetry; Cluster validity indices; Semi-supervised clustering; Feature selection; Multi-center; Automatic determination of number of clusters,,0.0,1.0
An experimental evaluation of some classification methods,"Doumpos, M; Chatzi, E; Zopounidis, C",JOURNAL OF GLOBAL OPTIMIZATION,2006.0,"The classification problem is of major importance to a plethora of research fields. The outgrowth in the development of classification methods has led to the development of several techniques. The objective of this research is to provide some insight on the relative performance of some well-known classification methods, through an experimental analysis covering data sets with different characteristics. The methods used in the analysis include statistical techniques, machine learning methods and multicriteria decision aid. The results of the study can be used to support the design of classification systems and the identification of the proper methods that could be used given the data characteristics.",10.1007/s10898-005-6152-y,classification; machine learning; Monte Carlo simulation; multicriteria decision aid; Statistical techniques,,0.0,1.0
An evolutionary parallel multiobjective feature selection framework,"Kiziloz, HE; Deniz, A",COMPUTERS & INDUSTRIAL ENGINEERING,2021.0,"Feature selection has become an indispensable preprocessing step in data mining problems as high amount of data become prevalent with the advances in technology. The objective of feature selection is twofold: reducing data amount and improving learning performance. In this study, we leverage the multi-core nature of a regular PC to build a robust framework for feature selection. This framework executes the feature selection algorithm on four processors, in parallel. As per the No Free Lunch Theorem, we facilitate 40 different execution settings for the processors by employing two multiobjective selection algorithms, four initial population generation methods, and five machine learning techniques. Besides, we introduce six setting selection schemes to decide the most fruitful setting for each processor. We carry out extensive experiments on 11 UCI benchmark datasets and analyze the results with statistical tests. Finally, we compare our proposed method with state-of-the-art studies and record remarkable improvement in terms of maximum accuracy.",10.1016/j.cie.2021.107481,Feature selection; Multiobjective optimization; Parallel processing; Evolutionary computation,,0.0,1.0
Designing parallelism in Surrogate-assisted multiobjective optimization based on decomposition,"Berveglieri, N; Derbel, B; Liefooghe, A; Aguirre, H; Zhang, QF; Tanaka, K",GECCO'20: PROCEEDINGS OF THE 2020 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE,2020.0,"On the one hand, surrogate-assisted evolutionary algorithms are established as a method of choice for expensive black-box optimization problems. On the other hand, the growth in computing facilities has seen a massive increase in potential computational power, granted the users accommodate their approaches with the offered parallelism. While a number of studies acknowledge the impact of parallelism for single-objective expensive optimization assisted by surrogates, extending such techniques to the multi-objective setting has not yet been properly investigated, especially within the state-of-the-art decomposition framework. We first highlight the different degrees of parallelism in existing surrogate-assisted multi-objective evolutionary algorithms based on decomposition (S-MOEA/D). We then provide a comprehensive analysis of the key steps towards a successful parallel S-MOEA/D approach. Through an extensive benchmarking effort relying on the well-established bbob-biobj test functions, we analyze the performance of the different algorithm designs with respect to the problem dimensionality and difficulty, the amount of parallel cores available, and the supervised learning models considered. In particular, we show the difference in algorithm scalability based on the selected surrogate-assisted approaches, the performance impact of distributing the model training task and the efficacy of the designed parallel-surrogate methods.",10.1145/3377930.3390202,Multiobjective optimization; surrogates; parallelism benchmarking,,0.0,1.0
On the efficient use of uncertainty when performing expensive ROC optimisation,"Fieldsend, JE; Everson, RM","2008 IEEE CONGRESS ON EVOLUTIONARY COMPUTATION, VOLS 1-8",2008.0,"When optimising receiver operating characteristic (ROC) curves there is an inherent degree of uncertainty associated with the operating point evaluation of a model parameterisation x. This is due to the finite amount of training data used to evaluate the true and false positive rates of x. The uncertainty associated with any particular x can be reduced, but only at the computation cost of evaluating more data. Here we explicitly represent this uncertainty through the use of probabilistically non-dominated archives, and show how expensive ROC optimisation problems may be tackled by only evaluating a small subset of the available data at each generation of an optimisation algorithm. Illustrative results are given on data sets from the well known UCI machine learning repository.",10.1109/CEC.2008.4631340,,,0.0,1.0
Interpretable neural networks based on continuous-valued logic and multicriteria decision operators,"Csiszar, O; Csiszar, G; Dombi, J",KNOWLEDGE-BASED SYSTEMS,2020.0,"Combining neural networks with continuous logic and multicriteria decision-making tools can reduce the black-box nature of neural models. In this study, we show that nilpotent logical systems offer an appropriate mathematical framework for hybridization of continuous nilpotent logic and neural models, helping to improve the interpretability and safety of machine learning. In our concept, perceptrons model soft inequalities; namely membership functions and continuous logical operators. We design the network architecture before training, using continuous logical operators and multicriteria decision tools with given weights working in the hidden layers. Designing the structure appropriately leads to a drastic reduction in the number of parameters to be learned. The theoretical basis offers a straightforward choice of activation functions (the cutting function or its differentiable approximation, the squashing function), and also suggests an explanation to the great success of the rectified linear unit (ReLU). In this study, we focus on the architecture of a hybrid model and introduce the building blocks for future applications in deep neural networks. (C) 2020 Elsevier B.V. All rights reserved.",10.1016/j.knosys.2020.105972,Explainable artificial intelligence; Continuous logic; Nilpotent logic; Neural network; Adversarial problems,,0.0,1.0
A Multicriteria Decision Making Approach for Estimating the Number of Clusters in a Data Set,"Peng, Y; Zhang, Y; Kou, G; Shi, Y",PLOS ONE,2012.0,"Determining the number of clusters in a data set is an essential yet difficult step in cluster analysis. Since this task involves more than one criterion, it can be modeled as a multiple criteria decision making (MCDM) problem. This paper proposes a multiple criteria decision making (MCDM)-based approach to estimate the number of clusters for a given data set. In this approach, MCDM methods consider different numbers of clusters as alternatives and the outputs of any clustering algorithm on validity measures as criteria. The proposed method is examined by an experimental study using three MCDM methods, the well-known clustering algorithm-k-means, ten relative measures, and fifteen public-domain UCI machine learning data sets. The results show that MCDM methods work fairly well in estimating the number of clusters in the data and outperform the ten relative measures considered in the study.",10.1371/journal.pone.0041713,,,0.0,1.0
K-modes and Entropy Cluster Centers Initialization Methods,"Ali, DS; Ghoneim, A; Saleh, M",PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON OPERATIONS RESEARCH AND ENTERPRISE SYSTEMS (ICORES),2017.0,"Data clustering is an important unsupervised technique in data mining which aims to extract the natural partitions in a dataset without a priori class information. Unfortunately, every clustering model is very sensitive to the set of randomly initialized centers, since such initial clusters directly influence the formation of final clusters. Thus, determining the initial cluster centers is an important issue in clustering models. Previous work has shown that using multiple clustering validity indices in a multiobjective clustering model (e.g., MODEK-Modes model) yields more accurate results than using a single validity index. In this study, we enhance the performance of MODEK-Modes model by introducing two new initialization methods. The two proposed methods are the K-Modes initialization method and the entropy initialization method. The two proposed methods are tested using ten benchmark real life datasets obtained from the UCI Machine Learning Repository. Experimental results show that the two initialization methods achieve significant improvement in the clustering performance compared to other existing initialization methods.",10.5220/0006245504470454,Multiobjective Data Clustering; Categorical Datasets; K-modes Clustering Algorithm; Entropy,,0.0,1.0
Robust ordinal regression in preference learning and ranking,"Corrente, S; Greco, S; Kadzinski, M; Slowinski, R",MACHINE LEARNING,2013.0,"Multiple Criteria Decision Aiding (MCDA) offers a diversity of approaches designed for providing the decision maker (DM) with a recommendation concerning a set of alternatives (items, actions) evaluated from multiple points of view, called criteria. This paper aims at drawing attention of the Machine Learning (ML) community upon recent advances in a representative MCDA methodology, called Robust Ordinal Regression (ROR). ROR learns by examples in order to rank a set of alternatives, thus considering a similar problem as Preference Learning (ML-PL) does. However, ROR implements the interactive preference construction paradigm, which should be perceived as a mutual learning of the model and the DM. The paper clarifies the specific interpretation of the concept of preference learning adopted in ROR and MCDA, comparing it to the usual concept of preference learning considered within ML. This comparison concerns a structure of the considered problem, types of admitted preference information, a character of the employed preference models, ways of exploiting them, and techniques to arrive at a final ranking.",10.1007/s10994-013-5365-4,Robust ordinal regression; Ranking; Preference learning; Multiple criteria decision aiding; Comparison; Preference modeling; Preference construction,,0.0,1.0
Learning and exploitation do not conflict under minimax optimality,Szepesvári C.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1997.0,We show that adaptive real time dynamic programming extended with the action selection strategy which chooses the best action according to the latest estimate of the cost function yields asymptotically optimal policies within finite time under the minimax optimality criterion. From this it follows that learning and exploitation do not conflict under this special optimality criterion. We relate this result to learning optimal strategies in repeated two-player zero-sum deterministic games. © Springer-Verlag Berlin Heidelberg 1997.,10.1007/3-540-62858-4_89,Dynamic games; Reinforcement learning; Self-optimizing systems,6.0,0.0,1.0
Neural minimax classifiers,"Alaiz-Rodríguez R., Cid-Sueiro J.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2002.0,"Many supervised learning algorithms are based on the assumption that the training data set reflects the underlying statistical model of the real data. However, this stationarity assumption may be partially violated in practice: for instance, if the cost of collecting data is class dependent, the class priors of the training data set may be different from that of the test set. A robust solution to this problem is selecting the classifier that minimize the error probability under the worst case conditions. This is known as the minimax strategy. In this paper we propose a mechanism to train a neural network in order to estimate the minimax classifier that is robust to changes in the class priors. This procedure is illustrated on a softmax-based neural network, although it can be applied to other structures. Several experimental results show the advantages of the proposed methods with respect to other approaches. © Springer-Verlag Berlin Heidelberg 2002.",10.1007/3-540-46084-5_66,,,0.0,1.0
Evolutionary model selection in unsupervised learning,"Kim Y., Street W.N., Menczer F.",Intelligent Data Analysis,2002.0,"Feature subset selection is important not only for the insight gained from determining relevant modeling variables but also for the improved understandability, scalability, and possibly, accuracy of the resulting models. Feature selection has traditionally been studied in supervised learning situations, with some estimate of accuracy used to evaluate candidate subsets. However, we often cannot apply supervised learning for lack of a training signal. For these cases, we propose a new feature selection approach based on clustering. A number of heuristic criteria can be used to estimate the quality of clusters built from a given feature subset. Rather than combining such criteria, we use ELSA, an evolutionary local selection algorithm that maintains a diverse population of solutions that approximate the Pareto front in a multi-dimensional objective space. Each evolved solution represents a feature subset and a number of clusters; two representative clustering algorithms, K-means and EM, are applied to form the given number of clusters based on the selected features. Experimental results on both real and synthetic data show that the method can consistently find approximate Pareto-optimal solutions through which we can identify the significant features and an appropriate number of clusters. This results in models with better and clearer semantic relevance. © 2002-IOS Press. All rights reserved.",10.3233/ida-2002-6605,,76.0,0.0,1.0
Pareto neuro-ensembles,Abbass H.A.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2003.0,"The formation of a neural network ensemble has attracted much attention in the machine learning literature. A set of trained neural networks are combined using a post-gate to form a single super-network. One main challenge in the literature is to decide on which network to include in, or exclude from the ensemble. Another challenge is how to define an optimum size for the ensemble. Some researchers also claim that for an ensemble to be effective, the networks need to be different. However, there is not a consistent definition of what “different” means. Some take it to mean weakly correlated networks, networks with different bias-variance trade-off, and/or networks which are specialized on different parts of the input space. In this paper, we present a theoretically sound approach for the formation of neural network ensembles. The approach is based on the dominance concept that determines which network to include/exclude, identifies a suitable size for the ensemble, and provides a mechanism for quantifying differences between networks. The approach was tested on a number of standard dataset and showed competitive results. © Springer-Verlag Berlin Heidelberg 2003.",10.1007/978-3-540-24581-0_47,,28.0,0.0,1.0
Pareto-optimal patterns in logical analysis of data,"Hammer P.L., Kogan A., Simeone B., Szedmák S.",Discrete Applied Mathematics,2004.0,"Patterns are the key building blocks in the logical analysis of data (LAD). It has been observed in empirical studies and practical applications that some patterns are more ""suitable"" than others for use in LAD. In this paper, we model various such suitability criteria as partial preorders defined on the set of patterns. We introduce three such preferences, and describe patterns which are Pareto-optimal with respect to any one of them, or to certain combinations of them. We develop polynomial time algorithms for recognizing Pareto-optimal patterns, as well as for transforming an arbitrary pattern to a better Pareto-optimal one with respect to any one of the considered criteria, or their combinations. We obtain analytical representations characterizing some of the sets of Pareto-optimal patterns, and investigate the computational complexity of generating all Pareto-optimal patterns. The empirical evaluation of the relative merits of various types of Pareto-optimality is carried out by comparing the classification accuracy of Pareto-optimal theories on several real life data sets. This evaluation indicates the advantages of ""strong patterns"", i.e. those patterns which are Pareto-optimal with respect to the ""evidential preference"" introduced in this paper. © 2004 Elsevier B.V. All rights reserved.",10.1016/j.dam.2003.08.013,"Boolean functions; Classification accuracy; Extremal patterns, Data mining; Machine learning",60.0,0.0,1.0
Semi-supervised feature selection via multiobjective optimization,"Handl J., Knowles J.",IEEE International Conference on Neural Networks - Conference Proceedings,2006.0,"In previous work, we have shown that both unsupervised feature selection and the semi-supervised clustering problem can be usefully formulated as multiobjective optimization problems. In this paper, we discuss the logical extension of this prior work to cover the problem of semi-supervised feature selection. Our extensive experimental results provide evidence for the advantages of semi-supervised feature selection when both labelled and unlabelled data are available. Moreover, the particular effectiveness of a Pareto-based optimization approach can also be seen. © 2006 IEEE.",10.1109/ijcnn.2006.247330,,17.0,0.0,1.0
Pattern classification by evolutionary RBF networks ensemble based on multi-objective optimization,"Kondo N., Hatanaka T., Uosaki K.",IEEE International Conference on Neural Networks - Conference Proceedings,2006.0,"In this paper, evolutionary multi-objective selection method of RBF networks structure and its application to the ensemble learning is considered. The candidates of RBF network structure are encoded into the chromosomes in GAs. Then, they evolve toward Pareto-optimal front defined by several objective functions concerning with model accuracy, model complexity and model smoothness. RBF network ensemble is constructed of the obtained Pareto-optimal models since such models are diverse. This method is applied to the pattern classification problem. Experiments on the benchmark problem demonstrate that the proposed method has comparable generalization ability to conventional ensemble methods. © 2006 IEEE.",10.1109/ijcnn.2006.247224,,8.0,0.0,1.0
Pareto-coevolutionary genetic programming classifier,"Lemczyk V., Heywood M.",GECCO 2006 - Genetic and Evolutionary Computation Conference,2006.0,"The conversion and extension of the Incremental Pareto-Coevolution Archive algorithm (IPCA) into the domain of Genetic Programming classifier evolution is presented. In order to accomplish efficiency in regards to classifier evaluation on training data, the coevolutionary aspect of the IPCA algorithm is utilized to simultaneously evolve a subset of the training data that provides distinctions between candidate classifiers. The algorithm is compared in terms of classification ""score"" (equal weight to detection rate, and 1 - false positive rate), and run-time against a traditional GP classifier using the entinety of the training data for evaluation, and a GP classifier which performs Dynamic Subset Selection. The results indicate that the presented algorithm outperforms the subset, selection algorithm in terms of classification score, and outperforms the traditional classifier while requiring roughly 1/430 of the wall-clock time.",10.1145/1143997.1144162,Co-evolution; Evolutionary Computation; Genetic Programming; Subset Selection; Supervised Learning,4.0,0.0,1.0
Information preserving multi-objective feature selection for unsupervised learning,"Mierswa I., Wurst M.",GECCO 2006 - Genetic and Evolutionary Computation Conference,2006.0,"In this work we propose a novel, sound framework for evolutionary feature selection in unsupervised machine learning problems. We show that unsupervised feature selection is inhemulti-objectiverently multi-objective and behaves differently from supervised feature selection in that the number of features must be maximized instead of being minimized. Although this might sound surprising from a supervised learning point of view, we exemplify this relationship on the problem of data clustering and show that existing approaches do not pose the optimization problem in an appropriate way. Another important consequence of this paradigm change is a method which segments the Pareto sets produced by our approach. Inspecting only prototypical points from these segments drastically reduces the amount of work for selecting a final solution. We compare our methods against existing approaches on eight data sets. Copyright 2006 ACM.",10.1145/1143997.1144248,Multi-objective feature selection; Pareto front segmentation; Unsupervised learning,34.0,0.0,1.0
A DC-programming algorithm for kernel selection,"Argyriou A., Hauser R., Micchelli C.A., Pontil M.",ACM International Conference Proceeding Series,2006.0,"We address the problem of learning a kernel for a given supervised learning task. Our approach consists in searching within the convex hull of a prescribed set of basic kernels for one which minimizes a convex regularization functional. A unique feature of this approach compared to others in the literature is that the number of basic kernels can be infinite. We only require that they are continuously parameterized. For example, the basic kernels could be isotropic Gaussians with variance in a prescribed interval or even Gaussians parameterized by multiple continuous parameters. Our work builds upon a formulation involving a minimax optimization problem and a recently proposed greedy algorithm for learning the kernel. Although this optimization problem is not convex, it belongs to the larger class of DC (difference of convex functions) programs. Therefore, we apply recent results from DC optimization theory to create a new algorithm for learning the kernel. Our experimental results on benchmark data sets show that this algorithm outperforms a previously proposed method.",10.1145/1143844.1143850,,31.0,0.0,1.0
An agent-based approach to the multiple-objective selection of reference vectors,"Czarnowski I., Jȩdrzejowicz P.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2007.0,"The paper proposes an agent-based approach to the multipleobjective selection of reference vectors from original datasets. Effective and dependable selection procedures are of vital importance to machine learning and data mining. The suggested approach is based on the multiple agent paradigm. The authors propose using JABAT middleware as a tool and the original instance reduction procedure as a method for selecting reference vectors under multiple objectives. The paper contains a brief introduction to the multiple objective optimization, followed by the formulation of the multiple-objective, agent-based, reference vectors selection optimization problem. Further sections of the paper provide details on the proposed algorithm generating a non-dominated (or Pareto-optimal) set of reference vector sets. To validate the approach the computational experiment has been planned and carried out. Presentation and discussion of experiment results conclude the paper. © Springer-Verlag Berlin Heidelberg 2007.",10.1007/978-3-540-73499-4_10,,7.0,0.0,1.0
Supervised tensor learning,"Tao D., Li X., Wu X., Hu W., Maybank S.J.",Knowledge and Information Systems,2007.0,"Tensor representation is helpful to reduce the small sample size problem in discriminative subspace selection. As pointed by this paper, this is mainly because the structure information of objects in computer vision research is a reasonable constraint to reduce the number of unknown parameters used to represent a learning model. Therefore, we apply this information to the vector-based learning and generalize the vector-based learning to the tensor-based learning as the supervised tensor learning (STL) framework, which accepts tensors as input. To obtain the solution of STL, the alternating projection optimization procedure is developed. The STL framework is a combination of the convex optimization and the operations in multilinear algebra. The tensor representation helps reduce the overfitting problem in vector-based learning. Based on STL and its alternating projection optimization procedure, we generalize support vector machines, minimax probability machine, Fisher discriminant analysis, and distance metric learning, to support tensor machines, tensor minimax probability machine, tensor Fisher discriminant analysis, and the multiple distance metrics learning, respectively. We also study the iterative procedure for feature extraction within STL. To examine the effectiveness of STL, we implement the tensor minimax probability machine for image classification. By comparing with minimax probability machine, the tensor version reduces the overfitting problem. © Springer-Verlag London Limited 2006.",10.1007/s10115-006-0050-6,Alternating projection; Convex optimization; Supervised learning; Tensor,286.0,0.0,1.0
Controlling overfitting with multi-objective support vector machines,Mierswa I.,Proceedings of GECCO 2007: Genetic and Evolutionary Computation Conference,2007.0,"Recently, evolutionary computation has been successfully integrated into statistical learning methods. A Support Vector Machine (SVM) using evolution strategies for its optimization problem frequently deliver better results with respect to the optimization criterion and the prediction accuracy. Moreover, evolutionary computation allows for the efficient large margin optimization of a huge family of new kernel functions, namely non-positive semi definite kernels as the Epanechnikov kernel. For these kernel functions, evolutionary SVM even outperform other learning methods like the Relevance Vector Machine. In this paper, we will discuss another major advantage of evolutionary SVM compared to traditional SVM solutions: we can explicitly optimize the inherent trade-off between training error and model complexity by embedding multi-objective optimization into the evolutionary SVM. This leads to three advantages: first, it is no longer necessary to tune the SVM parameter C which weighs both conflicting criteria. This is a very time-consuming task for traditional SVM. Second, the shape and size of the Pareto front give interesting insights about the complexity of the learning task at hand. Finally, the user can actually see the point where overfitting occurs and can easily select a solution from the Pareto front best suiting his or her needs. Copyright 2007 ACM.",10.1145/1276958.1277323,Evolution strategies; Kernel methods; Machine learning; Support vector machines,20.0,0.0,1.0
Pareto-coevolutionary genetic programming for problem decomposition in multi-class classification,"Lichodzijewski P., Heywood M.I.",Proceedings of GECCO 2007: Genetic and Evolutionary Computation Conference,2007.0,"A bid-based approach for coevolving Genetic Programming classifiers is presented. The approach coevolves a population of learners thatdecompose the instance space by way of their aggregate bidding behaviour. To reduce computation overhead, a small, relevant, subsetof training exemplars is (competitively) coevolved alongside the learners. The approach solves multi-class problems using a single population and is evaluated on three large datasets. It is found tobe competitive, especially compared to classifier systems, whilesignificantly reducing the computation overhead associated withtraining. Copyright 2007 ACM.",10.1145/1276958.1277058,Classification; Coevolution; Genetic programming; Problem decomposition; Subset selection; Supervised learning; Training efficiency,17.0,0.0,1.0
Pareto analysis for the selection of classifier ensembles,"Dos Santos E.M., Sabourin R., Maupin P.",GECCO'08: Proceedings of the 10th Annual Conference on Genetic and Evolutionary Computation 2008,2008.0,"The overproduce-and-choose strategy involves the generation of an initial large pool of candidate classifiers and it is intended to test different candidate ensembles in order to select the best performing solution. The ensemble's error rate, ensemble size and diversity measures are the most frequent search criteria employed to guide this selection. By applying the error rate, we may accomplish the main objective in Pattern Recognition and Machine Learning, which is to find high-performance predictors. In terms of ensemble size, the hope is to increase the recognition rate while minimizing the number of classifiers in order to meet both the performance and low ensemble size requirements. Finally, ensembles can be more accurate than individual classifiers only when classifier members present diversity among themselves. In this paper we apply two Pareto front spread quality measures to analyze the relationship between the three main search criteria used in the overproduce-and-choose strategy. Experimental results conducted demonstrate that the combination of ensemble size and diversity does not produce conflicting multi-objective optimization problems. Moreover, we cannot decrease the generalization error rate by combining this pair of search criteria. However, when the error rate is combined with diversity or the ensemble size, we found that these measures are conflicting objective functions and that the performances of the solutions are much higher. Copyright 2008 ACM.",10.1145/1389095.1389229,Classifier ensembles; Diversity measures; Ensemble selection; Pareto analysis,10.0,0.0,1.0
Evolutionary multi-objective rule selection for classification rule mining,"Ishibuchi H., Kuwajima I., Nojima Y.",Studies in Computational Intelligence,2008.0,"This chapter discusses the application of evolutionary multi-objective optimization (EMO) to classification rule mining. In the field of classification rule mining, classifiers are designed through the following two phases: rule discovery and rule selection. In the rule discovery phase, a large number of classification rules are extracted from training data. This phase is based on two rule evaluation criteria: support and confidence. An association rule mining technique such as Apriori is usually used to extract classification rules satisfying pre-specified threshold values of the minimum support and confidence. In some studies, EMO algorithms were used to search for Pareto-optimal rules with respect to support and confidence. On the other hand, a small number of rules are selected from the extracted rules to design an accurate and compact classifier in the rule selection phase. A heuristic rule sorting criterion is usually used for rule selection. In some studies, EMO algorithms were used for multi-objective rule selection to maximize the accuracy of rule sets and minimize their complexity. In this chapter, first we explain the above-mentioned two phases in classification rule mining. Next we explain the search for Pareto-optimal rules and the search for Pareto-optimal rule sets. Then we explain evolutionary multi-objective rule selection as a post processing procedure in the second phase of classification rule mining. A number of Pareto-optimal rule sets are found from a large number of candidate rules, which are extracted from training data in the first phase. Finally we show experimental results on some data sets from the UCI machine learning repository. Through computational experiments, we demonstrate that evolutionary rule selection can drastically decrease the number of extracted rules without severely degrading their classification accuracy. We also examine the relation between Paretooptimal rules and Pareto- optimal rule sets. © 2008 Springer-Verlag Berlin Heidelberg.",10.1007/978-3-540-77467-9_3,,8.0,0.0,1.0
Simultaneous feature selection and classification via minimax probability machine,"Yang L., Wang L., Sun Y., Zhang R.",International Journal of Computational Intelligence Systems,2010.0,"This paper presents a novel method for simultaneous feature selection and classification by incorporating a robust L1-norm into the objective function of Minimax Probability Machine (MPM). A fractional programming framework is derived by using a bound on the misclassification error involving the mean and covariance of the data. Furthermore, the problems are solved by the Quadratic Interpolation method. Experiments show that our methods can select fewer features to improve the generalization compared to MPM, which illustrates the effectiveness of the proposed algorithms. © 2010 Taylor & Francis Group, LLC.",10.1080/18756891.2010.9727738,Feature selection; Machine learning; Minimax probability machine; Probability of misclassification,19.0,0.0,1.0
Generalization improvement of radial basis function network based on multi-objective particle swarm optimization,"Qasem S.N., Shamsuddin S.M.",Journal of Artificial Intelligence,2010.0,"The problem of unsupervised and supervised learning of RBF networks is discussed with Multi-Objective Particle Swarm Optimization (MOPSO). This study presents an evolutionary multi-objective selection method of RBF networks structure. The candidates of RBF networks structures are encoded into particles in PSO. These particles evolve toward Pareto-optimal front defined by several objective functions with model accuracy and complexity. This study suggests an approach of RBF network training through simultaneous optimization of architectures and connections with PSO-based multi-objective algorithm. Present goal is to determine whether MOPSO can train RBF networks and the performance is validated on accuracy and complexity. The experiments are conducted on two benchmark datasets obtained from the machine learning repository. The results show that; the best results are obtained for our proposed method that has obtained 100 and 80.21 % classification accuracy from the experiments made on the data taken from breast cancer and diabetes diseases database, respectively. The results also show that our approach provides an effective means to solve multi-objective RBF networks and outperforms multi-objective genetic algorithm. © 2010 Asian Network for Scientific Information.",10.3923/jai.2010.1.16,Elitist non-dominated sorting genetic algorithm; Hybrid learning; Multi-objective optimization; Multi-objective particle swarm optimization; Radial basis function network,16.0,0.0,1.0
DBSCAN-based multi-objective niching to approximate equivalent pareto-subsets,"Kramer O., Danielsiek H.","Proceedings of the 12th Annual Genetic and Evolutionary Computation Conference, GECCO '10",2010.0,"In systems optimization and machine learning multiple alternative solutions may exist in different parts of decision space for the same parts of the Pareto-front. The detection of equivalent Pareto-subsets may be desirablethis paper we introduce a niching method that approximates Paretooptimal solutions with diversity mechanisms in objective and decision space. For diversity in objective space we use rake selection, a selection method based on the distances to reference lines in objective space. For diversity in decision space we introduce a niching approach that uses the density-based clustering method DBSCAN. The clustering process assigns the population to niches while the multi-objective optimization process concentrates on each niche independently. We introduce an indicator for the adaptive control of clustering processes, and extend rake selection by the concept of adaptive corner points. The niching method is experimentally validated on parameterized test function with the help of the S-metric. Copyright 2010 ACM.",10.1145/1830483.1830575,Hybrid evolutionary multiobjective algorithm; Hybrid metaheuristics; Local search; Memetic algorithms,16.0,0.0,1.0
Feature selection for multi-purpose predictive models: A many-objective task,"Reynolds A.P., Corne D.W., Chantler M.J.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2010.0,"The target of machine learning is a predictive model that performs well on unseen data. Often, such a model has multiple intended uses, related to different points in the tradeoff between (e.g.) sensitivity and specificity. Moreover, when feature selection is required, different feature subsets will suit different target performance characteristics. Given a feature selection task with such multiple distinct requirements, one is in fact faced with a very-many-objective optimization task, whose target is a Pareto surface of feature subsets, each specialized for (e.g.) a different sensitivity/specificity tradeoff profile. We argue that this view has many advantages. We motivate, develop and test such an approach. We show that it can be achieved successfully using a dominance-based multiobjective algorithm, despite an arbitrarily large number of objectives. © 2010 Springer-Verlag.",10.1007/978-3-642-15844-5_39,,7.0,0.0,1.0
Inducing multi-objective clustering ensembles with genetic programming,"Coelho A.L.V., Fernandes E., Faceli K.",Neurocomputing,2010.0,"The recent years have witnessed a growing interest in two advanced strategies to cope with the data clustering problem, namely, clustering ensembles and multi-objective clustering. In this paper, we present a genetic programming based approach that can be considered as a hybrid of these strategies, thereby allowing that different hierarchical clustering ensembles be simultaneously evolved taking into account complementary validity indices. Results of computational experiments conducted with artificial and real datasets indicate that, in most of the cases, at least one of the Pareto optimal partitions returned by the proposed approach compares favorably or go in par with the consensual partitions yielded by two well-known clustering ensemble methods in terms of clustering quality, as gauged by the corrected Rand index. © 2010 Elsevier B.V.",10.1016/j.neucom.2010.09.014,Cluster analysis; Ensembles; Genetic programming; Multi-objective optimization,14.0,0.0,1.0
Smart PAC-learners,"Darnstädt M., Simon H.U.",Theoretical Computer Science,2011.0,"The PAC-learning model is distribution-independent in the sense that the learner must reach a learning goal with a limited number of labeled random examples without any prior knowledge of the underlying domain distribution. In order to achieve this, one needs generalization error bounds that are valid uniformly for every domain distribution. These bounds are (almost) tight in the sense that there is a domain distribution which does not admit a generalization error being significantly smaller than the general bound. Note however that this leaves open the possibility to achieve the learning goal faster if the underlying distribution is ""simple"". Informally speaking, we say a PAC-learner L is ""smart"" if, for a ""vast majority"" of domain distributions D, L does not require significantly more examples to reach the ""learning goal"" than the best learner whose strategy is specialized to D. In this paper, focusing on sample complexity and ignoring computational issues, we show that smart learners do exist. This implies (at least from an information-theoretical perspective) that full prior knowledge of the domain distribution (or access to a huge collection of unlabeled examples) does (for a vast majority of domain distributions) not significantly reduce the number of labeled examples required to achieve the learning goal. © 2010 Elsevier B.V. All rights reserved.",10.1016/j.tcs.2010.12.053,Learning under a fixed distribution; Machine learning; Minimax theorem; PAC-learning; Semi-supervised learning; Smart PAC-learner; Value of unlabeled data,5.0,0.0,1.0
Evolving ensembles in Multi-objective Genetic Programming for classification with unbalanced data,"Bhowan U., Johnston M., Zhang M.","Genetic and Evolutionary Computation Conference, GECCO'11",2011.0,"Machine learning algorithms can suffer a performance bias when data sets are unbalanced. This paper proposes a Multi-objective Genetic Programming approach using negative correlation learning to evolve accurate and diverse ensembles of non-dominated solutions where members vote on class membership. We also compare two popular Pareto-based fitness schemes on the classification tasks. We show that the evolved ensembles achieve high accuracy on both classes using six unbalanced binary data sets, and that this performance is usually better than many of its individual members. Copyright 2011 ACM.",10.1145/2001576.2001756,Class imbalance; Classification; Evolutionary multi-objective optimisation; Genetic programming,25.0,0.0,1.0
Ensembles and multiple classifiers: A game-theoretic view,Cesa-Bianchi N.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2011.0,"The study of multiple classifier systems is a fundamental topic in modern machine learning. However, early work on aggregation of predictors can be traced back to the Fifties, in the area of game theory. At that time, the pioneering work of James Hannan [11] and David Blackwell [2] laid down the foundations of repeated game theory. In a nutshell, a repeated game is the game-theoretic interpretation of learning. In games played once, lacking any information about the opponent, the best a player can do is to play the minimax strategy (the best strategy against the worst possible opponent). In repeated games, by examining the history of past opponent moves, the player acquires information about the opponent's behavior and can adapt to it, in order to achieve a better payoff than that guaranteed by the minimax strategy. © 2011 Springer-Verlag.",10.1007/978-3-642-21557-5_2,,,0.0,1.0
On the curse of dimensionality in supervised learning of smooth regression functions,"Liitiäinen E., Corona F., Lendasse A.",Neural Processing Letters,2011.0,"In this paper, the effect of dimensionality on the supervised learning of infinitely differentiable regression functions is analyzed. By invoking the Van Trees lower bound, we prove lower bounds on the generalization error with respect to the number of samples and the dimensionality of the input space both in a linear and non-linear context. It is shown that in non-linear problems without prior knowledge, the curse of dimensionality is a serious problem. At the same time, we speculate counter-intuitively that sometimes supervised learning becomes plausible in the asymptotic limit of infinite dimensionality. © 2011 Springer Science+Business Media, LLC.",10.1007/s11063-011-9188-7,Analytic function; High dimensional; Minimax; Nonparametric regression; Supervised learning; Van Trees,4.0,0.0,1.0
Manifold-regularized minimax probability machine,"Yoshiyama K., Sakurai A.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2012.0,"In this paper we propose Manifold-Regularized Minimax Probability Machine, called MRMPM. We show that Minimax Probability Machine can properly be extended to semi-supervised version in the manifold regularization framework and that its kernelized version is obtained for non-linear case. Our experiments show that the proposed methods achieve results competitive to existing learning methods, such as Laplacian Support Vector Machine and Laplacian Regularized Least Square for publicly available datasets from UCI machine learning repository. © 2012 Springer-Verlag.",10.1007/978-3-642-28258-4_5,,4.0,0.0,1.0
Coevolutionary learning of neural network ensemble for complex classification tasks,"Tian J., Li M., Chen F., Kou J.",Pattern Recognition,2012.0,"Ensemble approaches to classification have attracted a great deal of interest recently. This paper presents a novel method for designing the neural network ensemble using coevolutionary algorithm. The bootstrap resampling procedure is employed to obtain different training subsets that are used to estimate different component networks of the ensemble. Then the cooperative coevolutionary algorithm is developed to optimize the ensemble model via the divide-and-cooperative mechanism. All component networks are coevolved in parallel in the scheme of interacting co-adapted subpopulations. The fitness of an individual from a particular subpopulation is assessed by associating it with the representatives from other subpopulations. In order to promote the cooperation of all component networks, the proposed method considers both the accuracy and the diversity among the component networks that are evaluated using the multi-objective Pareto optimality measure. A hybrid output-combination method is designed to determine the final ensemble output. Experimental results illustrate that the proposed method is able to obtain neural network ensemble models with better classification accuracy in comparison with currently popular ensemble algorithms. © 2011 Elsevier Ltd All rights reserved.",10.1016/j.patcog.2011.09.012,Classification; Coevolutionary algorithm; Ensemble learning; Neural network,30.0,0.0,1.0
A computational geometry approach for pareto-optimal selection of neural networks,"Torres L.C.B., Castro C.L., Braga A.P.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2012.0,"This paper presents a Pareto-optimal selection strategy for multiobjective learning that is based on the geometry of the separation margin between classes. The Gabriel Graph, a method borrowed from Computational Geometry, is constructed in order to obtain margin patterns and class borders. From border edges, a target separator is obtained in order to obtain a large margin classifier. The selected model from the generated Pareto-set is the one that is closer to the target separator. The method presents robustness in both synthetic and real benchmark datasets. It is efficient for Pareto-Optimal selection of neural networks and no claim is made that the obtained solution is equivalent to a maximum margin separator. © 2012 Springer-Verlag.",10.1007/978-3-642-33266-1_13,classification; decision-making; gabriel graph; multiobjective machine learning,2.0,0.0,1.0
A classification model based on incomplete information on features in the form of their average values,"Utkin L.V., Zhuk Y.A., Selikhovkin I.A.",Scientific and Technical Information Processing,2012.0,"This paper presents a model of classification under incomplete information in the form of mathematical expectations of features; it is based on the minimax (minimin) strategy of decision making. The discriminant function is calculated by maximization (minimization) of the risk functional as a measure of misclassification, by a set of distributions of probabilities with bounds determined by information on features, and minimization by the set of parameters. The algorithm is reduced to solution of the parametric problem of linear programming. © 2012 Allerton Press, Inc.",10.3103/S0147688212060068,classification; linear programming; machine learning; mathematical expectation; minimax strategy; risk functional; the loss function,3.0,0.0,1.0
Categorical feature reduction using multi objective genetic Algorithm in cluster analysis,"Dutta D., Dutta P., Sil J.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2013.0,"In the paper, real coded multi objective genetic algorithm based K-clustering method has been studied, K represents the number of clusters. In K-clustering algorithm value of K is known. The searching power of Genetic Algorithm (GA) is exploited to search for suitable clusters and centers of clusters so that intra-cluster distance (Homogeneity, H) and inter-cluster distances (Separation, S) are simultaneously optimized. It is achieved by measuring H and S using Mod distance per feature metric, suitable for categorical features (attributes). We have selected 3 benchmark data sets from UCI Machine Learning Repository containing categorical features only. The paper proposes two versions of MOGA based K-clustering algorithm. In proposed MOGA (H, S), all features are taking part in building chromosomes and calculation of H and S values. In MOGA-Feature-Selection (H, S), selected features take part to build chromosomes, relevant for clusters. Here, K-modes is hybridized with GA. We have used hybridized GA to combine global searching capabilities of GA with local searching capabilities of K-modes. Considering context sensitivity, we have used a special crossover operator called ""pairwise crossover"" and ""substitution"". The main contribution of this paper is simultaneous dimensionality reduction and optimization of objectives using MOGA. © 2013 Springer-Verlag Berlin Heidelberg.",10.1007/978-3-642-45318-2_7,Clustering; dimensionality reduction; homogeneity and separation; Pareto optimal front; real coded multi objective genetic algorithm,3.0,0.0,1.0
Robust novelty detection in the framework of a contamination neighbourhood,"Utkin L.V., Zhuk Y.A.",International Journal of Intelligent Information and Database Systems,2013.0,A novelty detection robust model is studied in the paper. It is based on contaminated (robust) models which produce a set of probability distributions of data points instead of the empirical distribution. The minimax and minimin strategies are used to construct optimal separating functions. An algorithm for computing the optimal parameters of the novelty detection model is reduced to a finite number of standard SVM tasks with weighted data points. Experimental results with synthetic and some real data illustrate the proposed novelty detection robust model. Copyright © 2013 Inderscience Enterprises Ltd.,10.1504/IJIIDS.2013.053830,Classification; Machine learning; Minimax strategy; Novelty detection; Quadratic programming; Support vector machine; SVM,5.0,0.0,1.0
Robust extreme learning machine,"Horata P., Chiewchanwattana S., Sunat K.",Neurocomputing,2013.0,"The output weights computing of extreme learning machine (ELM) encounters two problems, the computational and outlier robustness problems. The computational problem occurs when the hidden layer output matrix is a not full column rank matrix or an ill-conditioned matrix because of randomly generated input weights and biases. An existing solution to this problem is Singular Value Decomposition (SVD) method. However, the training speed is still affected by the large complexity of SVD when computing the Moore-Penrose (MP) pseudo inverse. The outlier robustness problem may occur when the training data set contaminated with outliers then the accuracy rate of ELM is extremely affected. This paper proposes the Extended Complete Orthogonal Decomposition (ECOD) method to solve the computational problem in ELM weights computing via ECODLS algorithm. And the paper also proposes the other three algorithms, i.e. the iteratively reweighted least squares (IRWLS-ELM), ELM based on the multivariate least-trimmed squares (MLTS-ELM), and ELM based on the one-step reweighted MLTS (RMLTS-ELM) to solve the outlier robustness problem. However, they also encounter the computational problem. Therefore, the ECOD via ECODLS algorithm is also used successfully in the three proposed algorithms. The experiments of regression problems were conducted on both toy and real-world data sets. The outlier types are one-sided and two-sided outliers. Each experiment was randomly contaminated with outliers, of one type only, with 10%, 20%, 30%, 40%, and 50% of the total training data size. Meta-metrics evaluation was used to measure the outlier robustness of the proposed algorithms compared to the existing algorithms, i.e. the minimax probability machine regression (MPMR) and the ordinary ELM. The experimental results showed that ECOD can effectively replace SVD. The ECOD is robust to the not full column rank or the ill-conditional problem. The speed of the ELM training using ECOD is also faster than the ordinary training algorithm. Moreover, the meta-metrics measure showed that the proposed algorithms are less affected by the increasing number of outliers than the existing algorithms. © 2012 Elsevier B.V.",10.1016/j.neucom.2011.12.045,Extended Complete Orthogonal Decomposition; Extreme learning machine; Iteratively reweighted least squares; Meta-metrics evaluation; Minimax probability machine regression; Moore-Penrose pseudo inverse; Multivariate least-trimmed squares; Outlier; Robustness; Singular Value Decomposition,128.0,0.0,1.0
Comparing ensemble learning approaches in genetic programming for classification with unbalanced data,"Bhowan U., Johnston M., Zhang M.",GECCO 2013 - Proceedings of the 2013 Genetic and Evolutionary Computation Conference Companion,2013.0,"This paper compares three approaches to evolving ensembles in Genetic Programming (GP) for binary classification with unbalanced data. The first uses bagging with sampling, while the other two use Pareto-based multi-objective GP (MOGP) for the trade-off between the two (unequal) classes. In MOGP, two ways are compared to build the ensembles: using the evolved Pareto front alone, and using the whole evolved population of dominated and non-dominated individuals alike. Experiments on several benchmark (binary) unbalanced tasks find that smaller, more diverse ensembles chosen during ensemble selection perform best due to better generalisation, particularly when the combined knowledge of the whole evolved MOGP population forms the ensemble.",10.1145/2464576.2464643,Clasfisification; Class imbalance; Genetic programming; Multi-objective optimisation,1.0,0.0,1.0
Label free change detection on streaming data with cooperative multi-objective genetic programming,"Rahimi S., McIntyre A.R., Heywood M.I., Zincir-Heywood N.",GECCO 2013 - Proceedings of the 2013 Genetic and Evolutionary Computation Conference Companion,2013.0,"Classification under streaming data conditions requires that the machine learning (ML) approach operate interactively with the stream content. Thus, given some initial ML clas- sification capability, it is not possible to assume that stream content will be stationary. It is therefore necessary to first detect when the stream content changes. Only after detect- ing a change, can classifier retraining be triggered. Current methods for change detection tend to assume an entropy fil- ter approach, where class labels are necessary. In practice, labeling the stream would be extremely expensive. This work proposes an approach in which the behaviour of GP individuals is used to detect change without the use of la- bels. Only after detecting a change is label information re- quested. Benchmarking under a computer network traffic analysis scenario demonstrates that the proposed approach performs at least as well as the filter method, while retaining the advantage of requiring no labels.",10.1145/2464576.2464652,Coevolution; Dynamic environments; Genetic programming; Pareto archiving; Streaming data,2.0,0.0,1.0
One-side probability machine: Learning imbalanced classifiers locally and globally,"Zhang R., Huang K.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2013.0,"Imbalanced learning is a challenged task in machine learning, where the data associated with one class are far fewer than those associated with the other class. In this paper, we propose a novel model called One-Side Probability Machine (OSPM) able to learn from imbalanced data rigorously and accurately. In particular, OSPM can lead to a rigorous treatment on biased or imbalanced classification tasks, which is significantly different from previous approaches. Importantly, the proposed OSPM exploits the reliable global information from one side only, i.e., the majority class , while engaging the robust local learning [2] from the other side, i.e., the minority class. Such setting proves much effective than other models such as Biased Minimax Probability Machine (BMPM). To our best knowledge, OSPM presents the first model capable of learning from imbalanced data both locally and globally. Our proposed model has also established close connections with various famous models such as BMPM and Support Vector Machine. One appealing feature is that the optimization problem involved can be cast as a convex second order conic programming problem with a global optimum guaranteed. A series of experiments on three data sets demonstrate the advantages of our proposed method against four competitive approaches. © Springer-Verlag 2013.",10.1007/978-3-642-42042-9_18,,1.0,0.0,1.0
Machine learning enhanced multi-objective evolutionary algorithm based on decomposition,"Liau Y.S., Tan K.C., Hu J., Qiu X., Gee S.B.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2013.0,"We address the problem of expensive multi-objective optimization using a machine learning assisted model of evolutionary computation. Specifically, we formulate a meta-objective function tailored to the framework of MOEA/D, which can be solved by means of supervised regression learning using the Support Vector Machine (SVM) algorithm. The learned model constitutes the knowledge which can be then utilized to guide the evolution process within MOEA/D so as to reach better regions in the search space more quickly. Simulation results on a variety of benchmark problems show that the machine-learning enhanced MOEA/D is able to obtain better estimation of Pareto fronts when the allowed computational budget, measured in terms of number of objective function evaluation, is scarce. © 2013 Springer-Verlag.",10.1007/978-3-642-41278-3_67,Evolutionary multi-objective optimization; expensive optimization; support vector machine; surrogate modelling,6.0,0.0,1.0
The randomized greedy modularity clustering algorithm and the core groups graph clustering scheme,"Geyer-Schulz A., Ovelgönne M.","Studies in Classification, Data Analysis, and Knowledge Organization",2014.0,"The modularity measure of Newman and Girvan is a popular formal cluster criterium for graph clustering. Although the modularity maximization problem has been shown to be NP-hard, a large number of heuristic modularity maximization algorithms have been developed. In the 10th DIMACS Implementation Challenge of the Center for Discrete Mathematics & Theoretical Computer Science (DIMACS) for graph clustering our core groups graph clustering scheme combined with a randomized greedy modularity clustering algorithm won both modularity optimization challenges: the Modularity (highest modularity) and the Pareto Challenge (tradeoff between modularity and performance). The core groups graph clustering scheme is an ensemble learning clustering method which combines the local solutions of several base algorithms to form a good start solution for the final algorithm. The randomized greedy modularity algorithm is a nondeterministic agglomerative hierarchical clustering approach which finds locally optimal solutions. In this contribution we analyze the similarity of the randomized greedy modularity algorithm with incomplete solvers for the satisfiability problem and we establish an analogy between the cluster core group heuristic used in core groups graph clustering and a sampling of restart points on the Morse graph of a continuous optimization problem with the same local optima. © Springer International Publishing Switzerland 2014.",10.1007/978-3-319-01264-3_2,,6.0,0.0,1.0
Bayesian reinforcement learning with exploration,"Lattimore T., Hutter M.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2014.0,We consider a general reinforcement learning problem and show that carefully combining the Bayesian optimal policy and an exploring policy leads to minimax sample-complexity bounds in a very general class of (history-based) environments. We also prove lower bounds and show that the new algorithm displays adaptive behaviour when the environment is easier than worst-case. © Springer International Publishing Switzerland 2014.,10.1007/978-3-319-11662-4_13,,3.0,0.0,1.0
Feature selection with positive region constraint for test-cost-sensitive data,"Liu J., Min F., Zhao H., Zhu W.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2014.0,"In many data mining and machine learning applications, data are not free, and there is a test cost for each data item. Due to economic, technological and legal reasons, it is neither possible nor necessary to obtain a classifier with 100 % accuracy. In this paper, we consider such a situation and propose a new constraint satisfaction problem to address it. With this in mind, one has to minimize the test cost to keep the accuracy of the classification under a budget. The constraint is expressed by the positive region, whereas the object is to minimizing the total test cost. The new problem is essentially a dual of the test cost constraint attribute reduction problem, which has been addressed recently. We propose a heuristic algorithm based on the information gain, the test cost, and a user specified parameter to deal with the new problem. The algorithm is tested on four University of California - Irvine datasets with various test cost settings. Experimental results indicate that the algorithm finds optimal feature subset in most cases, the rational setting of is different among datasets, and the algorithm is especially stable when the test cost is subject to the Pareto distribution. © 2014 Springer-Verlag.",10.1007/978-3-662-44680-5_2,Cost-sensitive learning; Feature selection; Positive region; Test cost,1.0,0.0,1.0
Transductive minimax probability machine,"Huang G., Song S., Xu Z., Weinberger K.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2014.0,"The Minimax Probability Machine (MPM) is an elegant machine learning algorithm for inductive learning. It learns a classifier that minimizes an upper bound on its own generalization error. In this paper, we extend its celebrated inductive formulation to an equally elegant transductive learning algorithm. In the transductive setting, the label assignment of a test set is already optimized during training. This optimization problem is an intractable mixed-integer programming. Thus, we provide an efficient label-switching approach to solve it approximately. The resulting method scales naturally to large data sets and is very efficient to run. In comparison with nine competitive algorithms on eleven data sets, we show that the proposed Transductive MPM (TMPM) almost outperforms all the other algorithms in both accuracy and speed. © 2014 Springer-Verlag.",10.1007/978-3-662-44848-9_37,minimax probability machine; semi-supervised learning; transductive learning,3.0,0.0,1.0
A community detection method based on multi-objective optimization method,"Wang L., Liang Y.Q., Tian Q.J., Yang J., Song C., Wu Z.",Applied Mechanics and Materials,2014.0,"Community detection in complex network has been an active research area in data mining and machine learning. This paper proposed a community detection method based on multi-objective evolutionary algorithm, named CDMOEA, which tries to find the Pareto front by maximize two objectives, community score and community fitness. Fast and Elitist Multi-objective Genetic Algorithm is used to attained a set of optimal solutions, and then use Modularity function to choose the best one from them. The locus based adjacency representation is used to realize genetic representation, which ensures the effective connections of the nodes in the network during the process of population Initialization and other genetic operator. Uniform crossover is introduced to ensure population's diversity. We compared it with some popular community detection algorithms in computer generated network and real world networks. Experiment results show that it is more efficient in community detection. © (2014) Trans Tech Publications, Switzerland.",10.4028/www.scientific.net/AMM.571-572.177,Community detection; Complex network; Modularity; Multi-objective evolutionary algorithm; Pareto front,3.0,0.0,1.0
A framework for imprecise robust one-class classification models,Utkin L.V.,International Journal of Machine Learning and Cybernetics,2014.0,"A framework for constructing robust one-class classification models is proposed in the paper. It is based on Walley's imprecise extensions of contaminated models which produce a set of probability distributions of data points instead of a single empirical distribution. The minimax and minimin strategies are used to choose an optimal probability distribution from the set and to construct optimal separating functions. It is shown that an algorithm for computing optimal parameters is determined by extreme points of the probability set and is reduced to a finite number of standard SVM tasks with weighted data points. Important special cases of the models, including pari-mutuel, constant odd-ratio, contaminated models and Kolmogorov-Smirnov bounds are studied. Experimental results with synthetic and real data illustrate the proposed models. © 2012 Springer-Verlag Berlin Heidelberg.",10.1007/s13042-012-0140-6,Classification; Machine learning; Minimax strategy; Novelty detection; Quadratic programming; Support vector machine,25.0,0.0,1.0
Robust boosting classification models with local sets of probability distributions,"Utkin L.V., Zhuk Y.A.",Knowledge-Based Systems,2014.0,Robust classification models based on the ensemble methodology are proposed in the paper. The main feature of the models is that the precise vector of weights assigned for examples in the training set at each iteration of boosting is replaced by a local convex set of weight vectors. The minimax strategy is used for building weak classifiers at each iteration. The local sets of weights are constructed by means of imprecise statistical models. The proposed models are called RILBoost (Robust Imprecise Local Boost). Numerical experiments with real data show that the proposed models outperform the standard AdaBoost algorithm for several well-known data sets. © 2014 Elsevier B.V. All rights reserved.,10.1016/j.knosys.2014.02.007,Boosting; Classification; Imprecise model; Machine learning; Robust,11.0,0.0,1.0
Imprecise prior knowledge incorporating into one-class classification,"Utkin L.V., Zhuk Y.A.",Knowledge and Information Systems,2014.0,"An extension of Campbell and Bennett’s novelty detection or one-class classification model incorporating prior knowledge is studied in the paper. The proposed extension relaxes the strong assumption of the empirical probability distribution over elements of a training set and deals with a set of probability distributions produced by prior knowledge about training data. The classification problem is solved by considering extreme points of the probability distribution set or by means of the conjugate duality technique. Special cases of prior knowledge are considered in detail, including the imprecise linear-vacuous mixture model and interval-valued moments of feature values. Numerical experiments show that the proposed models outperform Campbell and Bennett’s model for many real and synthetic data. © 2013, Springer-Verlag London.",10.1007/s10115-013-0661-7,Extreme points; Imprecise statistical model; Linear programming; Machine learning; Minimax strategy; Novelty detection; One-class classification,8.0,0.0,1.0
Privacy aware learning,"Duchi J.C., Jordan M.I., Wainwright M.J.",Journal of the ACM,2014.0,"We study statistical risk minimization problems under a privacy model in which the data is kept confidential even from the learner. In this local privacy framework, we establish sharp upper and lower bounds on the convergence rates of statistical estimation procedures. As a consequence, we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility, as measured by convergence rate, of any statistical estimator or learning procedure. © 2014 ACM.",10.1145/2666468,Differential privacy; Lower bounds; Machine learning; Minimax convergence rates; Saddle points,58.0,0.0,1.0
A hybrid meta-learning architecture for multi-objective optimization of SVM parameters,"Miranda P.B.C., Prudêncio R.B.C., de Carvalho A.P.L.F., Soares C.",Neurocomputing,2014.0,"Support Vector Machines (SVMs) have achieved a considerable attention due to their theoretical foundations and good empirical performance when compared to other learning algorithms in different applications. However, the SVM performance strongly depends on the adequate calibration of its parameters. In this work we proposed a hybrid multi-objective architecture which combines meta-learning (ML) with multi-objective particle swarm optimization algorithms for the SVM parameter selection problem. Given an input problem, the proposed architecture uses a ML technique to suggest an initial Pareto front of SVM configurations based on previous similar learning problems; the suggested Pareto front is then refined by a multi-objective optimization algorithm. In this combination, solutions provided by ML are possibly located in good regions in the search space. Hence, using a reduced number of successful candidates, the search process would converge faster and be less expensive. In the performed experiments, the proposed solution was compared to traditional multi-objective algorithms with random initialization, obtaining Pareto fronts with higher quality on a set of 100 classification problems. © 2014 Elsevier B.V.",10.1016/j.neucom.2014.06.026,Meta-learning; Multi-objective optimization; Parameter selection; Particles swarm optimization; Support vector machines,29.0,0.0,1.0
A flexible cluster-oriented alternative clustering algorithm for choosing from the Pareto front of solutions,"Truong D.T., Battiti R.",Machine Learning,2015.0,"Supervised alternative clustering is the problem of finding a set of clusterings which are of high quality and different from a given negative clustering. The task is therefore a clear multi-objective optimization problem. Optimizing two conflicting objectives at the same time requires dealing with trade-offs. Most approaches in the literature optimize these objectives sequentially (one objective after another one) or indirectly (by some heuristic combination of the objectives). Solving a multi-objective optimization problem in these ways can result in solutions which are dominated, and not Pareto-optimal. We develop a direct algorithm, called COGNAC, which fully acknowledges the multiple objectives, optimizes them directly and simultaneously, and produces solutions approximating the Pareto front. COGNAC performs the recombination operator at the cluster level instead of at the object level, as in the traditional genetic algorithms. It can accept arbitrary clustering quality and dissimilarity objectives and provides solutions dominating those obtained by other state-of-the-art algorithms. Based on COGNAC, we propose another algorithm called SGAC for the sequential generation of alternative clusterings where each newly found alternative clustering is guaranteed to be different from all previous ones. The experimental results on widely used benchmarks demonstrate the advantages of our approach. © 2013, The Author(s).",10.1007/s10994-013-5350-y,Alternative clustering; Cluster-oriented recombination; Genetic algorithms; Multi-objective optimization,4.0,0.0,1.0
Robust classifiers using imprecise probability models and importance of classes,"Utkin L.V., Zhuk Y.A.",International Journal on Artificial Intelligence Tools,2015.0,"A framework for constructing robust classification models is proposed in the paper. An assumption about importance of one of the classes in comparison with other classes is incorporated into the models. It often takes place in the real application, for example, in reliability, in medical diagnostic, etc. A main idea underlying the models is to consider a set of probability distributions on training examples produced by the imprecise probability models such as linear-vacuous mixture and constant odd-ratio contaminated models. Extreme points of the sets of probability distributions are a main tool for constructing the robust classifiers. It is shown that algorithms for computing optimal classification parameters are reduced to a finite number of weighted support vector machines with weights determined by the extreme points. Experimental results with synthetic and real data illustrate the proposed models. © 2015 World Scientific Publishing Company.",10.1142/S0218213015500086,classification; extreme points; imprecise probability model; Machine learning; minimax strategy; quadratic programming; support vector machine,,0.0,1.0
A new robust model of one-class classification by interval-valued training data using the triangular kernel,"Utkin L.V., Chekh A.I.",Neural Networks,2015.0,A robust one-class classification model as an extension of Campbell and Bennett's (C-B) novelty detection model on the case of interval-valued training data is proposed in the paper. It is shown that the dual optimization problem to a linear program in the C-B model has a nice property allowing to represent it as a set of simple linear programs. It is proposed also to replace the Gaussian kernel in the obtained linear support vector machines by the well-known triangular kernel which can be regarded as an approximation of the Gaussian kernel. This replacement allows us to get a finite set of simple linear optimization problems for dealing with interval-valued data. Numerical experiments with synthetic and real data illustrate performance of the proposed model. © 2015 Elsevier Ltd.,10.1016/j.neunet.2015.05.004,Extreme points; Interval-valued data; Kernel; Linear programming; Minimax strategy; Novelty detection; One-class classification; Support vector machine,10.0,0.0,1.0
Hybrid multi-objective optimization approach for neural network classification using local search,"Mane S., Sonawani S., Sakhare S.",Advances in Intelligent Systems and Computing,2016.0,"Classification is inherently multi-objective problem. It is one of the most important tasks of data mining to extract important patterns from large volume of data. Traditionally, either only one objective is considered or the multiple objectives are accumulated to one objective function. In the last decade, Pareto-based multi-objective optimization approach have gained increasing popularity due to the use of multi-objective optimization using evolutionary algorithms and population-based search methods. Multi-objective optimization approaches are more powerful than traditional single-objective methods as it addresses various topics of data mining such as classification, clustering, feature selection, ensemble learning, etc. This paper proposes improved approach of non-dominated sorting algorithm II (NSGA II) for classification using neural network model by augmenting with local search. It tries to enhance two conflicting objectives of classifier: Accuracy and mean squared error. NSGA II is improved by augmenting backpropagation as a local search method to deal with the disadvantage of genetic algorithm, i.e. slow convergence to best solutions. By using backpropagation we are able to speed up the convergence. This approach is applied in various classification problems obtained from UCI repository. The neural network modes obtained shows high accuracy and low mean squared error. © Springer Science+Business Media Singapore 2016.",10.1007/978-981-10-0419-3_21,Classificationm; Local search; Multi-objective optimization; Neural network; NSGA II; Pareto optimality,2.0,0.0,1.0
Multi-objective reinforcement learning through continuous pareto manifold approximation,"Parisi S., Pirotta M., Restelli M.",Journal of Artificial Intelligence Research,2016.0,"Many real-world control applications, from economics to robotics, are characterized by the presence of multiple conicting objectives. In these problems, the standard concept of optimality is replaced by Pareto{optimality and the goal is to find the Pareto frontier, a set of solutions representing different compromises among the objectives. Despite recent advances in multi{objective optimization, achieving an accurate representation of the Pareto frontier is still an important challenge. In this paper, we propose a reinforcement learning policy gradient approach to learn a continuous approximation of the Pareto frontier in multi{objective Markov Decision Problems (MOMDPs). Differently from previous policy gradient algorithms, where n optimization routines are executed to have n solutions, our approach performs a single gradient ascent run, generating at each step an improved continuous approximation of the Pareto frontier. The idea is to optimize the parameters of a function defining a manifold in the policy parameters space, so that the corresponding image in the objectives space gets as close as possible to the true Pareto frontier. Besides deriving how to compute and estimate such gradient, we will also discuss the non{trivial issue of defining a metric to assess the quality of the candidate Pareto frontiers. Finally, the properties of the proposed approach are empirically evaluated on two problems, a linear-quadratic Gaussian regulator and a water reservoir control task. © 2016 AI Access Foundation. All rights reserved.",10.1613/jair.4961,,15.0,0.0,1.0
Benchmarking the peformance of a machine learning classifier enabled multiobjective genetic algorithm on six standard test functions,"Zeliff K., Bennette W., Ferguson S.",Proceedings of the ASME Design Engineering Technical Conference,2017.0,"Previous work tested a multi-objective genetic algorithm that was integrated with a machine learning classifier to reduce the number of objective function calls. Four machine learning classifiers and a baseline ""No Classifier"" option were evaluated. Using a machine learning classifier to create a hybrid multiobjective genetic algorithm reduced objective function calls by 75-85% depending on the classifier used. This work expands the analysis of algorithm performance by considering six standard benchmark problems from the literature. The problems are designed to test the ability of the algorithm to identify the Pareto frontier and maintain population diversity. Results indicate a tradeoff between the objectives of Pareto frontier identification and solution diversity. The ""No Classifier"" baseline multiobjective genetic algorithm produces the frontier with the closest proximity to the true frontier while a classifier option provides the greatest diversity when the number of generations is fixed. However, there is a significant reduction in computational expense as the number of objective function calls required is significantly reduced, highlighting the advantage of this hybrid approach. © Copyright 2017 ASME.",10.1115/DETC2017-68332,,3.0,0.0,1.0
Multi-objective particle swarm optimization approach for cost-based feature selection in classification,"Zhang Y., Gong D.-W., Cheng J.",IEEE/ACM Transactions on Computational Biology and Bioinformatics,2017.0,"Feature selection is an important data-preprocessing technique in classification problems such as bioinformatics and signal processing. Generally, there are some situations where a user is interested in not only maximizing the classification performance but also minimizing the cost that may be associated with features. This kind of problem is called cost-based feature selection. However, most existing feature selection approaches treat this task as a single-objective optimization problem. This paper presents the first study of multi-objective particle swarm optimization (PSO) for cost-based feature selection problems. The task of this paper is to generate a Pareto front of nondominated solutions, that is, feature subsets, to meet different requirements of decision-makers in real-world applications. In order to enhance the search capability of the proposed algorithm, a probability-based encoding technology and an effective hybrid operator, together with the ideas of the crowding distance, the external archive, and the Pareto domination relationship, are applied to PSO. The proposed PSO-based multi-objective feature selection algorithm is compared with several multi-objective feature selection algorithms on five benchmark datasets. Experimental results show that the proposed algorithm can automatically evolve a set of nondominated solutions, and it is a highly competitive feature selection method for solving cost-based feature selection problems. © 2004-2012 IEEE.",10.1109/TCBB.2015.2476796,Cost; Feature selection; Multi-objective; Particle swarm optimization,231.0,0.0,1.0
From Extraction to Generation of Design Information -Paradigm Shift in Data Mining via Evolutionary Learning Classifier System,"Chiba K., Nakata M.",Procedia Computer Science,2017.0,"This paper aims at generating as well as extracting design strategies for a real world problem using an evolutionary learning classifier system. Data mining for a design optimization result as a virtual database specifies design information and discovers latent design knowledge. It is essential for decision making in real world problems. Although we employed several methods from classic statistics to artificial intelligence to obtain design information from optimization results, we may not cognize anything beyond a prepared database. In this study, we have applied an evolutionary learning classifier system as a data mining technique to a real world engineering problem. Consequently, not only it extracted known design information but also it successfully generated design strategies not to extract from the database. The generated design rules do not physically become innovative knowledge because the prepared dataset include Pareto solutions owing to complete exploration to the edge of the feasible region in the optimization. However, this problem is independent of the method, our evolutionary learning classifier system is a useful method for incomplete datasets. © 2017 The Authors. Published by Elsevier B.V.",10.1016/j.procs.2017.05.233,data mining; design information generation; evolutionary machine learning; knowledge discovery; learning classifier system; real-world application,,0.0,1.0
Morphological perceptrons: Geometry and training algorithms,"Charisopoulos V., Maragos P.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017.0,"Neural networks have traditionally relied on mostly linear models, such as the multiply-accumulate architecture of a linear perceptron that remains the dominant paradigm of neuronal computation. However, from a biological standpoint, neuron activity may as well involve inherently nonlinear and competitive operations. Mathematical morphology and minimax algebra provide the necessary background in the study of neural networks made up from these kinds of nonlinear units. This paper deals with such a model, called the morphological perceptron. We study some of its geometrical properties and introduce a training algorithm for binary classification. We point out the relationship between morphological classifiers and the recent field of tropical geometry, which enables us to obtain a precise bound on the number of linear regions of the maxout unit, a popular choice for deep neural networks introduced recently. Finally, we present some relevant numerical results. © Springer International Publishing AG 2017.",10.1007/978-3-319-57240-6_1,Machine learning; Mathematical morphology; Neural networks; Optimization; Tropical geometry,17.0,0.0,1.0
A pareto-based ensemble with feature and instance selection for learning from multi-class imbalanced datasets,"Fernández A., Carmona C.J., José Del Jesus M., Herrera F.",International Journal of Neural Systems,2017.0,"Imbalanced classification is related to those problems that have an uneven distribution among classes. In addition to the former, when instances are located into the overlapped areas, the correct modeling of the problem becomes harder. Current solutions for both issues are often focused on the binary case study, as multi-class datasets require an additional effort to be addressed. In this research, we overcome these problems by carrying out a combination between feature and instance selections. Feature selection will allow simplifying the overlapping areas easing the generation of rules to distinguish among the classes. Selection of instances from all classes will address the imbalance itself by finding the most appropriate class distribution for the learning task, as well as possibly removing noise and difficult borderline examples. For the sake of obtaining an optimal joint set of features and instances, we embedded the searching for both parameters in a Multi-Objective Evolutionary Algorithm, using the C4.5 decision tree as baseline classifier in this wrapper approach. The multi-objective scheme allows taking a double advantage: the search space becomes broader, and we may provide a set of different solutions in order to build an ensemble of classifiers. This proposal has been contrasted versus several state-of-the-art solutions on imbalanced classification showing excellent results in both binary and multi-class problems. © 2017 World Scientific Publishing Company.",10.1142/S0129065717500289,ensembles; feature selection; Imbalanced classification; instance selection; multi-class; multi-objective evolutionary algorithms; overlapping,36.0,0.0,1.0
A temporal difference method for multi-objective reinforcement learning,"Ruiz-Montiel M., Mandow L., Pérez-de-la-Cruz J.-L.",Neurocomputing,2017.0,"This work describes MPQ-learning, an algorithm that approximates the set of all deterministic non-dominated policies in multi-objective Markov decision problems, where rewards are vectors and each component stands for an objective to maximize. MPQ-learning generalizes directly the ideas of Q-learning to the multi-objective case. It can be applied to non-convex Pareto frontiers and finds both supported and unsupported solutions. We present the results of the application of MPQ-learning to some benchmark problems. The algorithm solves successfully these problems, so showing the feasibility of this approach. We also compare MPQ-learning to a standard linearization procedure that computes only supported solutions and show that in some cases MPQ-learning can be as effective as the scalarization method. © 2017 Elsevier B.V.",10.1016/j.neucom.2016.10.100,MOMDPs; Multi-objective optimization; Q-learning; Reinforcement learning,12.0,0.0,1.0
Manifold-based multi-objective policy search with sample reuse,"Parisi S., Pirotta M., Peters J.",Neurocomputing,2017.0,"Many real-world applications are characterized by multiple conflicting objectives. In such problems optimality is replaced by Pareto optimality and the goal is to find the Pareto frontier, a set of solutions representing different compromises among the objectives. Despite recent advances in multi-objective optimization, achieving an accurate representation of the Pareto frontier is still an important challenge. Building on recent advances in reinforcement learning and multi-objective policy search, we present two novel manifold-based algorithms to solve multi-objective Markov decision processes. These algorithms combine episodic exploration strategies and importance sampling to efficiently learn a manifold in the policy parameter space such that its image in the objective space accurately approximates the Pareto frontier. We show that episode-based approaches and importance sampling can lead to significantly better results in the context of multi-objective reinforcement learning. Evaluated on three multi-objective problems, our algorithms outperform state-of-the-art methods both in terms of quality of the learned Pareto frontier and sample efficiency. © 2017 Elsevier B.V.",10.1016/j.neucom.2016.11.094,Black-box optimization; Importance sampling; Multi-objective; Policy search; Reinforcement learning,14.0,0.0,1.0
Context-aware generative adversarial privacy,"Huang C., Kairouz P., Chen X., Sankar L., Rajagopal R.",Entropy,2017.0,"Preserving the utility of published datasets while simultaneously providing provable privacy guarantees is a well-known challenge. On the one hand, context-free privacy solutions, such as differential privacy, provide strong privacy guarantees, but often lead to a significant reduction in utility. On the other hand, context-aware privacy solutions, such as information theoretic privacy, achieve an improved privacy-utility tradeoff, but assume that the data holder has access to dataset statistics. We circumvent these limitations by introducing a novel context-aware privacy framework called generative adversarial privacy (GAP). GAP leverages recent advancements in generative adversarial networks (GANs) to allow the data holder to learn privatization schemes from the dataset itself. Under GAP, learning the privacy mechanism is formulated as a constrained minimax game between two players: a privatizer that sanitizes the dataset in a way that limits the risk of inference attacks on the individuals' private variables, and an adversary that tries to infer the private variables from the sanitized dataset. To evaluate GAP's performance, we investigate two simple (yet canonical) statistical dataset models: (a) the binary data model; and (b) the binary Gaussian mixture model. For both models, we derive game-theoretically optimal minimax privacy mechanisms, and show that the privacy mechanisms learned from data (in a generative adversarial fashion) match the theoretically optimal ones. This demonstrates that our framework can be easily applied in practice, even in the absence of dataset statistics. © 2017 by the authors.",10.3390/e19120656,Adversarial network; Differential privacy; Error probability games; Generative adversarial networks; Generative adversarial privacy; Information theoretic privacy; Machine learning; Mutual information privacy; Privatizer network; Statistical data privacy,50.0,0.0,1.0
A comparative study on large scale kernelized support vector machines,"Horn D., Demircioğlu A., Bischl B., Glasmachers T., Weihs C.",Advances in Data Analysis and Classification,2018.0,"Kernelized support vector machines (SVMs) belong to the most widely used classification methods. However, in contrast to linear SVMs, the computation time required to train such a machine becomes a bottleneck when facing large data sets. In order to mitigate this shortcoming of kernel SVMs,many approximate training algorithms were developed. While most of these methods claim to be much faster than the state-of-the-art solver LIBSVM, a thorough comparative study is missing.We aim to fill this gap.We choose several well-known approximate SVM solvers and compare their performance on a number of large benchmark data sets. Our focus is to analyze the trade-off between prediction error and runtime for different learning and accuracy parameter settings. This includes simple subsampling of the data, the poor-man’s approach to handling large scale problems. We employ model-based multi-objective optimization, which allows us to tune the parameters of learning machine and solver over the full range of accuracy/runtime trade-offs. We analyze (differences between) solvers by studying and comparing the Pareto fronts formed by the two objectives classification error and training time. Unsurprisingly, givenmore runtimemost solvers are able to find more accurate solutions, i.e., achieve a higher prediction accuracy. It turns out that LIBSVM with subsampling of the data is a strong baseline. Some solvers systematically outperform others, which allows us to give concrete recommendations of when to use which solver. © Springer-Verlag Berlin Heidelberg 2016.",10.1007/s11634-016-0265-7,Large scale; Machine learning; Multi-objective optimization; Nonlinear SVM; Parameter tuning; Supervised learning; Support vector machine,7.0,0.0,1.0
Multi-task Learning by Pareto Optimality,"Dyankov D., Riccio S.D., Di Fatta G., Nicosia G.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019.0,"Deep Neural Networks (DNNs) are often criticized because they lack the ability to learn more than one task at a time: Multitask Learning is an emerging research area whose aim is to overcome this issue. In this work, we introduce the Pareto Multitask Learning framework as a tool that can show how effectively a DNN is learning a shared representation common to a set of tasks. We also experimentally show that it is possible to extend the optimization process so that a single DNN simultaneously learns how to master two or more Atari games: using a single weight parameter vector, our network is able to obtain sub-optimal results for up to four games. © Springer Nature Switzerland AG 2019.",10.1007/978-3-030-37599-7_50,Atari 2600 Games; Deep artificial neural networks; Deep neuroevolution; Evolution Strategy; Hypervolume; Kullback-Leibler Divergence; Multitask learning; Neural and evolutionary computing,1.0,0.0,1.0
On Selection of Optimal Classifiers,"Rado O., Neagu D.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019.0,"The current advances of computational power and storage allow more models to be created and stored from significant data resources. This progress opens the opportunity to re-cycle and re-use such models in similar exercises. The evaluation of the machine learning algorithms and selection of an appropriate classifier from an existing collection of classifiers are still challenging tasks. In most cases, the decision of selecting the classifier is left to the user. When the selection is not performed accurately, the outcomes can have unexpected performance results. Classification algorithms aim to optimise some of the distinct objectives such as minimising misclassification error, maximising the accuracy, or maximising the model quality. The right choice for each of these objectives is critical to the quality of the classifier selected. This work aims to study the use of a multi-objective method that can be undertaken to find a set of suitable classifiers for a problem at hand. In this study, we applied seven classifiers on mental health data sets for classifier selection in terms of correctness and reliability. The experimental results suggest that this approach is useful in finding the best trade-off among the objectives of selecting a suitable classifier framework. © 2019, Springer Nature Switzerland AG.",10.1007/978-3-030-34885-4_42,Classification algorithms; Optimization; Pareto set,1.0,0.0,1.0
Adversarial machine learning with double oracle,Wang K.,IJCAI International Joint Conference on Artificial Intelligence,2019.0,"We aim to improve the general adversarial machine learning solution by introducing the double oracle idea from game theory, which is commonly used to solve a sequential zero-sum game, where the adversarial machine learning problem can be formulated as a zero-sum minimax problem between learner and attacker. © 2019 International Joint Conferences on Artificial Intelligence. All rights reserved.",10.24963/ijcai.2019/925,,1.0,0.0,1.0
Model-Based Multi-objective Reinforcement Learning with Unknown Weights,"Yamaguchi T., Nagahama S., Ichikawa Y., Takadama K.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019.0,"This paper describes solving multi-objective reinforcement learning problems where there are multiple conflicting objectives with unknown weights. Reinforcement learning (RL) is a popular algorithm for automatically solving sequential decision problems and most of them are focused on single-objective settings to decide a single solution. In multi-objective reinforcement learning (MORL), the reward function emits a reward vector instead of a scalar reward. A scalarization function with a vector of n weights (weight vector) is a commonly used to decide a single solution. The simple scalarization function is linear scalarization such as weighted sum. The main problem of previous MORL methods is a huge learning cost required to collect all Pareto optimal policies. Hence, it is hard to learn the high dimensional Pareto optimal policies. To solve this, this paper proposes the novel model-based MORL method by reward occurrence probability (ROP) with unknown weights. There are two main features. The first feature is that the average reward of a policy is defined by inner product of the ROP vector and the weight vector. The second feature is that it learns ROP in each policy instead of Q-values. Pareto optimal deterministic policies directly form the vertices of a convex hull in the ROP vector space. Therefore, Pareto optimal policies are calculated independently with weights and just once. The experimental results show that our proposed method collected all optimal policies under four dimensional Pareto optimal policies, and it takes a small computation time though previous MORL methods learn at most two or three dimensions. © 2019, Springer Nature Switzerland AG.",10.1007/978-3-030-22649-7_25,Average reward; Model-based; Multi-objective reinforcement learning; Reward occurrence probability; Reward vector,1.0,0.0,1.0
Multilayer Perceptron: NSGA II for a New Multi-objective Learning Method for Training and Model Complexity,"Senhaji K., Ramchoun H., Ettaouil M.",Advances in Intelligent Systems and Computing,2019.0,"The multi-layer perceptron has proved its efficiencies in several fields as pattern and voice recognition. Unfortunately, the classical training for MLP suffers from a poor generalization. In this respect, we have proposed a new multi-objective training model with constraints, satisfies two objectives. The first one is the learning objective: minimizing the perceptron error and the second is the complexity objective: optimizing number of weights and neurons. The proposed model will provide a balance between the multi-layer perceptron learning and the complexity to get a good generalization. Our model has been solved using an evolutionary approach called the Non-Dominated Sorting Genetic Algorithm (NSGA II). This approach has led to a good representation of the Pareto set for the MLP network, from which an improved generalization performance model is selected. © 2019, Springer International Publishing AG, part of Springer Nature.",10.1007/978-3-319-91337-7_15,Multi-objective training; Multilayer perceptron; Non-dominated Sorting Genetic Algorithm II (NSGA II); Non-linear optimization; Pareto front; Supervised learning,1.0,0.0,1.0
A targeted Bayesian network learning for classification,"Gruber A., Ben-Gal I.",Quality Technology and Quantitative Management,2019.0,"A targeted Bayesian network learning (TBNL) method is proposed to account for a classification objective during the learning stage of the network model. The TBNL approximates the expected conditional probability distribution of the class variable. It effectively manages the trade-off between the classification accuracy and the model complexity by using a discriminative approach, constrained by information theory measurements. The proposed approach also provides a mechanism for maximizing the accuracy via a Pareto frontier over a complexity–accuracy plane, in cases of missing data in the data-sets. A comparative study over a set of classification problems shows the competitiveness of the TBNL mainly with respect to other graphical classifiers. © 2017, © 2017 International Chinese Association of Quantitative Management.",10.1080/16843703.2017.1395109,AI; Bayesian classifiers; complexity–accuracy trade-off; information theory; machine learning; target-oriented learning,2.0,0.0,1.0
On the global optima of kernelized adversarial representation learning,"Sadeghi B., Yu R., Boddeti V.",Proceedings of the IEEE International Conference on Computer Vision,2019.0,"Adversarial representation learning is a promising paradigm for obtaining data representations that are invariant to certain sensitive attributes while retaining the information necessary for predicting target attributes. Existing approaches solve this problem through iterative adversarial minimax optimization and lack theoretical guarantees. In this paper, we first study the ''linear' form of this problem i.e., the setting where all the players are linear functions. We show that the resulting optimization problem is both non-convex and non-differentiable. We obtain an exact closed-form expression for its global optima through spectral learning and provide performance guarantees in terms of analytical bounds on the achievable utility and invariance. We then extend this solution and analysis to non-linear functions through kernel representation. Numerical experiments on UCI, Extended Yale B and CIFAR-100 datasets indicate that, (a) practically, our solution is ideal for ''imparting' provable invariance to any biased pre-trained data representation, and (b) the global optima of the ''kernel' form can provide a comparable trade-off between utility and invariance in comparison to iterative minimax optimization of existing deep neural network based approaches, but with provable guarantees. © 2019 IEEE.",10.1109/ICCV.2019.00806,,10.0,0.0,1.0
Online Anomaly Detection in Multivariate Settings,"Mozaffari M., Yilmaz Y.","IEEE International Workshop on Machine Learning for Signal Processing, MLSP",2019.0,This paper considers the real-time and nonparametric detection of anomalies in high-dimensional systems. The goal is to detect anomalies quickly and accurately such that the appropriate countermeasures could be taken before any possible harm is caused by the anomalous event. We propose a k NN-based sequential anomaly detection method in both semi-supervised and supervised settings. We prove that the proposed method is asymptotically optimum in the minimax sense under certain conditions in terms of minimizing the average detection delay for a given false alarm constraint. The proposed method is shown to be capable of multivariate anomaly detection and also scalable to high-dimensional datasets. We further propose an online learning scheme that combines the desirable properties of our semi-supervised and supervised methods. © 2019 IEEE.,10.1109/MLSP.2019.8918893,anomaly detection; nonparametric methods; online learning,8.0,0.0,1.0
Multi-objective artificial immune algorithm for fuzzy clustering based on multiple kernels,"Shang R., Zhang W., Li F., Jiao L., Stolkin R.",Swarm and Evolutionary Computation,2019.0,"This paper presents a multi-objective artificial immune algorithm for fuzzy clustering based on multiple kernels (MAFC). MAFC extends the classical Fuzzy C-Means (FCM) algorithm and improves some of its important limitations, such as vulnerability to local optima convergence, which can lead to poor clustering quality. MAFC unifies multi-kernel learning and multi-objective optimization in a joint clustering framework, which preserves the geometric information of the dataset. The multi-kernel method maps data from the feature space to kernel space by using kernel functions. Additionally, the introduction of multi-objective optimization helps to optimize between-cluster separation and within-cluster compactness simultaneously via two different clustering validity criteria. These properties help the proposed algorithm to avoid becoming stuck at local optima. Furthermore, this paper utilizes an artificial immune algorithm to address the multi-objective clustering problem and acquire a Pareto optimal solution set. The solution set is obtained through the process of antibody population initialization, clone proliferation, non-uniform mutation and uniformity maintaining strategy, which avoids the problems of degradation and prematurity which can occur with conventional genetic algorithms. Finally, we choose the best solution from the Pareto optimal solution set. We use a semi-supervised method to achieve the final clustering results. We compare our method against state-of-the-art methods from the literature by performing experiments with both UCI datasets and face datasets. The results suggest that MAFC is significantly more efficient for clustering and has a wider scope of application. © 2019 Elsevier B.V.",10.1016/j.swevo.2019.01.001,Artificial immune algorithm; Fuzzy c-means (FCM); Multi-objective optimization; Multiple kernel learning,14.0,0.0,1.0
Automatic clustering by multi-objective genetic algorithm with numeric and categorical features,"Dutta D., Sil J., Dutta P.",Expert Systems with Applications,2019.0,"Many clustering algorithms categorized as K-clustering algorithm require the user to predict the number of clusters (K) to do clustering. Due to lack of domain knowledge an accurate value of K is difficult to predict. The problem becomes critical when the dimensionality of data points is large; clusters differ widely in shape, size, and density; and when clusters are overlapping in nature. Determining the suitable K is an optimization problem. Automatic clustering algorithms can discover the optimal K. This paper presents an automatic clustering algorithm which is superior to K-clustering algorithm as it can discover an optimal value of K. Iterative hill-climbing algorithms like K-Means work on a single solution and converge to a local optimum solution. Here, Genetic Algorithms (GAs) find out near global optimum solutions, i.e. optimal K as well as the optimal cluster centroids. Single-objective clustering algorithms are adequate for efficiently grouping linearly separable clusters. For non-linearly separable clusters they are not so good. So for grouping non-linearly separable clusters, we apply Multi-Objective Genetic Algorithm (MOGA) by minimizing the intra-cluster distance and maximizing inter-cluster distance. Many existing MOGA based clustering algorithms are suitable for either numeric or categorical features. This paper pioneered employing MOGA for automatic clustering with mixed types of features. Statistical testing on experimental results on real-life benchmark data sets from the University of California at Irvine (UCI) machine learning repository proves the superiority of the proposed algorithm. © 2019 Elsevier Ltd",10.1016/j.eswa.2019.06.056,Automatic clustering; Multi-Objective Genetic Algorithm (MOGA); Pareto approach; Statistical test,19.0,0.0,1.0
Relaxed regularization for linear inverse problems,"Luiken N., Van Leeuwen T.",SIAM Journal on Scientific Computing,2020.0,"We consider regularized least-squares problems of the form min {equation presented}. Recently, Zheng et al. [IEEE Access, 7 (2019), pp. 1404-1423], proposed an algorithm called Sparse Relaxed Regularized Regression (SR3) that employs a splitting strategy by introducing an auxiliary variable y and solves min {equation presented}. By minimizing out the variable x, we obtain an equivalent optimization problem min {equation presented}. In our work, we view the SR3 method as a way to approximately solve the regularized problem. We analyze the conditioning of the relaxed problem in general and give an expression for the SVD of Fκ as a function of κ. Furthermore, we relate the Pareto curve of the original problem to the relaxed problem, and we quantify the error incurred by relaxation in terms of κ. Finally, we propose an efficient iterative method for solving the relaxed problem with inexact inner iterations. Numerical examples illustrate the approach. Copyright © by SIAM.",10.1137/20M1348091,Inverse problems; Machine learning; Optimization; Regularization; Sparsity; Total variation,,0.0,1.0
DBQ: A Differentiable Branch Quantizer for Lightweight Deep Neural Networks,"Dbouk H., Sanghvi H., Mehendale M., Shanbhag N.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020.0,"Deep neural networks have achieved state-of-the art performance on various computer vision tasks. However, their deployment on resource-constrained devices has been hindered due to their high computational and storage complexity. While various complexity reduction techniques, such as lightweight network architecture design and parameter quantization, have been successful in reducing the cost of implementing these networks, these methods have often been considered orthogonal. In reality, existing quantization techniques fail to replicate their success on lightweight architectures such as MobileNet. To this end, we present a novel fully differentiable non-uniform quantizer that can be seamlessly mapped onto efficient ternary-based dot product engines. We conduct comprehensive experiments on CIFAR-10, ImageNet, and Visual Wake Words datasets. The proposed quantizer (DBQ) successfully tackles the daunting task of aggressively quantizing lightweight networks such as MobileNetV1, MobileNetV2, and ShuffleNetV2. DBQ achieves state-of-the art results with minimal training overhead and provides the best (pareto-optimal) accuracy-complexity trade-off. © 2020, Springer Nature Switzerland AG.",10.1007/978-3-030-58583-9_6,Deep learning; Low-complexity neural networks; Quantization,,0.0,1.0
Improved Adversarial Training via Learned Optimizer,"Xiong Y., Hsieh C.-J.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020.0,"Adversarial attack has recently become a tremendous threat to deep learning models. To improve the robustness of machine learning models, adversarial training, formulated as a minimax optimization problem, has been recognized as one of the most effective defense mechanisms. However, the non-convex and non-concave property poses a great challenge to the minimax training. In this paper, we empirically demonstrate that the commonly used PGD attack may not be optimal for inner maximization, and improved inner optimizer can lead to a more robust model. Then we leverage a learning-to-learn (L2L) framework to train an optimizer with recurrent neural networks, providing update directions and steps adaptively for the inner problem. By co-training optimizer’s parameters and model’s weights, the proposed framework consistently improves over PGD-based adversarial training and TRADES. © 2020, Springer Nature Switzerland AG.",10.1007/978-3-030-58598-3_6,Adversarial training; Learning to learn; Optimization,1.0,0.0,1.0
Feature Selection Using PSO: A Multi Objective Approach,"Vashishtha J., Puri V.H., Mukesh",Communications in Computer and Information Science,2020.0,"Feature selection is a pre-processing technique in which a subset or a small number of features, which are relevant and non-redundant, are selected for better classification performance. Multi-objective optimization is applied in the fields where finest decisions need to be taken in presence of trade-offs between two or more differing objectives. Therefore, feature selection is considered as a multi-objective problem with conflicting measures like classification error rate and feature reduction rate. The existing algorithms, Non-dominated Sorting based particle swarm optimization for Feature Selection (NSPSOFS) and Crowding Mutation Dominance based particle swarm optimization for Feature Selection (CMDPSOFS) are the two multi-objective PSO algorithms for feature selection. This work presents the enhanced form of NSPSOFS and CMDPSOFS. A novel selection mechanism for gbest is incorporated and hybrid mutation is also added to the algorithms in order to generate a better pareto optimal front of non-dominated solutions. The experimental results show that the proposed algorithm generates non-dominated solutions and produce better result than existing algorithms. © 2020, Springer Nature Singapore Pte Ltd.",10.1007/978-981-15-6318-8_10,Feature selection; Multi-objective optimization; PSO,,0.0,1.0
Ensemble Learning via Multimodal Multiobjective Differential Evolution and Feature Selection,"Wang J., Wang B., Liang J., Yu K., Yue C., Ren X.",Communications in Computer and Information Science,2020.0,"Ensemble learning is an important element in machine learning. However, two essential tasks, including training base classifiers and finding a suitable ensemble balance for the diversity and accuracy of these base classifiers, are need to be achieved. In this paper, a novel ensemble method, which utilizes a multimodal multiobjective differential evolution (MMODE) algorithm to select feature subsets and optimize base classifiers parameters, is proposed. Moreover, three methods including minimum error ensemble, all Pareto sets ensemble, and error reduction ensemble are employed to construct ensemble classifiers for executing classification tasks. Experimental results on several benchmark classification databases evidence that the proposed algorithm is valid. © 2020, Springer Nature Singapore Pte Ltd.",10.1007/978-981-15-3425-6_34,Classifier parameter; Ensemble learning; Feature selection; Multimodal multiobjective optimization,,0.0,1.0
Efficient Multiagent Policy Optimization Based on Weighted Estimators in Stochastic Cooperative Environments,"Zheng Y., Hao J.-Y., Zhang Z.-Z., Meng Z.-P., Hao X.-T.",Journal of Computer Science and Technology,2020.0,"Multiagent deep reinforcement learning (MA-DRL) has received increasingly wide attention. Most of the existing MA-DRL algorithms, however, are still inefficient when faced with the non-stationarity due to agents changing behavior consistently in stochastic environments. This paper extends the weighted double estimator to multiagent domains and proposes an MA-DRL framework, named Weighted Double Deep Q-Network (WDDQN). By leveraging the weighted double estimator and the deep neural network, WDDQN can not only reduce the bias effectively but also handle scenarios with raw visual inputs. To achieve efficient cooperation in multiagent domains, we introduce a lenient reward network and scheduled replay strategy. Empirical results show that WDDQN outperforms an existing DRL algorithm (double DQN) and an MA-DRL algorithm (lenient Q-learning) regarding the averaged reward and the convergence speed and is more likely to converge to the Pareto-optimal Nash equilibrium in stochastic cooperative environments. © 2020, Institute of Computing Technology, Chinese Academy of Sciences.",10.1007/s11390-020-9967-6,cooperative Markov game; deep reinforcement learning; lenient reinforcement learning; multiagent system; weighted double estimator,6.0,0.0,1.0
Sparse principal component analysis via axis-aligned random projections,"Gataric M., Wang T., Samworth R.J.",Journal of the Royal Statistical Society. Series B: Statistical Methodology,2020.0,"We introduce a new method for sparse principal component analysis, based on the aggregation of eigenvector information from carefully selected axis-aligned random projections of the sample covariance matrix. Unlike most alternative approaches, our algorithm is non-iterative, so it is not vulnerable to a bad choice of initialization. We provide theoretical guarantees under which our principal subspace estimator can attain the minimax optimal rate of convergence in polynomial time. In addition, our theory provides a more refined understanding of the statistical and computational trade-off in the problem of sparse principal component estimation, revealing a subtle interplay between the effective sample size and the number of random projections that are required to achieve the minimax optimal rate. Numerical studies provide further insight into the procedure and confirm its highly competitive finite sample performance. © 2020 The Authors Journal of the Royal Statistical Society: Series B (Statistical Methodology) Published by John Wiley & Sons Ltd on behalf of the Royal Statistical Society.",10.1111/rssb.12360,Dimensionality reduction; Eigenspace estimation; Ensemble learning; Sketching; Statistical and computational trade-offs,4.0,0.0,1.0
Imparting fairness to pre-trained biased representations,"Sadeghi B., Boddeti V.N.",IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops,2020.0,"Adversarial representation learning is a promising paradigm for obtaining data representations that are invariant to certain sensitive attributes while retaining the information necessary for predicting target attributes. Existing approaches solve this problem through iterative adversarial minimax optimization and lack theoretical guarantees. In this paper, we first study the ""linear"" form of this problem i.e., the setting where all the players are linear functions. We show that the resulting optimization problem is both non-convex and non-differentiable. We obtain an exact closed-form expression for its global optima through spectral learning. We then extend this solution and analysis to non-linear functions through kernel representation. Numerical experiments on UCI and CIFAR-100 datasets indicate that, (a) practically, our solution is ideal for ""imparting"" provable invariance to any biased pre-trained data representation, and (b) empirically, the trade-off between utility and invariance provided by our solution is comparable to iterative minimax optimization of existing deep neural network based approaches.Code is available at Human Analysis Lab. © 2020 IEEE.",10.1109/CVPRW50498.2020.00016,,3.0,0.0,1.0
GeneCAI: <u>gene</u>tic evolution for acquiring <u>c</u>ompact <u>AI</u>,"Javaheripi M., Samragh M., Javidi T., Koushanfar F.",GECCO 2020 - Proceedings of the 2020 Genetic and Evolutionary Computation Conference,2020.0,"In the contemporary big data realm, Deep Neural Networks (DNNs) are evolving towards more complex architectures to achieve higher inference accuracy. Model compression techniques can be leveraged to efficiently deploy these compute-intensive architectures on resource-limited mobile devices. Such methods comprise various hyperparameters that require per-layer customization to ensure high accuracy. Choosing the hyperparameters is cumbersome as the pertinent search space grows exponentially with model layers. This paper introduces GeneCAI, a novel optimization method that automatically learns how to tune per-layer compression hyperparameters. We devise a bijective translation scheme that encodes compressed DNNs to the genotype space. Each genotype's optimality is measured using a multi-objective score based on the accuracy and number of floating-point operations. We develop customized genetic operations to iteratively evolve the non-dominated solutions towards the optimal Pareto front, thus, capturing the optimal trade-off between model accuracy and complexity. GeneCAI optimization method is highly scalable and can achieve a near-linear performance boost on distributed multi-GPU platforms. Our extensive evaluations demonstrate that GeneCAI outperforms existing rule-based and reinforcement learning methods in DNN compression by finding models that lie on a better accuracy/complexity Pareto curve. © 2020 ACM.",10.1145/3377930.3390226,Computer aided/automated design; Deep learning; Genetic algorithms; Multi-objective optimization; Parallel optimization,3.0,0.0,1.0
An evolutionary approach for constructing multi-stage classifiers,"Hamilton N.H., Fulp E.W.",GECCO 2020 Companion - Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion,2020.0,"Multi-stage classification is a supervised learning approach that distributes a set of features, each with an associated importance and cost of generation, across a series of classifiers (stages). Inputs are processed in a pipeline fashion through the stages, each of which utilizes only a subset of the complete feature set, until a confident classification decision can be made or until all features and stages have been exhausted. This design benefits from processing inputs in parallel and ensures that labels are assigned using only the necessary features, but the number and composition of stages used by the model can have significant impact on overall performance. Unfortunately, identifying these critical design aspects becomes more difficult as the number of features and possible stages increases, often making brute-force search or human intuition impractical. This paper introduces a novel evolutionary approach for discovering multi-stage configurations that provide high classification performance and fast processing times. Using this approach, multistage classifier configurations are modeled as chromosomes, and a series of selection, recombination, and mutation operations are iteratively performed to find better configurations. Since the problem has multiple objectives, a Pareto-based fitness measure is developed to rank chromosomes, where better chromosomes have high accuracy, high conclusiveness, and fast processing time. Experimental results indicate this approach is able to consistently find accurate and fast multi-stage classifier configurations under various conditions, including an increasing number of features and different feature synthesis time distributions. © 2020 ACM.",10.1145/3377929.3398088,,,0.0,1.0
A two-stage multi-objective deep reinforcement learning framework,"Chen D., Wang Y., Gao W.",Frontiers in Artificial Intelligence and Applications,2020.0,"In multi-objective decision making problems, multi-objective reinforcement learning (MORL) algorithms aim to approximate the Pareto frontier uniformly. A naive approach is to learn multiple policies by repeatedly running a single-objective reinforcement learning (RL) algorithm on scalarized rewards. The scalarization methods denote the preferences of objectives, which are different in each run. However, in this way, the model representation and computation are redundant. Furthermore, uniform preferences can not guarantee a uniformly approximated Pareto frontier. To address these problems and leverage the expressive power of deep neural networks, we propose a two-stage MORL framework integrating a multi-policy deep RL algorithm and an evolution strategy algorithm. Firstly, a multi-policy soft actor-critic algorithm is proposed to collaboratively learn multiple policies which are assigned with different scalarization weights. The lower layers of all policy networks are shared. The first-stage learning can be regarded as representation learning. Secondly, the multi-objective covariance matrix adaptation evolution strategy (MO-CMA-ES) is applied to fine-tune policy-independent parameters to approach a dense and uniform estimation of the Pareto frontier. Experimental results on two benchmarks (Deep Sea Treasure and Adaptive Streaming) show the superiority of the proposed method. © 2020 The authors and IOS Press.",10.3233/FAIA200202,,1.0,0.0,1.0
Regularization methods based on the Lq-likelihood for linear models with heavy-tailed errors,Hirose Y.,Entropy,2020.0,"We propose regularization methods for linear models based on the Lq-likelihood, which is a generalization of the log-likelihood using a power function. Regularization methods are popular for the estimation in the normal linear model. However, heavy-tailed errors are also important in statistics and machine learning. We assume q-normal distributions as the errors in linear models. A q-normal distribution is heavy-tailed, which is defined using a power function, not the exponential function. We find that the proposed methods for linear models with q-normal errors coincide with the ordinary regularization methods that are applied to the normal linear model. The proposed methods can be computed using existing packages because they are penalized least squares methods. We examine the proposed methods using numerical experiments, showing that the methods perform well, even when the error is heavy-tailed. The numerical experiments also illustrate that our methods work well in model selection and generalization, especially when the error is slightly heavy-tailed. © 2020 by the author.",10.3390/E22091036,Least absolute shrinkage and selection operator (LASSO); Minimax concave penalty (MCP); Power function; Q-normal distribution; Smoothly clipped absolute deviation (SCAD); Sparse estimation,,0.0,1.0
Combining a gradient-based method and an evolution strategy for multi-objective reinforcement learning,"Chen D., Wang Y., Gao W.",Applied Intelligence,2020.0,"Multi-objective reinforcement learning (MORL) algorithms aim to approximate the Pareto frontier uniformly in multi-objective decision making problems. In the scenario of deep reinforcement learning (RL), gradient-based methods are often adopted to learn deep policies/value functions due to the fast convergence speed, while pure gradient-based methods can not guarantee a uniformly approximated Pareto frontier. On the other side, evolution strategies straightly manipulate in the solution space to achieve a well-distributed Pareto frontier, but applying evolution strategies to optimize deep networks is still a challenging topic. To leverage the advantages of both kinds of methods, we propose a two-stage MORL framework combining a gradient-based method and an evolution strategy. First, an efficient multi-policy soft actor-critic algorithm is proposed to learn multiple policies collaboratively. The lower layers of all policy networks are shared. The first-stage learning can be regarded as representation learning. Secondly, the multi-objective covariance matrix adaptation evolution strategy (MO-CMA-ES) is applied to fine-tune policy-independent parameters to approach a dense and uniform estimation of the Pareto frontier. Experimental results on three benchmarks (Deep Sea Treasure, Adaptive Streaming, and Super Mario Bros) show the superiority of the proposed method. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",10.1007/s10489-020-01702-7,Multi-objective reinforcement learning; Multi-policy reinforcement learning; Pareto frontier; Sampling efficiency,2.0,0.0,1.0
Learning adversarial attack policies through multi-objective reinforcement learning,"García J., Majadas R., Fernández F.",Engineering Applications of Artificial Intelligence,2020.0,"Deep Reinforcement Learning has shown promising results in learning policies for complex sequential decision-making tasks. However, different adversarial attack strategies have revealed the weakness of these policies to perturbations to their observations. Most of these attacks have been built on existing adversarial example crafting techniques used to fool classifiers, where an adversarial attack is considered a success if it makes the classifier outputs any wrong class. The major drawback of these approaches when applied to decision-making tasks is that they are blind for long-term goals. In contrast, this paper suggests that it is more appropriate to view the attack process as a sequential optimization problem, with the aim of learning a sequence of attacks, where the attacker must consider the long-term effects of each attack. In this paper, we propose that such an attack policy must be learned with two objectives in view. On the one hand, the attack must pursue the maximum performance loss of the attacked policy. On the other hand, it also should minimize the cost of the attacks. Therefore, in this paper we propose a novel modelization of the process of learning an attack policy as a Multi-objective Markov Decision Process with two objectives: maximizing the performance loss of the attacked policy and minimizing the cost of the attacks. We also reveal the conflicting nature of these two objectives and use a Multi-objective Reinforcement Learning algorithm to draw the Pareto fronts for four well-known tasks: the GridWorld, the Cartpole, the Mountain car and the Breakout. © 2020 Elsevier Ltd",10.1016/j.engappai.2020.104021,Adversarial reinforcement learning; Multi-objective reinforcement learning,6.0,0.0,1.0
A multi-objective deep reinforcement learning framework,"Nguyen T.T., Nguyen N.D., Vamplew P., Nahavandi S., Dazeley R., Lim C.P.",Engineering Applications of Artificial Intelligence,2020.0,"This paper introduces a new scalable multi-objective deep reinforcement learning (MODRL) framework based on deep Q-networks. We develop a high-performance MODRL framework that supports both single-policy and multi-policy strategies, as well as both linear and non-linear approaches to action selection. The experimental results on two benchmark problems (two-objective deep sea treasure environment and three-objective Mountain Car problem) indicate that the proposed framework is able to find the Pareto-optimal solutions effectively. The proposed framework is generic and highly modularized, which allows the integration of different deep reinforcement learning algorithms in different complex problem domains. This therefore overcomes many disadvantages involved with standard multi-objective reinforcement learning methods in the current literature. The proposed framework acts as a testbed platform that accelerates the development of MODRL for solving increasingly complicated multi-objective problems. © 2020 Elsevier Ltd",10.1016/j.engappai.2020.103915,Deep learning; Multi-objective; Multi-policy; Reinforcement learning; Single-policy,6.0,0.0,1.0
AutoML Technologies for the Identification of Sparse Models,"Liuliakov A., Hammer B.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021.0,"Automated machine learning (AutoML) technologies constitute promising tools to automatically infer model architecture, meta-parameters or processing pipelines for specific machine learning tasks given suitable training data. At present, the main objective of such technologies typically relies on the accuracy of the resulting model. Additional objectives such as sparsity can be integrated by pre-processing steps or according penalty terms in the objective function. Yet, sparsity and model accuracy are often contradictory goals, and optimum solutions form a Pareto front. Thereby, it is not guaranteed that solutions at different positions of the Pareto front share the same architectural choices, hence current AutoML technologies might yield sub-optimal results. In this contribution, we propose a novel method, based on the AutoML method TPOT, which enables an automated optimization of ML pipelines with sparse input features along the whole Pareto front. We demonstrate that, indeed, different architectures are found at different points of the Pareto front for benchmark examples from the domain of systems security. © 2021, Springer Nature Switzerland AG.",10.1007/978-3-030-91608-4_7,AutoML; Feature selection; TPOT,,0.0,1.0
Comparing the Forecast Performance of Advanced Statistical and Machine Learning Techniques Using Huge Big Data: Evidence from Monte Carlo Experiments,"Khan F., Urooj A., Khan S.A., Alsubie A., Almaspoor Z., Muhammadullah S.",Complexity,2021.0,"This research compares factor models based on principal component analysis (PCA) and partial least squares (PLS) with Autometrics, elastic smoothly clipped absolute deviation (E-SCAD), and minimax concave penalty (MCP) under different simulated schemes like multicollinearity, heteroscedasticity, and autocorrelation. The comparison is made with varying sample size and covariates. We found that in the presence of low and moderate multicollinearity, MCP often produces superior forecasts in contrast to small sample case, whereas E-SCAD remains better. In the case of high multicollinearity, the PLS-based factor model remained dominant, but asymptotically the prediction accuracy of E-SCAD significantly enhances compared to other methods. Under heteroscedasticity, MCP performs very well and most of the time beats the rival methods. In some circumstances under large samples, Autometrics provides a similar forecast as MCP. In the presence of low and moderate autocorrelation, MCP shows outstanding forecasting performance except for the small sample case, whereas E-SCAD produces a remarkable forecast. In the case of extreme autocorrelation, E-SCAD outperforms the rival techniques under both the small and medium samples, but further augmentation in sample size enables MCP forecast more accurate comparatively. To compare the predictive ability of all methods, we split the data into two halves (i.e., data over 1973-2007 as training data and data over 2008-2020 as testing data). Based on the root mean square error and mean absolute error, the PLS-based factor model outperforms the competitor models in terms of forecasting performance. © 2021 Faridoon Khan et al.",10.1155/2021/6117513,,1.0,0.0,1.0
Training-Free Multi-objective Evolutionary Neural Architecture Search via Neural Tangent Kernel and Number of Linear Regions,"Do T., Luong N.H.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021.0,"A newly introduced training-free neural architecture search (TE-NAS) framework suggests that candidate network architectures can be ranked via a combined metric of expressivity and trainability. Expressivity is measured by the number of linear regions in the input space that can be divided by a network. Trainability is assessed based on the condition number of the neural tangent kernel (NTK), which affects the convergence rate of training a network with gradient descent. These two measurements have been found to be correlated with network test accuracy. High-performance architectures can thus be searched for without incurring the intensive cost of network training as in a typical NAS run. In this paper, we suggest that TE-NAS can be incorporated with a multi-objective evolutionary algorithm (MOEA), in which expressivity and trainability are kept separate as two different objectives rather than being combined. We also add the minimization of floating-point operations (FLOPs) as the third objective to be optimized simultaneously. On NAS-Bench-101 and NAS-Bench-201 benchmarks, our approach achieves excellent efficiency in finding Pareto fronts of a wide range of architectures exhibiting optimal trade-offs among network expressivity, trainability, and complexity. Network architectures obtained by our approach on CIFAR-10 also show high transferability on CIFAR-100 and ImageNet. © 2021, Springer Nature Switzerland AG.",10.1007/978-3-030-92270-2_29,Deep learning; Evolutionary computation; Multi-objective optimization; Neural architecture search; Neural tangent kernels,,0.0,1.0
On the Treatment of Optimization Problems with L1 Penalty Terms via Multiobjective Continuation,"Bieker K., Gebken B., Peitz S.",IEEE Transactions on Pattern Analysis and Machine Intelligence,2021.0,"We present a novel algorithm that allows us to gain detailed insight into the effects of sparsity in linear and nonlinear optimization. Sparsity is of great importance in many scientific areas such as image and signal processing, medical imaging, compressed sensing, and machine learning, as it ensures robustness against noisy data and yields models that are easier to interpret due to the small number of relevant terms. It is common practice to enforce sparsity by adding the l1-norm as a penalty term. In order to gain a better understanding and to allow for an informed model selection, we directly solve the corresponding multiobjective optimization problem (MOP) that arises when minimizing the main objective and the l1-norm simultaneously. As this MOP is in general non-convex for nonlinear objectives, the penalty method will fail to provide all optimal compromises. To avoid this issue, we present a continuation method specifically tailored to MOPs with two objective functions one of which is the l1-norm. Our method can be seen as a generalization of homotopy methods for linear regression problems to the nonlinear case. Several numerical examples - including neural network training- demonstrate our theoretical findings and the additional insight gained by this multiobjective approach. Author",10.1109/TPAMI.2021.3114962,Linear programming; Machine Learning; Mathematical models; Multiobjective Optimization; Neural networks; Nonsmooth Optimization; Optimization; Pareto optimization; Signal processing; Sparsity; Training,,0.0,1.0
Multi-task Learning with Riemannian Optimization,"Cai T., Song L., Li G., Liao M.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021.0,"Multi-task learning (MTL) is a promising research field of machine learning, in which the training process of the neural network is equivalent to multi-objective optimization. On one hand, MTL trains all the network weights simultaneously to converge the multi-task loss. On the other hand, multi-objective optimization aims to find the optimum solution, which satisfies the constraints and optimizes the vector of objective functions. Therefore, the performance of MTL is dominated by the computation of the multi-objective solution. This paper proposes a method based on Riemannian optimization to solve the multi-objective optimization in MTL. Firstly, multi-objective optimization is reduced to its Karush-Kuhn-Tucker (KKT) condition as the optimum solution of constrained quadratic optimization. Secondly, by mapping the Euclidean space of the constraint into manifold, the quadratic optimization is transformed to an unconstrained problem. Finally, Riemannian optimization algorithm is used to compute the solution of this problem, which gives a Pareto direction towards the KKT condition. We perform experiments on the MultiMNIST and Fashion MNIST datasets, and the experimental results demonstrate the efficiency of our method. © 2021, Springer Nature Switzerland AG.",10.1007/978-3-030-84529-2_42,Multi-objective optimization; Multi-task learning; Riemannian optimization,,0.0,1.0
Facing Many Objectives for Fairness in Machine Learning,"Villar D., Casillas J.",Communications in Computer and Information Science,2021.0,"Fairness is an increasingly important topic in the world of Artificial Intelligence. Machine learning techniques are widely used nowadays to solve huge amounts of problems, but those techniques may be biased against certain social groups due to different reasons. Using fair classification methods we can attenuate this discrimination source. Nevertheless, there are lots of valid fairness definitions which may be mutually incompatible. The aim of this paper is to propose a method which generates fair solutions for machine learning binary classification problems with one sensitive attribute. As we want accurate, fair and interpretable solutions, our method is based on Many Objective Evolutionary Algorithms (MaOEAs). The decision space will represent hyperparameters for training our classifiers, which will be decision trees, while the objective space will be a four-dimensional space representing the quality of the classifier in terms of an accuracy measure, two contradictory fairness criteria and an interpretability indicator. Experimentation have been done using four well known fairness datasets. As we will see, our algorithm generates good solutions compared to previous work, and a presumably well populated pareto-optimal population is found so that different classifiers could be used depending on our needs. © 2021, Springer Nature Switzerland AG.",10.1007/978-3-030-85347-1_27,Decision trees; Fairness in machine learning; Many objective evolutionary algorithm,,0.0,1.0
The multi-task learning with an application of Pareto improvement,"Cai T., Gao X., Song L., Liao M.",ACM International Conference Proceeding Series,2021.0,"Multi-task learning is a promising field in machine learning, which aims to improve the performance of multiple related learning tasks by taking advantage of useful information between them. Multi-task learning is essentially equivalent to multi-objective optimization problem, the purpose is to find the most appropriate weight, and because the performance of many deep learning systems based on multi-task learning largely depends on the relative weight of each task loss. It's a problem that we need to study how to calculate the weight value under some constraint conditions by reasonable method. Therefore, this paper employs a powerful method based on convex optimization theory, whose purpose is to find the Pareto optimal solution and get the specific task loss weight. The optimization process is closely related to the gradient in deep learning. In addition, to improve the accuracy, we add the modules of gradient normalization and weight standardization. The experimental results show that the performance of our method is better than that of single task experiment or multi-task experiment under fixed weight, and multi-task experiment based on uncertainty based adaptive learning, and the accuracy is further improved after adding the above modules. © 2021 ACM.",10.1145/3448734.3450463,multi-objective optimization; Multi-task learning; Pareto improvement,,0.0,1.0
Mitigating bias in set selection with noisy protected attributes,"Mehrotra A., Celis L.E.","FAccT 2021 - Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",2021.0,"Subset selection algorithms are ubiquitous in AI-driven applications, including, online recruiting portals and image search engines, so it is imperative that these tools are not discriminatory on the basis of protected attributes such as gender or race. Currently, fair subset selection algorithms assume that the protected attributes are known as part of the dataset. However, protected attributes may be noisy due to errors during data collection or if they are imputed (as is often the case in real-world settings). While a wide body of work addresses the effect of noise on the performance of machine learning algorithms, its effect on fairness remains largely unexamined. We find that in the presence of noisy protected attributes, in attempting to increase fairness without considering noise, one can, in fact, decrease the fairness of the result! Towards addressing this, we consider an existing noise model in which there is probabilistic information about the protected attributes (e.g., [19, 32, 44, 56]), and ask is fair selection possible under noisy conditions? We formulate a ""denoised""selection problem which functions for a large class of fairness metrics; given the desired fairness goal, the solution to the denoised problem violates the goal by at most a small multiplicative amount with high probability. Although this denoised problem turns out to be NP-hard, we give a linear-programming based approximation algorithm for it. We evaluate this approach on both synthetic and real-world datasets. Our empirical results show that this approach can produce subsets which significantly improve the fairness metrics despite the presence of noisy protected attributes, and, compared to prior noise-oblivious approaches, has better Pareto-tradeoffs between utility and fairness. © 2021 ACM.",10.1145/3442188.3445887,,,0.0,1.0
Exploring multiobjective training in multiclass classification,"Raimundo M.M., Drumond T.F., Marques A.C.R., Lyra C., Rocha A., Von Zuben F.J.",Neurocomputing,2021.0,"Multinomial logistic loss and L2 regularization are often conflicting objectives as more robust regularization leads to restrained multinomial parameters. For many practical problems, leveraging the best of both worlds would be invaluable for better decision-making processes. This research proposes a novel framework to obtain representative and diverse L2-regularized multinomial models, based on valuable trade-offs between prediction error and model complexity. The framework relies upon the Non-Inferior Set Estimation (NISE) method – a deterministic multiobjective solver. NISE automatically implements hyperparameter tuning in a multiobjective context. Given the diverse set of efficient learning models, model selection and aggregation of the multiple models in an ensemble framework promote high performance in multiclass classification. Additionally, NISE uses the weighted sum method as scalarization, thus being able to deal with the learning formulation directly. Its deterministic nature and the convexity of the learning problem confer scalability to the proposal. The experiments show competitive performance in various setups, taking a broad set of multiclass classification methods as contenders. © 2021 Elsevier B.V.",10.1016/j.neucom.2020.12.087,Diversity of Pareto-optimal models; Ensemble learning; Multiclass classification; Multiobjective optimization,1.0,0.0,1.0
Lexicographically fair learning: Algorithms and generalization,"Diana E., Gill W., Globus-Harris I., Kearns M., Roth A., Sharifi-Malvajerdi S.","Leibniz International Proceedings in Informatics, LIPIcs",2021.0,"We extend the notion of minimax fairness in supervised learning problems to its natural conclusion: lexicographic minimax fairness (or lexifairness for short). Informally, given a collection of demographic groups of interest, minimax fairness asks that the error of the group with the highest error be minimized. Lexifairness goes further and asks that amongst all minimax fair solutions, the error of the group with the second highest error should be minimized, and amongst all of those solutions, the error of the group with the third highest error should be minimized, and so on. Despite its naturalness, correctly defining lexifairness is considerably more subtle than minimax fairness, because of inherent sensitivity to approximation error. We give a notion of approximate lexifairness that avoids this issue, and then derive oracle-efficient algorithms for finding approximately lexifair solutions in a very general setting. When the underlying empirical risk minimization problem absent fairness constraints is convex (as it is, for example, with linear and logistic regression), our algorithms are provably efficient even in the worst case. Finally, we show generalization bounds - approximate lexifairness on the training sample implies approximate lexifairness on the true distribution with high probability. Our ability to prove generalization bounds depends on our choosing definitions that avoid the instability of naive definitions. © Emily Diana, Wesley Gill, Ira Globus-Harris, Michael Kearns, Aaron Roth, and Saeed Sharifi-Malvajerdi; licensed under Creative Commons License CC-BY 4.0 2nd Symposium on Foundations of Responsible Computing (FORC 2021).",10.4230/LIPIcs.FORC.2021.6,Fair learning; Game theory; Lexicographic fairness; Online learning,,0.0,1.0
TPOT-NN: augmenting tree-based automated machine learning with neural network estimators,"Romano J.D., Le T.T., Fu W., Moore J.H.",Genetic Programming and Evolvable Machines,2021.0,"Automated machine learning (AutoML) and artificial neural networks (ANNs) have revolutionized the field of artificial intelligence by yielding incredibly high-performing models to solve a myriad of inductive learning tasks. In spite of their successes, little guidance exists on when to use one versus the other. Furthermore, relatively few tools exist that allow the integration of both AutoML and ANNs in the same analysis to yield results combining both of their strengths. Here, we present TPOT-NN—a new extension to the tree-based AutoML software TPOT—and use it to explore the behavior of automated machine learning augmented with neural network estimators (AutoML+NN), particularly when compared to non-NN AutoML in the context of simple binary classification on a number of public benchmark datasets. Our observations suggest that TPOT-NN is an effective tool that achieves greater classification accuracy than standard tree-based AutoML on some datasets, with no loss in accuracy on others. We also provide preliminary guidelines for performing AutoML+NN analyses, and recommend possible future directions for AutoML+NN methods research, especially in the context of TPOT. © 2021, The Author(s).",10.1007/s10710-021-09401-z,Artificial neural networks; Automated machine learning; Evolutionary algorithms; Genetic programming; Pareto optimization,3.0,0.0,1.0
Rawlsian Fair Adaptation of Deep Learning Classifiers,"Shah K., Gupta P., Deshpande A., Bhattacharyya C.","AIES 2021 - Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",2021.0,"Group-fairness in classification aims for equality of a predictive utility across different sensitive sub-populations, e.g., race or gender. Equality or near-equality constraints in group-fairness often worsen not only the aggregate utility but also the utility for the least advantaged sub-population. In this paper, we apply the principles of Pareto-efficiency and least-difference to the utility being accuracy, as an illustrative example, and arrive at the Rawls classifier that minimizes the error rate on the worst-off sensitive sub-population. Our mathematical characterization shows that the Rawls classifier uniformly applies a threshold to an ideal score of features, in the spirit of fair equality of opportunity. In practice, such a score or a feature representation is often computed by a black-box model that has been useful but unfair. Our second contribution is practical Rawlsian fair adaptation of any given black-box deep learning model, without changing the score or feature representation it computes. Given any score function or feature representation and only its second-order statistics on the sensitive sub-populations, we seek a threshold classifier on the given score or a linear threshold classifier on the given feature representation that achieves the Rawls error rate restricted to this hypothesis class. Our technical contribution is to formulate the above problems using ambiguous chance constraints, and to provide efficient algorithms for Rawlsian fair adaptation, along with provable upper bounds on the Rawls error rate. Our empirical results show significant improvement over state-of-the-art group-fair algorithms, even without retraining for fairness. © 2021 ACM.",10.1145/3461702.3462592,fair adaptation; fairness for deep learning classifiers; Rawlsian fairness,,0.0,1.0
Minimax Group Fairness: Algorithms and Experiments,"Diana E., Gill W., Kearns M., Kenthapadi K., Roth A.","AIES 2021 - Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",2021.0,"We consider a recently introduced framework in which fairness is measured by worst-case outcomes across groups, rather than by the more standard differences between group outcomes. In this framework we provide provably convergent oracle-efficient learning algorithms (or equivalently, reductions to non-fair learning) for minimax group fairness. Here the goal is that of minimizing the maximum loss across all groups, rather than equalizing group losses. Our algorithms apply to both regression and classification settings and support both overall error and false positive or false negative rates as the fairness measure of interest. They also support relaxations of the fairness constraints, thus permitting study of the tradeoff between overall accuracy and minimax fairness. We compare the experimental behavior and performance of our algorithms across a variety of fairness-sensitive data sets and show empirical cases in which minimax fairness is strictly and strongly preferable to equal outcome notions. © 2021 ACM.",10.1145/3461702.3462523,fair machine learning; game theory; minimax fairness,3.0,0.0,1.0
Cascaded Classifier for Pareto-Optimal Accuracy-Cost Trade-Off Using Off-the-Shelf ANNs,"Latotzke C., Loh J., Gemmeke T.",Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2022.0,"Machine-learning classifiers provide high quality of service in classification tasks. Research now targets cost reduction measured in terms of average processing time or energy per solution. Revisiting the concept of cascaded classifiers, we present a first of its kind analysis of optimal pass-on criteria between the classifier stages. Based on this analysis, we derive a methodology to maximize accuracy and efficiency of cascaded classifiers. On the one hand, our methodology allows cost reduction of 1.32 × while preserving reference classifier’s accuracy. On the other hand, it allows to scale cost over two orders while gracefully degrading accuracy. Thereby, the final classifier stage sets the top accuracy. Hence, the multi-stage realization can be employed to optimize any state-of-the-art classifier. © 2022, Springer Nature Switzerland AG.",10.1007/978-3-030-95470-3_32,Cascaded classifier; Design methodology; Edge devices; Machine learning; Pareto analysis; Preliminary classifier,,0.0,1.0
A Logarithmic Distance-Based Multi-Objective Genetic Programming Approach for Classification of Imbalanced Data,"Kumar A., Goel S., Sinha N., Bhardwaj A.",Communications in Computer and Information Science,2022.0,"Standard classification algorithms give biased results when data sets are imbalanced. Genetic Programming, a machine learning algorithm based on the evolution of species in nature, also suffers from the same issue. In this research work, we introduced a logarithmic distance-based multi-objective genetic programming (MOGP) approach for classifying imbalanced data. The proposed approach utilizes the logarithmic value of the distance between predicted and expected values. This logarithmic value for the minority and the majority classes is treated as two separate objectives while learning. In the final generation, the proposed approach generated a Pareto-front of classifiers with a balanced surface representing the majority and the minority class accuracies for binary classification. The primary advantage of the MOGP technique is that it can produce a set of good-performing classifiers in a single experimental execution. Against the MOGP approach, the canonical GP method requires multiple experimental runs and a priori objective-based fitness function. Another benefit of MOGP is that it explicitly includes the learning bias into the algorithms. For evaluation of the proposed approach, we performed extensive experimentation of five imbalanced problems. The proposed approach’s results have proven its superiority over the traditional method, where the minority and majority class accuracies are taken as two separate objectives. © 2022, Springer Nature Switzerland AG.",10.1007/978-3-030-95502-1_23,Fitness function; Genetic programming; Imbalanced data classification; Multi-objective optimization; Pareto front,,0.0,1.0
Evaluating the Performance of Feature Selection Methods Using Huge Big Data: A Monte Carlo Simulation Approach,"Khan F., Urooj A., Khan S.A., Khosa S.K., Muhammadullah S., Almaspoor Z.",Mathematical Problems in Engineering,2022.0,"In this article, we compare autometrics and machine learning techniques including Minimax Concave Penalty (MCP), Elastic Smoothly Clipped Absolute Deviation (E-SCAD), and Adaptive Elastic Net (AEnet). For simulation experiments, three kinds of scenarios are considered by allowing the multicollinearity, heteroscedasticity, and autocorrelation conditions with varying sample sizes and the varied number of covariates. We found that all methods show improved their performance for a large sample size. In the presence of low and moderate multicollinearity and low and moderate autocorrelation, the considered methods retain all relevant variables. However, for low and moderate multicollinearity, excluding AEnet, all methods keep many irrelevant predictors as well. In contrast, under low and moderate autocorrelation, along with AEnet, the Autometrics retain less irrelevant predictors. Considering the case of extreme multicollinearity, AEnet retains more than 93 percent correct variables with an outstanding gauge (zero percent). However, the potency of remaining techniques, specifically MCP and E-SCAD, tends towards unity with augmenting sample size but capturing massive irrelevant predictors. Similarly, in case of high autocorrelation, E-SCAD has shown good performance in the selection of relevant variables for a small sample, while in gauge, Autometrics and AEnet are performed better and often retained less than 5 percent irrelevant variables. In the presence of heteroscedasticity, all techniques often hold all relevant variables but also suffer from overspecification problems except AEnet and Autometrics which circumvent the irrelevant predictors and establish the true model precisely. For an empirical application, we take into account the workers' remittance data for Pakistan along its twenty-seven determinants spanning from 1972 to 2020 for Pakistan. The AEnet selected thirteen relevant covariates of workers' remittance while E-SCAD and MCP suffered from an overspecification problem. Hence, the policymakers and practitioners should focus on the relevant variables selected by AEnet to improve workers' remittance in the case of Pakistan. In this regard, the Pakistan government has devised policies that make it easy to transfer remittances legally and mitigate the cost of transferring remittances from abroad. The AEnet approach can help policymakers arrive at relevant variables in the presence of a huge set of covariates, which in turn produce accurate predictions. © 2022 Faridoon Khan et al.",10.1155/2022/6607330,,,0.0,1.0
An efficient primal-dual method for solving non-smooth machine learning problem,"Lyaqini S., Nachaoui M., Hadri A.","Chaos, Solitons and Fractals",2022.0,"This paper deals with the machine learning model as a framework of regularized loss minimization problem in order to obtain a generalized model. Recently, some studies have proved the success and the efficiency of nonsmooth loss function for supervised learning problems Lyaqini et al. [1]. Motivated by the success of this choice, in this paper we formulate the supervised learning problem based on L1 fidelity term. To solve this nonsmooth optimization problem we transform it into a mini-max one. Then we propose a Primal-Dual method that handles the mini-max problem. This method leads to an efficient and significantly faster numerical algorithm to solve supervised learning problems in the most general case. To illustrate the effectiveness of the proposed approach we present some experimental-numerical validation examples, which are made through synthetic and real-life data. Thus, we show that our approach is outclassing existing methods in terms of convergence speed, quality, and stability of the predicted models. © 2021 Elsevier Ltd",10.1016/j.chaos.2021.111754,ECG; Kernel methods EMG; Non-smooth optimization; Primal-dual algorithm; Supervised learning,,0.0,1.0
Efficient and sparse neural networks by pruning weights in a multiobjective learning approach,"Reiners M., Klamroth K., Heldmann F., Stiglmayr M.",Computers and Operations Research,2022.0,"Overparameterization and overfitting are common concerns when designing and training deep neural networks, that are often counteracted by pruning and regularization strategies. However, these strategies remain secondary to most learning approaches and suffer from time and computational intensive procedures. We suggest a multiobjective perspective on the training of neural networks by treating its prediction accuracy and the network complexity as two individual objective functions in a biobjective optimization problem. As a showcase example, we use the cross entropy as a measure of the prediction accuracy while adopting an l1-penalty function to assess the total cost (or complexity) of the network parameters. The latter is combined with an intra-training pruning approach that reinforces complexity reduction and requires only marginal extra computational cost. From the perspective of multiobjective optimization, this is a truly large-scale optimization problem. We compare two different optimization paradigms: On the one hand, we adopt a scalarization-based approach that transforms the biobjective problem into a series of weighted-sum scalarizations. On the other hand we implement stochastic multi-gradient descent algorithms that generate a single Pareto optimal solution without requiring or using preference information. In the first case, favorable knee solutions are identified by repeated training runs with adaptively selected scalarization parameters. Numerical results on exemplary convolutional neural networks confirm that large reductions in the complexity of neural networks with negligible loss of accuracy are possible. © 2022 Elsevier Ltd",10.1016/j.cor.2021.105676,Automated machine learning; l1-regularization; Multiobjective learning; Stochastic multi-gradient descent; Unstructured pruning,,0.0,1.0
Multi-label learning via minimax probability machine,"Rastogi (nee Khemchandani) R., Jain S.",International Journal of Approximate Reasoning,2022.0,"In this paper, we propose Minimax Probability Machine for Multi-label data classification and is termed as Multi-Label Minimax Probability Machine (MLMPM). Based on data mean and covariance information, MLMPM builds a classifier that minimizes an upper bound on the mis-classification probability of unseen future data. For capturing label correlation we have considered asymmetric co-occurrency matrix into the model. The proposed model has also been extended to non-linear settings using the Mercer Kernel trick. To accelerate the training procedure, iterative weighted least squares is used to train the underlying optimization model efficiently. Extensive experimental comparisons of our proposed method with related multi-label algorithms on synthetic as well as real world multi-label datasets, along with Amazon rainforest satellite images dataset, prove its efficacy. © 2022 Elsevier Inc.",10.1016/j.ijar.2022.02.002,Label correlation; Minimax probability machine; Multi-label classification; Second order cone programming problem; Weighted least squares,,0.0,1.0
TensorOpt: Exploring the Tradeoffs in Distributed DNN Training with Auto-Parallelism,"Cai Z., Yan X., Ma K., Wu Y., Huang Y., Cheng J., Su T., Yu F.",IEEE Transactions on Parallel and Distributed Systems,2022.0,"Effective parallelization strategies are crucial for the performance of distributed deep neural network (DNN) training. Recently, several methods have been proposed to search parallelization strategies but they all optimize a single objective (e.g., execution time, memory consumption) and produce only one strategy. We propose Frontier Tracking (FT), an efficient algorithm that finds a set of Pareto-optimal parallelization strategies to explore the best trade-off among different objectives. FT can minimize the memory consumption when the number of devices is limited and fully utilize additional resources to reduce the execution time. Based on FT, we develop a user-friendly system, called TensorOpt, which allows users to run their distributed DNN training jobs without caring the details about searching and coding parallelization strategies. Experimental results show that TensorOpt is more flexible in adapting to resource availability compared with existing frameworks. © 1990-2012 IEEE.",10.1109/TPDS.2021.3132413,Deep learning; distributed systems; large-scale model training,,0.0,1.0
Accurate Multi-Criteria Decision Making Methodology for Recommending Machine Learning Algorithm,"Ali, Rahman and Lee, Sungyoung and Chung, Tae Choong","Pergamon Press, Inc.",2017.0,"A multi-criteria decision making methodology is proposed to select best classifier.NGT-based method is adapted to choose suitable classifier's evaluation metrics.Accuracy, time complexity and consistency based new ranking criteria is designed.AHP-based method is adapted to estimate relative weights for evaluation metrics.Classifiers are ranked based on relative closeness score, computed using TOPSIS. ObjectiveManual evaluation of machine learning algorithms and selection of a suitable classifier from the list of available candidate classifiers, is highly time consuming and challenging task. If the selection is not carefully and accurately done, the resulting classification model will not be able to produce the expected performance results. In this study, we present an accurate multi-criteria decision making methodology (AMD) which empirically evaluates and ranks classifiers' and allow end users or experts to choose the top ranked classifier for their applications to learn and build classification models for them. Methods and materialExisting classifiers performance analysis and recommendation methodologies lack (a) appropriate method for suitable evaluation criteria selection, (b) relative consistent weighting mechanism, (c) fitness assessment of the classifiers' performances, and (d) satisfaction of various constraints during the analysis process. To assist machine learning practitioners in the selection of suitable classifier(s), AMD methodology is proposed that presents an expert group-based criteria selection method, relative consistent weighting scheme, a new ranking method, called optimum performance ranking criteria, based on multiple evaluation metrics, statistical significance and fitness assessment functions, and implicit and explicit constraints satisfaction at the time of analysis. For ranking the classifiers performance, the proposed ranking method integrates Wgt.Avg.F-score, CPUTimeTesting, CPUTimeTraining, and Consistency measures using the technique for order performance by similarity to ideal solution (TOPSIS). The final relative closeness score produced by TOPSIS, is ranked and the practitioners select the best performance (top-ranked) classifier for their problems in-hand. FindingsBased on the extensive experiments performed on 15 publically available UCI and OpenML datasets using 35 classification algorithms from heterogeneous families of classifiers, an average Spearman's rank correlation coefficient of 0.98 is observed. Similarly, the AMD method has showed improved performance of 0.98 average Spearman's rank correlation coefficient as compared to 0.83 and 0.045 correlation coefficient of the state-of-the-art ranking methods, performance of algorithms (PAlg) and adjusted ratio of ratio (ARR). Conclusion and implicationThe evaluation, empirical analysis of results and comparison with state-of-the-art methods demonstrate the feasibility of AMD methodology, especially the selection and weighting of right evaluation criteria, accurate ranking and selection of optimum performance classifier(s) for the user's application's data in hand. AMD reduces expert's time and efforts and improves system performance by designing suitable classifier recommended by AMD methodology.",10.1016/j.eswa.2016.11.034,"Classification algorithms, Algorithm recommendation, Ranking classifiers, TOPSIS, Classifiers recommendation, Multi-criteria decision making, Algorithm selection",,0.0,1.0
Multi-Objective Model Type Selection,"Rosales-P\'{e}rez, Alejandro and Gonzalez, Jesus A. and Coello Coello, Carlos A. and Escalante, Hugo Jair and Reyes-Garcia, Carlos A.",Elsevier Science Publishers B. V.,2014.0,"Classification is a mainstream within the machine learning community. As a result, a large number of learning algorithms have been proposed. The performance of many of these could highly depend on the chosen values of their hyper-parameters. This paper introduces a novel method for addressing the model selection problem for a given classification task. In our model selection formulation, both the learning algorithm and its hyper-parameters are considered. In our proposed approach, model selection is tackled as a multi-objective optimization problem. The empirical error, or training error, and the model complexity are defined as the objectives. We adopt a multi-objective evolutionary algorithm as the search engine, due to its high performance and its advantages for solving multi-objective problems. The model complexity is estimated experimentally, in a general fashion, for any learning algorithm, through the VC dimension. Strategies for choosing a single model or for constructing an ensemble of models from the resulting non-dominated set are also proposed. Experimental results on benchmark data sets indicate the effectiveness of the proposed approach. Furthermore, a comparative study shows that the obtained models are highly competitive, in terms of generalization performance, with other methods in the state of the art that focus on a single-learning algorithm, or a single-objective approach.",10.1016/j.neucom.2014.05.077,"VC dimension, Multi-objective optimization, Model type selection, Ensemble methods",,0.0,1.0
A Multi-Objective Optimization Design Framework for Ensemble Generation,"Ribeiro, Victor Henrique Alves and Reynoso-Meza, Gilberto",Association for Computing Machinery,2018.0,"Machine learning algorithms have found to be useful for the solution of complex engineering problems. However, due to problem's characteristics, such as class imbalance, classical methods may not be formidable. The authors believe that the application of multi-objective optimization design can improve the results of machine learning algorithms on such scenarios. Thus, this paper proposes a novel methodology for the creation of ensembles of classifiers. To do so, a multi-objective optimization design approach composed of two steps is used. The first step focus on generating a set of diverse classifiers, while the second step focus on the selection of such classifiers as ensemble members. The proposed method is tested on a real-world competition data set, using both decision trees and logistic regression classifiers. Results show that the ensembles created with such technique outperform the best ensemble members.",10.1145/3205651.3208219,"logistic regression, decision trees, multi-objective optimization, ensemble methods",,0.0,1.0
Optimal Set of Overlapping Clusters Using Multi-Objective Genetic Algorithm,"Das, Sunanda and Chaudhuri, Shreya and Das, Asit K.",Association for Computing Machinery,2017.0,"Clustering is an important unsupervised machine learning techniqueused in diverse fields to explore the inherent structure of the data. In most of the real life datasets, one object resides in many clusters with different membership values. Many clustering algorithms have been proposed for finding such overlapping clusters for knowledge extraction and future trend prediction. In the paper, multi-objective genetic algorithm based cluster analysis technique is proposed for finding the optimal set of overlapping clusters. As most of the real world search and optimization problems involve multiple objectives, multi-objective Genetic Algorithm is an obvious choice for capturing multiple optimal solutions. Thus the usefulness of applying the multi-objective Genetic Algorithm is to grouping the objects based on different objective functions for finding optimal set of overlapping clusters. The advantage of this algorithm is that it assigns a membership value only to the objects which are the members of several clusters, instead of assigning membership values for all clusters like fuzzy clustering algorithm. If any object positively belongs only to a single cluster, its membership value for this cluster is ' 1' and '0' for all other clusters. The overall performance of the method is investigated on some popular UCI and microarray datasets and the optimality of the clusters is measured by some important cluster validation indices. The experimental results show the effectiveness of the proposed method.",10.1145/3055635.3056653,"Cluster Analysis, Cluster validation index, Fuzzy Clustering, Multi-objective Genetic Algorithm, Overlapping Cluster",,0.0,1.0
Multi-Objective Hyperparameter Tuning and Feature Selection Using Filter Ensembles,"Binder, Martin and Moosbauer, Julia and Thomas, Janek and Bischl, Bernd",Association for Computing Machinery,2020.0,"Both feature selection and hyperparameter tuning are key tasks in machine learning. Hyperparameter tuning is often useful to increase model performance, while feature selection is undertaken to attain sparse models. Sparsity may yield better model interpretability and lower cost of data acquisition, data handling and model inference. While sparsity may have a beneficial or detrimental effect on predictive performance, a small drop in performance may be acceptable in return for a substantial gain in sparseness. We therefore treat feature selection as a multi-objective optimization task. We perform hyperparameter tuning and feature selection simultaneously because the choice of features of a model may influence what hyperparameters perform well.We present, benchmark, and compare two different approaches for multi-objective joint hyperparameter optimization and feature selection: The first uses multi-objective model-based optimization. The second is an evolutionary NSGA-II-based wrapper approach to feature selection which incorporates specialized sampling, mutation and recombination operators. Both methods make use of parameterized filter ensembles.While model-based optimization needs fewer objective evaluations to achieve good performance, it incurs computational overhead compared to the NSGA-II, so the preferred choice depends on the cost of evaluating a model on given data.",10.1145/3377930.3389815,"model-based optimization, multiobjective optimization, hyperparameter optimization, evolutionary algorithms, feature selection",,0.0,1.0
Surrogate-Assisted Multi-Objective Model Selection for Support Vector Machines,"Rosales-P\'{e}rez, Alejandro and Gonzalez, Jesus A. and Coello Coello, Carlos A. and Escalante, Hugo Jair and Reyes-Garcia, Carlos A.",Elsevier Science Publishers B. V.,2015.0,"Classification is one of the most well-known tasks in supervised learning. A vast number of algorithms for pattern classification have been proposed so far. Among these, support vector machines (SVMs) are one of the most popular approaches, due to the high performance reached by these methods in a wide number of pattern recognition applications. Nevertheless, the effectiveness of SVMs highly depends on their hyper-parameters. Besides the fine-tuning of their hyper-parameters, the way in which the features are scaled as well as the presence of non-relevant features could affect their generalization performance. This paper introduces an approach for addressing model selection for support vector machines used in classification tasks. In our formulation, a model can be composed of feature selection and pre-processing methods besides the SVM classifier. We formulate the model selection problem as a multi-objective one, aiming to minimize simultaneously two components that are closely related to the error of a model: bias and variance components, which are estimated in an experimental fashion. A surrogate-assisted evolutionary multi-objective optimization approach is adopted to explore the hyper-parameters space. We adopted this approach due to the fact that estimating the bias and variance could be computationally expensive. Therefore, by using surrogate-assisted optimization, we expect to reduce the number of solutions evaluated by the fitness functions so that the computational cost would also be reduced. Experimental results conducted on benchmark datasets widely used in the literature, indicate that highly competitive models with a fewer number of fitness function evaluations are obtained by our proposal when it is compared to state of the art model selection methods.",10.1016/j.neucom.2014.08.075,"Support vector machines, Surrogate-assisted optimization, Model selection, Multi-objective optimization",,0.0,1.0
Multi-Criteria Feature Selection on Cost-Sensitive Data with Missing Values,"Shu, Wenhao and Shen, Hong",Elsevier Science Inc.,2016.0,"Feature selection plays an important role in pattern recognition and machine learning. Confronted with high dimensional data in many data analysis tasks, feature selection techniques are designed to find a relevant feature subset of the original features which can facilitate classification. However, in many real-world applications, missing feature values that contribute to test and misclassification costs are emerging to be an issue of increasing concern for most data sets, particularly dealing with big data. The existing feature selection approaches do not address this issue effectively. In this paper, based on rough set theory we address the problem of feature selection for cost-sensitive data with missing values. We first propose a multi-criteria evaluation function to characterize the significance of candidate features, by taking into consideration not only the power in the positive region and boundary region but also their associated costs. On this basis, we develop a forward greedy feature selection algorithm for selecting a feature subset of minimized cost that preserves the same information as the whole feature set. In addition, to improve the efficiency of this algorithm, we implement the selection of candidate features in a dwindling object set. Finally, we demonstrate the superior performance of the proposed algorithm to the existing feature selection algorithms through experimental results on different data sets. HighlightsA multi-criteria based evaluation function is proposed for measuring features from different viewpoints.A dwindling universe is provided to accelerate the feature selection process.A feature selection algorithm is developed on cost-sensitive data with missing values.The efficiency and effectiveness of the proposed algorithm are demonstrated on different data sets.",10.1016/j.patcog.2015.09.016,"Cost-sensitive data, Feature selection, Incomplete data, Algorithm MCFSMulti-criteria based feature selection algorithm on cost-sensitive data with missing values, Multi-criteria, Rough sets",,0.0,1.0
Parallel Alternatives for Evolutionary Multi-Objective Optimization in Unsupervised Feature Selection,"Kimovski, Dragi and Ortega, Julio and Ortiz, Andr\'{e}s and Ba\~{n}os, Ra\'{u}l","Pergamon Press, Inc.",2015.0,"Multiobjective unsupervised feature selection with many decision variables is tackled.EEG signals for Brain-Computer Interface (BCI) applications are used as benchmarks.Cooperative evolutionary algorithms for multiobjective optimization are given.Parallel implementations obtain quality results in terms of hypervolume and speedup.Superlinear speedups are justified by adjusting models to experimental results. Many machine learning and pattern recognition applications require reducing dimensionality to improve learning accuracy while irrelevant inputs are removed. This way, feature selection has become an important issue on these researching areas. Nevertheless, as in past years the number of patterns and, more specifically, the number of features to be selected have grown very fast, parallel processing constitutes an important tool to reach efficient approaches that make possible to tackle complex problems within reasonable computing times. In this paper we propose parallel multi-objective optimization approaches to cope with high-dimensional feature selection problems. Several parallel multi-objective evolutionary alternatives are proposed, and experimentally evaluated by using some synthetic and BCI (Brain-Computer Interface) benchmarks. The experimental results show that the cooperation of parallel evolving subpopulations provides improvements in the solution quality and computing time speedups depending on the parallel alternative and data profile.",10.1016/j.eswa.2015.01.061,"Parallel evolutionary algorithms, Unsupervised classification, Multi-objective clustering, Feature selection, High-dimensional data, Speedup models",,0.0,1.0
"Feature Weighting for Na\""{\i}ve Bayes Using Multi Objective Artificial Bee Colony Algorithm","Chaudhuri, Abhilasha and Sahu, Tirath Prasad",Inderscience Publishers,2021.0,"Na\""{\i}ve Bayes (NB) is a widely used classifier in the field of machine learning. However, its conditional independence assumption does not hold true in real-world applications. In literature, various feature weighting approaches have attempted to alleviate this assumption. Almost all of these approaches consider the relationship between feature-class (relevancy) and feature-feature (redundancy) independently, to determine the weights of features. We argue that these two relationships are mutually dependent and both cannot be improved simultaneously, i.e., form a trade-off. This paper proposes a new paradigm to determine the feature weight by formulating it as a multi-objective optimisation problem to balance the trade-off between relevancy and redundancy. Multi-objective artificial bee colony-based feature weighting technique for na\""{\i}ve Bayes (MOABC-FWNB) is proposed. An extensive experimental study was conducted on 20 benchmark UCI datasets. Experimental results show that MOABC-FWNB outperforms NB and other existing state-of-the-art feature weighting techniques.",10.1504/ijcse.2021.113655,"na\""{\i}ve Bayes, multi objective optimisation, artificial bee colony, feature weighting",,0.0,1.0
Neural Network for Learning and Analyzing Preferences for Multi-Criteria Services,"Haddar, Imane and Raouyane, Brahim and Bellafkih, Mostafa",Association for Computing Machinery,2020.0,"In the latest years, service selection is becoming more and more important due to the significant effect of internet based services in the telecom industry. When it comes to selecting the best service, different candidate services with similar settings are proposed by different service providers. The selection should take into consideration the respect of the constraints of consumers in terms of Service Level Agreement contracts, what makes the modelling of the preferences of decision-makers for choice problems the main focus of this work. In order to model these preferences, we propose contextual preference functions based on machine learning techniques from neural networks. It will therefore be possible to further explain and decode preferences in order to facilitate negotiation and thus decision-making, thereby improving the quality of service providers while being on customer preferences.",10.1145/3419604.3419799,"Decision aid, Preferences, Neural network, Service selection",,0.0,1.0
Multi-Objective Search of Robust Neural Architectures against Multiple Types of Adversarial Attacks,"Liu, Jia and Jin, Yaochu",Elsevier Science Publishers B. V.,2021.0,,10.1016/j.neucom.2021.04.111,"Adversarial attacks, Robustness, Multi-objective evolutionary algorithm, Neural architecture search",,0.0,1.0
NSGA-Net: Neural Architecture Search Using Multi-Objective Genetic Algorithm,"Lu, Zhichao and Whalen, Ian and Boddeti, Vishnu and Dhebar, Yashesh and Deb, Kalyanmoy and Goodman, Erik and Banzhaf, Wolfgang",Association for Computing Machinery,2019.0,"This paper introduces NSGA-Net --- an evolutionary approach for neural architecture search (NAS). NSGA-Net is designed with three goals in mind: (1) a procedure considering multiple and conflicting objectives, (2) an efficient procedure balancing exploration and exploitation of the space of potential neural network architectures, and (3) a procedure finding a diverse set of trade-off network architectures achieved in a single run. NSGA-Net is a population-based search algorithm that explores a space of potential neural network architectures in three steps, namely, a population initialization step that is based on prior-knowledge from hand-crafted architectures, an exploration step comprising crossover and mutation of architectures, and finally an exploitation step that utilizes the hidden useful knowledge stored in the entire history of evaluated neural architectures in the form of a Bayesian Network. Experimental results suggest that combining the dual objectives of minimizing an error metric and computational complexity, as measured by FLOPs, allows NSGA-Net to find competitive neural architectures. Moreover, NSGA-Net achieves error rate on the CIFAR-10 dataset on par with other state-of-the-art NAS methods while using orders of magnitude less computational resources. These results are encouraging and shows the promise to further use of EC methods in various deep-learning paradigms.",10.1145/3321707.3321729,"deep learning, image classification, multi objective, bayesian optimization, neural architecture search",,0.0,1.0
What's inside the Black-Box? A Genetic Programming Method for Interpreting Complex Machine Learning Models,"Evans, Benjamin P. and Xue, Bing and Zhang, Mengjie",Association for Computing Machinery,2019.0,"Interpreting state-of-the-art machine learning algorithms can be difficult. For example, why does a complex ensemble predict a particular class? Existing approaches to interpretable machine learning tend to be either local in their explanations, apply only to a particular algorithm, or overly complex in their global explanations. In this work, we propose a global model extraction method which uses multi-objective genetic programming to construct accurate, simplistic and model-agnostic representations of complex black-box estimators. We found the resulting representations are far simpler than existing approaches while providing comparable reconstructive performance. This is demonstrated on a range of datasets, by approximating the knowledge of complex black-box models such as 200 layer neural networks and ensembles of 500 trees, with a single tree.",10.1145/3321707.3321726,"interpretable machine learning, evolutionary multi-objective optimisation, explainable artificial intelligence",,0.0,1.0
WGNCS: A Robust Hybrid Cross-Version Defect Model via Multi-Objective Optimization and Deep Enhanced Feature Representation,"Zhang, Nana and Ying, Shi and Ding, Weiping and Zhu, Kun and Zhu, Dandan",Elsevier Science Inc.,2021.0,,10.1016/j.ins.2021.05.008,"Wasserstein GAN with Gradient Penalty, Convolutional neural network, Cross-version defect prediction, Multi-objective feature selection, Deep learning techniques",,0.0,1.0
Towards Enhancing Fault Tolerance in Neural Networks,"Duddu, Vasisht and Rao, D Vijay and Balas, Valentina",Association for Computing Machinery,2020.0," Deep Learning Accelerators and Neuromorphic hardware, used in many real-time safety-critical applications, are prone to faults that manifest in the form of errors in Neural Networks. Fault Tolerance in Neural Networks is a critical attribute for applications that require reliable computation for long duration such as IoT and mobile devices. The inherent fault tolerance of Neural Networks can be improved with regularization, however, the current techniques exhibit a trade-off between generalization and classification accuracy. To this extent, in this work, a Neural Network is modelled as two distinct functional components: a Feature Extractor with an unsupervised learning objective and a Fully Connected Classifier with a supervised learning objective. Traditional approaches to train the entire network using a single supervised learning objective are insufficient to achieve the objectives of the individual functional goals optimally. In this work, a novel two phase framework with multi-criteria objective function combining unsupervised training of the Feature Extractor followed by supervised training of the Classifier Network is proposed. In the Phase I, the unsupervised training of the Feature Extractor is modeled using two games solved simultaneously in the presence of Neural Networks with conflicting objectives. The first game with a generative model, trains the Feature Extractor to generate robust features for the input image by minimizing a reconstruction loss between the input and reconstructed image. The second game with a binary classification network, updates the Feature Extractor to smoothen the feature space and match with a prior Gaussian distribution. In Phase II, the resultant Feature Extractor, which is strongly regularized, is combined with the Fully Connected Classifier for fine-tuning on the classification task. The proposed two phase training algorithm is evaluated on four architectures with varying model complexity on standard image classification datasets: FashionMNIST and CIFAR10. The proposed framework is scalable and independent of the network architecture that provides superior tolerance to stuck at “0” faults as compared to existing regularization functions without loss in classification accuracy.",10.1145/3448891.3448936,"Fault Tolerance, Regularization., Neural Networks, Adversarial Game, Classification",,0.0,1.0
Automated Feature Engineering for Algorithmic Fairness,"Salazar, Ricardo and Neutatz, Felix and Abedjan, Ziawasch",VLDB Endowment,2021.0,"One of the fundamental problems of machine ethics is to avoid the perpetuation and amplification of discrimination through machine learning applications. In particular, it is desired to exclude the influence of attributes with sensitive information, such as gender or race, and other causally related attributes on the machine learning task. The state-of-the-art bias reduction algorithm Capuchin breaks the causality chain of such attributes by adding and removing tuples. However, this horizontal approach can be considered invasive because it changes the data distribution. A vertical approach would be to prune sensitive features entirely. While this would ensure fairness without tampering with the data, it could also hurt the machine learning accuracy. Therefore, we propose a novel multi-objective feature selection strategy that leverages feature construction to generate more features that lead to both high accuracy and fairness. On three well-known datasets, our system achieves higher accuracy than other fairness-aware approaches while maintaining similar or higher fairness.",10.14778/3461535.3463474,,,0.0,1.0
Evolutionary Discovery of Coresets for Classification,"Barbiero, Pietro and Squillero, Giovanni and Tonda, Alberto",Association for Computing Machinery,2019.0,"When a machine learning algorithm is able to obtain the same performance given a complete training set, and a small subset of samples from the same training set, the subset is termed coreset. As using a coreset improves training speed and allows human experts to gain a better understanding of the data, by reducing the number of samples to be examined, coreset discovery is an active line of research. Often in literature the problem of coreset discovery is framed as i. single-objective, attempting to find the candidate coreset that best represents the training set, and ii. independent from the machine learning algorithm used. In this work, an approach to evolutionary coreset discovery is presented. Building on preliminary results, the proposed approach uses a multi-objective evolutionary algorithm to find compromises between two conflicting objectives, i. minimizing the number of samples in a candidate coreset, and ii. maximizing the accuracy of a target classifier, trained with the coreset, on the whole original training set. Experimental results on popular classification benchmarks show that the proposed approach is able to identify candidate coresets with better accuracy and generality than state-of-the-art coreset discovery algorithms found in literature.",10.1145/3319619.3326846,"evolutionary algorithms, multi-objective, explain AI, coreset discovery, classification, machine learning",,0.0,1.0
FTT-NAS: Discovering Fault-Tolerant Neural Architecture,"Li, Wenshuo and Ning, Xuefei and Ge, Guangjun and Chen, Xiaoming and Wang, Yu and Yang, Huazhong",IEEE Press,2020.0,"With the fast evolvement of deep-learning specific embedded computing systems, applications powered by deep learning are moving from the cloud to the edge. When deploying NNs onto the edge devices under complex environments, there are various types of possible faults: soft errors caused by atmospheric neutrons and radioactive impurities, voltage instability, aging, temperature variations, and malicious attackers. Thus the safety risk of deploying neural networks at edge computing devices in safety-critic applications is now drawing much attention. In this paper, we implement the random bit-flip, Gaussian, and Salt-and-Pepper fault models and establish a multi-objective fault-tolerant neural architecture search framework. On top of the NAS framework, we propose Fault-Tolerant Neural Architecture Search (FT-NAS) to automatically discover convolutional neural network (CNN) architectures that are reliable to various faults in nowadays edge devices. Then we incorporate fault-tolerant training (FTT) in the search process to achieve better results, which we called FTT-NAS. Experiments show that the discovered architecture FT-NAS-Net and FTT-NAS-Net outperform other hand-designed baseline architectures (58.1%/86.6% VS. 10.0%/52.2%), with comparable FLOPs and less parameters. What is more, the architectures trained under a single fault model can also defend against other faults. By inspecting the discovered architecture, we find that there are redundant connections learned to protect the sensitive paths. This insight can guide future fault-tolerant neural architecture design, and we verify it by a modification on ResNet-20–ResNet-M.",10.1109/ASP-DAC47756.2020.9045324,,,0.0,1.0
Semi-Supervised Extensions of Multi-Task Tree Ensembles,"Ad\i{}yeke, Esra and Baydo\u{g}an, Mustafa G\""{o}k\c{c}e",Elsevier Science Inc.,2022.0,,10.1016/j.patcog.2021.108393,"Ensemble learning, Multi-task learning, Totally randomized trees, Semi-supervised learning, Multi-objective trees",,0.0,1.0
Predictive Models with End User Preference,"Zhao, Yifan and Yang, Xian and Bolnykh, Carolina and Harenberg, Steve and Korchiev, Nodirbek and Yerramsetty, Saavan Raj and Vellanki, Bhanu Prasad and Kodumagulla, Ramakanth and Samatova, Nagiza F.","John Wiley &amp; Sons, Inc.",2022.0,"Classical machine learning models typically try to optimize the model based on the most discriminatory features of the data; however, they do not usually account for end user preferences. In certain applications, this can be a serious issue as models not aware of user preferences could become costly, untrustworthy, or privacy‐intrusive to use, thus becoming irrelevant and/or uninterpretable. Ideally, end users with domain knowledge could propose preferable features that the predictive model could then take into account. In this paper, we propose a generic modeling method that respects end user preferences via a relative ranking system to express multi‐criteria preferences and a regularization term in the model's objective function to incorporate the ranked preferences. In a more generic perspective, this method is able to plug user preferences into existing predictive models without creating completely new ones. We implement this method in the context of decision trees and are able to achieve a comparable classification accuracy while reducing the use of undesirable features.",10.1002/sam.11545,"decision tree, predictive model, child support, relative ranking, user preference, regularization",,0.0,1.0
