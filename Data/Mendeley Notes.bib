@article{Kumar1999,
   abstract = {Generalisation is a non-trivial problem in machine learning and more so with neural networks which have the capabilities of inducing varying degrees of freedom. It is influenced by many factors in network design, such as network size, initial conditions, learning rate, weight decay factor, pruning algorithms, and many more. In spite of continuous research efforts, we could not arrive at a practical solution which can offer a superior generalisation. We present a novel approach for handling complex problems of machine learning. A multiobjective genetic algorithm is used for identifying (near-) optimal subspaces for hierarchical learning. This strategy of explicitly partitioning the data for subsequent mapping onto a hierarchical classifier is found both to reduce the learning complexity and the classification time. The classification performance of various algorithms is compared and it is argued that the neural modules are superior for learning the localised decision surfaces of such partitions and offer better generalisation.},
   author = {Rajeev Kumar},
   doi = {10.1109/ICCIMA.1999.798512},
   isbn = {0769503004},
   journal = {Proceedings - 3rd International Conference on Computational Intelligence and Multimedia Applications, ICCIMA 1999},
   note = {\{'id': 1,<br/>'keep': True\}},
   pages = {112-116},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {On generalisation of machine learning with neural-evolutionary computations},
   year = {1999},
}
@article{Ahmad1992,
   author = {Ziauddin Ahmad and Allon Guez and Ziauddin Ahmad and Allon Guez},
   doi = {10.1117/12.140027},
   issn = {0277-786X},
   journal = {SPIE},
   month = {9},
   note = {\{'id': 2,<br/>'keep': False\}},
   pages = {486-492},
   publisher = {SPIE},
   title = {Preliminary report on machine learning via multiobjective optimization},
   volume = {1709},
   url = {https://ui.adsabs.harvard.edu/abs/1992SPIE.1709..486A/abstract},
   year = {1992},
}
@article{Kim2002,
   abstract = {Feature subset selection is important not only for the insight gained from determining relevant modeling variables but also for the improved understandability, scalability, and possibly, accuracy of the resulting models. Feature selection has traditionally been studied in supervised learning situations, with some estimate of accuracy used to evaluate candidate subsets. However, we often cannot apply supervised learning for lack of a training signal. For these cases, we propose a new feature selection approach based on clustering. A number of heuristic criteria can be used to estimate the quality of clusters built from a given feature subset. Rather than combining such criteria, we use ELSA, an evolutionary local selection algorithm that maintains a diverse population of solutions that approximate the Pareto front in a multi-dimensional objective space. Each evolved solution represents a feature subset and a number of clusters; two representative clustering algorithms, K-means and EM, are applied to form the given number of clusters based on the selected features. Experimental results on both real and synthetic data show that the method can consistently find approximate Pareto-optimal solutions through which we can identify the significant features and an appropriate number of clusters. This results in models with better and clearer semantic relevance. © 2002-IOS Press. All rights reserved.},
   author = {Yongseog Kim and W. Nick Street and Filippo Menczer},
   doi = {10.3233/IDA-2002-6605},
   issn = {1088-467X},
   issue = {6},
   journal = {Intelligent Data Analysis},
   month = {1},
   note = {\{'id': 3,<br/>'keep': True, <br/>'Optimization': ["Evolutionary", "ELSA"], <br/>'ML task': ["Feature Selection","Clustering"], <br/>'Objective functions': ["cluster cohesiveness", "distance from glocal centroid","number of clusters","complexity"], <br/>'Single/Multi Solutions': "Multi" <br/>\}},
   pages = {531-556},
   publisher = {IOS Press},
   title = {Evolutionary model selection in unsupervised learning},
   volume = {6},
   year = {2002},
}
@article{Abbass2003,
   abstract = {The formation of a neural network ensemble has attracted much attention in the machine learning literature. A set of trained neural networks are combined using a post-gate to form a single super-network. One main challenge in the literature is to decide on which...},
   author = {Hussein A. Abbass},
   doi = {10.1007/978-3-540-24581-0_47},
   isbn = {9783540206460},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   note = {\{'id': 4,<br/>'keep': True, <br/>'Optimization': ["Evolutive","MPANN"], <br/>'ML task': ["Neural Networks","Classification","Ensemble"], <br/>'Objective functions': ["MSE for two data partitions"], <br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pages = {554-566},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Pareto Neuro-Ensembles},
   volume = {2903},
   url = {https://link.springer.com/chapter/10.1007/978-3-540-24581-0_47},
   year = {2003},
}
@article{,
   abstract = {This paper presents a cooperative coevolutive approach for designing neural network ensembles. Cooperative coevolution is a recent paradigm in evolutionary computation that allows the effective modeling of cooperative environments. Although theoretically, a single neural network with a sufficient number of neurons in the hidden layer would suffice to solve any problem, in practice many real-world problems are too hard to construct the appropriate network that solve them. In such problems, neural network ensembles are a successful alternative. Nevertheless, the design of neural network ensembles is a complex task. In this paper, we propose a general framework for designing neural network ensembles by means of cooperative coevolution. The proposed model has two main objectives: first, the improvement of the combination of the trained individual networks; second, the cooperative evolution of such networks, encouraging collaboration among them, instead of a separate training of each network. In order to favor the cooperation of the networks, each network is evaluated throughout the evolutionary process using a multiobjective method. For each network, different objectives are defined, considering not only its performance in the given problem, but also its cooperation with the rest of the networks. In addition, a population of ensembles is evolved, improving the combination of networks and obtaining subsets of networks to form ensembles that perform better than the combination of all the evolved networks. The proposed model is applied to ten real-world classificatio n problems of a very different nature from the UCI machine learning repository and proben1 benchmark set. In all of them the performance of the model is better than the performance of standard ensembles in terms of generalization error. Moreover, the size of the obtained ensembles is also smaller. © 2005 IEEE.},
   author = {Nicolás García-Pedrajas and César Hervás-Martínez and Domingo Ortiz-Boyer},
   doi = {10.1109/TEVC.2005.844158},
   issn = {1089778X},
   issue = {3},
   journal = {IEEE Transactions on Evolutionary Computation},
   keywords = {Classification,Cooperative coevolution,Multi-objective optimization,Neural network ensembles},
   month = {6},
   note = {\{'id': 5,<br/>'keep': True,<br/>'Optimization': ["Evolutionary", "NSGA"],<br/>'ML task': ["Classification","Ensemble","Neural Networks"],<br/>'Objective functions': "Several performanca and diversity measures",<br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pages = {271-302},
   title = {Cooperative coevolution of artificial neural network ensembles for pattern classification},
   volume = {9},
   year = {2005},
}
@article{Chen2006,
   abstract = {This paper proposes to incorporate evolutionary multiobjective algorithm and Bayeslan Automatic Relevance Determination (ARD) to automatically design and train ensemble. The algorithm determines almost all the parameters of ensemble automatically. Our algorithm adopts different feature subsets, selected by Bayeslan ARD, to maintain accuracy and promote diversity among Individual NNs in an ensemble. The multiobjective evaluation of the fitness of the networks encourages the networks with lower error rate and fewer features. The proposed algorithm is applied to several realworld classification problems and in all cases the performance of the method is better than the performance of other ensemble construction algorithms. © 2006 IEEE.},
   author = {Huanhuan Chen and Xin Yao},
   doi = {10.1109/CEC.2006.1688318},
   isbn = {0780394879},
   journal = {2006 IEEE Congress on Evolutionary Computation, CEC 2006},
   note = {\{'id': 6,<br/>'keep': True, <br/>'Optimization': ["Evolutive","NSGA"], <br/>'ML task': ["Neural Networks","Classification","Ensemble"], <br/>'Objective functions': ["MSE for two data partitions"], <br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pages = {267-274},
   title = {Evolutionary multiobjective ensemble learning based on Bayesian feature selection},
   year = {2006},
}
@article{Lemczyk2006,
   abstract = {The conversion and extension of the Incremental Pareto-Coevolution Archive algorithm (IPCA) into the domain of Genetic Programming classifier evolution is presented. In order to accomplish efficiency in regards to classifier evaluation on training data, the coevolutionary aspect of the IPCA algorithm is utilized to simultaneously evolve a subset of the training data that provides distinctions between candidate classifiers. The algorithm is compared in terms of classification "score" (equal weight to detection rate, and 1 - false positive rate), and run-time against a traditional GP classifier using the entinety of the training data for evaluation, and a GP classifier which performs Dynamic Subset Selection. The results indicate that the presented algorithm outperforms the subset, selection algorithm in terms of classification score, and outperforms the traditional classifier while requiring roughly 1/430 of the wall-clock time.},
   author = {Vlichal Lemczyk and Malcolm Heywood},
   doi = {10.1145/1143997.1144162},
   isbn = {1595931864},
   journal = {GECCO 2006 - Genetic and Evolutionary Computation Conference},
   keywords = {Co-evolution,Evolutionary Computation,Genetic Programming,Subset Selection,Super-vised Learning,Supervised Learning,and Search General Terms Algorithms},
   note = {\{'id': 7,<br/>'keep': False\}},
   pages = {945-946},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Pareto-coevolutionary genetic programming classifier},
   volume = {1},
   url = {http://www.ics.uci.edu/},
   year = {2006},
}
@article{Kondo2006,
   abstract = {In this paper, evolutionary multi-objective selection method of RBF networks structure and its application to the ensemble learning is considered. The candidates of RBF network structure are encoded into the chromosomes in GAs. Then, they evolve toward Pareto-optimal front defined by several objective functions concerning with model accuracy, model complexity and model smoothness. RBF network ensemble is constructed of the obtained Pareto-optimal models since such models are diverse. This method is applied to the pattern classification problem. Experiments on the benchmark problem demonstrate that the proposed method has comparable generalization ability to conventional ensemble methods. © 2006 IEEE.},
   author = {Nobuhiko Kondo and Toshiharu Hatanaka and Katsuji Uosaki},
   doi = {10.1109/IJCNN.2006.247224},
   isbn = {0780394909},
   issn = {10987576},
   journal = {IEEE International Conference on Neural Networks - Conference Proceedings},
   note = {\{'id': 8,<br/>'keep': True,<br/>'Optimization': ["Evolutionary", "NSGA-II"],<br/>'ML task': ["RBF Networks","Ensemble","Classification"],<br/>'Objective functions': ["number of hidden layer neuron", "MSE", "sum of absolute weights"],<br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pages = {2919-2925},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Pattern classification by evolutionary RBF networks ensemble based on multi-objective optimization},
   year = {2006},
}
@article{Nojima2006,
   abstract = {In this paper, we propose a multi-classifier coding scheme and an entropy-based diversity criterion in evolutionary multiobjective optimization algorithms for the design of fuzzy ensemble classifiers. In a multi-classifier coding scheme, an ensemble classifier is coded as an integer string. Each string is evaluated by using its accuracy and diversity. We use two accuracy criteria. One is the overall classification rate of the string as an ensemble classifier. The other is the average classification rate of component classifiers in the ensemble classifier. As a diversity criterion, we use the entropy of outputs from component classifiers in the ensemble classifier. We examine four formulations based on the above criteria through computational experiments on benchmark data sets in the UCI machine learning repository. The experimental results show the effectiveness of the multi-classifier coding scheme and the entropy-based diversity criterion. © 2006 IEEE.},
   author = {Yusuke Nojima and Hisao Ishibuchi},
   doi = {10.1109/HIS.2006.264942},
   isbn = {0769526624},
   journal = {Proceedings - Sixth International Conference on Hybrid Intelligent Systems and Fourth Conference on Neuro-Computing and Evolving Intelligence, HIS-NCEI 2006},
   note = {\{'id': 9,<br/>'keep': False\}},
   pages = {59},
   publisher = {IEEE Computer Society},
   title = {Designing fuzzy ensemble classifiers by evolutionary multiobjective optimization with an entropy-based diversity criterion},
   year = {2006},
}
@article{Mierswa2006,
   abstract = {In this work we propose a novel, sound framework for evolutionary feature selection in unsupervised machine learning problems. We show that unsupervised feature selection is inhemulti-objectiverently multi-objective and behaves differently from supervised feature selection in that the number of features must be maximized instead of being minimized. Although this might sound surprising from a supervised learning point of view, we exemplify this relationship on the problem of data clustering and show that existing approaches do not pose the optimization problem in an appropriate way. Another important consequence of this paradigm change is a method which segments the Pareto sets produced by our approach. Inspecting only prototypical points from these segments drastically reduces the amount of work for selecting a final solution. We compare our methods against existing approaches on eight data sets. Copyright 2006 ACM.},
   author = {Ingo Mierswa and Michael Wurst},
   doi = {10.1145/1143997.1144248},
   isbn = {1595931864},
   journal = {GECCO 2006 - Genetic and Evolutionary Computation Conference},
   keywords = {Multi-objective feature selection,Pareto front segmentation,Unsupervised learning,unsupervised learning},
   note = {\{'id': 10,<br/>'keep': True, <br/>'Optimization': ["Evolutive","NSGA-II"], <br/>'ML task': ["Feature Selection","Clustering"], <br/>'Objective functions': ["Number of features","DB index"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {1545-1552},
   publisher = {Association for Computing Machinery},
   title = {Information preserving multi-objective feature selection for unsupervised learning},
   volume = {2},
   year = {2006},
}
@article{Tan2006,
   abstract = {This paper presents a dual-objective evolutionary algorithm (DOEA) for extracting multiple decision rule lists in data mining, which aims at satisfying the classification criteria of high accuracy and ease of user comprehension. Unlike existing approaches, the algorithm incorporates the concept of Pareto dominance to evolve a set of non-dominated decision rule lists each having different classification accuracy and number of rules over a specified range. The classification results of DOEA are analyzed and compared with existing rule-based and non-rule based classifiers based upon 8 test problems obtained from UCI Machine Learning Repository. It is shown that the DOEA produces comprehensible rules with competitive classification accuracy as compared to many methods in literature. Results obtained from box plots and t-tests further examine its invariance to random partition of datasets.},
   author = {K. C. Tan and Q. Yu and J. H. Ang},
   doi = {10.1007/S10589-005-3907-9},
   issn = {1573-2894},
   issue = {2},
   journal = {Computational Optimization and Applications 2006 34:2},
   keywords = {Convex and Discrete Geometry,Management Science,Operations Research,Operations Research/Decision Theory,Optimization,Statistics,general},
   month = {3},
   note = {\{'id': 11,<br/>'keep': True,<br/>'Optimization': ["Evolutionary"],<br/>'ML task': ["decision rules","Classification"],<br/>'Objective functions': ["Accuracy", "number of rules"],<br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {273-294},
   publisher = {Springer},
   title = {A Dual-Objective Evolutionary Algorithm for Rules Extraction in Data Mining},
   volume = {34},
   url = {https://link.springer.com/article/10.1007/s10589-005-3907-9},
   year = {2006},
}
@article{Jin2006,
   abstract = {Handling catastrophic forgetting is an interesting and challenging topic in modeling the memory mechanisms of the human brain using machine learning models. From a more general point of view, catastrophic forgetting reflects the stability-plasticity dilemma, which is one of the several dilemmas to be addressed in learning systems: to retain the stored memory while learning new information. Different to the existing approaches, we introduce a Pareto-optimality based multi-objective learning framework for alleviating catastrophic learning. Compared to the single-objective learning methods, multi-objective evolutionary learning with the help of pseudorehearsal is shown to be more promising in dealing with the stability-plasticity dilemma. © 2006 IEEE.},
   author = {Yaochu Jin and Bernhard Sendhoff},
   doi = {10.1109/IJCNN.2006.247332},
   isbn = {0780394909},
   issn = {10987576},
   journal = {IEEE International Conference on Neural Networks - Conference Proceedings},
   note = {\{'id': 12,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II"], <br/>'ML task': ["Catastrophic Forgetting","Neural Networks"], <br/>'Objective functions': ["Error on new patterns","Error on pseudo-patterns"], <br/>'Single/Multi Solutions': "Single"<br/>\}},
   pages = {3335-3342},
   title = {Alleviating catastrophic forgetting via multi-objective learning},
   year = {2006},
}
@article{Handl2006,
   abstract = {In previous work, we have shown that both unsupervised feature selection and the semi-supervised clustering problem can be usefully formulated as multiobjective optimization problems. In this paper, we discuss the logical extension of this prior work to cover the problem of semi-supervised feature selection. Our extensive experimental results provide evidence for the advantages of semi-supervised feature selection when both labelled and unlabelled data are available. Moreover, the particular effectiveness of a Pareto-based optimization approach can also be seen. © 2006 IEEE.},
   author = {Julia Handl and Joshua Knowles},
   doi = {10.1109/IJCNN.2006.247330},
   isbn = {0780394909},
   issn = {10987576},
   journal = {IEEE International Conference on Neural Networks - Conference Proceedings},
   note = {\{'id': 13,<br/>'keep': True, <br/>'Optimization': ["Evolutive","PESA-II"], <br/>'ML task': ["Feature Selection","Semi-supervised"], <br/>'Objective functions': ["Silhoute Width","Feature cardinality","Adjusted Rand Index"], <br/>'Single/Multi Solutions': "Single - Solution that performs best in supervised"<br/>\}},
   pages = {3319-3326},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Semi-supervised feature selection via multiobjective optimization},
   year = {2006},
}
@article{Jin2008,
   abstract = {Machine learning is inherently a multiobjective task. Traditionally, however, either only one of the objectives is adopted as the cost function or multiple objectives are aggregated to a scalar cost function. This can be mainly attributed to the fact that most conventional learning algorithms can only deal with a scalar cost function. Over the last decade, efforts on solving machine learning problems using the Pareto-based multiobjective optimization methodology have gained increasing impetus, particularly due to the great success of multiobjective optimization using evolutionary algorithms and other population-based stochastic search methods. It has been shown that Pareto-based multiobjective learning approaches are more powerful compared to learning algorithms with a scalar cost function in addressing various topics of machine learning, such as clustering, feature selection, improvement of generalization ability, knowledge extraction, and ensemble generation. One common benefit of the different multiobjective learning approaches is that a deeper insight into the learning problem can be gained by analyzing the Pareto front composed of multiple Pareto-optimal solutions. This paper provides an overview of the existing research on multiobjective machine learning, focusing on supervised learning. In addition, a number of case studies are provided to illustrate the major benefits of the Pareto-based approach to machine learning, e.g., how to identify interpretable models and models that can generalize on unseen data from the obtained Pareto-optimal solutions. Three approaches to Pareto-based multiobjective ensemble generation are compared and discussed in detail. Finally, potentially interesting topics in multiobjective machine learning are suggested. © 2008 IEEE.},
   author = {Yaochu Jin and Berhard Sendhoff},
   doi = {10.1109/TSMCC.2008.919172},
   issn = {10946977},
   issue = {3},
   journal = {IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews},
   keywords = {Ensemble,Evolutionary multiobjective optimization,Generalization,Machine learning,Multiobjective learning,Multiobjective optimization,Neural networks,Pareto optimization},
   month = {5},
   note = {\{'id': 14,<br/>'keep': False,<br/>'obs': "Overview"\}},
   pages = {397-415},
   title = {Pareto-based multiobjective machine learning: An overview and case studies},
   volume = {38},
   year = {2008},
}
@article{Kondo2007,
   author = {Nobuhiko Kondo and Toshiharu Hatanaka and Katsuji Uosaki},
   doi = {10.1007/978-3-540-46375-7_50},
   journal = {Frontiers of Computational Science},
   note = {\{'id': 15,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Genetic"], <br/>'ML task': ["Ensebmle","RBF Networks","Classification"], <br/>'Objective functions': ["Accuracy","Complexity"], <br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pages = {321-325},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Multiobjective Evolutionary RBF Networks and Its Application to Ensemble Learning},
   url = {https://link.springer.com/chapter/10.1007/978-3-540-46375-7_50},
   year = {2007},
}
@article{Mierswa2007,
   abstract = {Recently, evolutionary computation has been successfully integrated into statistical learning methods. A Support Vector Machine (SVM) using evolution strategies for its optimization problem frequently deliver better results with respect to the optimization criterion and the prediction accuracy. Moreover, evolutionary computation allows for the efficient large margin optimization of a huge family of new kernel functions, namely non-positive semidefinite kernels as the Epanechnikov kernel. For these kernel functions, evolutionary SVM even outperform other learning methods like the Relevance Vector Machine. In this paper, we will discuss another major advantage of evolutionary SVM compared to traditional SVM solutions: we can explicitly optimize the inherent trade-off between training error and model complexity by embedding multi-objective optimization into the evolutionary SVM. This leads to three advantages: first, it is no longer necessary to tune the SVM parameter C which weighs both conflicting criteria. This is a very time-consuming task for traditional SVM. Second, the shape and size of the Pareto front give interesting insights about the complexity of the learning task at hand. Finally, the user can actually see the point where overfitting occurs and can easily select a solution from the Pareto front best suiting his or her needs.},
   author = {Ingo Mierswa},
   city = {New York, New York, USA},
   doi = {10.1145/1276958},
   isbn = {9781595936974},
   journal = {Proceedings of the 9th annual conference on Genetic and evolutionary computation  - GECCO '07},
   keywords = {Experimentation Keywords: Support vector machines,I26 [Computing Methodologies]: Learning General Terms: Algorithms,Theory,evolution strategies,kernel methods,machine learning},
   note = {\{'id': 16,<br/>'keep': True, <br/>'Optimization': ["Evolutive","NSGA-II"], <br/>'ML task': ["Classification","SVM"], <br/>'Objective functions': ["Dual of max margin and min error"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   publisher = {ACM Press},
   title = {Controlling Overfitting with Multi-Objective Support Vector Machines},
   year = {2007},
}
@article{He2007,
   abstract = {In this paper, we propose a maximum separation margin (MSM) training method for multiple-prototype(MP)-based pattern classifiers in which a sample separation margin defined as the distance from the training sample to the classification boundary can be calculated precisely. Similar to support vector machine (SVM) methodology, MSM training is formulated as a multicriteria optimization problem which aims at maximizing the separation margin and minimizing the empirical error rate on training data simultaneously. By making certain relaxation assumptions, MSM training can be reformulated as a semidefinite programming (SDP) problem that can be solved efficiently by some standard optimization algorithms designed for SDP. Evaluation experiments are conducted on the task of the recognition of most confusable Kanji character pairs identified from popular Nakayosi and Kuchibue handwritten Japanese character databases. It is observed that the MSM-trained MP-based classifier achieves a similar character recognition accuracy as that of the state-of-the-art SVM-based classifier, yet requires much fewer classifier parameters. © 2007 IEEE.},
   author = {Tingting He and Yu Hu and Qiang Huo},
   doi = {10.1109/ICASSP.2007.366313},
   isbn = {1424407281},
   issn = {15206149},
   journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
   keywords = {Large margin,Machine learning,Pattern classification,Semidefinite programming,Support vector machine},
   note = {\{'id': 17,<br/>'keep': False\}},
   title = {An approach to large margin design of prototype-based pattern classifiers},
   volume = {2},
   year = {2007},
}
@article{Kokshenev2008,
   abstract = {In this paper, the problem of multi-objective supervised learning is discussed within the non-evolutionary optimization framework. The proposed MOBJ learning algorithm performs the search of Pareto-optimal models determining weights, width, prototype vectors, and the quantity of basis functions of the RBF network. In combination with the Akaike information criterion, the algorithm provides high quality solutions. © 2008 IEEE.},
   author = {Illya Kokshenev and Antonio Padua Braga},
   doi = {10.1109/SBRN.2008.39},
   isbn = {9780769533612},
   journal = {Proceedings - 10th Brazilian Symposium on Neural Networks, SBRN 2008},
   note = {\{'id': 18,<br/>'keep': True, <br/>'Optimization': ["Evolutive"], <br/>'ML task': ["Classification","RBF-Networks"], <br/>'Objective functions': ["MSE","L1"], <br/>'Single/Multi Solutions': "Single - AIC in Pareto fronteir"<br/>\}},
   pages = {9-14},
   title = {A multi-objective learning algorithm for RBF neural network},
   year = {2008},
}
@article{,
   abstract = {This paper presents a new algorithm that approximates real function evaluations using supervised learning with a surrogate method called support vector machine (SVM). We perform a comparative study among different leader selection schemes in a Multi-Objective Particle Swarm Optimizer (MOPSO), in order to determine the most appropriate approach to be adopted for solving the sort of problems of our interest. The resulting hybrid presents a poor spread of solutions, which motivates the introduction of a second phase to our algorithm, in which an approach called rough sets is adopted in order to improve the spread of solutions along the Pareto front. Rough sets are used as a local search engine, which is able to generate solutions in the neighborhood of the nondominated solutions previously generated by the surrogate-based algorithm. The resulting approach is able to generate reasonably good approximations of the Pareto front of problems of up to 30 decision variables with only 2,000 fitness function evaluations. Our results are compared with respect to the NSGA-II, which is a multi-objective evolutionary algorithm representative of the state-of-the-art in the area. ©2008 IEEE.},
   author = {Luis V. Santana-Quintero and Carlos A. Coello Coello and Alfredo G. Hernández-Díaz and Jesús Moisés Osorio Velázquez},
   doi = {10.1109/SIS.2008.4668300},
   isbn = {9781424427055},
   journal = {2008 IEEE Swarm Intelligence Symposium, SIS 2008},
   keywords = {Hybrid algorithms,Multi-objective optimization,PSO,Rough sets,Support vector machines,Surrogates},
   note = {\{'id': 19,<br/>'keep': False\}},
   title = {Surrogate-based multi-objective particle swarm optimization},
   year = {2008},
}
@article{,
   abstract = {Multi-dimensional classification is a generalization of supervised classification that considers more than one class variable to classify. In this paper we review the existing multi-dimesional Bayesian classifiers and introduce a new one: the KDB multi-dimensional classifier. Then we define different classification rules for multi-dimensional scope. Finally, we introduce a structural learning approach of a multi-dimensional Bayesian classifier based on the multi-objective evolutionary algorithm NSGA-II. The solution of the learning approach is a Pareto front representing different multi-dimensional classifiers and their accuracy values for the different classes, so a decision maker can easily choose the classifier which is more interesting for the particular problem and domain. © 2008 IEEE.},
   author = {Juan D. Rodríguez and Jose A. Lozano},
   doi = {10.1109/HIS.2008.143},
   isbn = {9780769533261},
   journal = {Proceedings - 8th International Conference on Hybrid Intelligent Systems, HIS 2008},
   note = {\{'id': 20,<br/>'keep': True, <br/>'Optimization': ["Evolutive","NSGA-II"], <br/>'ML task': ["Classification","Bayesian classifiers"], <br/>'Objective functions': ["Error for each classification variable"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {501-506},
   title = {Multi-objective learning of multi-dimensional Bayesian classifiers},
   year = {2008},
}
@article{Silva2009,
   abstract = {Support Vector Machines (SVMs) are considered state-of-the-art learning machines techniques for classification problems. This paper studies the training of SVMs in the special case of problems in which the raw data to be used for training purposes is composed of both labeled and unlabeled data - the semi-supervised learning problem. This paper proposes the definition of an intermediate problem of attributing labels to the unlabeled data as a multiobjective optimization problem, with the conflicting objectives of minimizing the classification error over the training data set and maximizing the regularity of the resulting classifier. This intermediate problem is solved using an evolutionary multiobjective algorithm, the SPEA2. Simulation results are presented in order to illustrate the suitability of the proposed technique. © 2009 IEEE.},
   author = {Cidiney Silva and Jésus S. Santos and Elizabeth F. Wanner and Eduardo G. Carrano and Ricardo H.C. Takahashi},
   doi = {10.1109/CEC.2009.4983321},
   isbn = {9781424429592},
   journal = {2009 IEEE Congress on Evolutionary Computation, CEC 2009},
   note = {\{'id': 21,<br/>'keep': True,<br/>'Optimization': ["Evolutionary", "SPEA2"],<br/>'ML task': ["SVM","Classification","semi-supervised"],<br/>'Objective functions': ["error over the labeled points", "norm of the vector of weights"],<br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {2996-3002},
   title = {Semi-supervised training of least squares support vector machine using a multiobjective evolutionary algorithm},
   year = {2009},
}
@article{,
   abstract = {This paper proposes a multi-classification pattern algorithm using multilayer perceptron neural network models which try to boost two conflicting main objectives of a classifier, a high correct classification rate and a high classification rate for each class. To solve this machine learning problem, we consider a Memetic Pareto Evolutionary approach based on the 2SGA2 algorithm (MPE2SGA2), where we defined two objectives for determining the goodness of a classifier: the cross-entropy error function and the variation coefficient of its sensitivities, because both measures are continuous functions, making the convergence more robust. Once the Pareto front is built, we use an automatic selection methodology of individuals: the best model in accuracy (upper extreme in the Pareto front). This methodology is applied to solve six benchmark classification problems, obtaining promising results and achieving a high classification rate in the generalization set with an acceptable level of accuracy for each class. © 2009 IEEE.},
   author = {J. C. Fernández and C. Hervás and F. J. Martínez and M. Cruz},
   doi = {10.1109/ISDA.2009.153},
   isbn = {9780769538723},
   journal = {ISDA 2009 - 9th International Conference on Intelligent Systems Design and Applications},
   note = {\{'id': 22,<br/>'keep': True,<br/>'Optimization': ["Evolutionary", "MPENSGA2"],<br/>'ML task': ["Neural Network","Classification"],<br/>'Objective functions': ["Cross entropy", "Variation coefficient"],<br/>'Single/Multi Solutions': "Single"<br/>\}},
   pages = {408-413},
   title = {Design of artificial neural networks using a memetic pareto evolutionary algorithm using as objectives entropy versus variation coefficient},
   year = {2009},
}
@article{Qasem2009,
   abstract = {In this paper, an adaptive evolutionary multiobjective selection method of RBF Networks structure is discussed. The candidates of RBF Network structures are encoded into particles in Particle Swarm Optimization (PSO). These particles evolve toward Pareto-optimal front defined by several objective functions with model accuracy and complexity. The problem of unsupervised and supervised learning is discussed with Adaptive Multi-Objective PSO (AMOPSO). This study suggests an approach of RBF Network training through simultaneous optimization of architectures and weights with Adaptive PSO-based multi-objective algorithm. Our goal is to determine whether Adaptive Multi-objective PSO can train RBF Networks, and the performance is validated on accuracy and complexity. The experiments are conducted on two benchmark datasets obtained from the machine learning repository. The results show that our proposed method provides an effective means for training RBF Networks that is competitive with PSO-based multi-objective algorithm. ©2009 IEEE.},
   author = {Sultan Noman Qasem and Siti Mariyam Hj Shamsuddin},
   doi = {10.1109/ICSMC.2009.5346876},
   isbn = {9781424427949},
   issn = {1062922X},
   journal = {Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
   keywords = {Adaptive multiobjective particle swarm optimization,Multi-objective particle swarm optimization,Radial basis function network},
   note = {\{'id': 23,<br/>'keep': True, <br/>'Optimization': ["Evolutive","AMPSO"], <br/>'ML task': ["Classification","RBF Networks"], <br/>'Objective functions': ["MSE","Sum of square weights"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {534-540},
   title = {Improving generalization of radial basis function network with adaptive multi-objective particle swarm optimization},
   year = {2009},
}
@article{Qasem2009,
   abstract = {The problem of unsupervised and supervised learning is discussed within the context of multi-objective optimization. In this paper, an evolutionary multi-objective selection method of RBF Networks structure is discussed. The candidates of RBF Network structure are encoded into the particles in PSO. Then, they evolve toward Pareto-optimal front defined by several objective functions concerning with model accuracy and model complexity. This study suggests an approach of RBF Network training through simultaneous optimization of architectures and weights with PSO-based multi-objective algorithm. Our goal is to determine whether Multi-objective PSO can train RBF Networks, and the performance is validated on accuracy and complexity. The experiments are conducted on benchmark datasets obtained from the UCI machine learning repository. The results show that our proposed method provides an effective means for training RBF Networks that is competitive with other evolutionary computational-based methods. © 2009 IEEE.},
   author = {Sultan Noman Qasem and Siti Mariyam Hj Shamsuddin},
   doi = {10.1109/ISMA.2009.5164833},
   isbn = {9781424434817},
   journal = {2009 6th International Symposium on Mechatronics and its Applications, ISMA 2009},
   note = {\{'id': 24,<br/>'keep': True, <br/>'Optimization': ["Evolutive","MOPSO"], <br/>'ML task': ["Classification","RBF Networks"], <br/>'Objective functions': ["MSE","Sum of square weights"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   title = {Radial basis function network based on multi-objective particle swarm optimization},
   year = {2009},
}
@article{Ishibuchi2009,
   abstract = {Recently evolutionary multiobjective optimization (EMO) algorithms have been actively used for the design of accurate and interpretable fuzzy rule-based systems. This research area is often referred to as multiobjective genetic fuzzy systems where EMO algorithms are used to search for a number of non-dominated fuzzy rule-based systems with respect to their accuracy and interpretability. The main advantage of the use of EMO algorithms for fuzzy system design over single-objective optimizers is that multiple alternative fuzzy rule-based systems with different accuracy-interpretability tradeoffs are obtained by their single run. The decision maker can choose a single fuzzy rule-based system according to their preference. There still exist several important issues to be discussed in this research area such as the definition of interpretability, the formulation of interpretability measures, the visualization of tradeoff relations, and the interpretability of the explanation of fuzzy reasoning results. In this paper, we discuss the ability of EMO algorithms as multiobjective optimizers to search for Pareto optimal or near Pareto optimal fuzzy rule-based systems. More specifically, we examine whether EMO algorithms can find non-dominated fuzzy rule-based systems that approximate the entire Pareto fronts of multiobjective fuzzy system design problems. ©2009 IEEE.},
   author = {Hisao Ishibuchi and Yusuke Nakashima and Yusuke Nojima},
   doi = {10.1109/FUZZY.2009.5277370},
   isbn = {9781424435975},
   issn = {10987584},
   journal = {IEEE International Conference on Fuzzy Systems},
   note = {\{'id': 25,<br/>'keep': False\}},
   pages = {1724-1729},
   title = {Search ability of evolutionary multiobjective optimization algorithms for multiobjective fuzzy genetics-based machine learning},
   year = {2009},
}
@article{Ethridge2010,
   abstract = {Using a machine learning algorithm for a given application often requires tuning design parameters of the classifier to obtain optimal classification performance without overfitting. In this contribution, we present an evolutionary algorithm based approach for multi-objective optimization of the sensitivity and specificity of a ν-SVM. The ν-SVM is often preferred over the standard C-SVM due to smaller dynamic range of the ν parameter compared to the unlimited dynamic range of the C parameter. Instead of looking for a single optimization result, we look for a set of optimal solutions that lie along the Pareto optimality front. The traditional advantage of using the Pareto optimality is of course the flexibility to choose any of the solutions that lies on the Pareto optimality front. However, we show that simply maximizing sensitivity and specificity over the Pareto front leads to parameters that appear to be mathematically optimal yet still cause overfitting. We propose a multiple objective optimization approach with three objective functions to find additional parameter values that do not cause overfitting. © 2010 IEEE.},
   author = {James Ethridge and Gregory Ditzler and Robi Polikar},
   doi = {10.1109/CEC.2010.5586029},
   isbn = {9781424469109},
   journal = {2010 IEEE World Congress on Computational Intelligence, WCCI 2010 - 2010 IEEE Congress on Evolutionary Computation, CEC 2010},
   keywords = {evolutionary algorithms,multi-objective optimization,ν-SVM},
   note = {\{'id': 26,<br/>'keep': True,<br/>'Optimization': ["Evolutionary", "NSGA-II"],<br/>'ML task': ["v-SVM","Classification"],<br/>'Objective functions': ["Sensitivity", "Specificity", "Generalization error"],<br/>'Single/Multi Solutions': "Multi"<br/>\}},
   title = {Optimal ν-SVM parameter estimation using multi objective evolutionary algorithms},
   year = {2010},
}
@article{Chen2010,
   abstract = {Negative Correlation Learning (NCL) [CHECK END OF SENTENCE], [CHECK END OF SENTENCE] is a neural network ensemble learning algorithm which introduces a correlation penalty term to the cost function of each individual network so that each neural network minimizes its mean-square-error (MSE) together with the correlation. This paper describes NCL in detail and observes that the NCL corresponds to training the entire ensemble as a single learning machine that only minimizes the MSE without regularization. This insight explains that NCL is prone to overfitting the noise in the training set. The paper analyzes this problem and proposes the multiobjective regularized negative correlation learning (MRNCL) algorithm which incorporates an additional regularization term for the ensemble and uses the evolutionary multiobjective algorithm to design ensembles. In MRNCL, we define the crossover and mutation operators and adopt nondominated sorting algorithm with fitness sharing and rank-based fitness assignment. The experiments on synthetic data as well as real-world data sets demonstrate that MRNCL achieves better performance than NCL, especially when the noise level is nontrivial in the data set. In the experimental discussion, we give three reasons why our algorithm outperforms others. © 2006 IEEE.},
   author = {Huanhuan Chen and Xin Yao},
   doi = {10.1109/TKDE.2010.26},
   issn = {10414347},
   issue = {12},
   journal = {IEEE Transactions on Knowledge and Data Engineering},
   keywords = {Multiobjective algorithm,multiobjective learning,negative correlation learning,neural network ensembles,neural networks,regularization},
   note = {\{'id': 27,<br/>'keep': True,<br/>'Optimization': ["Evolutionary"],<br/>'ML task': ["Ensemble","Negative Correlation Learning","RBF","Classification"],<br/>'Objective functions': ["MSE", "Correlation measure", "weight decay"],<br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pages = {1738-1751},
   title = {Multiobjective neural network ensembles based on regularized negative correlation learning},
   volume = {22},
   year = {2010},
}
@article{Coelho2010,
   abstract = {The recent years have witnessed a growing interest in two advanced strategies to cope with the data clustering problem, namely, clustering ensembles and multi-objective clustering. In this paper, we present a genetic programming based approach that can be considered as a hybrid of these strategies, thereby allowing that different hierarchical clustering ensembles be simultaneously evolved taking into account complementary validity indices. Results of computational experiments conducted with artificial and real datasets indicate that, in most of the cases, at least one of the Pareto optimal partitions returned by the proposed approach compares favorably or go in par with the consensual partitions yielded by two well-known clustering ensemble methods in terms of clustering quality, as gauged by the corrected Rand index. © 2010 Elsevier B.V.},
   author = {André L.V. Coelho and Everlândio Fernandes and Katti Faceli},
   doi = {10.1016/J.NEUCOM.2010.09.014},
   issn = {0925-2312},
   issue = {1-3},
   journal = {Neurocomputing},
   keywords = {Cluster analysis,Ensembles,Genetic programming,Multi-objective optimization},
   month = {12},
   note = {\{'id': 28,<br/>'keep': False<br/>\}},
   pages = {494-498},
   publisher = {Elsevier},
   title = {Inducing multi-objective clustering ensembles with genetic programming},
   volume = {74},
   year = {2010},
}
@article{Ekbal2010,
   abstract = {Appropriate feature selection is a very crucial issue in any machine learning framework, specially in Maximum Entropy (ME). In this paper, the selection of appropriate features for constructing a ME based Named Entity Recognition (NER) system is posed as a multiobjective optimization (MOO) problem. Two classification quality measures, namely recall and precision are simultaneously optimized using the search capability of a popular evolutionary MOO technique, NSGA-II. The proposed technique is evaluated to determine suitable feature combinations for NER in two languages, namely Bengali and English that have significantly different characteristics. Evaluation results yield the recall, precision and F-measure values of 70.76%, 81.88% and 75.91%, respectively for Bengali, and 78.38%, 81.27% and 79.80%, respectively for English. Comparison with an existing ME based NER system shows that our proposed feature selection technique is more efficient than the heuristic based feature selection. © 2010 IEEE.},
   author = {Asif Ekbal and Sriparna Saha and Christoph S. Garbe},
   doi = {10.1109/ICPR.2010.477},
   isbn = {9780769541099},
   issn = {10514651},
   journal = {Proceedings - International Conference on Pattern Recognition},
   keywords = {Feature selection,Maximum entropy,Multiobjective optimization,Named entity recognition},
   note = {\{'id': 29,<br/>'keep': True,<br/>'Optimization': ["Evolutionary", "NSGA-II"],<br/>'ML task': ["Feature Selection","Named  Entity Recognition"],<br/>'Objective functions': ["recall", "precision"],<br/>'Single/Multi Solutions': "Single - Performance in training"<br/>\}},
   pages = {1937-1940},
   title = {Feature selection using multiobjective optimization for named entity recognition},
   year = {2010},
}
@article{Reynolds2010,
   abstract = {The target of machine learning is a predictive model that performs well on unseen data. Often, such a model has multiple intended uses, related to different points in the tradeoff between (e.g.) sensitivity and specificity. Moreover, when feature selection is...},
   author = {Alan P. Reynolds and David W. Corne and Michael J. Chantler},
   doi = {10.1007/978-3-642-15844-5_39},
   isbn = {3642158439},
   issn = {03029743},
   issue = {PART 1},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   note = {\{'id': 30,<br/>'keep': True, <br/>'Optimization': ["Evolutionary", "NSGA-II"], <br/>'ML task': ["Feature Selection","Classification","Binary Classification"], <br/>'Objective functions': ["Specificity", "Sensitivity", "Confidence"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {384-393},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Feature Selection for Multi-purpose Predictive Models: A Many-Objective Task},
   volume = {6238 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-15844-5_39},
   year = {2010},
}
@article{Silva2010,
   abstract = {This paper presents a novel algorithm for multiobjective training of Radial Basis Function (RBF) networks based on least-squares and Particle Swarm Optimization methods. The formulation is based on the fundamental concept that supervised learning is a bi-objective optimization problem, in which two conflicting objectives should be minimized. The objectives are related to the empirical training error and the machine complexity. The training is done in three steps: i) a conventional minimization of the training error, ii) multiobjective least-squares optimization for the linear parameters and, iii) particle swarm optimization for the nonlinear parameters. Some results are presented and they show the effectiveness of the proposed approach. © 2010 IEEE.},
   author = {G. R.L. Silva and D. A.G. Vieira and A. C. Lisboa and Vasile Palade},
   doi = {10.1109/ICTAI.2010.112},
   isbn = {9780769542638},
   issn = {10823409},
   journal = {Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
   note = {\{'id': 31,<br/>'keep': True, <br/>'Optimization': ["Evolutionary", "Particle Swarm"], <br/>'ML task': ["RBF"], <br/>'Objective functions': ["Least Squares", "Complexity - Gradient"], <br/>'Single/Multi Solutions': "?"<br/>\}},
   pages = {282-285},
   title = {On a multiobjective training algorithm for RBF networks using particle swarm optimization},
   volume = {2},
   year = {2010},
}
@article{Kaylani2010,
   abstract = {In this paper, we present the evolution of adaptive resonance theory (ART) neural network architectures (classifiers) using a multiobjective optimization approach. In particular, we propose the use of a multiobjective evolutionary approach to simultaneously evolve the weights and the topology of three well-known ART architectures; fuzzy ARTMAP (FAM), ellipsoidal ARTMAP (EAM), and Gaussian ARTMAP (GAM). We refer to the resulting architectures as MO-GFAM, MO-GEAM, and MO-GGAM, and collectively as MO-GART. The major advantage of MO-GART is that it produces a number of solutions for the classification problem at hand that have different levels of merit [accuracy on unseen data (generalization) and size (number of categories created)]. MO-GART is shown to be more elegant (does not require user intervention to define the network parameters), more effective (of better accuracy and smaller size), and more efficient (faster to produce the solution networks) than other ART neural network architectures that have appeared in the literature. Furthermore, MO-GART is shown to be competitive with other popular classifiers, such as classification and regression tree (CART) and support vector machines (SVMs). © 2006 IEEE.},
   author = {Assem Kaylani and Michael Georgiopoulos and Mansooreh Mollaghasemi and Georgios C. Anagnostopoulos and Christopher Sentelle and Mingyu Zhong},
   doi = {10.1109/TNN.2009.2037813},
   issn = {10459227},
   issue = {4},
   journal = {IEEE Transactions on Neural Networks},
   keywords = {ARTMAP,Category proliferation,Classification,Genetic algorithms (GAs),Genetic operators,Machine learning},
   month = {4},
   note = {\{'id': 32,<br/>'keep': True, <br/>'Optimization': ["Evolutionary"], <br/>'ML task': ["Classification", "ART"], <br/>'Objective functions': ["Accuracy", "Complexity - Size"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {529-550},
   pmid = {20172827},
   title = {An adaptive multiobjective approach to evolving ART architectures},
   volume = {21},
   year = {2010},
}
@article{,
   abstract = {This paper proposes a multiclassification algorithm using multilayer perceptron neural network models. It tries to boost two conflicting main objectives of multiclassifiers: a high correct classification rate level and a high classification rate for each class. This last objective is not usually optimized in classification, but is considered here given the need to obtain high precision in each class in real problems. To solve this machine learning problem, we use a Pareto-based multiobjective optimization methodology based on a memetic evolutionary algorithm. We consider a memetic Pareto evolutionary approach based on the NSGA2 evolutionary algorithm (MPENSGA2). Once the Pareto front is built, two strategies or automatic individual selection are used: the best model in accuracy and the best model in sensitivity (extremes in the Pareto front). These methodologies are applied to solve 17 classification benchmark problems obtained from the University of California at Irvine (UCI) repository and one complex real classification problem. The models obtained show high accuracy and a high classification rate for each class. © 2010 IEEE.},
   author = {Juan Carlos Fernández Caballero and Francisco José Martinez and César Hervas and Pedro Antonio Gutierrez},
   doi = {10.1109/TNN.2010.2041468},
   issn = {10459227},
   issue = {5},
   journal = {IEEE Transactions on Neural Networks},
   keywords = {Accuracy,Local search,Multiclassification,Multiobjective evolutionary algorithms,Neural networks,Sensitivity},
   month = {5},
   note = {\{'id': 33,<br/>'keep': True, <br/>'Optimization': ["Evolutionary", "MPENSGA2"], <br/>'ML task': ["Classification", "Multiclass", "Multilayer Perceptron"], <br/>'Objective functions': ["Accuracy", "Sensitivity"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {750-770},
   pmid = {20227976},
   title = {Sensitivity versus accuracy in multiclass problems using memetic pareto evolutionary neural networks},
   volume = {21},
   year = {2010},
}
@article{Kokshenev2010,
   abstract = {Most of modern multi-objective machine learning methods are based on evolutionary optimization algorithms. They are known to be global convergent, however, usually deliver nondeterministic results. In this work we propose the deterministic global solution to a multi-objective problem of supervised learning with the methodology of nonlinear programming. As the result, the proposed multi-objective algorithm performs a global search of Pareto-optimal hypotheses in the space of RBF networks, determining their weights and basis functions. In combination with the Akaike and Bayesian information criteria, the algorithm demonstrates a high generalization efficiency on several synthetic and real-world benchmark problems. © 2010 Elsevier B.V.},
   author = {Illya Kokshenev and Antonio Padua Braga},
   doi = {10.1016/J.NEUCOM.2010.06.022},
   issn = {0925-2312},
   issue = {16-18},
   journal = {Neurocomputing},
   keywords = {Model selection,Multi-objective learning,Pareto-optimality,Radial-basis functions,Regularization},
   month = {10},
   note = {\{'id': 34,<br/>'keep': True, <br/>'Optimization': ["Deterministic"], <br/>'ML task': ["RBF"], <br/>'Objective functions': ["Squared Error", "Complexity"], <br/>'Single/Multi Solutions': "Single - Information Criteria"<br/>\}},
   pages = {2799-2808},
   publisher = {Elsevier},
   title = {An efficient multi-objective learning algorithm for RBF neural network},
   volume = {73},
   year = {2010},
}
@article{Giusti2010,
   abstract = {Most Machine Learning systems target into inducing classifiers with optimal coverage and precision measures. Although this constitutes a good approach for prediction, it might not provide good results when the user is more interested in description. In this case, the induced models should present other properties such as novelty, interestingness and so forth. In this paper we present a research work based in Multi-Objective Evolutionary Computing to construct individual knowledge rules targeting arbitrary user-defined criteria via objective quality measures such as precision, support, novelty etc. This paper also presents a comparison among multi-objective and ranking composition techniques. It is shown that multi-objective-based methods attain better results than ranking-based methods, both in terms of solution dominance and diversity of solutions in the Pareto front. © 2010 IEEE.},
   author = {Rafael Giusti and Gustavo E.A.P.A. Batista},
   doi = {10.1109/ICMLA.2010.25},
   isbn = {9780769543000},
   journal = {Proceedings - 9th International Conference on Machine Learning and Applications, ICMLA 2010},
   keywords = {Evolutionary computing,Knowledge discovery in databases,Multi-objective machine learning},
   note = {\{'id': 35,<br/>'keep': False<br/>\}},
   pages = {119-124},
   title = {Discovering knowledge rules with multi-objective evolutionary computing},
   year = {2010},
}
@article{McIntyre2011,
   abstract = {Intuitively population based algorithms such as genetic programming provide a natural environment for supporting solutions that learn to decompose the overall task between multiple individuals, or ...},
   author = {Andrew R. McIntyre and Malcolm I. Heywood},
   doi = {10.1162/EVCO_A_00016},
   issn = {10636560},
   issue = {1},
   journal = {Evolutionary Computation},
   keywords = {Genetic programming,Pareto multi-objective optimization,classification,coevolution,problem decomposition},
   month = {3},
   note = {\{'id': 36,<br/>'keep': True, <br/>'Optimization': ["Evolutionary"], <br/>'ML task': ["Classification", "Multiclass"], <br/>'Objective functions': ["Sensitivity", "Specificity"], <br/>'Single/Multi Solutions': "Multi?"<br/>\}},
   pages = {137-166},
   pmid = {20879899},
   publisher = {
		MIT Press
		PUB1010
		Cambridge, MA, USA
	},
   title = {Classification as Clustering: A Pareto Cooperative-Competitive GP Approach},
   volume = {19},
   url = {https://dl.acm.org/doi/abs/10.1162/EVCO_a_00016},
   year = {2011},
}
@article{Zhang2011,
   abstract = {In this paper, we present a generic, optimising feature extraction method using multiobjective genetic programming. We re-examine the feature extraction problem and show that effective feature extraction can significantly enhance the performance of pattern recognition systems with simple classifiers. A framework is presented to evolve optimised feature extractors that transform an input pattern space into a decision space in which maximal class separability is obtained. We have applied this method to real world datasets from the UCI Machine Learning and StatLog databases to verify our approach and compare our proposed method with other reported results. We conclude that our algorithm is able to produce classifiers of superior (or equivalent) performance to the conventional classifiers examined, suggesting removal of the need to exhaustively evaluate a large family of conventional classifiers on any new problem. © 2010 Elsevier B.V. All rights reserved.},
   author = {Yang Zhang and Peter I. Rockett},
   doi = {10.1016/J.ASOC.2010.02.008},
   issn = {1568-4946},
   issue = {1},
   journal = {Applied Soft Computing},
   keywords = {Feature extraction,Genetic programming,Multiobjective optimisation,Pattern recognition},
   month = {1},
   note = {\{'id': 37,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Genetic","SPEA-2"], <br/>'ML task': ["Feature Extraction", "Classification"], <br/>'Objective functions': ["Tree Complexity", "Missclassification Error", "Bayes Error"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {1087-1097},
   publisher = {Elsevier},
   title = {A generic optimising feature extraction method using multiobjective genetic programming},
   volume = {11},
   year = {2011},
}
@article{Bhowan2011,
   abstract = {Machine learning algorithms can suffer a performance bias when data sets are unbalanced. This paper proposes a Multi-objective Genetic Programming approach using negative correlation learning to evolve accurate and diverse ensembles of non-dominated solutions where members vote on class membership. We also compare two popular Pareto-based fitness schemes on the classification tasks. We show that the evolved ensembles achieve high accuracy on both classes using six unbalanced binary data sets, and that this performance is usually better than many of its individual members .},
   author = {Urvesh Bhowan and Mark Johnston and Mengjie Zhang},
   city = {New York, New York, USA},
   doi = {10.1145/2001576},
   isbn = {9781450305570},
   journal = {Proceedings of the 13th annual conference on Genetic and evolutionary computation - GECCO '11},
   keywords = {Algorithms Keywords Genetic Programming,Class Imbalance,Classification,Evolutionary Multi-objective Opti-misation,I28 [Problem Solving,Search]: Heuristic methods; I52 [Design Methodology]: Classifier design and evaluation General Terms Design},
   note = {\{'id': 38,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Genetic","NSGA-II"], <br/>'ML task': ["Ensemble", "Classification"], <br/>'Objective functions': ["Minority Accuracy", "Majority Accuracy"], <br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   publisher = {ACM Press},
   title = {Evolving Ensembles in Multi-objective Genetic Programming for Classification with Unbalanced Data},
   year = {2011},
}
@article{Ishibuchi2011,
   abstract = {Recently, evolutionary multiobjective optimization (EMO) algorithms have been utilized for the design of accurate and interpretable fuzzy rule-based systems. This research area is often referred to as multiobjective genetic fuzzy systems (MoGFS), where EMO algorithms are used to search for non-dominated fuzzy rule-based systems with respect to their accuracy and interpretability. In this paper, we examine the ability of EMO algorithms to efficiently search for Pareto optimal or near Pareto optimal fuzzy rule-based systems for classification problems. We use NSGA-II (elitist non-dominated sorting genetic algorithm), its variants, and MOEA/D (multiobjective evolutionary algorithm based on decomposition) in our multiobjective fuzzy genetics-based machine learning (MoFGBML) algorithm. Classification performance of obtained fuzzy rule-based systems by each EMO algorithm is evaluated for training data and test data under various settings of the available computation load and the granularity of fuzzy partitions. Experimental results in this paper suggest that reported classification performance of MoGFS in the literature can be further improved using more computation load, more efficient EMO algorithms, and/or more antecedent fuzzy sets from finer fuzzy partitions. © 2010 Springer-Verlag.},
   author = {Hisao Ishibuchi and Yusuke Nakashima and Yusuke Nojima},
   doi = {10.1007/S00500-010-0669-9/TABLES/10},
   issn = {14327643},
   issue = {12},
   journal = {Soft Computing},
   keywords = {Evolutionary multiobjective optimization,Fuzzy rule-based classification,Genetic algorithms,Genetics-based machine learning,Multiobjective machine learning},
   month = {12},
   note = {\{'id': 39,<br/>'keep': False\}},
   pages = {2415-2434},
   publisher = {Springer},
   title = {Performance evaluation of evolutionary multiobjective optimization algorithms for multiobjective fuzzy genetics-based machine learning},
   volume = {15},
   url = {https://link.springer.com/article/10.1007/s00500-010-0669-9},
   year = {2011},
}
@article{Tian2012,
   abstract = {Ensemble approaches to classification have attracted a great deal of interest recently. This paper presents a novel method for designing the neural network ensemble using coevolutionary algorithm. The bootstrap resampling procedure is employed to obtain different training subsets that are used to estimate different component networks of the ensemble. Then the cooperative coevolutionary algorithm is developed to optimize the ensemble model via the divide-and-cooperative mechanism. All component networks are coevolved in parallel in the scheme of interacting co-adapted subpopulations. The fitness of an individual from a particular subpopulation is assessed by associating it with the representatives from other subpopulations. In order to promote the cooperation of all component networks, the proposed method considers both the accuracy and the diversity among the component networks that are evaluated using the multi-objective Pareto optimality measure. A hybrid output-combination method is designed to determine the final ensemble output. Experimental results illustrate that the proposed method is able to obtain neural network ensemble models with better classification accuracy in comparison with currently popular ensemble algorithms. © 2011 Elsevier Ltd All rights reserved.},
   author = {Jin Tian and Minqiang Li and Fuzan Chen and Jisong Kou},
   doi = {10.1016/J.PATCOG.2011.09.012},
   issn = {0031-3203},
   issue = {4},
   journal = {Pattern Recognition},
   keywords = {Classification,Coevolutionary algorithm,Ensemble learning,Neural network},
   month = {4},
   note = {\{'id': 40,<br/>'keep': False\}},
   pages = {1373-1385},
   publisher = {Pergamon},
   title = {Coevolutionary learning of neural network ensemble for complex classification tasks},
   volume = {45},
   year = {2012},
}
@article{Peng2012,
   abstract = {Determining the number of clusters in a data set is an essential yet difficult step in cluster analysis. Since this task involves more than one criterion, it can be modeled as a multiple criteria decision making (MCDM) problem. This paper proposes a multiple criteria decision making (MCDM)-based approach to estimate the number of clusters for a given data set. In this approach, MCDM methods consider different numbers of clusters as alternatives and the outputs of any clustering algorithm on validity measures as criteria. The proposed method is examined by an experimental study using three MCDM methods, the well-known clustering algorithm–k-means, ten relative measures, and fifteen public-domain UCI machine learning data sets. The results show that MCDM methods work fairly well in estimating the number of clusters in the data and outperform the ten relative measures considered in the study.},
   author = {Yi Peng and Yong Zhang and Gang Kou and Yong Shi},
   doi = {10.1371/JOURNAL.PONE.0041713},
   issn = {1932-6203},
   issue = {7},
   journal = {PLOS ONE},
   keywords = {Breast cancer,Clustering algorithms,Diabetes mellitus,Inflammation,Ionosphere,Machine learning,Machine learning algorithms,Yeast},
   month = {7},
   note = {\{'id': 41,<br/>'keep': False<br/>\}},
   pages = {e41713},
   pmid = {22870181},
   publisher = {Public Library of Science},
   title = {A Multicriteria Decision Making Approach for Estimating the Number of Clusters in a Data Set},
   volume = {7},
   url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0041713},
   year = {2012},
}
@article{Dutta2012,
   abstract = {Clustering is unsupervised learning where ideally class levels and number of clusters (K) are not known. K-clustering can be categorized as semi-supervised learning where K is known. Here we have considered K-Clustering with simultaneous feature selection. Feature subset selection helps to identify relevant features for clustering, increase understandability, better scalability and improve accuracy. Here we have used two measures, intra-cluster distance (Homogeneity, H) and inter-cluster distances (Separation, S) for clustering. Measures are using mod distance per feature suitable for categorical features (attributes). Rather than combining H and S to frame the problem as single objective optimization problem, we use multi objective genetic algorithm (MOGA) to find out diverse solutions near to Pareto optimal front in the two-dimensional objective space. Each evolved solution represents a set of cluster modes (CMs) build by selected feature subset. Here, K-modes is hybridized with MOGA. We have used hybridized GA to combine global searching powers of GA with local searching powers of K-modes. Considering context sensitivity, we have used a special crossover operator called 'pairwise crossover' and 'substitution'. The main contribution of this paper is simultaneous dimensionality reduction and optimization of objectives using MOGA. Results on 3 benchmark data sets from UCI Machine Learning Repository containing categorical features shows the superiority of the algorithm. © 2012 IEEE.},
   author = {Dipankar Dutta and Paramartha Dutta and Jaya Sil},
   doi = {10.1109/HIS.2012.6421332},
   isbn = {9781467351157},
   journal = {Proceedings of the 2012 12th International Conference on Hybrid Intelligent Systems, HIS 2012},
   note = {\{'id': 42,<br/>'keep': True, <br/>'Optimization': ["Evolutive","Genetic"], <br/>'ML task': ["Feature Selection","Clustering"], <br/>'Objective functions': ["Homogeneity","Separation"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {191-196},
   title = {Simultaneous feature selection and clustering for categorical features using multi objective genetic algorithm},
   year = {2012},
}
@article{Saha2012,
   abstract = {Semi-supervised clustering uses the information of unsupervised and supervised learning to overcome the problems associated with them. Extracted information are given in the form of class labels and data distribution during clustering process. In this paper the problem of semi-supervised clustering is formulated under the framework of multiobjective optimization (MOO). Thereafter, a multiobjective based clustering technique is extended to solve the semi-supervised clustering problem. The newly developed semi-supervised multiobjective clustering algorithm (Semi-GenClustMOO), is used for appropriate partitioning of data into appropriate number of clusters. Four objective functions are optimized, out of which first three use some unsupervised information and the last one uses supervised information. These four objective functions represent, respectively, the, total compactness of the partitioning, total symmetry present in the clusters, cluster connectedness and Adjust Rand Index. These four objective functions are optimized simultaneously using AMOSA, a newly developed simulated annealing based multiobjective optimization method. Results show that it can easily detect the appropriate number of clusters as well as the appropriate partitioning from data sets having either well-separated clusters of any shape or symmetrical clusters with or without overlaps. Seven artificial and four real-life data sets have been used for evaluation to show the effectiveness of the Semi-GenClustMOO technique. In each case class information of 10% randomly chosen data point is known to us 1. © 2012 IEEE.},
   author = {Sriparna Saha and Asif Ekbal and Abhay Kumar Alok},
   doi = {10.1109/HIS.2012.6421361},
   isbn = {9781467351157},
   journal = {Proceedings of the 2012 12th International Conference on Hybrid Intelligent Systems, HIS 2012},
   keywords = {AMOSA,Adjusted Rand Index (ARI),Cluster validity index,Con-index,I-Index,Multiobjective optimization,Semi-supervised clustering,Sym-index},
   note = {\{'id': 43,<br/>'keep': True, <br/>'Optimization': ["Evolutive","AMOSA"], <br/>'ML task': ["Semi-supervised","Clustering"], <br/>'Objective functions': ["Compactedness of partitions","Total symmetry","Cluster connectedness","Adjusted Rand Index"], <br/>'Single/Multi Solutions': "Single - ARI value"<br/>\}},
   pages = {360-365},
   title = {Semi-supervised clustering using multiobjective optimization},
   year = {2012},
}
@article{Dutta2012,
   abstract = {The aim of the paper is to study a real coded multi objective genetic algorithm based K-clustering, where K represents the number of clusters, may be known or unknown. If the value of K is known, it is called K-clustering algorithm. The searching power of Genetic Algorithm (GA) is exploited to get for proper clusters and centers of clusters in the feature space to optimize simultaneously intra-cluster distance (Homogeneity) (H) and inter-cluster distances (Separation) (S). Maximization of 1/H and S are the twin objectives of Multi Objective Genetic Algorithm (MOGA) achieved by measuring H and S using Euclidean distance metric, suitable for continuous features (attributes). We have selected 10 data sets from the UCI machine learning repository containing continuous features only to validate the proposed algorithms. All-important steps of algorithms are shown here. At the end, classification accuracies obtained by best chromosomes are shown. © 2012 IEEE.},
   author = {Dipankar Dutta and Paramartha Dutta and Jaya Sil},
   doi = {10.1109/RAIT.2012.6194619},
   isbn = {9781457706974},
   journal = {2012 1st International Conference on Recent Advances in Information Technology, RAIT-2012},
   keywords = {Clustering,Pareto optimal front,homogeneity and separation,real coded multi objective genetic algorithm},
   note = {\{'id': 44,<br/>'keep': True, <br/>'Optimization': ["Evolutive","Genetic"], <br/>'ML task': ["Clustering"], <br/>'Objective functions': ["Homogeneity","Separation"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {548-553},
   title = {Clustering by multi objective genetic algorithm},
   year = {2012},
}
@article{Torres2012,
   abstract = {This paper presents a Pareto-optimal selection strategy for multiobjective learning that is based on the geometry of the separation margin between classes. The Gabriel Graph, a method borrowed from Computational Geometry, is constructed in order to obtain margin...},
   author = {Luiz C.B. Torres and Cristiano L. Castro and Antônio P. Braga},
   doi = {10.1007/978-3-642-33266-1_13},
   isbn = {9783642332654},
   issn = {03029743},
   issue = {PART 2},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {classification,decision-making,gabriel graph,multiobjective machine learning},
   note = {\{'id': 45,<br/>'keep': True, <br/>'Optimization': [], <br/>'ML task': ["Neural Networks","Classification"], <br/>'Objective functions': ["Training Error","Norm of weights"], <br/>'Single/Multi Solutions': "Single - Separation Margin"<br/>\}},
   pages = {100-107},
   publisher = {Springer, Berlin, Heidelberg},
   title = {A Computational Geometry Approach for Pareto-Optimal Selection of Neural Networks},
   volume = {7553 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-33266-1_13},
   year = {2012},
}
@article{Zafra2012,
   abstract = {Multiple instance learning (MIL) is considered a generalization of traditional supervised learning which deals with uncertainty in the information. Together with the fact that, as in any other learning framework, the classifier performance evaluation maintains a trade-off relationship between different conflicting objectives, this makes the classification task less straightforward. This paper introduces a multi-objective proposal that works in a MIL scenario to obtain well-distributed Pareto solutions to multi-instance problems. The algorithm developed, Multi-Objective Grammar Guided Genetic Programming for Multiple Instances (MOG3P-MI), is based on grammar-guided genetic programming, which is a robust tool for classification. Thus, this proposal combines the advantages of the grammar-guided genetic programming with benefits provided by multi-objective approaches. First, a study of multi-objective optimization for MIL is carried out. To do this, three different extensions of MOG3P-MI are designed and implemented and their performance is compared. This study allows us on the one hand, to check the performance of multi-objective techniques in this learning paradigm and on the other hand, to determine the most appropriate evolutionary process for MOG3P-MI. Then, MOG3P-MI is compared with some of the most significant proposals developed throughout the years in MIL. Computational experiments show that MOG3P-MI often obtains consistently better results than the other algorithms, achieving the most accurate models. Moreover, the classifiers obtained are very comprehensible. © 2011 Springer-Verlag.},
   author = {Amelia Zafra and Sebastián Ventura},
   doi = {10.1007/S00500-011-0794-0/TABLES/13},
   issn = {14327643},
   issue = {6},
   journal = {Soft Computing},
   keywords = {Evolutionary rule learning,Grammar guided genetic programming,Multiple instance learning,Multiple objective learning},
   month = {6},
   note = {\{'id': 46,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Genetic","Grammar Guided Genetic Programming", "G3P"], <br/>'ML task': ["Multiple instance learning"], <br/>'Objective functions': ["sensitivity", "specificity"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {955-977},
   publisher = {Springer},
   title = {Multi-objective approach based on grammar-guided genetic programming for solving multiple instance problems},
   volume = {16},
   url = {https://link.springer.com/article/10.1007/s00500-011-0794-0},
   year = {2012},
}
@article{Corrente2013,
   abstract = {Multiple Criteria Decision Aiding (MCDA) offers a diversity of approaches designed for providing the decision maker (DM) with a recommendation concerning a set of alternatives (items, actions) evaluated from multiple points of view, called criteria. This paper aims at drawing attention of the Machine Learning (ML) community upon recent advances in a representative MCDA methodology, called Robust Ordinal Regression (ROR). ROR learns by examples in order to rank a set of alternatives, thus considering a similar problem as Preference Learning (ML-PL) does. However, ROR implements the interactive preference construction paradigm, which should be perceived as a mutual learning of the model and the DM. The paper clarifies the specific interpretation of the concept of preference learning adopted in ROR and MCDA, comparing it to the usual concept of preference learning considered within ML. This comparison concerns a structure of the considered problem, types of admitted preference information, a character of the employed preference models, ways of exploiting them, and techniques to arrive at a final ranking. © 2013 The Author(s).},
   author = {Salvatore Corrente and Salvatore Greco and Miłosz Kadziński and Roman Słowiński},
   doi = {10.1007/S10994-013-5365-4/FIGURES/2},
   issn = {08856125},
   issue = {2-3},
   journal = {Machine Learning},
   keywords = {Comparison,Multiple criteria decision aiding,Preference construction,Preference learning,Preference modeling,Ranking,Robust ordinal regression},
   month = {11},
   note = {\{'id': 47,<br/>'keep': False<br/>\}},
   pages = {381-422},
   publisher = {Springer},
   title = {Robust ordinal regression in preference learning and ranking},
   volume = {93},
   url = {https://link.springer.com/article/10.1007/s10994-013-5365-4},
   year = {2013},
}
@article{Dutta2013,
   abstract = {In the paper, real coded multi objective genetic algorithm based K-clustering method has been studied, K represents the number of clusters. In K-clustering algorithm value of K is known. The searching power of Genetic Algorithm (GA) is exploited to search for...},
   author = {Dipankar Dutta and Paramartha Dutta and Jaya Sil},
   doi = {10.1007/978-3-642-45318-2_7},
   isbn = {9783642453175},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Clustering,Pareto optimal front,dimensionality reduction,homogeneity and separation,real coded multi objective genetic algorithm},
   note = {\{'id': 48,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Genetic"], <br/>'ML task': ["Clustering"], <br/>'Objective functions': ["homogeneity", "separation"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {164-189},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Categorical Feature Reduction Using Multi Objective Genetic Algorithm in Cluster Analysis},
   volume = {8160},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-45318-2_7},
   year = {2013},
}
@article{Bhowan2013,
   abstract = {In classification, machine learning algorithms can suffer a performance bias when data sets are unbalanced. Data sets are unbalanced when at least one class is represented by only a small number of training examples (called the minority class), while the other class(es) make up the majority. In this scenario, classifiers can have good accuracy on the majority class, but very poor accuracy on the minority class(es). This paper proposes a multiobjective genetic programming (MOGP) approach to evolving accurate and diverse ensembles of genetic program classifiers with good performance on both the minority and majority of classes. The evolved ensembles comprise of nondominated solutions in the population where individual members vote on class membership. This paper evaluates the effectiveness of two popular Pareto-based fitness strategies in the MOGP algorithm (SPEA2 and NSGAII), and investigates techniques to encourage diversity between solutions in the evolved ensembles. Experimental results on six (binary) class imbalance problems show that the evolved ensembles outperform their individual members, as well as single-predictor methods such as canonical GP, naive Bayes, and support vector machines, on highly unbalanced tasks. This highlights the importance of developing an effective fitness evaluation strategy in the underlying MOGP algorithm to evolve good ensemble members. © 1997-2012 IEEE.},
   author = {Urvesh Bhowan and Mark Johnston and Mengjie Zhang and Xin Yao},
   doi = {10.1109/TEVC.2012.2199119},
   issn = {1089778X},
   issue = {3},
   journal = {IEEE Transactions on Evolutionary Computation},
   keywords = {Classification,class imbalance learning,genetic programming (GP),multiobjective machine learning (ML)},
   note = {\{'id': 49,<br/>'keep': True,<br/>'Optimization': ["Evolutionary", "NSGA-II", "SPEA2"],<br/>'ML task': ["Classification","Ensemble", "Unbalanced Data"],<br/>'Objective functions': ["Minority Accuracy", "Majority Accuracy"],<br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pages = {368-386},
   title = {Evolving diverse ensembles using genetic programming for classification with unbalanced data},
   volume = {17},
   year = {2013},
}
@article{Rizoiu2013,
   abstract = {Attribute-based format is the main data representation format used by machine learning algorithms. When the attributes do not properly describe the initial data, performance starts to degrade. Some algorithms address this problem by internally changing the representation space, but the newly constructed features rarely have any meaning. We seek to construct, in an unsupervised way, new attributes that are more appropriate for describing a given dataset and, at the same time, comprehensible for a human user. We propose two algorithms that construct the new attributes as conjunctions of the initial primitive attributes or their negations. The generated feature sets have reduced correlations between features and succeed in catching some of the hidden relations between individuals in a dataset. For example, a feature like sky \wedge \neg building \wedge panorama would be true for non-urban images and is more informative than simple features expressing the presence or the absence of an object. The notion of Pareto optimality is used to evaluate feature sets and to obtain a balance between total correlation and the complexity of the resulted feature set. Statistical hypothesis testing is employed in order to automatically determine the values of the parameters used for constructing a data-dependent feature set. We experimentally show that our approaches achieve the construction of informative feature sets for multiple datasets. © 2013 Springer Science+Business Media New York.},
   author = {Marian Andrei Rizoiu and Julien Velcin and Stéphane Lallich},
   doi = {10.1007/S10844-013-0235-X/FIGURES/12},
   issn = {09259902},
   issue = {3},
   journal = {Journal of Intelligent Information Systems},
   keywords = {Algorithms for data and knowledge management,Clustering,Data mining,Feature evaluation,Heuristic methods,Nonparametric statistics,Pattern analysis,Representations,Unsupervised feature construction},
   month = {6},
   note = {\{'id': 50,<br/>'keep': False<br/>\}},
   pages = {501-527},
   publisher = {Springer},
   title = {Unsupervised feature construction for improving data representation and semantics},
   volume = {40},
   url = {https://link.springer.com/article/10.1007/s10844-013-0235-x},
   year = {2013},
}
@article{Miranda2014,
   abstract = {Support Vector Machines (SVMs) have achieved a considerable attention due to their theoretical foundations and good empirical performance when compared to other learning algorithms in different applications. However, the SVM performance strongly depends on the adequate calibration of its parameters. In this work we proposed a hybrid multi-objective architecture which combines meta-learning (ML) with multi-objective particle swarm optimization algorithms for the SVM parameter selection problem. Given an input problem, the proposed architecture uses a ML technique to suggest an initial Pareto front of SVM configurations based on previous similar learning problems; the suggested Pareto front is then refined by a multi-objective optimization algorithm. In this combination, solutions provided by ML are possibly located in good regions in the search space. Hence, using a reduced number of successful candidates, the search process would converge faster and be less expensive. In the performed experiments, the proposed solution was compared to traditional multi-objective algorithms with random initialization, obtaining Pareto fronts with higher quality on a set of 100 classification problems. © 2014 Elsevier B.V.},
   author = {Péricles B.C. Miranda and Ricardo B.C. Prudêncio and André P.L.F. de Carvalho and Carlos Soares},
   doi = {10.1016/J.NEUCOM.2014.06.026},
   issn = {0925-2312},
   journal = {Neurocomputing},
   keywords = {Meta-learning,Multi-objective optimization,Parameter selection,Particles swarm optimization,Support vector machines},
   month = {11},
   note = {\{'id': 51,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Particle Swarm Optimization"], <br/>'ML task': ["SVM", "Classification", "Meta-Learning"], <br/>'Objective functions': ["success rate in classification", "number of support vectors"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {27-43},
   publisher = {Elsevier},
   title = {A hybrid meta-learning architecture for multi-objective optimization of SVM parameters},
   volume = {143},
   year = {2014},
}
@article{Bhowan2014,
   abstract = {Classification algorithms can suffer from performance degradation when the class distribution is unbalanced. This paper develops a two-step approach to evolving ensembles using genetic programming (GP) for unbalanced data. The first step uses multiobjective (MO) GP to evolve a Pareto-approximated front of GP classifiers to form the ensemble by trading-off the minority and the majority class against each other during learning. The MO component alleviates the reliance on sampling to artificially rebalance the data. The second step, which is the focus this paper, proposes a novel ensemble selection approach using GP to automatically find/choose the best individuals for the ensemble. This new GP approach combines multiple Pareto-approximated front members into a single composite genetic program solution to represent the (optimized) ensemble. This ensemble representation has two main advantages/novelties over traditional genetic algorithm (GA) approaches. First, by limiting the depth of the composite solution trees, we use selection pressure during evolution to find small highly-cooperative groups of individuals for the ensemble. This means that ensemble sizes are not fixed a priori (as in GA), but vary depending on the strength of the base learners. Second, we compare different function set operators in the composite solution trees to explore new ways to aggregate the member outputs and thus, control how the ensemble computes its output. We show that the proposed GP approach evolves smaller more diverse ensembles compared to an established ensemble selection algorithm, while still performing as well as, or better than the established approach. The evolved GP ensembles also perform well compared to other bagging and boosting approaches, particularly on tasks with high levels of class imbalance.},
   author = {Urvesh Bhowan and Mark Johnston and Mengjie Zhang and Xin Yao},
   doi = {10.1109/TEVC.2013.2293393},
   issn = {1089778X},
   issue = {6},
   journal = {IEEE Transactions on Evolutionary Computation},
   keywords = {Classification,ensemble machine learning,genetic programming,unbalanced data},
   month = {12},
   note = {\{'id': 52,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Genetic"], <br/>'ML task': ["Classification", "Ensemble"], <br/>'Objective functions': ["minority acc", "majority acc"], <br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pages = {893-908},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Reusing genetic programming for ensemble selection in classification of unbalanced data},
   volume = {18},
   year = {2014},
}
@article{,
   abstract = {This study investigates the evaluation of machine learning models based on multiple criteria. The criteria included are: predictive model accuracy, model complexity, and algorithmic complexity (related to the learning/adaptation algorithm and prediction delivery) captured by monitoring the execution time. Furthermore, it compares the models generated from optimising the criteria using two approaches. The first approach is a scalarized multi objective optimisation, where the models are generated from optimising a single cost function that combines the criteria. On the other hand the second approach uses a Pareto-based multi objective optimisation to trade-off the three criteria and to generate a set of non-dominated models. This study shows that defining universal measures for the three criteria is not always feasible. Furthermore, it was shown that, the models generated from Pareto-based multi objective optimisation approach can be more accurate and more diverse than the models generated from scalarized multi objective optimisation approach.},
   author = {Bassma Al-Jubouri and Bogdan Gabrys},
   doi = {10.1109/MCDM.2014.7007189},
   isbn = {9781479944682},
   journal = {IEEE SSCI 2014 - 2014 IEEE Symposium Series on Computational Intelligence - MCDM 2014: 2014 IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making, Proceedings},
   month = {1},
   note = {\{'id': 53,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II"], <br/>'ML task': ["Neural Networks","Classification"], <br/>'Objective functions': ["Accuracy","Model Complexity","Algorithmic Complexity"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {64-71},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Multicriteria approaches for predictive model generation: A comparative experimental study},
   year = {2015},
}
@article{You2014,
   abstract = {Regression plays a major role in many scientific and engineering problems. The goal of regression is to learn the unknown underlying function from a set of sample vectors with known outcomes. In recent years, kernel methods in regression have facilitated the estimation of nonlinear functions. However, two major (interconnected) problems remain open. The first problem is given by the bias-versus-variance tradeoff. If the model used to estimate the underlying function is too flexible (i.e., high model complexity), the variance will be very large. If the model is fixed (i.e., low complexity), the bias will be large. The second problem is to define an approach for selecting the appropriate parameters of the kernel function. To address these two problems, this paper derives a new smoothing kernel criterion, which measures the roughness of the estimated function as a measure of model complexity. Then, we use multiobjective optimization to derive a criterion for selecting the parameters of that kernel. The goal of this criterion is to find a tradeoff between the bias and the variance of the learned function. That is, the goal is to increase the model fit while keeping the model complexity in check. We provide extensive experimental evaluations using a variety of problems in machine learning, pattern recognition, and computer vision. The results demonstrate that the proposed approach yields smaller estimation errors as compared with methods in the state of the art.},
   author = {Di You and Carlos Fabian Benitez-Quiroz and Aleix M. Martinez},
   doi = {10.1109/TNNLS.2013.2297686},
   issn = {21622388},
   issue = {10},
   journal = {IEEE Transactions on Neural Networks and Learning Systems},
   keywords = {Kernel methods,Pareto optimality,kernel optimization,optimization,regression},
   month = {10},
   note = {\{'id': 54,<br/>'keep': True, <br/>'Optimization': ["Deterministic","ε-Constraint Optimization"], <br/>'ML task': ["Kernel methods","Regression"], <br/>'Objective functions': ["RMSE","Roughness Penalty"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {1879-1893},
   pmid = {25291740},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Multiobjective optimization for model selection in kernel methods in regression},
   volume = {25},
   year = {2014},
}
@article{Saha2014,
   abstract = {In this paper we have coupled feature selection problem with semi-supervised clustering. Semi-supervised clustering utilizes the information of unsupervised and supervised learning in order to overcome the problems related to them. But in general all the features present in the data set may not be important for clustering purpose. Thus appropriate selection of features from the set of all features is very much relevant from clustering point of view. In this paper we have solved the problem of automatic feature selection and semi-supervised clustering using multiobjective optimization. A recently created simulated annealing based multiobjective optimization technique titled archived multiobjective simulated annealing (AMOSA) is used as the underlying optimization technique. Here features and cluster centers are encoded in the form of a string. We assume that for each data set for 10% data points class level information are known to us. Two internal cluster validity indices reflecting different data properties, an external cluster validity index measuring the similarity between the obtained partitioning and the true labelling for 10% data points and a measure counting the number of features present in a particular string are optimized using the search capability of AMOSA. AMOSA is utilized to detect the appropriate subset of features, appropriate number of clusters as well as the appropriate partitioning from any given data set. The effectiveness of the proposed semi-supervised feature selection technique as compared to the existing techniques is shown for seven real-life data sets of varying complexities.},
   author = {Sriparna Saha and Asif Ekbal and Abhay Kumar Alok and Rachamadugu Spandana},
   doi = {10.1186/2193-1801-3-465/TABLES/3},
   issn = {21931801},
   issue = {1},
   journal = {SpringerPlus},
   keywords = {Automatic determination of number of clusters,Cluster validity indices,Clustering,Feature selection,Multi-center,Multiobjective optimization (MOO),Semi-supervised clustering,Symmetry},
   month = {8},
   note = {\{'id': 55,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","AMOSA"], <br/>'ML task': ["Feature selection", "semi-supervised clustering"], <br/>'Objective functions': ["sym-index", "xb-index", "adjusted rand index", "number of features"], <br/>'Single/Multi Solutions': "Single - Minkowski Score"<br/>\}},
   pages = {1-12},
   publisher = {SpringerOpen},
   title = {Feature selection and semi-supervised clustering using multiobjective optimization},
   volume = {3},
   url = {https://link.springer.com/articles/10.1186/2193-1801-3-465 https://link.springer.com/article/10.1186/2193-1801-3-465},
   year = {2014},
}
@article{Piltaver2014,
   abstract = {We propose a multi-objective machine learning approach guaranteed to find the Pareto optimal set of hybrid classification models consisting of comprehensible and incomprehensible sub-models. The algorithm run-times are below 1 s for typical applications despite the exponential worst-case time complexity. The user chooses the model with the best comprehensibility-accuracy trade-off from the Pareto front which enables a well informed decision or repeats finding new Pareto fronts with modified seeds. For a classification trees as the comprehensible seed, the hybrids include single black-box model, invoked in hybrid leaves. The comprehensibility of such hybrid classifiers is measured with the proportion of examples classified by the regular leaves. We propose one simple and one computationally efficient algorithm for finding the Pareto optimal hybrid trees, starting from an initial classification tree and a black-box classifier. We evaluate the proposed algorithms empirically, comparing them to the baseline solution set, showing that they often provide valuable improvements. Furthermore, we show that the efficient algorithm outperforms the NSGA-II algorithm in terms of quality of the result set and efficiency (for this optimisation problem). Finally we show that the algorithm returns hybrid classifiers that reflect the expert's knowledge on activity recognition problem well.},
   author = {Rok Piltaver and Mitja Luštrek and Jernej Zupančič and Sašo Džeroski and Matjaž Gams},
   doi = {10.3233/978-1-61499-419-0-717},
   note = {\{'id': 56,<br/>'keep': True, <br/>'Optimization': ["Deterministic", "search"], <br/>'ML task': ["Classification", "hybrid classifiers", "hybrid trees"], <br/>'Objective functions': ["accuracy", "comprehensibility"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   title = {Multi-objective learning of hybrid classifiers},
   url = {http://evaal.aaloa.org/},
   year = {2014},
}
@article{,
   abstract = {Classification is a mainstream within the machine learning community. As a result, a large number of learning algorithms have been proposed. The performance of many of these could highly depend on the chosen values of their hyper-parameters. This paper introduces a novel method for addressing the model selection problem for a given classification task. In our model selection formulation, both the learning algorithm and its hyper-parameters are considered. In our proposed approach, model selection is tackled as a multi-objective optimization problem. The empirical error, or training error, and the model complexity are defined as the objectives. We adopt a multi-objective evolutionary algorithm as the search engine, due to its high performance and its advantages for solving multi-objective problems. The model complexity is estimated experimentally, in a general fashion, for any learning algorithm, through the VC dimension. Strategies for choosing a single model or for constructing an ensemble of models from the resulting non-dominated set are also proposed. Experimental results on benchmark data sets indicate the effectiveness of the proposed approach. Furthermore, a comparative study shows that the obtained models are highly competitive, in terms of generalization performance, with other methods in the state of the art that focus on a single-learning algorithm, or a single-objective approach. © 2014 Elsevier B.V.},
   author = {Alejandro Rosales-Pérez and Jesus A. Gonzalez and Carlos A. Coello Coello and Hugo Jair Escalante and Carlos A. Reyes-Garcia},
   doi = {10.1016/J.NEUCOM.2014.05.077},
   issn = {0925-2312},
   journal = {Neurocomputing},
   keywords = {Ensemble methods,Model type selection,Multi-objective optimization,VC dimension},
   month = {12},
   note = {\{'id': 57,<br/>'keep': True, <br/>'Optimization': ["Evolutionary", "MOEA/D"], <br/>'ML task': ["Classification", "model selection"], <br/>'Objective functions': ["error", "vc dimension"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {83-94},
   publisher = {Elsevier},
   title = {Multi-objective model type selection},
   volume = {146},
   year = {2014},
}
@article{Basterrech2016,
   abstract = {In last years-especially due to the development of telecommunications-fairness modelling has received a strong attention. This article presents an approach for categorizing unknown relations according to their closeness to known relations. We consider as reference relations, the well-known: Pareto dominance, Leximin and Proportional fairness relation. We simulate each relation generating a learning dataset that is used for learning Neural Networks. The learning performance evaluation is based in several metrics, which are used as a signature of each relation. Besides, we develop a new function that gives an estimation about the closeness between relations. This concept permits us to categorise a new dataset (generated by an unknown relation) according its closeness with the Pareto dominance, Leximin and Proportional fairness relations know relations. Our experimental results are coherent with the alpha fairness concept.},
   author = {Sebastian Basterrech and Kei Ohnishi and Mario Koppen},
   doi = {10.1109/SMC.2015.365},
   isbn = {9781479986965},
   journal = {Proceedings - 2015 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2015},
   keywords = {Fairness,Maxmin Fairness,Neural Networks,Priority Fairness,Supervised Learning},
   month = {1},
   note = {\{'id': 58\}},
   pages = {2090-2095},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Neural Signature of Efficiency Relations},
   year = {2016},
}
@article{Wang2014,
   abstract = {Community detection in complex network has been an active research area in data mining and machine learning. This paper proposed a community detection method based on multi-objective evolutionary algorithm, named CDMOEA, which tries to find the Pareto front by maximize two objectives, community score and community fitness. Fast and Elitist Multi-objective Genetic Algorithm is used to attained a set of optimal solutions, and then use Modularity function to choose the best one from them. The locus based adjacency representation is used to realize genetic representation, which ensures the effective connections of the nodes in the network during the process of population Initialization and other genetic operator. Uniform crossover is introduced to ensure population's diversity. We compared it with some popular community detection algorithms in computer generated network and real world networks. Experiment results show that it is more efficient in community detection. © (2014) Trans Tech Publications, Switzerland.},
   author = {Lu Wang and Yong Quan Liang and Qi Jia Tian and Jie Yang and Chao Song and Zhuang Wu},
   doi = {10.4028/WWW.SCIENTIFIC.NET/AMM.571-572.177},
   isbn = {9783038351399},
   issn = {16627482},
   journal = {Applied Mechanics and Materials},
   keywords = {Community detection,Complex network,Modularity,Multi-objective evolutionary algorithm,Pareto front},
   note = {\{'id': 59,<br/>'keep': False<br/>\}},
   pages = {177-182},
   publisher = {Trans Tech Publications Ltd},
   title = {A community detection method based on multi-objective optimization method},
   volume = {571-572},
   year = {2014},
}
@article{Sikdar2015,
   abstract = {In this paper, we propose a multiobjective differential evolution (MODE)-based feature selection and ensemble learning approaches for entity extraction in biomedical texts. The first step of the algorithm concerns with the problem of automatic feature selection in a machine learning framework, namely conditional random field. The final Pareto optimal front which is obtained as an output of the feature selection module contains a set of solutions, each of which represents a particular feature representation. In the second step of our algorithm, we combine a subset of these classifiers using a MODE-based ensemble technique. Our experiments on three benchmark datasets namely GENIA, GENETAG and AIMed show the F-measure values of 76.75, 94.15 and 91.91 %, respectively. Comparisons with the existing systems show that our proposed algorithm achieves the performance levels which are at par with the state of the art. These results also exhibit that our method is general in nature and because of this it performs well across the several domain of datasets. The key contribution of this work is the development of MODE-based generalized feature selection and ensemble learning techniques with the aim of extracting entities from the biomedical texts of several domains.},
   author = {Utpal Kumar Sikdar and Asif Ekbal and Sriparna Saha},
   doi = {10.1007/S00500-014-1565-5/TABLES/13},
   issn = {14337479},
   issue = {12},
   journal = {Soft Computing},
   keywords = {Artificial Intelligence,Computational Intelligence,Control,Mathematical Logic and Foundations,Mechatronics,Robotics},
   month = {12},
   note = {\{'id': 60,<br/>'keep': False<br/>\}},
   pages = {3529-3549},
   publisher = {Springer Verlag},
   title = {MODE: multiobjective differential evolution for feature selection and classifier ensemble},
   volume = {19},
   url = {https://link.springer.com/article/10.1007/s00500-014-1565-5},
   year = {2015},
}
@article{Boryczka2015,
   abstract = {Data mining and visualization techniques for high-dimensional data provide helpful information to substantially augment decision-making. Optimization techniques provide a way to efficiently search for these solutions. ACO applied to data mining tasks - a decision tree construction - is one of these methods and the focus of this paper. The Ant Colony Decision Tree (ACDT) approach generates solutions efficiently and effectively but scales poorly to large problems. This article merges the methods that have been developed for better construction of decision trees by ants. The ACDT approach is tested in the context of the bi-criteria evaluation function by focusing on two problems: the size of the decision trees and the accuracy of classification obtained during ACDT performance. This approach is tested in co-learning mechanism, it means agents-ants can interact during the construction decision trees via pheromone values. This cooperation is a chance of getting better results. The proposed methodology of analysis of ACDT is tested in a number of well-known benchmark data sets from the UCI Machine Learning Repository. The empirical results clearly show that the ACDT algorithm creates good solutions which are located in the Pareto front. The software that implements the ACDT algorithm used to generate the results of this study can be downloaded freely from http://www.acdtalgorithm.com.},
   author = {Urszula Boryczka and Jan Kozak},
   doi = {10.1016/J.ASOC.2014.12.036},
   issn = {1568-4946},
   journal = {Applied Soft Computing},
   keywords = {Ant Colony Decision Trees,Ant Colony Optimization,Ant-Miner,Decision trees,Pareto front,Quality of decision trees},
   month = {5},
   note = {\{'id': 61,<br/>'keep': True, <br/>'Optimization': ["Evolutionary", "Ant Colony Optimization"], <br/>'ML task': ["Classification", "Decision trees"], <br/>'Objective functions': ["Size of decision tree", "Accuracy"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {166-178},
   publisher = {Elsevier},
   title = {Enhancing the effectiveness of Ant Colony Decision Tree algorithms by co-learning},
   volume = {30},
   year = {2015},
}
@article{Nojima2015,
   abstract = {Fuzzy genetics-based machine learning (FGBML) has frequently been used for fuzzy classifier design. It is one of the promising evolutionary machine learning (EML) techniques from the viewpoint of data mining. This is because FGBML can generate accurate classifiers with linguistically interpretable fuzzy if-then rules. Of course, a classifier with tens of thousands of if-then rules is not linguistically understandable. Thus, the complexity minimization of fuzzy classifiers should be considered together with the accuracy maximization. In previous studies, we proposed hybrid FGBML and its multiobjective formulation (MoFGBML) to handle both the accuracy maximization and the complexity minimization simultaneously. MoFGBML can obtain a number of non-dominated classifiers with different tradeoffs between accuracy and complexity. In this paper, we focus on heuristic rule generation in MoFGBML to improve the search performance. In the original heuristic rule generation, each if-then rule is generated from a randomly-selected training pattern in a heuristic manner. This operation is performed at population initialization and during evolution. To generate more generalized rules according to the training data, we propose new heuristic rule generation where each rule is generated from multiple training patterns. Through computational experiments using some benchmark data sets, we discuss the effects of the proposed operation on the search performance of our MoFGBML.},
   author = {Yusuke Nojima and Kazuhiro Watanabe and Hisao Ishibuchi},
   doi = {10.1109/CEC.2015.7257262},
   isbn = {9781479974924},
   journal = {2015 IEEE Congress on Evolutionary Computation, CEC 2015 - Proceedings},
   keywords = {Fuzzy genetics-based machine learning,evolutionary multiobjective optimization,heuristic rule generation},
   month = {9},
   note = {\{'id': 62,<br/>'keep': False,<br/>'comment': 'Extends MOO paper with heuristic'\}},
   pages = {2996-3003},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Effects of heuristic rule generation from multiple patterns in multiobjective fuzzy genetics-Based machine learning},
   year = {2015},
}
@article{Li2015,
   abstract = {A traditional and intuitively appealing Multitask Multiple Kernel Learning (MT-MKL) method is to optimize the sum (thus, the average) of objective functions with (partially) shared kernel function, which allows information sharing among the tasks. We point out that the obtained solution corresponds to a single point on the Pareto Front (PF) of a multiobjective optimization problem, which considers the concurrent optimization of all task objectives involved in the Multitask Learning (MTL) problem. Motivated by this last observation and arguing that the former approach is heuristic, we propose a novel support vector machine MT-MKL framework that considers an implicitly defined set of conic combinations of task objectives. We show that solving our framework produces solutions along a path on the aforementioned PF and that it subsumes the optimization of the average of objective functions as a special case. Using the algorithms we derived, we demonstrate through a series of experimental results that the framework is capable of achieving a better classification performance, when compared with other similar MTL approaches.},
   author = {Cong Li and Michael Georgiopoulos and Georgios C. Anagnostopoulos},
   doi = {10.1109/TNNLS.2014.2309939},
   issn = {21622388},
   issue = {1},
   journal = {IEEE Transactions on Neural Networks and Learning Systems},
   keywords = {Machine learning,optimization methods,pattern recognition,supervised learning,support vector machines (SVM).},
   month = {1},
   note = {\{'id': 63,<br/>'keep': True, <br/>'Optimization': ["Deterministic", "Tseng’s algorithm"], <br/>'ML task': ["Classification", "Multitask", "Multiple Kernel Learning", "SVM"], <br/>'Objective functions': ["SVM objective"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {51-61},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Pareto-path multitask multiple kernel learning},
   volume = {26},
   year = {2015},
}
@article{Mousavi2015,
   abstract = {Ensemble learning is a system that improves the performance and robustness of the classification problems. How to combine the outputs of base classifiers is one of the fundamental challenges in ensemble learning systems. In this paper, an optimized Static Ensemble Selection (SES) approach is first proposed on the basis of NSGA-II multi-objective genetic algorithm (called SES-NSGAII), which selects the best classifiers along with their combiner, by simultaneous optimization of error and diversity objectives. In the second phase, the Dynamic Ensemble Selection-Performance (DES-P) is improved by utilizing the first proposed method. The second proposed method is a hybrid methodology that exploits the abilities of both SES and DES approaches and is named Improved DES-P (IDES-P). Accordingly, combining static and dynamic ensemble strategies as well as utilizing NSGA-II are the main contributions of this research. Findings of the present study confirm that the proposed methods outperform the other ensemble approaches over 14 datasets in terms of classification accuracy. Furthermore, the experimental results are described from the view point of Pareto front with the aim of illustrating the relationship between diversity and the over-fitting problem.},
   author = {R. Mousavi and M. Eftekhari},
   doi = {10.1016/J.ASOC.2015.09.009},
   issn = {1568-4946},
   journal = {Applied Soft Computing},
   keywords = {Classifier combination,Classifier diversity,Dynamic ensemble selection,Ensemble learning system,Multi-objective optimization,Static ensemble selection},
   month = {12},
   note = {\{'id': 64,<br/>'keep': True, <br/>'Optimization': ["Evolutive", "NSGA-II"], <br/>'ML task': ["Classification", "Ensemble", "NN", "DT", "SVM", "KNN"], <br/>'Objective functions': ["Accuracy","Correlation","Disagreement","Double Fault","Q-statistic"], <br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pages = {652-666},
   publisher = {Elsevier},
   title = {A new ensemble learning methodology based on hybridization of classifier ensemble selection approaches},
   volume = {37},
   year = {2015},
}
@article{Kimovski2015,
   abstract = {Many machine learning and pattern recognition applications require reducing dimensionality to improve learning accuracy while irrelevant inputs are removed. This way, feature selection has become an important issue on these researching areas. Nevertheless, as in past years the number of patterns and, more specifically, the number of features to be selected have grown very fast, parallel processing constitutes an important tool to reach efficient approaches that make possible to tackle complex problems within reasonable computing times. In this paper we propose parallel multi-objective optimization approaches to cope with high-dimensional feature selection problems. Several parallel multi-objective evolutionary alternatives are proposed, and experimentally evaluated by using some synthetic and BCI (Brain-Computer Interface) benchmarks. The experimental results show that the cooperation of parallel evolving subpopulations provides improvements in the solution quality and computing time speedups depending on the parallel alternative and data profile.},
   author = {Dragi Kimovski and Julio Ortega and Andrés Ortiz and Raúl Baños},
   doi = {10.1016/J.ESWA.2015.01.061},
   issn = {0957-4174},
   issue = {9},
   journal = {Expert Systems with Applications},
   keywords = {Feature selection,High-dimensional data,Multi-objective clustering,Parallel evolutionary algorithms,Speedup models,Unsupervised classification},
   month = {6},
   note = {\{'id': 65,<br/>'keep': True, <br/>'Optimization': ["Evolutive", "NSGA-II"], <br/>'ML task': ["Feature Selection", "Classification"], <br/>'Objective functions': [], <br/>'Single/Multi Solutions': "Single - Combination"<br/>\}},
   pages = {4239-4252},
   publisher = {Pergamon},
   title = {Parallel alternatives for evolutionary multi-objective optimization in unsupervised feature selection},
   volume = {42},
   year = {2015},
}
@article{Wang2015,
   abstract = {The receiver operating characteristic (ROC) is commonly used to analyze the performance of classifiers in data mining. An important topic in ROC analysis is the ROC convex hull (ROCCH), which is the least convex majorant (LCM) of the empirical ROC curve and covers potential optima for a given set of classifiers. ROCCH maximization problems have been taken as multiobjective optimization problem (MOPs) in some previous work. However, the special characteristics of ROCCH maximization problem makes it different from traditional MOPs. In this paper, the difference will be discussed in detail and a new convex hull-based multiobjective genetic programming (CH-MOGP) is proposed to solve ROCCH maximization problems. Specifically, convex hull-based without redundancy sorting (CWR-sorting) is introduced, which is an indicator-based selection scheme that aims to maximize the area under the convex hull. A novel selection procedure is also proposed based on the proposed sorting scheme. It is hypothesized that by using a tailored indicator-based selection, CH-MOGP becomes more efficient for ROC convex hull approximation than algorithms that compute all Pareto optimal points. Empirical studies are conducted to compare CH-MOGP to both existing machine learning approaches and multiobjective genetic programming (MOGP) methods with classical selection schemes. Experimental results show that CH-MOGP outperforms the other approaches significantly.},
   author = {Pu Wang and Michael Emmerich and Rui Li and Ke Tang and Thomas Bäck and Xin Yao},
   doi = {10.1109/TEVC.2014.2305671},
   issn = {1089778X},
   issue = {2},
   journal = {IEEE Transactions on Evolutionary Computation},
   keywords = {Classification,evolutionary multiobjective algorithm,genetic programming,memetic algorithm,receiver operating characteristic (ROC) convex hull},
   month = {4},
   note = {\{'id': 66,<br/>'keep': True, <br/>'Optimization': ["Evolutive"], <br/>'ML task': ["Classification","ROCCH"], <br/>'Objective functions': ["False Positive Rate","True Positive Rate"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {188-200},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Convex Hull-Based Multiobjective Genetic Programming for Maximizing Receiver Operating Characteristic Performance},
   volume = {19},
   year = {2015},
}
@article{Prakash2015,
   abstract = {Clustering is an unsupervised classification method in the field of data mining. Many population based evolutionary and swarm intelligence optimization methods are proposed to optimize clustering solutions globally based on a single selected objective function which lead to produce a single best solution. In this sense, optimized solution is biased towards a single objective, hence it is not equally well to the data set having clusters of different geometrical properties. Thus, clustering having multiple objectives should be naturally optimized through multiobjective optimization methods for capturing different properties of the data set. To achieve this clustering goal, many multiobjective population based optimization methods, e.g., multiobjective genetic algorithm, mutiobjective particle swarm optimization (MOPSO), are proposed to obtain diverse tradeoff solutions in the pareto-front. As single directional diversity mechanism in particle swarm optimization converges prematurely to local optima, this paper presents a two-stage diversity mechanism in MOPSO to improve its exploratory capabilities by incorporating crossover operator of the genetic algorithm. External archive is used to store non-dominated solutions, which is further utilized to find one best solution having highest F-measure value at the end of the run. Two conceptually orthogonal internal measures SSE and connectedness are used to estimate the clustering quality. Results demonstrate effectiveness of the proposed method over its competitors MOPSO, non-dominated sorting genetic algorithm, and multiobjective artificial bee colony on seven real data sets from UCI machine learning repository.},
   author = {Jay Prakash and P. K. Singh},
   doi = {10.1007/S12293-014-0147-5/FIGURES/3},
   issn = {18659292},
   issue = {2},
   journal = {Memetic Computing},
   keywords = {Data clustering,Evolutionary and swarm intelligence,Multiobjective optimization,Mutiobjective particle swarm optimization},
   month = {6},
   note = {\{'id': 67,<br/>'keep': True, <br/>'Optimization': ["Evolutive","Particle Swarm Optimization"], <br/>'ML task': ["Clustering"], <br/>'Objective functions': ["SSE","Connectedness"], <br/>'Single/Multi Solutions': "Single - Highest F-measure"<br/>\}},
   pages = {93-104},
   publisher = {Springer Verlag},
   title = {An effective multiobjective approach for hard partitional clustering},
   volume = {7},
   url = {https://link.springer.com/article/10.1007/s12293-014-0147-5},
   year = {2015},
}
@article{Zhang2015,
   abstract = {Multi-objective model selection, which is an important aspect of Machine Learning, refers to the problem of identifying a set of Pareto optimal models from a given ensemble of models. This paper proposes SPRINT-Race, a multi-objective racing algorithm based on the Sequential Probability Ratio Test with an Indifference Zone. In SPRINT-Race, a non-parametric ternary-decision sequential analogue of the sign test is adopted to identify pair-wise dominance and non-dominance relationship. In addition, a Bonferroni approach is employed to control the overall probability of any erroneous decisions. In the fixed confidence setting, SPRINT-Race tries to minimize the computational effort needed to achieve a predefined confidence about the quality of the returned models. The efficiency of SPRINT-Race is analyzed on artificially-constructed multi-objective model selection problems with known ground-truth. Moreover, SPRINT-Race is applied to identifying the Pareto optimal parameter settings of Ant Colony Optimization algorithms in the context of solving Traveling Salesman Problems. The experimental results confirm the advantages of SPRINT-Race for multi-objective model selection.},
   author = {Tiantian Zhang and Michael Georgiopoulos and Georgios C Anagnostopoulos},
   city = {New York, NY, USA},
   doi = {10.1145/2739480},
   isbn = {9781450334723},
   journal = {Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation},
   keywords = {CCS Concepts •Mathematics of computing → Hypothesis testing and confidence interval computation,Keywords Racing Algorithm,Model Selection,Multi-objective Opti-mization,Sequential Probability Ratio Test,•Appl-ied computing → Multi-criterion optimization and decision-making,•Computing methodologies → Sequential decision making},
   note = {\{'id': 68,<br/>'keep': True, <br/>'Optimization': ["Deterministic"], <br/>'ML task': ["Racing","Model Selection"], <br/>'Objective functions': ["User defined"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   publisher = {ACM},
   title = {SPRINT Multi-Objective Model Racing},
   url = {http://dx.doi.org/10.1145/2739480.2754791},
   year = {2015},
}
@article{,
   abstract = {Classification is one of the most well-known tasks in supervised learning. A vast number of algorithms for pattern classification have been proposed so far. Among these, support vector machines (SVMs) are one of the most popular approaches, due to the high performance reached by these methods in a wide number of pattern recognition applications. Nevertheless, the effectiveness of SVMs highly depends on their hyper-parameters. Besides the fine-tuning of their hyper-parameters, the way in which the features are scaled as well as the presence of non-relevant features could affect their generalization performance. This paper introduces an approach for addressing model selection for support vector machines used in classification tasks. In our formulation, a model can be composed of feature selection and pre-processing methods besides the SVM classifier. We formulate the model selection problem as a multi-objective one, aiming to minimize simultaneously two components that are closely related to the error of a model: bias and variance components, which are estimated in an experimental fashion. A surrogate-assisted evolutionary multi-objective optimization approach is adopted to explore the hyper-parameters space. We adopted this approach due to the fact that estimating the bias and variance could be computationally expensive. Therefore, by using surrogate-assisted optimization, we expect to reduce the number of solutions evaluated by the fitness functions so that the computational cost would also be reduced. Experimental results conducted on benchmark datasets widely used in the literature, indicate that highly competitive models with a fewer number of fitness function evaluations are obtained by our proposal when it is compared to state of the art model selection methods.},
   author = {Alejandro Rosales-Pérez and Jesus A. Gonzalez and Carlos A. Coello Coello and Hugo Jair Escalante and Carlos A. Reyes-Garcia},
   doi = {10.1016/J.NEUCOM.2014.08.075},
   issn = {0925-2312},
   issue = {Part A},
   journal = {Neurocomputing},
   keywords = {Model selection,Multi-objective optimization,Support vector machines,Surrogate-assisted optimization},
   month = {2},
   note = {\{'id': 69,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","SAMOEA"], <br/>'ML task': ["Classification","Pre-Processing","Feature Selection","Model Selection","SVM"], <br/>'Objective functions': ["Bias","Variance"], <br/>'Single/Multi Solutions': "Single - Performance in validation"<br/>\}},
   pages = {163-172},
   publisher = {Elsevier},
   title = {Surrogate-assisted multi-objective model selection for support vector machines},
   volume = {150},
   year = {2015},
}
@article{Truong2015,
   abstract = {Supervised alternative clustering is the problem of finding a set of clusterings which are of high quality and different from a given negative clustering. The task is therefore a clear multi-objective optimization problem. Optimizing two conflicting objectives at the same time requires dealing with trade-offs. Most approaches in the literature optimize these objectives sequentially (one objective after another one) or indirectly (by some heuristic combination of the objectives). Solving a multi-objective optimization problem in these ways can result in solutions which are dominated, and not Pareto-optimal. We develop a direct algorithm, called COGNAC, which fully acknowledges the multiple objectives, optimizes them directly and simultaneously, and produces solutions approximating the Pareto front. COGNAC performs the recombination operator at the cluster level instead of at the object level, as in the traditional genetic algorithms. It can accept arbitrary clustering quality and dissimilarity objectives and provides solutions dominating those obtained by other state-of-the-art algorithms. Based on COGNAC, we propose another algorithm called SGAC for the sequential generation of alternative clusterings where each newly found alternative clustering is guaranteed to be different from all previous ones. The experimental results on widely used benchmarks demonstrate the advantages of our approach.},
   author = {Duy Tin Truong and Roberto Battiti},
   doi = {10.1007/S10994-013-5350-Y/TABLES/5},
   issn = {15730565},
   issue = {1-2},
   journal = {Machine Learning},
   keywords = {Alternative clustering,Cluster-oriented recombination,Genetic algorithms,Multi-objective optimization},
   month = {1},
   note = {\{'id': 70,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Genetic","NSGA-II"], <br/>'ML task': ["Clustering","Supervised Alternative clustering"], <br/>'Objective functions': ["Vector Quantization Error","Adjusted Rand Index"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {57-91},
   publisher = {Kluwer Academic Publishers},
   title = {A flexible cluster-oriented alternative clustering algorithm for choosing from the Pareto front of solutions},
   volume = {98},
   url = {https://link.springer.com/article/10.1007/s10994-013-5350-y},
   year = {2015},
}
@article{Zhang2016,
   abstract = {Model selection is a core aspect in machine learning and is, occasionally, multi-objective in nature. For instance, hyper-parameter selection in a multi-task learning context is of multi-objective nature, since all the tasks' objectives must be optimized simultaneously. In this paper, a novel multi-objective racing algorithm (RA), namely S-Race, is put forward. Given an ensemble of models, our task is to reliably identify Pareto optimal models evaluated against multiple objectives, while minimizing the total computational cost. As a RA, S-Race attempts to eliminate non-promising models with confidence as early as possible, so as to concentrate computational resources on promising ones. Simultaneously, it addresses the problem of multi-objective model selection (MOMS) in the sense of Pareto optimality. In S-Race, the nonparametric sign test is utilized for pair-wise dominance relationship identification. Moreover, a discrete Holm's step-down procedure is adopted to control the family-wise error rate of the set of hypotheses made simultaneously. The significance level assigned to each family is adjusted adaptively during the race. In order to illustrate its merits, S-Race is applied on three MOMS problems: 1) selecting support vector machines for classification; 2) tuning the parameters of artificial bee colony algorithms for numerical optimization; and 3) constructing optimal hybrid recommendation systems for movie recommendation. The experimental results confirm that S-Race is an efficient and effective MOMS algorithm compared to a brute-force approach.},
   author = {Tiantian Zhang and Michael Georgiopoulos and Georgios C. Anagnostopoulos},
   doi = {10.1109/TCYB.2015.2456187},
   issn = {21682267},
   issue = {8},
   journal = {IEEE Transactions on Cybernetics},
   keywords = {Discrete Holm's step-down procedure,model selection,multi-objective optimization,racing algorithm (RA),sign test (ST)},
   month = {8},
   note = {\{'id': 71,<br/>'keep': False<br/>\}},
   pages = {1863-1876},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Multi-Objective Model Selection via Racing},
   volume = {46},
   year = {2016},
}
@article{Urbanowicz2016,
   abstract = {Learning classifier systems (LCSs) are rule-based evolutionary algorithms uniquely suited to classification and data mining in complex, multi-factorial, and heterogeneous problems. LCS rule fitness is commonly based on accuracy, but this metric alone is not ideal for assessing global rule 'value' in noisy problem domains, and thus impedes effective knowledge extraction. Multi-objective fitness functions are promising but rely on knowledge of how to weigh objective importance. Prior knowledge would be unavailable in most real-world problems. The Pareto-front concept offers a strategy for multi-objective machine learning that is agnostic to objective importance. We propose a Pareto-inspired multi-objective rule fitness (PIMORF) for LCS, and combine it with a complimentary rule-compaction approach (SRC). We implemented these strategies in ExSTraCS, a successful supervised LCS and evaluated performance over an array of complex simulated noisy and clean problems (i.e. genetic and multiplexer) that each concurrently model pure interaction effects and heterogeneity. While evaluation over multiple performance metrics yielded mixed results, this work represents an important first step towards efficiently learning complex problem spaces without the advantage of prior problem knowledge. Overall the results suggest that PI-MORF paired with SRC improved rule set interpretability, particularly with regard to heterogeneous patterns.},
   author = {Ryan J. Urbanowicz and Randal S. Olson and Jason H. Moore},
   doi = {10.1145/2908961.2931736},
   isbn = {9781450343237},
   keywords = {classifier systems,data mining,fitness evaluation,machine learning,multi-objective optimization},
   month = {7},
   note = {\{'id': 72,<br/>'keep': False<br/>\}},
   pages = {1403-1403},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Pareto Inspired Multi-objective Rule Fitness for Adaptive Rule-based Machine Learning},
   url = {http://dx.doi.org/10.1145/2908961.2931736},
   year = {2016},
}
@article{Mane2016,
   abstract = {Classification is inherently multi-objective problem. It is one of the most important tasks of data mining to extract important patterns from large volume of data. Traditionally, either only one objective is considered or the multiple objectives are accumulated to...},
   author = {Seema Mane and Shilpa Sonawani and Sachin Sakhare},
   doi = {10.1007/978-981-10-0419-3_21},
   isbn = {9789811004179},
   issn = {21945357},
   journal = {Advances in Intelligent Systems and Computing},
   keywords = {Classificationm,Local search,Multi-objective optimization,NSGA II,Neural network,Pareto optimality},
   note = {\{'id': 73,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II"], <br/>'ML task': ["Classification","Neural Network"], <br/>'Objective functions': ["MSE","Accuracy"], <br/>'Single/Multi Solutions': "Single"<br/>\}},
   pages = {171-179},
   publisher = {Springer, Singapore},
   title = {Hybrid Multi-objective Optimization Approach for Neural Network Classification Using Local Search},
   volume = {413},
   url = {https://link.springer.com/chapter/10.1007/978-981-10-0419-3_21},
   year = {2016},
}
@article{Smithson2016,
   abstract = {Artificial neural networks have gone through a recent rise in popularity, achieving state-of-the-art results in various fields, including image classification, speech recognition, and automated control. Both the performance and computational complexity of such models are heavily dependant on the design of characteristic hyper-parameters (e.g., number of hidden layers, nodes per layer, or choice of activation functions), which have traditionally been optimized manually. With machine learning penetrating low-power mobile and embedded areas, the need to optimize not only for performance (accuracy), but also for implementation complexity, becomes paramount. In this work, we present a multi-objective design space exploration method that reduces the number of solution networks trained and evaluated through response surface modelling. Given spaces which can easily exceed 1020 solutions, manually designing a near-optimal architecture is unlikely as opportunities to reduce network complexity, while maintaining performance, may be overlooked. This problem is exacerbated by the fact that hyper-parameters which perform well on specific datasets may yield sub-par results on others, and must therefore be designed on a per-application basis. In our work, machine learning is leveraged by training an artificial neural network to predict the performance of future candidate networks. The method is evaluated on the MNIST and CIFAR-10 image datasets, optimizing for both recognition accuracy and computational complexity. Experimental results demonstrate that the proposed method can closely approximate the Pareto-optimal front, while only exploring a small fraction of the design space.},
   author = {Sean C. Smithson and Guang Yang and Warren J. Gross and Brett H. Meyer},
   doi = {10.1145/2966986.2967058},
   isbn = {9781450344661},
   issn = {10923152},
   journal = {IEEE/ACM International Conference on Computer-Aided Design, Digest of Technical Papers, ICCAD},
   month = {11},
   note = {\{'id': 74,<br/>'keep': False,<br/>'obs': 'No MOO method applied, only compares Pareto dominance"<br/>\}},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Neural networks designing neural networks: Multi-objective hyper-parameter optimization},
   volume = {07-10-November-2016},
   url = {http://dx.doi.org/10.1145/2966986.2967058},
   year = {2016},
}
@article{Fang2016,
   abstract = {In current supervised machine learning research spectrum, there are several attribute reduction methodologies to acquire reducts with low test cost. They can deal with symbolic data, or numeric data with error ranges. In many cases, they consider the situation with only one type of cost; therefore the problem is single-objective. This paper addresses the attribute reduction problem on data with multi-type-costs and error ranges. First, we define the multi-objective attribute reduction problem where multi-type-costs are involved. Second, we propose three metrics to evaluate the quality of a reduct set. Third, we design a backtrack algorithm to compute the Pareto optimal set, and a heuristic algorithm to find a sub-optimal reduct set. Finally, we compare these algorithms on seven UCI (University of California-Irvine) datasets. Experimental results indicate that our heuristic algorithm has good capability of tackling the proposed problem.},
   author = {Yu Fang and Zhong Hui Liu and Fan Min},
   doi = {10.1007/S13042-014-0296-3/FIGURES/10},
   issn = {1868808X},
   issue = {5},
   journal = {International Journal of Machine Learning and Cybernetics},
   keywords = {Attribute reduction,Cost-sensitive learning,Error range,Test cost},
   month = {10},
   note = {\{'id': 75,<br/>'keep': True, <br/>'Optimization': ["Heuristic","Backtrack reduction","k-weighted heuristic reduction"], <br/>'ML task': ["Attribute reduction","Classification"], <br/>'Objective functions': ["Coverage","Accuracy","Similarity"], <br/>'Single/Multi Solutions': "Single"<br/>\}},
   pages = {783-793},
   publisher = {Springer Verlag},
   title = {Multi-objective cost-sensitive attribute reduction on data with error ranges},
   volume = {7},
   url = {https://link.springer.com/article/10.1007/s13042-014-0296-3},
   year = {2016},
}
@article{Horn2017,
   abstract = {The performance of many machine learning algorithms heavily depends on the setting of their respective hyperparameters. Many different tuning approaches exist, from simple grid or random search approaches to evolutionary algorithms and Bayesian optimization. Often, these algorithms are used to optimize a single performance criterion. But in practical applications, a single criterion may not be sufficient to adequately characterize the behavior of the machine learning method under consideration and the Pareto front of multiple criteria has to be considered. We propose to use model-based multi-objective optimization to efficiently approximate such Pareto fronts.},
   author = {Daniel Horn and Bernd Bischl},
   doi = {10.1109/SSCI.2016.7850221},
   isbn = {9781509042401},
   journal = {2016 IEEE Symposium Series on Computational Intelligence, SSCI 2016},
   month = {2},
   note = {\{'id': 76,<br/>'keep': True, <br/>'Optimization': ["Evolutionary"], <br/>'ML task': ["Parameter Configuration","Classification"], <br/>'Objective functions': ["False Negative Rate","False Positive Rate"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Multi-objective parameter configuration of machine learning algorithms using model-based optimization},
   year = {2017},
}
@article{Nojima2016,
   abstract = {Classifier design for a classification problem with M classes can be viewed as finding an optimal partition of its pattern space into M disjoint subspaces. However, this is not always a good strategy especially when training patterns from different classes are heavily overlapping in the pattern space. A simple but practically useful idea is the use of a reject option. In this case, the pattern space is partitioned into (M+1) disjoint subspace where the classification of new patterns is rejected in the (M+1)th subspace. In this paper, we discuss the design of fuzzy rule-based classifiers with a reject option. The rejection subspace is specified by a threshold value for the difference of a kind of matching degrees between the best matching class and the second best matching class. The important research question is how to specify the threshold value. We examine the following two approaches: One is manual specification after designing a fuzzy rule-based classifier, and the other is simultaneous multiobjective optimization of a threshold value and a fuzzy rulebased classifier. In the latter approach, we use three objectives: maximization of the correct classification, and minimization of the rejection and the complexity of the classifier.},
   author = {Yusuke Nojima and Hisao Ishibuchi},
   doi = {10.1109/FUZZ-IEEE.2016.7737854},
   isbn = {9781509006250},
   journal = {2016 IEEE International Conference on Fuzzy Systems, FUZZ-IEEE 2016},
   keywords = {Evolutionary multiobjective optimization,Fuzzy genetics-based machine learning,Reject option},
   month = {11},
   note = {\{'id': 77,<br/>'keep': True, <br/>'Optimization': ["Evolutionary", "NSGA-II"], <br/>'ML task': ["Classification","Reject option","Fuzzy rule-based classifier"], <br/>'Objective functions': ["Number of rules","Error rate except for rejected patterns","Number of rejected patterns"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {1405-1412},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Multiobjective fuzzy genetics-based machine learning with a reject option},
   year = {2016},
}
@article{Urbanowicz2016,
   abstract = {Learning classifier systems (LCSs) are rule-based evolutionary algorithms uniquely suited to classification and data mining in complex, multi-factorial, and heterogeneous problems. The fitness of individual LCS rules is commonly based on accuracy, but this metric...},
   author = {Ryan J. Urbanowicz and Randal S. Olson and Jason H. Moore},
   doi = {10.1007/978-3-319-45823-6_48},
   isbn = {9783319458229},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Classifier systems,Data mining,Fitness evaluation,Machine learning,Multi-objective optimization},
   note = {\{'id': 78,<br/>'keep': False,<br/>'Obs': "Pareto-inspired"\}},
   pages = {514-524},
   publisher = {Springer, Cham},
   title = {Pareto Inspired Multi-objective Rule Fitness for Noise-Adaptive Rule-Based Machine Learning},
   volume = {9921 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-45823-6_48},
   year = {2016},
}
@article{Ojha2017,
   abstract = {Machine learning algorithms are inherently multiobjective in nature, where approximation error minimization and model's complexity simplification are two conflicting objectives. We proposed a multiobjective genetic programming (MOGP) for creating a heterogeneous flexible neural tree (HFNT), tree-like flexible feedforward neural network model. The functional heterogeneity in neural tree nodes was introduced to capture a better insight of data during learning because each input in a dataset possess different features. MOGP guided an initial HFNT population towards Pareto-optimal solutions, where the final population was used for making an ensemble system. A diversity index measure along with approximation error and complexity was introduced to maintain diversity among the candidates in the population. Hence, the ensemble was created by using accurate, structurally simple, and diverse candidates from MOGP final population. Differential evolution algorithm was applied to fine-tune the underlying parameters of the selected candidates. A comprehensive test over classification, regression, and time-series datasets proved the efficiency of the proposed algorithm over other available prediction methods. Moreover, the heterogeneous creation of HFNT proved to be efficient in making ensemble system from the final population.},
   author = {Varun Kumar Ojha and Ajith Abraham and Václav Snášel},
   doi = {10.1016/J.ASOC.2016.09.035},
   issn = {1568-4946},
   journal = {Applied Soft Computing},
   keywords = {Approximation,Ensemble,Feature selection,Flexible neural tree,Pareto-based multiobjectives},
   month = {3},
   note = {\{'id': 79,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Genetic","NSGA-II"], <br/>'ML task': ["Ensemble","Flexible neural tree","Classification","Regression","Time-Series"], <br/>'Objective functions': ["MSE","Tree size","Diversity index"], <br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pages = {909-924},
   publisher = {Elsevier},
   title = {Ensemble of heterogeneous flexible neural trees using multiobjective genetic programming},
   volume = {52},
   year = {2017},
}
@article{Nojima2017,
   abstract = {Various evolutionary multiobjective optimization (EMO) algorithms have been used in the field of evolutionary fuzzy systems (EFS), because EMO algorithms can easily handle multiple objective functions such as the accuracy maximization and complexity minimization for fuzzy system design. Most EMO algorithms used in EFS are Pareto dominance-based algorithms such as NSGA-II, SPEA2, and PAES. There are a few studies where other types of EMO algorithms are used in EFS. In this paper, we apply a multiobjective evolutionary algorithm based on decomposition called MOEA/D to EFS for fuzzy classifier design. MOEA/D is one of the most well-known decomposition-based EMO algorithms. The key idea is to divide a multiobjective optimization problem into a number of single-objective problems using a set of uniformly distributed weight vectors in a scalarizing function. We propose a new scalarizing function called an accuracy-oriented function (AOF) which is specialized for classifier design. We examine the effects of using AOF in MOEA/D on the search ability of our multiobjective fuzzy genetics-based machine learning (GBML). We also examine the synergy effect of MOEA/D with AOF and parallel distributed implementation of fuzzy GBML on the generalization ability.},
   author = {Yusuke Nojima and Koki Arahari and Shuji Takemura and Hisao Ishibuchi},
   doi = {10.1109/FUZZ-IEEE.2017.8015749},
   isbn = {9781509060344},
   issn = {10987584},
   journal = {IEEE International Conference on Fuzzy Systems},
   keywords = {Accuracy-oriented scalarizingfunction,Evolutionary fuzzy systems,Fuzzy classifier design,MOEA/D},
   month = {8},
   note = {\{'id': 80,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","MOEA/D"], <br/>'ML task': ["Fuzzy Systems","Classification"], <br/>'Objective functions': ["Error","Number of rules","Total rule length"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Multiobjective fuzzy genetics-based machine learning based on MOEA/D with its modifications},
   year = {2017},
}
@article{,
   abstract = {Imbalanced classification is related to those problems that have an uneven distribution among classes. In addition to the former, when instances are located into the overlapped areas, the correct modeling of the problem becomes harder. Current solutions for both issues are often focused on the binary case study, as multi-class datasets require an additional effort to be addressed. In this research, we overcome these problems by carrying out a combination between feature and instance selections. Feature selection will allow simplifying the overlapping areas easing the generation of rules to distinguish among the classes. Selection of instances from all classes will address the imbalance itself by finding the most appropriate class distribution for the learning task, as well as possibly removing noise and difficult borderline examples. For the sake of obtaining an optimal joint set of features and instances, we embedded the searching for both parameters in a Multi-Objective Evolutionary Algorithm, using the C4.5 decision tree as baseline classifier in this wrapper approach. The multi-objective scheme allows taking a double advantage: the search space becomes broader, and we may provide a set of different solutions in order to build an ensemble of classifiers. This proposal has been contrasted versus several state-of-the-art solutions on imbalanced classification showing excellent results in both binary and multi-class problems.},
   author = {Alberto Fernández and Cristobal José Carmona and María José Del Jesus and Francisco Herrera},
   doi = {10.1142/S0129065717500289},
   issn = {01290657},
   issue = {6},
   journal = {International Journal of Neural Systems},
   keywords = {Imbalanced classification,ensembles,feature selection,instance selection,multi-class,multi-objective evolutionary algorithms,overlapping},
   month = {9},
   note = {\{'id': 81,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II"], <br/>'ML task': ["Ensebmle","Classification","Multi-Class","Feature Selection","Instance Selection"], <br/>'Objective functions': ["M-AUC","RED"], <br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pmid = {28633551},
   publisher = {World Scientific Publishing Co. Pte Ltd},
   title = {A pareto-based ensemble with feature and instance selection for learning from multi-class imbalanced datasets},
   volume = {27},
   year = {2017},
}
@article{Mannion2017,
   abstract = {Reinforcement Learning (RL) is a powerful and well-studied Machine Learning paradigm, where an agent learns to improve its performance in an environment by maximising a reward signal. In multi-objective Reinforcement Learning (MORL) the reward signal is a vector, where each component represents the performance on a different objective. Reward shaping is a well-established family of techniques that have been successfully used to improve the performance and learning speed of RL agents in single-objective problems. The basic premise of reward shaping is to add an additional shaping reward to the reward naturally received from the environment, to incorporate domain knowledge and guide an agent's exploration. Potential-Based Reward Shaping (PBRS) is a specific form of reward shaping that offers additional guarantees. In this paper, we extend the theoretical guarantees of PBRS to MORL problems. Specifically, we provide theoretical proof that PBRS does not alter the true Pareto front in both single- and multi-agent MORL. We also contribute the first published empirical studies of the effect of PBRS in single- and multi-agent MORL problems.},
   author = {Patrick Mannion and Sam Devlin and Karl Mason and Jim Duggan and Enda Howley},
   doi = {10.1016/J.NEUCOM.2017.05.090},
   issn = {0925-2312},
   journal = {Neurocomputing},
   keywords = {Multi-agent systems,Multi-objective,Potential-based,Reinforcement learning,Reward shaping},
   month = {11},
   note = {\{'id': 82,<br/>'keep': False,<br/>'obs': 'Theoretical paper'<br/>\}},
   pages = {60-73},
   publisher = {Elsevier},
   title = {Policy invariance under reward transformations for multi-objective reinforcement learning},
   volume = {263},
   year = {2017},
}
@article{Das2017,
   abstract = {Feature Selection (FS) is an important pre-processing step in machine learning and it reduces the number of features/variables used to describe each member of a dataset. Such reduction occurs by eliminating some of the non-discriminating and redundant features and selecting a subset of the existing features with higher discriminating power among various classes in the data. In this paper, we formulate the feature selection as a bi-objective optimization problem of some real-valued weights corresponding to each feature. A subset of the weighted features is thus selected as the best subset for subsequent classification of the data. Two information theoretic measures, known as ‘relevancy’ and ‘redundancy’ are chosen for designing the objective functions for a very competitive Multi-Objective Optimization (MOO) algorithm called ‘Multi-Objective Evolutionary Algorithm based on Decomposition (MOEA/D)’. We experimentally determine the best possible constraints on the weights to be optimized. We evaluate the proposed bi-objective feature selection and weighting framework on a set of 15 standard datasets by using the popular k-Nearest Neighbor (k-NN) classifier. As is evident from the experimental results, our method appears to be quite competitive to some of the state-of-the-art FS methods of current interest. We further demonstrate the effectiveness of our framework by changing the choices of the optimization scheme and the classifier to Non-dominated Sorting Genetic Algorithm (NSGA)-II and Support Vector Machines (SVMs) respectively.},
   author = {Ayan Das and Swagatam Das},
   doi = {10.1016/J.PATREC.2017.01.004},
   issn = {0167-8655},
   journal = {Pattern Recognition Letters},
   keywords = {Classification,Feature selection,Feature weighting,Information measure,Multi-objective optimization},
   month = {3},
   note = {\{'id': 83,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","MOEA/D"], <br/>'ML task': ["Feature Selection","Classification"], <br/>'Objective functions': ["relevancy","redundancy"], <br/>'Single/Multi Solutions': "Single"<br/>\}},
   pages = {12-19},
   publisher = {North-Holland},
   title = {Feature weighting and selection with a Pareto-optimal trade-off between relevancy and redundancy},
   volume = {88},
   year = {2017},
}
@article{Yi2017,
   abstract = {In order to improve utilization rate of high dimensional data features, an ensemble learning method based on feature selection for entity resolution is developed. Entity resolution is regarded as a binary classification problem, an optimization model is designed to maximize each classifier's classification accuracy and dissimilarity between classifiers and minimize cardinality of features. A modified multiobjective ant colony optimization algorithm is employed to solve the model for each base classifier, two pheromone matrices are set up, weighted product method is applied to aggregate values of two pheromone matrices, and feature's Fisher discriminant rate of records' similarity vector is calculated as heuristic information. A solution which is called complementary subset is selected from Pareto archive according to the descending order of three objectives to train the given base classifier. After training all base classifiers, their classification outputs are aggregated by max-wins voting method to obtain the ensemble classifiers' final result. A simulation experiment is carried out on three classical datasets. The results show the effectiveness of our method, as well as a better performance compared with the other two methods.},
   author = {Liu Yi and Diao Xing-Chun and Cao Jian-Jun and Zhou Xing and Shang Yu-Ling},
   doi = {10.1155/2017/4953280},
   issn = {15635147},
   journal = {Mathematical Problems in Engineering},
   note = {\{'id': 84,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","MOACO"], <br/>'ML task': ["Ensemble","Classification","Entity Resolution","Feature Selection"], <br/>'Objective functions': ["Accuracy","Dissimilarity between classifiers","Cardinality of features"], <br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   publisher = {Hindawi Publishing Corporation},
   title = {A method for entity resolution in high dimensional data using ensemble classifiers},
   volume = {2017},
   year = {2017},
}
@article{Fang2017,
   abstract = {Multi-objective cost-sensitive attribute reduction is an attractive problem in supervised machine learning. Most research has focused on single-objective minimal test cost reduction or dealt with symbolic data. In this paper, we propose a particle swarm optimization algorithm for the attribute reduction problem on numeric data with multiple costs and error ranges and use three metrics with which to evaluate the performance of the algorithm. The proposed algorithm benefits from a fitness function based on the positive region, the selected n types of the test cost, a set of constant weight values wik, and a designated non-positive exponent λ. We design a learning strategy by setting dominance principles, which ensures the preservation of Pareto-optimal solutions and the rejection of redundant solutions. With different parameter settings, our PSO algorithm searches for a sub-optimal reduct set. Finally, we test our algorithm on seven UCI (University of California, Irvine) datasets. Comparisons with alternative approaches including the λ-weighted method and exhaustive calculation method of reduction are analyzed. Experimental results indicate that our heuristic algorithm outperforms existing algorithms.},
   author = {Yu Fang and Zhong Hui Liu and Fan Min},
   doi = {10.1007/S00500-016-2260-5/FIGURES/12},
   issn = {14337479},
   issue = {23},
   journal = {Soft Computing},
   keywords = {Attribute reduction,Cost-sensitive learning,Particle swarm optimization,Rough sets},
   month = {12},
   note = {\{'id': 85,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Particle Swarm Optimization"], <br/>'ML task': ["Cost-Sensitive Learning","Attribute reduction"], <br/>'Objective functions': ["Accuracy","Coverage","Similarity"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {7173-7189},
   publisher = {Springer Verlag},
   title = {A PSO algorithm for multi-objective cost-sensitive attribute reduction on numeric data with error ranges},
   volume = {21},
   url = {https://link.springer.com/article/10.1007/s00500-016-2260-5},
   year = {2017},
}
@article{Deniz2017,
   abstract = {This study investigates the success of a multiobjective genetic algorithm (GA) combined with state-of-the-art machine learning (ML) techniques for the feature subset selection (FSS) in binary classification problem (BCP). Recent studies have focused on improving the accuracy of BCP by including all of the features, neglecting to determine the best performing subset of features. However, for some problems, the number of features may reach thousands, which will cause too much computation power to be consumed during the feature evaluation and classification phases, also possibly reducing the accuracy of the results. Therefore, selecting the minimum number of features while preserving and/or increasing the accuracy of the results at a high level becomes an important issue for achieving fast and accurate binary classification. Our multiobjective evolutionary algorithm includes two phases, FSS using a GA and applying ML techniques for the BCP. Since exhaustively investigating all of the feature subsets is intractable, a GA is preferred for the first phase of the algorithm for intelligently detecting the most appropriate feature subset. The GA uses multiobjective crossover and mutation operators to improve a population of individuals (each representing a selected feature subset) and obtain (near-) optimal solutions through generations. In the second phase of the algorithms, the fitness of the selected subset is decided by using state-of-the-art ML techniques; Logistic Regression, Support Vector Machines, Extreme Learning Machine, K-means, and Affinity Propagation. The performance of the multiobjective evolutionary algorithm (and the ML techniques) is evaluated with comprehensive experiments and compared with state-of-the-art algorithms, Greedy Search, Particle Swarm Optimization, Tabu Search, and Scatter Search. The proposed algorithm was observed to be robust and it performed better than the existing methods on most of the datasets.},
   author = {Ayça Deniz and Hakan Ezgi Kiziloz and Tansel Dokeroglu and Ahmet Cosar},
   doi = {10.1016/J.NEUCOM.2017.02.033},
   issn = {0925-2312},
   journal = {Neurocomputing},
   keywords = {Binary classification,Evolutionary algorithm,Multiobjective feature selection,Supervised/unsupervised machine learning},
   month = {6},
   note = {\{'id': 86,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Genetic","NSGA-II"], <br/>'ML task': ["Feature subset selection","Binary Classification"], <br/>'Objective functions': ["Number of features","Accuracy"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {128-146},
   publisher = {Elsevier},
   title = {Robust multiobjective evolutionary feature subset selection algorithm for binary classification using machine learning techniques},
   volume = {241},
   year = {2017},
}
@article{Zutty2017,
   abstract = {Genetic algorithms and genetic programming lend themselves well to the field of machine learning, which involves solving test case based problems. However, most traditional multi-objective selection methods work with scalar objectives, such as minimizing false negative and false positive rates, that are computed from underlying test cases. In this paper, we propose a new fuzzy selection operator that takes into account the statistical nature of machine learning problems based on test cases. Rather than use a Pareto rank or strength computed from scalar objectives, such as with NSGA2 or SPEA2, we will compute a probability of Pareto optimality. This will be accomplished through covariance estimation and Markov chain Monte Carlo simulation in order to generate probabilistic objective scores for each individual. We then compute a probability that each individual will generate a Pareto optimal solution. This probability is directly used with a roulette wheel selection technique. Our method's performance is evaluated on the evolution of a feature selection vector for a binary classification on each of eight different activities. Fuzzy selection performance varies, outperforming both NSGA2 and SPEA2 in both speed (measured in generations) and solution quality (measured by area under the curve) in some cases, while underperforming in others.},
   author = {Jason Zutty and Gregory Rohling},
   doi = {10.1145/3071178.3071234},
   isbn = {9781450349208},
   journal = {GECCO 2017 - Proceedings of the 2017 Genetic and Evolutionary Computation Conference},
   keywords = {Genetic Algorithms,Machine Learning,Markov Chain Monte Carlo,Pareto Dominance},
   month = {7},
   note = {\{'id': 87,<br/>'keep': True, <br/>'Optimization': ["Evolutionary"], <br/>'ML task': ["Feature subset selection","Binary Classification"], <br/>'Objective functions': ["True Positive Rate","False Positive Rate"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {529-536},
   publisher = {Association for Computing Machinery, Inc},
   title = {Solving test case based problems with fuzzy dominance},
   volume = {8},
   url = {https://doi.org/http://dx.doi.org/10.1145/3071178.3071234},
   year = {2017},
}
@article{,
   abstract = {In this paper, a novel approach for classification rule mining is presented. The remarkable relationship between the rule extraction procedure and the concept of multiobjective optimization is emphasized. The range values of features composing the rules are handled as decision variables in the modelled multiobjective optimization problem. The proposed method is applied to three well-known datasets in literature. These are Iris, Haberman's Survival Data and Pima Indians Diabetes Datasets obtained from machine learning repository of University of California at Irvine (UCI). The classification rules are extracted with 100% accuracy for all datasets. These experimental results are the best outcomes found in literature so far.},
   author = {Tahir Saǧ and Humar Kahramanli},
   doi = {10.1109/IDAP.2017.8090264},
   isbn = {9781538618806},
   journal = {IDAP 2017 - International Artificial Intelligence and Data Processing Symposium},
   keywords = {Genetic algorithms,Multiobjective optimization,Rule extraction},
   month = {10},
   note = {\{'id': 88,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II"], <br/>'ML task': ["Rule Mining","Classification"], <br/>'Objective functions': ["Rate of the correct classification","Rate of wrong classification"], <br/>'Single/Multi Solutions': "Single"<br/>\}},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Classification rule mining approach based on multiobjective optimization},
   year = {2017},
}
@article{,
   abstract = {This work presents a novel approach for decision-making for multi-objective binary classification problems. The purpose of the decision process is to select within a set of Pareto-optimal solutions, one model that minimizes the structural risk (generalization error). This new approach utilizes a kind of prior knowledge that, if available, allows the selection of a model that better represents the problem in question. Prior knowledge about the imprecisions of the collected data enables the identification of the region of equivalent solutions within the set of Pareto-optimal solutions. Results for binary classification problems with sets of synthetic and real data indicate equal or better performance in terms of decision efficiency compared to similar approaches.},
   author = {Talles Henrique de Medeiros and Honovan Paz Rocha and Frank Sill Torres and Ricardo Hiroshi Caldeira Takahashi and Antônio Pádua Braga},
   doi = {10.1007/S40313-016-0295-6/FIGURES/11},
   issn = {21953899},
   issue = {2},
   journal = {Journal of Control, Automation and Electrical Systems},
   keywords = {Classification,Decision-making,Machine learning,Multi-objective optimization},
   month = {4},
   note = {\{'id': 89,<br/>'keep': True, <br/>'Optimization': ["Deterministic","Levenberg–Marquardt algorithm"], <br/>'ML task': ["Prior knowledge","Binary Classification"], <br/>'Objective functions': ["Training Error","Complexity"], <br/>'Single/Multi Solutions': "Single - Prior Knowledge"<br/>\}},
   pages = {217-227},
   publisher = {Springer New York LLC},
   title = {Multi-objective Decision in Machine Learning},
   volume = {28},
   url = {https://link.springer.com/article/10.1007/s40313-016-0295-6},
   year = {2017},
}
@article{Das2017,
   abstract = {Clustering is an important unsupervised machine learning techniqueused in diverse fields to explore the inherent structure of the data. In most of the real life datasets, one object resides in many clusters with different membership values. Many clustering algorithms have been proposed for finding such overlapping clusters for knowledge extraction and future trend prediction. In the paper, multi-objective genetic algorithm based cluster analysis technique is proposed for finding the optimal set of overlapping clusters. As most of the real world search and optimization problems involve multiple objectives, multi-objective Genetic Algorithm is an obvious choice for capturing multiple optimal solutions. Thus the usefulness of applying the multi-objective Genetic Algorithm is to grouping the objects based on different objective functions for finding optimal set of overlapping clusters. The advantage of this algorithm is that it assigns a membership value only to the objects which are the members of several clusters, instead of assigning membership values for all clusters like fuzzy clustering algorithm. If any object positively belongs only to a single cluster, its membership value for this cluster is '1' and '0' for all other clusters. The overall performance of the method is investigated on some popular UCI and microarray datasets and the optimality of the clusters is measured by some important cluster validation indices. The experimental results show the effectiveness of the proposed method.},
   author = {Sunanda Das and Shreya Chaudhuri and Asit K. Das},
   doi = {10.1145/3055635.3056653},
   isbn = {9781450348171},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Cluster analysis,Cluster validation index,Fuzzy clustering,Multi-objective Genetic Algorithm,Overlapping cluster},
   month = {2},
   note = {\{'id': 90,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Genetic","NSGA"], <br/>'ML task': ["Clustering","Overlapping groups"], <br/>'Objective functions': ["Dunn index","Overlapping"], <br/>'Single/Multi Solutions': "Single"<br/>\}},
   pages = {232-237},
   publisher = {Association for Computing Machinery},
   title = {Optimal set of overlapping clusters using multi-objective genetic algorithm},
   volume = {Part F128357},
   url = {http://dx.doi.org/10.1145/3055635.3056653},
   year = {2017},
}
@article{Acampora2018,
   abstract = {The Support Vector Machine (SVM) is one of the most powerful algorithms for machine learning and data mining in numerous and heterogenous application domains. However, in spite of its competitiveness, SVM suffers from scalability problems which drastically worsens its performance in terms of memory requirements and execution time. As a consequence, there is a strong emergence of approaches for supporting SVM in efficiently addressing the aforementioned problems without affecting its classification capabilities. In this scenario, methods for Training Set Selection (TSS) represent a suitable and consolidated pre-processing technique to compute a reduced but representative training dataset, and improve SVM's scalability without deprecating its classification accuracy. Recently, TSS has been formulated as an optimization problem characterized by two objectives (the classification accuracy and the reduction rate) and solved through the application of evolutionary algorithms. However, so far, all the evolutionary approaches for TSS have been based on a so-called multi-objective a priori technique, where multiple objectives are aggregated together into a single objective through a weighted combination. This paper proposes to apply, for the first time, a Pareto-based multi-objective optimization approach to the TSS problem in order to explicitly deal with both its objectives and offer a better trade-off between SVM's classification and reduction performance. The benefits of the proposed approach are validated by a set of experiments involving well-known datasets taken from the UCI Machine Learning Database Repository. As shown by statistical tests, the application of a Pareto-based multi-objective optimization approach improves on state-of-the-art TSS techniques and enhances SVM efficiency.},
   author = {Giovanni Acampora and Francisco Herrera and Genoveffa Tortora and Autilia Vitiello},
   doi = {10.1016/J.KNOSYS.2018.02.022},
   issn = {0950-7051},
   journal = {Knowledge-Based Systems},
   keywords = {Multi-objective optimization,Support vector machine,Training set selection},
   month = {5},
   note = {\{'id': 91,<br/>'keep': True, <br/>'Optimization': ["Evolutionary", "PESA-II"], <br/>'ML task': ["SVM","Classification","Training Set Selection"], <br/>'Objective functions': ["Accuracy","Reduction rate"], <br/>'Single/Multi Solutions': "Single - Model with highiest sum of objectives"<br/>\}},
   pages = {94-108},
   publisher = {Elsevier},
   title = {A multi-objective evolutionary approach to training set selection for support vector machine},
   volume = {147},
   year = {2018},
}
@article{Wu2018,
   abstract = {Extreme Learning Machine (ELM) is a popular machine learning method and has been widely applied to real-world problems due to its fast training speed and good generalization performance. However, in ELM, the randomly assigned input weights and hidden biases usually degrade the generalization performance. Furthermore, ELM is considered as an empirical risk minimization model and easily leads to overfitting when dataset exists some outliers. In this paper, we proposed a novel algorithm named Multiobjective Optimization-based Sparse Extreme Learning Machine (MO-SELM), where parameter optimization and structure learning are integrated into the learning process to simultaneously enhance the generalization performance and alleviate the overfitting problem. In MO-SELM, the training error and the connecting sparsity are taken as two conflicting objectives of the multiobjective model, which aims to find sparse connecting structures with optimal weights and biases. Then, a hybrid encoding-based MOEA/D is used to optimize the multiobjective model. In addition, ensemble learning is embedded into this algorithm to make decisions after multiobjective optimization. Experimental results of several classification and regression applications demonstrate the effectiveness of the proposed MO-SELM.},
   author = {Yu Wu and Yongshan Zhang and Xiaobo Liu and Zhihua Cai and Yaoming Cai},
   doi = {10.1016/J.NEUCOM.2018.07.060},
   issn = {0925-2312},
   journal = {Neurocomputing},
   keywords = {Extreme learning machine,Multiobjective optimization,Parameter optimization,Sparse connecting structure,Structure learning},
   month = {11},
   note = {\{'id': 92,<br/>'keep': True, <br/>'Optimization': ["Evolutionary", "MOEA/D"], <br/>'ML task': ["Extreme Learning Machine","Classification","Regression"], <br/>'Objective functions': ["RMSE","Connecting sparsity"], <br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pages = {88-100},
   publisher = {Elsevier},
   title = {A multiobjective optimization-based sparse extreme learning machine algorithm},
   volume = {317},
   year = {2018},
}
@article{Cai2018,
   abstract = {Deep neural networks have been successfully applied to many data mining problems in recent works. The training of deep neural networks relies heavily upon gradient descent methods, however, which may lead to the failure of training due to the vanishing gradient (or exploding gradient) and local optima problems. In this paper, we present SEvoAE method based on using Evolutionary Multiobjective optimization (EMO) algorithm to train single layer auto-encoder, and sequentially learning deeper representation in a stacking way. SEvoAE is able to achieve accurate feature representation with good sparseness by globally simultaneously optimizing two conflicting objective functions and allows users to flexibly design objective functions and evolutionary optimizers. We compare results of the proposed method with existing architectures for seven classification prob- lems, showing that the proposed method is able to outperform existing methods with a reduced risk of overfitting the training data.},
   author = {Yaoming Cai and Zhihua Cai and Meng Zeng and Xiaobo Liu and Jia Wu and Guangjun Wang},
   doi = {10.1109/IJCNN.2018.8489138},
   isbn = {9781509060146},
   journal = {Proceedings of the International Joint Conference on Neural Networks},
   keywords = {Auto-encoder,Deep learning,Evolutionary Multiobjective Optimization},
   month = {10},
   note = {\{'id': 93,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II"], <br/>'ML task': ["Neural Networks","Autoencoder","Extreme Learning Machine"], <br/>'Objective functions': ["MSE","L1-norm of the hidden-layer outputs"], <br/>'Single/Multi Solutions': "Single"<br/>\}},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Novel Deep Learning Approach: Stacked Evolutionary Auto-encoder},
   volume = {2018-July},
   year = {2018},
}
@article{,
   abstract = {Data and knowledge representation are fundamental concepts in machine learning. The quality of the representation impacts the performance of a learning model directly. Feature learning transforms or enhances raw data to structures that are effectively exploited by those methods. In recent years, several works have been using complex networks for data representation and analysis. However, no feature learning method has been proposed to enhance such category of representation. Here, we present an unsupervised feature learning mechanism that works on datasets with binary features. First, the dataset is mapped into a feature-sample network. Then, a multi-objective optimization process selects a set of new vertices to produce an enhanced version of the network. The new features depend on a nonlinear function of a combination of preexisting features. Effectively, the process projects the input data into a higher-dimensional space. To solve the optimization problem, we design two metaheuristics based on the lexicographic genetic algorithm and the improved strength Pareto evolutionary algorithm (SPEA2). We show that the enhanced network contains more useful information and can be exploited to improve the performance of machine learning methods. The advantages and disadvantages of each optimization strategy are discussed.},
   author = {Filipe Alves Neto Verri and Renato Tinós and Liang Zhao},
   doi = {10.1109/CEC.2018.8477891},
   isbn = {9781509060177},
   journal = {2018 IEEE Congress on Evolutionary Computation, CEC 2018 - Proceedings},
   keywords = {Feature learning,complex networks,genetic algorithm,multiobjective optimization},
   month = {9},
   note = {\{'id': 94,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","SPEA2"], <br/>'ML task': ["Feature Learning","Feature–Sample Networks","Clustering"], <br/>'Objective functions': ["Number of features","Disproportion between networks"], <br/>'Single/Multi Solutions': "Single - Max number of features"<br/>\}},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Feature Learning in Feature-Sample Networks Using Multi-Objective Optimization},
   year = {2018},
}
@article{Jesus2018,
   abstract = {One of the main issues of machine learning algorithms is the curse of dimensionality. With the fast growing of complex data in real world scenarios, the feature selection becomes a mandatory preprocessing step in any application to reduce both the complexity of the data and the computing time. Based on that, several works have been produced in order to develop efficient methods to perform this task. Most feature selection methods select the best attributes based on some specific criteria. Additionally, recent studies have successfully constructed models to select features considering the particularities of the data, assuming that similar samples should be treated separately. Although some advance has been made, a bad choice of one single criteria to evaluate the importance of the attributes and the arbitrary choice of the number of features made by the user can lead to a poor analysis. In order to overcome some of these issues, this work brings an improvement of a dynamic feature selection algorithm (DFS) by using the idea of pareto front multi-objective optimization, which allow us to both consider distinct perspectives of the features relevance and automatically set the number of attributes to select. We tested our approach using 15 artificial and real world data and results have shown that when compared to the original DFS method, the performance of the proposed method is remarkable superior. In fact, the results are very promising since the proposed method also achieved better performance than well-established dimensionality reduction methods and when using the original datasets, showing that the reduction of noisy and/or redundant attributes can have a positive effect in the performance of a classification task.},
   author = {Jhoseph Jesus and Anne Canuto and Daniel Araujo},
   doi = {10.1109/IJCNN.2018.8489680},
   isbn = {9781509060146},
   journal = {Proceedings of the International Joint Conference on Neural Networks},
   month = {10},
   note = {\{'id': 95,<br/>'keep': True, <br/>'Optimization': ["Deterministic","Test all possibilities"], <br/>'ML task': ["Feature Selection","Clustering","Classification"], <br/>'Objective functions': ["Mutual Information","Kullback-Leibler Divergence","Spearman Correlation"], <br/>'Single/Multi Solutions': "Single - All of the selected features"<br/>\}},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Dynamic Feature Selection Based on Pareto Front Optimization},
   volume = {2018-July},
   year = {2018},
}
@article{Ribeiro2018,
   abstract = {Machine learning algorithms have found to be useful for the solution of complex engineering problems. However, due to problem's characteristics, such as class imbalance, classical methods may not be formidable. The authors believe that the application of multiobjective optimization design can improve the results of machine learning algorithms on such scenarios. Thus, this paper proposes a novel methodology for the creation of ensembles of classifiers. To do so, a multi-objective optimization design approach composed of two steps is used. The first step focus on generating a set of diverse classifiers, while the second step focus on the selection of such classifiers as ensemble members. The proposed method is tested on a real-world competition data set, using both decision trees and logistic regression classifiers. Results show that the ensembles created with such technique outperform the best ensemble members.},
   author = {Victor Henrique Alves Ribeiro and Gilberto Reynoso-Meza},
   doi = {10.1145/3205651.3208219},
   isbn = {9781450357647},
   journal = {GECCO 2018 Companion - Proceedings of the 2018 Genetic and Evolutionary Computation Conference Companion},
   keywords = {Decision trees,Ensemble methods,Logistic regression,Multi-objective optimization},
   month = {7},
   note = {\{'id': 96,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","sp-MODE"], <br/>'ML task': ["Ensemble","Classification"], <br/>'Objective functions': ["Accuracy","True Positive Rate","True Negative Rate","F1 Score", "Model Complexity","Ensemble Complexity"], <br/>'Single/Multi Solutions': "Single - Ensemble",<br/>'Obs': "Uses MOO to generate and select the models for the ensemble"<br/>\}},
   pages = {1882-1885},
   publisher = {Association for Computing Machinery, Inc},
   title = {A multi-objective optimization design framework for ensemble generation},
   url = {https://doi.org/10.1145/3205651.3208219},
   year = {2018},
}
@article{Zhang2018,
   abstract = {In machine learning, the notion of multi-objective model selection (MOMS) refers to the problem of identifying the set of Pareto-optimal models that optimize by compromising more than one predefined objectives simultaneously. This paper introduces SPRINT-Race, the first multi-objective racing algorithm in a fixed-confidence setting, which is based on the sequential probability ratio with indifference zone test. SPRINT-Race addresses the problem of MOMS with multiple stochastic optimization objectives in the proper Pareto-optimality sense. In SPRINT-Race, a pairwise dominance or non-dominance relationship is statistically inferred via a non-parametric, ternary-decision, dual-sequential probability ratio test. The overall probability of falsely eliminating any Pareto-optimal models or mistakenly returning any clearly dominated models is strictly controlled by a sequential Holm's step-down family-wise error rate control method. As a fixed-confidence model selection algorithm, the objective of SPRINT-Race is to minimize the computational effort required to achieve a prescribed confidence level about the quality of the returned models. The performance of SPRINT-Race is first examined via an artificially constructed MOMS problem with known ground truth. Subsequently, SPRINT-Race is applied on two real-world applications: 1) hybrid recommender system design and 2) multi-criteria stock selection. The experimental results verify that SPRINT-Race is an effective and efficient tool for such MOMS problems.},
   author = {Tiantian Zhang and Michael Georgiopoulos and Georgios C. Anagnostopoulos},
   doi = {10.1109/TCYB.2017.2647821},
   issn = {21682267},
   issue = {2},
   journal = {IEEE Transactions on Cybernetics},
   keywords = {Model selection (MS),multi-objective optimization,racing algorithm,sequential probability ratio test (SPRT)},
   month = {2},
   note = {\{'id': 97,<br/>'keep': False,<br/>'ML task': ["Model  Selection","Racing"], <br/>'Objective functions': ["User defined stochastic objectives"], <br/>'Single/Multi Solutions': "Multi",<br/>'Obs': "Does not train/generate the models, but selects them from a provided set"<br/>\}},
   pages = {596-610},
   pmid = {28166512},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Pareto-Optimal Model Selection via SPRINT-Race},
   volume = {48},
   year = {2018},
}
@article{Kiziloz2018,
   abstract = {Teaching Learning Based Optimization (TLBO) is a new metaheuristic that has been successfully applied to several intractable optimization problems in recent years. In this study, we propose a set of novel multiobjective TLBO algorithms combined with supervised machine learning techniques for the solution of Feature Subset Selection (FSS) in Binary Classification Problems (FSS-BCP). Selecting the minimum number of features while not compromising the accuracy of the results in FSS-BCP is a multiobjective optimization problem. We propose TLBO as a FSS mechanism and utilize its algorithm-specific parameterless concept that does not require any parameters to be tuned during the optimization. Most of the classical metaheuristics such as Genetic and Particle Swarm Optimization algorithms need additional efforts for tuning their parameters (crossover ratio, mutation ratio, velocity of particle, inertia weight, etc.), which may have an adverse influence on their performance. Comprehensive experiments are carried out on the well-known machine learning datasets of UCI Machine Learning Repository and significant improvements have been observed when the proposed multiobjective TLBO algorithms are compared with state-of-the-art NSGA-II, Particle Swarm Optimization, Tabu Search, Greedy Search, and Scatter Search algorithms.},
   author = {Hakan Ezgi Kiziloz and Ayça Deniz and Tansel Dokeroglu and Ahmet Cosar},
   doi = {10.1016/J.NEUCOM.2018.04.020},
   issn = {0925-2312},
   journal = {Neurocomputing},
   keywords = {Multiobjective feature selection,Supervised learning,Teaching learning based optimization},
   month = {9},
   note = {\{'id': 98,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Teaching Learning Based Optimization","MTLBO-MD"], <br/>'ML task': ["Feature Subset Selection","Binary Classification"], <br/>'Objective functions': ["Accuracy","Number of features"], <br/>'Single/Multi Solutions': "Single - Accuracy in Validation"<br/>\}},
   pages = {94-107},
   publisher = {Elsevier},
   title = {Novel multiobjective TLBO algorithms for the feature subset selection problem},
   volume = {306},
   year = {2018},
}
@article{Kang2018,
   abstract = {To improve the effect of logistic regression in multiobjective classification and explore its greatest potential, a set of training and classification algorithms is constructed, by using the high accuracy of two-class classification. Multi-layer predictions are made under the premise of ensuring clear structure of the model. The method of outlier detection is introduced to choose a proper number of two-class classifiers for categories that are prone to be confused. Then further predictions are made with these two-class classifiers. The evaluation on MNIST dataset show that this method can effectively improve the classification accuracy of multi-class datasets with limited increase of running time.},
   author = {Kai Kang and Fengqiang Gao and Junguo Feng},
   doi = {10.1109/ICCSE.2018.8468725},
   isbn = {9781538654958},
   journal = {13th International Conference on Computer Science and Education, ICCSE 2018},
   keywords = {Logistic regression,MNIST,Machine learning,Multi-objective classification,Outlier},
   month = {1},
   note = {\{'id': 99,<br/>'keep': False\}},
   pages = {52-55},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A new multi-layer classification method based on logistic regression},
   year = {2018},
}
@article{Dutta2019,
   abstract = {Many clustering algorithms categorized as K-clustering algorithm require the user to predict the number of clusters (K) to do clustering. Due to lack of domain knowledge an accurate value of K is difficult to predict. The problem becomes critical when the dimensionality of data points is large; clusters differ widely in shape, size, and density; and when clusters are overlapping in nature. Determining the suitable K is an optimization problem. Automatic clustering algorithms can discover the optimal K. This paper presents an automatic clustering algorithm which is superior to K-clustering algorithm as it can discover an optimal value of K. Iterative hill-climbing algorithms like K-Means work on a single solution and converge to a local optimum solution. Here, Genetic Algorithms (GAs) find out near global optimum solutions, i.e. optimal K as well as the optimal cluster centroids. Single-objective clustering algorithms are adequate for efficiently grouping linearly separable clusters. For non-linearly separable clusters they are not so good. So for grouping non-linearly separable clusters, we apply Multi-Objective Genetic Algorithm (MOGA) by minimizing the intra-cluster distance and maximizing inter-cluster distance. Many existing MOGA based clustering algorithms are suitable for either numeric or categorical features. This paper pioneered employing MOGA for automatic clustering with mixed types of features. Statistical testing on experimental results on real-life benchmark data sets from the University of California at Irvine (UCI) machine learning repository proves the superiority of the proposed algorithm.},
   author = {Dipankar Dutta and Jaya Sil and Paramartha Dutta},
   doi = {10.1016/J.ESWA.2019.06.056},
   issn = {0957-4174},
   journal = {Expert Systems with Applications},
   keywords = {Automatic clustering,Multi-Objective Genetic Algorithm (MOGA),Pareto approach,Statistical test},
   month = {12},
   note = {\{'id': 99,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","MOGA"], <br/>'ML task': ["Clustering","Automatic clustering"], <br/>'Objective functions': ["Compactness","Separateness"], <br/>'Single/Multi Solutions': "Single"<br/>\}},
   pages = {357-379},
   publisher = {Pergamon},
   title = {Automatic clustering by multi-objective genetic algorithm with numeric and categorical features},
   volume = {137},
   year = {2019},
}
@article{Rathee2019,
   abstract = {Data reduction has always been an important field of research to enhance the performance of data mining algorithms. Instance selection, a data reduction technique, relates to selecting a subset of informative and non-redundant examples from data. This paper deals...},
   author = {Seema Rathee and Saroj Ratnoo and Jyoti Ahuja},
   doi = {10.1007/978-981-13-0586-3_48},
   issn = {23673389},
   journal = {Lecture Notes in Networks and Systems},
   keywords = {CHC algorithm,Instance selection,KNN,Multi-objective optimization},
   note = {\{'id': 100,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","MOGA"], <br/>'ML task': ["Instance Selection","Classification"], <br/>'Objective functions': ["Accuracy","Percentage of data reduction"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {475-484},
   publisher = {Springer, Singapore},
   title = {Instance Selection Using Multi-objective CHC Evolutionary Algorithm},
   volume = {40},
   url = {https://link.springer.com/chapter/10.1007/978-981-13-0586-3_48},
   year = {2019},
}
@article{Chouikhi2019,
   abstract = {The Multi-Layered Echo-State Network (ML-ESN) is a recently developed, highly powerful type of recurrent neural network. It has succeeded in dealing with several non-linear benchmark problems. On account of its rich dynamics, ML-ESN is exploited in this paper, for the first time, as a recurrent Autoencoder (ML-ESNAE) to extract new features from original data representations. Further, the challenging and crucial task of optimally determining the ML-ESNAE architecture and training parameters is addressed, in order to extract more efficient features from the data. Traditionally, in a ML-ESN, the number of parameters (hidden neurons, sparsity rates, weights) are randomly chosen and manually altered to achieve a minimum learning error. On one hand, this random setting may not guarantee best generalization results. On the other, it can increase the network's complexity. In this paper, a novel bi-level evolutionary optimization approach is thus proposed for the ML-ESNAE, to deal with these challenges. The first level offers Pareto multi-objective architecture optimization, providing maximum learning accuracy while maintaining a reduced complexity target. Next, every Pareto optimal solution obtained from the first level undergoes a mono-objective weights optimization at the second level. Particle Swarm Optimization (PSO) is used as an evolutionary tool for both levels 1 and 2. An empirical study shows that the evolved ML-ESNAE produces a noticeable improvement in extracting new, more expressive data features from original ones. A number of application case studies, using a range of benchmark datasets, show that the extracted features produce excellent results in terms of classification accuracy. The effectiveness of the evolved ML-ESNAE is demonstrated for both noisy and noise-free data. In conclusion, the evolutionary ML-ESNAE is proposed as a new benchmark for the evolutionary AI and machine learning research community.},
   author = {Naima Chouikhi and Boudour Ammar and Amir Hussain and Adel M. Alimi},
   doi = {10.1016/J.NEUCOM.2019.03.012},
   issn = {0925-2312},
   journal = {Neurocomputing},
   keywords = {Architecture optimization,Autoencoder,Data representation,Multi-Layered Echo State Network,Multi-objective optimization,PSO,Weights optimization},
   month = {5},
   note = {\{'id': 101,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Particle Swarm Optimization"], <br/>'ML task': ["Autoencoder","Multi-LayeredEcho-StateNetwork","Architecture optimization","Classification"], <br/>'Objective functions': ["RMSE","Average Reservoirs Connectivity Rate","Average reservoir sizes"], <br/>'Single/Multi Solutions': "Single - Optimize RMSE"<br/>\}},
   pages = {195-211},
   publisher = {Elsevier},
   title = {Bi-level multi-objective evolution of a Multi-Layered Echo-State Network Autoencoder for data representations},
   volume = {341},
   year = {2019},
}
@article{Lyubchenko2019,
   abstract = {Feature selection is known as a very useful technique in machine learning practice as it may result in the development of more straightforward models with better accuracy. Traditionally, feature selection is considered as a single-objective problem, however, it can be easily formulated in terms of two objectives. The solving of such problems requires the application of appropriate multi-objective optimization methods that do not always offer equally good solutions even under the same conditions. This paper focuses on the development of a metaheuristic optimization approach for bi-objective feature selection problem in two-class classification. We consider the solving of this problem in terms of minimization of both misclassification error and feature subset size. For solving the considered problem, an adaptation of the Multi-Objective Adaptive Memory Programming (MOAMP) metaheuristic based on the tabu search strategy is proposed. Our MOAMP adaption has been utilized to obtain the sets of most relevant features for two real classification problems with two classes. Finally, using popular Pareto front quality indicators, the obtained results have been compared with the sets of non-dominated solutions derived by the well-known NSGA2 algorithm. The conducted research allows concluding about the ability of the MOAMP adaptation to get a better efficient frontier for the same number of objective function calls.},
   author = {A. A. Lyubchenko and J. A. Pacheco and S. Casado and L. Nuñez},
   doi = {10.1088/1742-6596/1210/1/012086},
   issn = {17426596},
   issue = {1},
   journal = {Journal of Physics: Conference Series},
   month = {5},
   note = {\{'id': 102,<br/>'keep': True, <br/>'Optimization': ["Metaheuristic","Multi-Objective  Adaptive  Memory  Programming","MOAMP"], <br/>'ML task': ["Feature  Selection","Classification"], <br/>'Objective functions': ["Misclassification  error","Subset size"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   publisher = {Institute of Physics Publishing},
   title = {An Effective Metaheuristic for Bi-objective Feature Selection in Two-Class Classification Problem},
   volume = {1210},
   year = {2019},
}
@article{Senhaji2019,
   abstract = {The multi-layer perceptron has proved its efficiencies in several fields as pattern and voice recognition. Unfortunately, the classical training for MLP suffers from a poor generalization. In this respect, we have proposed a new multi-objective training model with...},
   author = {Kaoutar Senhaji and Hassan Ramchoun and Mohamed Ettaouil},
   doi = {10.1007/978-3-319-91337-7_15},
   isbn = {9783319913360},
   issn = {21945357},
   journal = {Advances in Intelligent Systems and Computing},
   keywords = {Multi-objective training,Multilayer perceptron,Non-dominated Sorting Genetic Algorithm II (NSGA II),Non-linear optimization,Pareto front,Supervised learning},
   note = {\{'id': 103,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II"], <br/>'ML task': ["Multilayer Perceptron","Classification"], <br/>'Objective functions': ["Training error","Network weights"], <br/>'Single/Multi Solutions': "Single - Crowding distance"<br/>\}},
   pages = {154-167},
   publisher = {Springer, Cham},
   title = {Multilayer Perceptron: NSGA II for a New Multi-objective Learning Method for Training and Model Complexity},
   volume = {756},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-91337-7_15},
   year = {2019},
}
@article{Gardner2019,
   abstract = {Automated machine learning has gained a lot of attention recently. Building and selecting the right machine learning models is often a multi-objective optimization problem. General purpose machine learning software that simultaneously supports multiple objectives and constraints is scant, though the potential benefits are great. In this work, we present a framework called Autotune that effectively handles multiple objectives and constraints that arise in machine learning problems. Autotune is built on a suite of derivative-free optimization methods, and utilizes multi-level parallelism in a distributed computing environment for automatically training, scoring, and selecting good models. Incorporation of multiple objectives and constraints in the model exploration and selection process provides the flexibility needed to satisfy trade-offs necessary in practical machine learning applications. Experimental results from standard multi-objective optimization benchmark problems show that Autotune is very efficient in capturing Pareto fronts. These benchmark results also show how adding constraints can guide the search to more promising regions of the solution space, ultimately producing more desirable Pareto fronts. Results from two real-world case studies demonstrate the effectiveness of the constrained multi-objective optimization capability offered by Autotune.},
   author = {Steven Gardner and Oleg Golovidov and Joshua Griffin and Patrick Koch and Wayne Thompson and Brett Wujek and Yan Xu},
   doi = {10.1109/DSAA.2019.00051},
   isbn = {9781728144931},
   journal = {Proceedings - 2019 IEEE International Conference on Data Science and Advanced Analytics, DSAA 2019},
   keywords = {Automated Machine Learning,Distributed Computing System,Multi-objective Optimization},
   month = {10},
   note = {\{'id': 104,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Genetic","Autotune"], <br/>'ML task': ["AutoML","Classification"], <br/>'Objective functions': ["Misclassification Rate","False Positive Rate","Black-box functions"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {364-373},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Constrained multi-objective optimization for automated machine learning},
   year = {2019},
}
@article{Lu2019,
   abstract = {This paper introduces NSGA-Net - an evolutionary approach for neural architecture search (NAS). NSGA-Net is designed with three goals in mind: (1) a procedure considering multiple and conflicting objectives, (2) an efficient procedure balancing exploration and exploitation of the space of potential neural network architectures, and (3) a procedure finding a diverse set of trade-off network architectures achieved in a single run. NSGA-Net is a population-based search algorithm that explores a space of potential neural network architectures in three steps, namely, a population initialization step that is based on prior-knowledge from hand-crafted architectures, an exploration step comprising crossover and mutation of architectures, and finally an exploitation step that utilizes the hidden useful knowledge stored in the entire history of evaluated neural architectures in the form of a Bayesian Network. Experimental results suggest that combining the dual objectives of minimizing an error metric and computational complexity, as measured by FLOPs, allows NSGA-Net to find competitive neural architectures. Moreover, NSGA-Net achieves error rate on the CIFAR-10 dataset on par with other state-of-the-art NAS methods while using orders of magnitude less computational resources. These results are encouraging and shows the promise to further use of EC methods in various deep-learning paradigms.},
   author = {Zhichao Lu and Ian Whalen and Vishnu Boddeti and Yashesh Dhebar and Kalyanmoy Deb and Erik Goodman and Wolfgang Banzhaf},
   doi = {10.1145/3321707.3321729},
   isbn = {9781450361118},
   journal = {GECCO 2019 - Proceedings of the 2019 Genetic and Evolutionary Computation Conference},
   keywords = {Bayesian Optimization,Deep Learning,Image classification,Multi objective,Neural Architecture Search},
   month = {7},
   note = {\{'id': 105,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II"], <br/>'ML task': ["Architecture Search","Neural Network","Classification"], <br/>'Objective functions': ["Classification Error","Computational complexity"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {419-427},
   publisher = {Association for Computing Machinery, Inc},
   title = {NSGA-Net: Neural architecture search using multi-objective genetic algorithm},
   year = {2019},
}
@article{Vu2019,
   abstract = {This paper proposes a multi-objective competitive co-evolutionary algorithm (MOCPCEA) based on the PreyPredator model to solve classification problems. In the MOCPCEA, a data population acts as preys. To be specific, each prey represents a selected subset of the training dataset. Another population is ANN classifiers which play as Predators. The task of the Predators is to try to classify the data sets as correctly as possible, whereas the Preys try to find the data sets that are difficult to be classified. Through this interaction process, MOCPCEA generates a set of classifiers that are able to classify difficult data sets. The final classification result is given by the ensemble voting mechanism among these sets of classifiers. The performance of the proposed algorithm is performed on seven benchmark problems. Through comparison with other algorithms, the proposed algorithm indicates that it could create an ensemble of ANN networks that give high and stable classification results.},
   author = {Van Truong Vu and Lam Thu Bui and Trung Thanh Nguyen},
   doi = {10.1109/NICS48868.2019.9023887},
   isbn = {9781728151632},
   journal = {Proceedings - 2019 6th NAFOSTED Conference on Information and Computer Science, NICS 2019},
   keywords = {Classification,Competitive co-evolutionary,Ensemble learning.,Multiobjective optimization,Prey-Predator},
   month = {12},
   note = {\{'id': 106,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Prey-Predator"], <br/>'ML task': ["Classification","Neural Networks","Ensemble"], <br/>'Objective functions': ["Accuracy","Diversity"], <br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pages = {49-54},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A multi-objective competitive co-evolutionary approach for classification problems},
   year = {2019},
}
@article{Evans2019,
   abstract = {Interpreting state-of-the-art machine learning algorithms can be difficult. For example, why does a complex ensemble predict a particular class? Existing approaches to interpretable machine learning tend to be either local in their explanations, apply only to a particular algorithm, or overly complex in their global explanations. In this work, we propose a global model extraction method which uses multi-objective genetic programming to construct accurate, simplistic and model-agnostic representations of complex black-box estimators. We found the resulting representations are far simpler than existing approaches while providing comparable reconstructive performance. This is demonstrated on a range of datasets, by approximating the knowledge of complex black-box models such as 200 layer neural networks and ensembles of 500 trees, with a single tree.},
   author = {Benjamin P. Evans and Bing Xue and Mengjie Zhang},
   doi = {10.1145/3321707.3321726},
   isbn = {9781450361118},
   journal = {GECCO 2019 - Proceedings of the 2019 Genetic and Evolutionary Computation Conference},
   keywords = {Evolutionary Multi-objective Optimisation,Explainable Artificial Intelligence,Interpretable Machine Learning},
   month = {7},
   note = {\{'id': 107,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Genetic","NSGA-II"], <br/>'ML task': ["Expainable AI","Interpretable ML","Classification"], <br/>'Objective functions': ["f1-score","Split points"], <br/>'Single/Multi Solutions': "Single - Largest f1"<br/>\}},
   pages = {1012-1020},
   publisher = {Association for Computing Machinery, Inc},
   title = {What's inside the black-box? A genetic programming method for interpreting complex machine learning models},
   url = {https://doi.org/10.1145/3321707.3321726},
   year = {2019},
}
@article{Yu2019,
   abstract = {Classification of high-dimensional data with very limited labels is a challenging task in the field of data mining and machine learning. In this paper, we propose the multiobjective semisupervised classifier ensemble (MOSSCE) approach to address this challenge. Specifically, a multiobjective subspace selection process (MOSSP) in MOSSCE is first designed to generate the optimal combination of feature subspaces. Three objective functions are then proposed for MOSSP, which include the relevance of features, the redundancy between features, and the data reconstruction error. Then, MOSSCE generates an auxiliary training set based on the sample confidence to improve the performance of the classifier ensemble. Finally, the training set, combined with the auxiliary training set, is used to select the optimal combination of basic classifiers in the ensemble, train the classifier ensemble, and generate the final result. In addition, diversity analysis of the ensemble learning process is applied, and a set of nonparametric statistical tests is adopted for the comparison of semisupervised classification approaches on multiple datasets. The experiments on 12 gene expression datasets and two large image datasets show that MOSSCE has a better performance than other state-of-the-art semisupervised classifiers on high-dimensional data.},
   author = {Zhiwen Yu and Yidong Zhang and C. L.Philip Chen and Jane You and Hau San Wong and Dan Dai and Si Wu and Jun Zhang},
   doi = {10.1109/TCYB.2018.2824299},
   issn = {21682267},
   issue = {6},
   journal = {IEEE Transactions on Cybernetics},
   keywords = {Ensemble learning,feature selection,multiobjective optimization,semisupervised learning},
   month = {6},
   note = {\{'id': 108,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Genetic","NSGA-II"], <br/>'ML task': ["Classification","Ensemble","Feature Selection"], <br/>'Objective functions': ["Relevance of features","Redundancy between features","Data reconstruction error"], <br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pages = {2280-2293},
   pmid = {29993923},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Multiobjective Semisupervised Classifier Ensemble},
   volume = {49},
   year = {2019},
}
@article{Dyankov2019,
   abstract = {Deep Neural Networks (DNNs) are often criticized because they lack the ability to learn more than one task at a time: Multitask Learning is an emerging research area whose aim is to overcome this issue. In this work, we introduce the Pareto Multitask Learning...},
   author = {Deyan Dyankov and Salvatore Danilo Riccio and Giuseppe Di Fatta and Giuseppe Nicosia},
   doi = {10.1007/978-3-030-37599-7_50},
   isbn = {9783030375980},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Atari 2600 Games,Deep artificial neural networks,Deep neuroevolution,Evolution Strategy,Hypervolume,Kullback-Leibler Divergence,Multitask learning,Neural and evolutionary computing},
   note = {\{'id': 109,<br/>'keep': True, <br/>'Optimization': ["Evolutionary"], <br/>'ML task': ["Multitask Learning","Deep Neural Networks"], <br/>'Objective functions': ["Utility function of each task"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {605-618},
   publisher = {Springer, Cham},
   title = {Multi-task Learning by Pareto Optimality},
   volume = {11943 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-37599-7_50},
   year = {2019},
}
@article{Barbiero2019,
   abstract = {When a machine learning algorithm is able to obtain the same performance given a complete training set, and a small subset of samples from the same training set, the subset is termed coreset. As using a coreset improves training speed and allows human experts to gain a better understanding of the data, by reducing the number of samples to be examined, coreset discovery is an active line of research. Often in literature the problem of coreset discovery is framed as i. single-objective, attempting to find the candidate coreset that best represents the training set, and ii. independent from the machine learning algorithm used. In this work, an approach to evolutionary coreset discovery is presented. Building on preliminary results, the proposed approach uses a multi-objective evolutionary algorithm to find compromises between two conflicting objectives, i. minimizing the number of samples in a candidate coreset, and ii. maximizing the accuracy of a target classifier, trained with the coreset, on the whole original training set. Experimental results on popular classification benchmarks show that the proposed approach is able to identify candidate coresets with better accuracy and generality than state-of-the-art coreset discovery algorithms found in literature.},
   author = {Pietro Barbiero and Giovanni Squillero and Alberto Tonda},
   city = {New York, NY, USA},
   doi = {10.1145/3319619},
   isbn = {9781450367486},
   journal = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
   keywords = {Classification,Coreset discovery,Evolutionary algorithms,Explain AI,Machine learning,Multi-objective},
   note = {\{'id': 110,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II"], <br/>'ML task': ["Classification","Coreset discovery","Explainable AI"], <br/>'Objective functions': ["Number of samples","Accuracy"], <br/>'Single/Multi Solutions': "Single - Accuracy in Validation"<br/>\}},
   pages = {pages},
   publisher = {ACM},
   title = {Evolutionary Discovery of Coresets for Classification},
   volume = {8},
   url = {https://doi.org/10.1145/3319619.3326846},
   year = {2019},
}
@article{Rado2019,
   abstract = {The current advances of computational power and storage allow more models to be created and stored from significant data resources. This progress opens the opportunity to re-cycle and re-use such models in similar exercises. The evaluation of the machine learning...},
   author = {Omesaad Rado and Daniel Neagu},
   doi = {10.1007/978-3-030-34885-4_42},
   isbn = {9783030348847},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Classification algorithms,Optimization,Pareto set},
   note = {\{'id': 111,<br/>'keep': False<br/>\}},
   pages = {494-499},
   publisher = {Springer, Cham},
   title = {On Selection of Optimal Classifiers},
   volume = {11927 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-34885-4_42},
   year = {2019},
}
@article{Pantula2019,
   abstract = {In the recent era, multi-criteria decision making under uncertainty is gaining importance due to its wide range of applicability. Among several types of uncertainty handling techniques, Robust Optimization (RO) is considered as an efficient and tractable approach provided one has accessibility to data in uncertain regions. However, solutions of RO may actually deviate from actual results in real scenarios, due to conservative sampling. This paper proposes a methodology to amalgamate unsupervised machine learning algorithms with RO which thereby makes it data-driven. A novel evolutionary fuzzy clustering mechanism is implemented to transcript the uncertain space such that the exact regions of uncertainty are identified. Subsequently, density based boundary point detection and Delaunay triangulation based boundary construction enables intelligent Sobol based sampling in these regions for use in RO. Results of two test cases with varying dimensions are presented along with a comprehensive comparison between conventional RO approach using box uncertainty set and proposed methodology. Considered case studies include highly nonlinear real life model for continuous casting from steelmaking industries, where a time expensive multi-objective optimization problem under uncertainty is formulated to resolve the conflict in productivity and energy consumption. Optimal Artificial Neural Network (ANN) surrogate assisted optimization under uncertainty for casting model is performed to obtain solutions in realistic time. The resulting RO problem being multi-objective in nature, the Pareto solutions are obtained by NSGA II.},
   author = {Priyanka D. Pantula and Kishalay Mitra},
   doi = {10.1109/CEC.2019.8790094},
   isbn = {9781728121536},
   journal = {2019 IEEE Congress on Evolutionary Computation, CEC 2019 - Proceedings},
   keywords = {ANN surrogate models,Data Driven Robust Optimization,Evolutionary Algorithms,Fuzzy Clustering,Multi objective Optimization},
   month = {6},
   note = {\{'id': 112,<br/>'keep': False,<br/>'Obs': "Method uses ML (clustering) to improove optimization, not the other way around"\}},
   pages = {2990-2997},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {An Evolutionary Machine Learning Approach Towards Less Conservative Robust Optimization},
   year = {2019},
}
@article{Hu2019,
   abstract = {The ensemble pruning system is an effective machine learning framework that combines several learners as experts to classify a test set. Generally, ensemble pruning systems aim to define a region of competence based on the validation set to select the most competent ensembles from the ensemble pool with respect to the test set. However, the size of the ensemble pool is usually fixed, and the performance of an ensemble pool heavily depends on the definition of the region of competence. In this paper, a dynamic pruning framework called margin-based Pareto ensemble pruning is proposed for ensemble pruning systems. The framework explores the optimized ensemble pool size during the overproduction stage and finetunes the experts during the pruning stage. The Pareto optimization algorithm is used to explore the size of the overproduction ensemble pool that can result in better performance. Considering the information entropy of the learners in the indecision region, the marginal criterion for each learner in the ensemble pool is calculated using margin criterion pruning, which prunes the experts with respect to the test set. The effectiveness of the proposed method for classification tasks is assessed using datasets. The results show that margin-based Pareto ensemble pruning can achieve smaller ensemble sizes and better classification performance in most datasets when compared with state-of-the-art models.},
   author = {Ruihan Hu and Songbin Zhou and Yisen Liu and Zhiri Tang},
   doi = {10.1155/2019/7560872},
   issn = {16875273},
   journal = {Computational Intelligence and Neuroscience},
   note = {\{'id': 113,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II"], <br/>'ML task': ["Classification","Ensemble","Ensemble Pruning"], <br/>'Objective functions': ["Classification Error","Number of models selected"], <br/>'Single/Multi Solutions': "Single"<br/>\}},
   pmid = {31281338},
   publisher = {Hindawi Limited},
   title = {Margin-Based Pareto Ensemble Pruning: An Ensemble Pruning Algorithm That Learns to Search Optimized Ensembles},
   volume = {2019},
   year = {2019},
}
@article{Yamaguchi2019,
   abstract = {This paper describes solving multi-objective reinforcement learning problems where there are multiple conflicting objectives with unknown weights. Reinforcement learning (RL) is a popular algorithm for automatically solving sequential decision problems and most of...},
   author = {Tomohiro Yamaguchi and Shota Nagahama and Yoshihiro Ichikawa and Keiki Takadama},
   doi = {10.1007/978-3-030-22649-7_25},
   isbn = {9783030226480},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Average reward,Model-based,Multi-objective reinforcement learning,Reward occurrence probability,Reward vector},
   note = {\{'id': 114,<br/>'keep': False<br/>\}},
   pages = {311-321},
   publisher = {Springer, Cham},
   title = {Model-Based Multi-objective Reinforcement Learning with Unknown Weights},
   volume = {11570 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-22649-7_25},
   year = {2019},
}
@article{Fernandes2019,
   abstract = {Inductive learning from multi-class and imbalanced datasets is one of the main challenges for machine learning. Most machine learning algorithms have their predictive performance negatively affected by imbalanced data. Although several techniques have been proposed to deal with this difficulty, they are usually restricted to binary classification datasets. Thus, one of the research challenges in this area is how to deal with imbalanced multiclass classification datasets. This challenge become more difficult when classes containing fewer instances are located in overlapping regions of the data attribute space. In fact, several studies have indicated that the degree of class overlapping has a higher effect on predictive performance than the global class imbalance ratio. This paper proposes a novel evolutionary ensemble-based method for multi-class imbalanced learning called the evolutionary inversion of class distribution in overlapping areas for multi-class imbalanced learning (EVINCI). EVINCI uses a multiobjective evolutionary algorithm (MOEA) to evolve a set of samples taken from an imbalanced dataset. It selectively reduces the concentration of less representative instances of the majority classes in the overlapping areas while selecting samples that produce more accurate models. In experiments performed to evaluate its predictive accuracy, EVINCI was superior to state-of-the-art ensemble-based methods for imbalanced learning.},
   author = {Everlandio R.Q. Fernandes and Andre C.P.L.F. de Carvalho},
   doi = {10.1016/J.INS.2019.04.052},
   issn = {0020-0255},
   journal = {Information Sciences},
   keywords = {Ensemble of classifiers,Evolutionary algorithms,Multi-Class imbalanced learning},
   month = {8},
   note = {\{'id': 115,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II"], <br/>'ML task': ["Ensemble","Classification","Multi-Class","Imbalanced Learning"], <br/>'Objective functions': ["Accuracy","percentage of instances of the majority classes that are at the border of separation with the minority classes"], <br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pages = {141-154},
   publisher = {Elsevier},
   title = {Evolutionary inversion of class distribution in overlapping areas for multi-class imbalanced learning},
   volume = {494},
   year = {2019},
}
@article{Pei2019,
   abstract = {We propose to use the relationship between the parameter of kernel function and its decisional angle or distance metrics for selecting the optimal setting of the parameter of kernel functions in kernel method-based algorithms. Kernel method is established in the reproducing kernel Hilbert space, the angle and distance are two metrics in such space. We analyse and investigate the relationship between the parameter of kernel function and the metrics (distance or angle) in the reproducing kernel Hilbert space. We design a target function of optimization to model the relationship between these two variables, and found that (1) the landscape shapes of parameter and the metrics are the same in Gaussian kernel function because the norm of all the vectors are equal to one in reproducing kernel Hilbert space; (2) the landscape monotonicity of that are opposite in polynomial kernel function from that of Gaussian kernel. The monotonicity of designed target functions of optimization using Gaussian kernel and polynomial kernel is different as well. The distance metric and angle metric have different distribution characteristics for the decision of parameter setting in kernel function. It needs to balance these two metrics when selecting a proper parameter of the kernel function in kernel-based algorithms. We use evolutionary multi-objective optimization algorithms to obtain the Pareto solutions for optimal selection of the parameter in kernel functions. We found that evolutionary multi-objective optimization algorithms are useful tools to balance the distance metric and angle metric in the decision of parameter setting in kernel method-based algorithms.},
   author = {Yan Pei},
   doi = {10.1109/SSCI44817.2019.9002691},
   isbn = {9781728124858},
   journal = {2019 IEEE Symposium Series on Computational Intelligence, SSCI 2019},
   keywords = {decision making,evolutionary multi-objective optimization,kernel function,kernel method,machine learning,reproducing kernel Hilbert space},
   month = {12},
   note = {\{'id': 116,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Chaotic evolution"], <br/>'ML task': ["Kernel methods","AutoML","Classification"], <br/>'Objective functions': ["Distance","Angle"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {3207-3214},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Automatic Decision Making for Parameters in Kernel Method},
   year = {2019},
}
@article{Li2019,
   abstract = {Modern machine learning datasets can have biases for certain representations that are leveraged by algorithms to achieve high performance without learning to solve the underlying task. This problem is referred to as 'representation bias'. The question of how to reduce the representation biases of a dataset is investigated and a new dataset REPresentAtion bIas Removal (REPAIR) procedure is proposed. This formulates bias minimization as an optimization problem, seeking a weight distribution that penalizes examples easy for a classifier built on a given feature representation. Bias reduction is then equated to maximizing the ratio between the classification loss on the reweighted dataset and the uncertainty of the ground-truth class labels. This is a minimax problem that REPAIR solves by alternatingly updating classifier parameters and dataset resampling weights, using stochastic gradient descent. An experimental set-up is also introduced to measure the bias of any dataset for a given representation, and the impact of this bias on the performance of recognition models. Experiments with synthetic and action recognition data show that dataset REPAIR can significantly reduce representation bias, and lead to improved generalization of models trained on REPAIRed datasets. The tools used for characterizing representation bias, and the proposed dataset REPAIR algorithm, are available at https://github.com/JerryYLi/Dataset-REPAIR/.},
   author = {Yi Li and Nuno Vasconcelos},
   doi = {10.1109/CVPR.2019.00980},
   isbn = {9781728132938},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   keywords = {Action Recognition,Datasets and Evaluation,Deep Learning,Representation Learning,Video Analytics},
   month = {6},
   note = {\{'id': 117,<br/>'keep': False\}},
   pages = {9564-9573},
   publisher = {IEEE Computer Society},
   title = {Repair: Removing representation bias by dataset resampling},
   volume = {2019-June},
   year = {2019},
}
@article{Yan2020,
   abstract = {This article proposes single-objective/multiobjective cat swarm optimization clustering algorithms for data partition. The proposed methods use the cat swarm to search the optimal. The position of the cat tightly associates with the clustering centers and is updated by two submodes: the seeking mode and the tracing mode. The seeking mode uses the simulated annealing strategy to update the cat position at a probability. Inspired by the quantum theories, the tracing mode adopts the quantum model to update the cat position in the whole solution space. First, the single-objective method is proposed and adopts the cohesion of clustering as the objective function, in which the kernel method is applied. For considering more objective functions to reveal diverse aspects of data, the multiobjective method is proposed and adopts both the cohesion and the connectivity as the objective functions. The Pareto optimization method is applied to balance the objectives. In the experiments, three kinds of data sets are used to examine the effectiveness of the proposed methods, which are three synthetic data sets, four data sets from the UCI Machine Learning Repository, and a field data set. Experimental results verified that the proposed methods perform better than the traditional clustering algorithms, and the proposed multiobjective method has the highest accuracy. Note to Practitioners-This article presents single-objective/multiobjective cat swarm optimization clustering analysis methods for data partition. Through automatically extracting meaningful or useful classes, clustering analysis could help the practitioners or the intelligent devices find the specific meanings of data, natural data structure, the data relationships, or other characteristics. The proposed methods use the cat swarm to search the optimal clustering result. One or more criterion functions could be selected as the optimization objectives. The time complexity of the multiobjective type is higher than that of the single-objective type. Therefore, in the industrial field, engineers should choose the number of the optimization objectives based on the actual requirements. The proposed methods could be widely used into industrial applications to deal with complex data sets. Future research could consider some more progressive optimization schemes to improve the effectiveness.},
   author = {Dapeng Yan and Hui Cao and Yajie Yu and Yanxia Wang and Xiang Yu},
   doi = {10.1109/TASE.2020.2969485},
   issn = {15583783},
   issue = {3},
   journal = {IEEE Transactions on Automation Science and Engineering},
   keywords = {Clustering analysis,data partition,quantum model,single-objective/multiobjective optimization},
   month = {7},
   note = {\{'id': 118,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Cat Swarm Optimization"], <br/>'ML task': ["Clustering","Data Partition"], <br/>'Objective functions': ["Cohesion","Connectivity"], <br/>'Single/Multi Solutions': "Single - Max crowding-distance"<br/>\}},
   pages = {1633-1646},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Single-Objective/Multiobjective Cat Swarm Optimization Clustering Analysis for Data Partition},
   volume = {17},
   year = {2020},
}
@article{Zhang2020,
   abstract = {Deep Neural Networks (DNNs) have been widely applied in safety- and security-critical aspects, where the robustness of the system is of great significance, especially for corner case inputs. Traditionally, a DNN is tested with manually labeled data, which is not only labor-consuming, but also unable to contain statistically rare case inputs.In our work, we design, implement and evaluate the test input generation framework guided by multi-objective functions. The multi-objective functions are formed from neuron coverage, behavioral divergence and perturbation degree. We leverage evolutionary algorithms (EAs) to resolve such optimization problem by generating approximation to Pareto-optimal solutions. By implementing our framework, we successfully generated more than 6,000 test inputs for a convolutional neural network. And the generated test inputs help to improve the system's accuracy by up to 4.4%.},
   author = {Lingfeng Zhang and Hiroyuki Sato},
   doi = {10.1109/CANDARW51189.2020.00040},
   isbn = {9781728199191},
   journal = {Proceedings - 2020 8th International Symposium on Computing and Networking Workshops, CANDARW 2020},
   keywords = {Automated test input generation,Deep learning testing,Evolutionary algorithms},
   month = {11},
   note = {\{'id': 119,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II"], <br/>'ML task': ["Deep Neural Networks","Input generation","Classification"], <br/>'Objective functions': ["Neuron coverage","Behavioral divergence","Pertubation degree"], <br/>'Single/Multi Solutions': "Single - Crowded tournament selection"<br/>\}},
   pages = {157-163},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Automated Test Input Generation for Convolutional Neural Networks by Implementing Multi-objective Evolutionary Algorithms},
   year = {2020},
}
@article{Marchisio2020,
   abstract = {Recently, Capsule Networks (CapsNets) have shown improved performance compared to the traditional Convolutional Neural Networks (CNNs), by encoding and preserving spatial relationships between the detected features in a better way. This is achieved through the so-called Capsules (i.e., groups of neurons) that encode both the instantiation probability and the spatial information. However, one of the major hurdles in the wide adoption of CapsNets is their gigantic training time, which is primarily due to the relatively higher complexity of their new constituting elements that are different from CNNs.In this paper, we implement different optimizations in the training loop of the CapsNets, and investigate how these optimizations affect their training speed and the accuracy. Towards this, we propose a novel framework FasTrCaps that integrates multiple lightweight optimizations and a novel learning rate policy called WarmAdaBatch (that jointly performs warm restarts and adaptive batch size), and steers them in an appropriate way to provide high training-loop speedup at minimal accuracy loss. We also propose weight sharing for capsule layers. The goal is to reduce the hardware requirements of CapsNets by removing unused/redundant connections and capsules, while keeping high accuracy through tests of different learning rate policies and batch sizes. We demonstrate that one of the solutions generated by the FasTrCaps framework can achieve 58.6% reduction in the training time, while preserving the accuracy (even 0.12% accuracy improvement for the MNIST dataset), compared to the CapsNet by Google Brain [25]. Moreover, the Pareto-optimal solutions generated by FasTrCaps can be leveraged to realize trade-offs between training time and achieved accuracy. We have open-sourced our framework on GitHub1.},
   author = {Alberto Marchisio and Beatrice Bussolino and Alessio Colucci and Muhammad Abdullah Hanif and Maurizio Martina and Guido Masera and Muhammad Shafique},
   doi = {10.1109/IJCNN48605.2020.9207533},
   isbn = {9781728169262},
   journal = {Proceedings of the International Joint Conference on Neural Networks},
   keywords = {Accuracy,Adaptivity,Batch Sizing,Capsule Networks,Decoder,Efficiency,Machine Learning,Performance,Training,Weight Sharing},
   month = {7},
   note = {\{'id': 120,<br/>'keep': False<br/>\}},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {FasTrCaps: An Integrated Framework for Fast yet Accurate Training of Capsule Networks},
   year = {2020},
}
@article{Yin2020,
   abstract = {With the advent of deeper, larger and more complex convolutional neural networks (CNN), manual design has become a daunting task, especially when hardware performance must be optimized. Sequential model-based optimization (SMBO) is an efficient method for hyperparameter optimization on highly parameterized machine learning (ML) algorithms, able to find good configurations with a limited number of evaluations by predicting the performance of candidates before evaluation. A case study on MNIST shows that SMBO regression model prediction error significantly impedes search performance in multi-objective optimization. To address this issue, we propose probabilistic SMBO, which selects candidates based on probabilistic estimation of their Pareto efficiency. With a formulation that incorporates error in accuracy prediction and uncertainty in latency measurement, probabilistic Pareto efficiency quantifies a candidate's quality in two ways: its likelihood of being Pareto optimal, and the expected number of current Pareto optimal solutions that it will dominate. We evaluate our proposed method on four image classification problems. Compared to a deterministic approach, probabilistic SMBO consistently generates Pareto optimal solutions that perform better, and that are competitive with state-of-the-art efficient CNN models, offering tremendous speedup in inference latency while maintaining comparable accuracy.},
   author = {Zixuan Yin and Warren Gross and Brett H. Meyer},
   doi = {10.23919/DATE48585.2020.9116535},
   isbn = {9783981926347},
   journal = {Proceedings of the 2020 Design, Automation and Test in Europe Conference and Exhibition, DATE 2020},
   month = {3},
   note = {\{'id': 121,<br/>'keep': False<br/>\}},
   pages = {1055-1060},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Probabilistic Sequential Multi-Objective Optimization of Convolutional Neural Networks},
   year = {2020},
}
@article{Kwon2020,
   abstract = {High-level synthesis (HLS) raises the level of design abstraction, expedites the process of hardware design, and enriches the set of final designs by automatically translating a behavioral specification into a hardware implementation. To obtain different implementations, HLS users can apply a variety of knobs, such as loop unrolling or function inlining, to particular code regions of the specification. The applied knob configuration significantly affects the synthesized design's performance and cost, e.g., application latency and area utilization. Hence, HLS users face the design-space exploration (DSE) problem, i.e. determine which knob configurations result in Pareto-optimal implementations in this multi-objective space. Whereas it can be costly in time and resources to run HLS flows with an enormous number of knob configurations, machine learning approaches can be employed to predict the performance and cost. Still, they require a sufficient number of sample HLS runs. To enhance the training performance and reduce the sample complexity, we propose a transfer learning approach that reuses the knowledge obtained from previously explored design spaces in exploring a new target design space. We develop a novel neural network model for mixed-sharing multi-domain transfer learning. Experimental results demonstrate that the proposed model outperforms both single-domain and hard-sharing models in predicting the performance and cost at early stages of HLS-driven DSE.},
   author = {Jihye Kwon and Luca P. Carloni},
   doi = {10.1145/3380446.3430636},
   isbn = {9781450375191},
   journal = {MLCAD 2020 - Proceedings of the 2020 ACM/IEEE Workshop on Machine Learning for CAD},
   keywords = {Design space exploration,High-level synthesis,Machine learning,Multi-task learning,Neural networks,Transfer learning},
   month = {11},
   note = {\{'id': 122,<br/>'keep': False<br/>\}},
   pages = {163-168},
   publisher = {Association for Computing Machinery, Inc},
   title = {Transfer learning for design-space exploration with high-level synthesis},
   year = {2020},
}
@article{Alsulaimawi2020,
   abstract = {Machine learning applications have emerged in many aspects of our lives, such as for credit lending, insurance rates, and employment applications. Consequently, it is required that such systems be nondiscriminatory and fair in sensitive features user, e.g., race, sexual orientation, and religion. To address this issue, this paper develops a minimax adversarial framework, called features protector (FP) framework, to achieve the information-theoretical trade-off between minimizing distortion of target data and ensuring that sensitive features have similar distributions. We evaluate the performance of the proposed framework on two real-world datasets. Preliminary empirical evaluation shows that our framework provides both accurate and fair decisions.},
   author = {Zahir Alsulaimawi},
   doi = {10.1109/MMSP48831.2020.9287139},
   isbn = {9781728193205},
   journal = {IEEE 22nd International Workshop on Multimedia Signal Processing, MMSP 2020},
   keywords = {Fairness,adversarial learning,big data security,deep learning,privacy-preserving,variational mutual information},
   month = {9},
   note = {\{'id': 123,<br/>'keep': False<br/>\}},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Variational Bound of Mutual Information for Fairness in Classification},
   year = {2020},
}
@article{Khorshidi2020,
   abstract = {Machine learning techniques have been developed to learn from complete data. When missing values exist in a dataset, the incomplete data should be preprocessed separately by removing data points with missing values or imputation. In this paper, we propose an online approach to handle missing values while a classification model is learnt. To reach this goal, we develop a multi-objective optimization model with two objective functions for imputation and model selection. We also propose three formulations for imputation objective function. We use an evolutionary algorithm based on NSGA II to find the optimal solutions as the Pareto solutions. We investigate the reliability and robustness of the proposed model using experiments by defining several scenarios in dealing with missing values and classification. We also describe how the proposed model can contribute to medical informatics. We compare the performance of three different formulations via experimental results. The proposed model results get validated by comparing with a comparable literature.},
   author = {Hadi A. Khorshidi and Michael Kirley and Uwe Aickelin},
   doi = {10.1109/IJCNN48605.2020.9206742},
   isbn = {9781728169262},
   journal = {Proceedings of the International Joint Conference on Neural Networks},
   keywords = {classification,incomplete data,model selection,multi-objective model,uncertainty},
   month = {7},
   note = {\{'id': 124,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II"], <br/>'ML task': ["Incomplete data","SVM","Classification"], <br/>'Objective functions': ["Cluster validity","Correlation","Variance ratio"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Machine learning with incomplete datasets using multi-objective optimization models},
   year = {2020},
}
@article{,
   abstract = {Ensemble learning methods have already shown to be powerful techniques for creating classifiers. However, when dealing with real-world engineering problems, class imbalance is usually found. In such scenario, canonical machine learning algorithms may not present desirable solutions, and techniques for overcoming this problem must be used. In addition to using learning algorithms that alleviate the imbalance between classes, multi-objective optimization design (MOOD) approaches can be used to improve the prediction performance of ensembles of classifiers. This paper proposes a study of different MOOD approaches for ensemble learning. First, a taxonomy on multi-objective ensemble learning (MOEL) is proposed. In it, four types of existing approaches are defined: multi-objective ensemble member generation, multi-objective ensemble member selection, multi-objective ensemble member combination, and multi-objective ensemble member selection and combination. Additionally, new approaches can be derived by combining the previous ones, such as multi-objective ensemble member generation and selection, multi-objective ensemble member generation and combination and multi-objective ensemble member generation, selection and combination. With the given taxonomy, two experiments are conducted for comparing (1) the performance of the MOEL techniques for generating and aggregating base models on several imbalanced benchmark problems and (2) the performance of MOEL techniques against other machine learning techniques in a real-world imbalanced drinking-water quality anomaly detection problem. Finally, results indicate that MOOD is able to improve the predictive performance of existing ensemble learning techniques.},
   author = {Victor Henrique Alves Ribeiro and Gilberto Reynoso-Meza},
   doi = {10.1016/J.ESWA.2020.113232},
   issn = {0957-4174},
   journal = {Expert Systems with Applications},
   keywords = {Ensemble learning,Imbalanced data sets,Multi-objective optimization},
   month = {6},
   note = {\{'id': 125,<br/>'keep': False,<br/>'obs': "Study of different MOO approaches for ensemble learning"<br/>\}},
   pages = {113232},
   publisher = {Pergamon},
   title = {Ensemble learning by means of a multi-objective optimization design approach for dealing with imbalanced data sets},
   volume = {147},
   year = {2020},
}
@article{Binder2020,
   abstract = {Both feature selection and hyperparameter tuning are key tasks in machine learning. Hyperparameter tuning is often useful to increase model performance, while feature selection is undertaken to attain sparse models. Sparsity may yield better model interpretability and lower cost of data acquisition, data handling and model inference. While sparsity may have a beneficial or detrimental effect on predictive performance, a small drop in performance may be acceptable in return for a substantial gain in sparseness. We therefore treat feature selection as a multi-objective optimization task. We perform hyperparameter tuning and feature selection simultaneously because the choice of features of a model may influence what hyperparameters perform well. We present, benchmark, and compare two different approaches for multi-objective joint hyperparameter optimization and feature selection: The first uses multi-objective model-based optimization. The second is an evolutionary NSGA-II-based wrapper approach to feature selection which incorporates specialized sampling, mutation and recombination operators. Both methods make use of parameterized filter ensembles. While model-based optimization needs fewer objective evaluations to achieve good performance, it incurs computational overhead compared to the NSGA-II, so the preferred choice depends on the cost of evaluating a model on given data.},
   author = {Martin Binder and Julia Moosbauer and Janek Thomas and Bernd Bischl},
   doi = {10.1145/3377930.3389815},
   isbn = {9781450371285},
   journal = {GECCO 2020 - Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
   keywords = {Evolutionary algorithms,Feature selection,Hyperparameter optimization,Model-based optimization,Multiobjective optimization},
   month = {6},
   note = {\{'id': 126,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II","Bayesian"], <br/>'ML task': ["Feature Selection","Hyperparameter tuning","Classification","Filter Ensemble"], <br/>'Objective functions': ["Generalization error","Fraction of selected features"], <br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pages = {471-479},
   publisher = {Association for Computing Machinery},
   title = {Multi-objective hyperparameter tuning and feature selection using filter ensembles},
   url = {https://doi.org/10.1145/3377930.3389815},
   year = {2020},
}
@article{Shi2020,
   abstract = {Clustering ensemble (CE) takes multiple clustering solutions into consideration in order to effectively improve the accuracy and robustness of the final result. To reduce redundancy as well as noise, a CE selection (CES) step is added to further enhance performance. Quality and diversity are two important metrics of CES. However, most of the CES strategies adopt heuristic selection methods or a threshold parameter setting to achieve tradeoff between quality and diversity. In this paper, we propose a transfer CES (TCES) algorithm which makes use of the relationship between quality and diversity in a source dataset, and transfers it into a target dataset based on three objective functions. Furthermore, a multiobjective self-evolutionary process is designed to optimize these three objective functions. Finally, we construct a transfer CE framework (TCE-TCES) based on TCES to obtain better clustering results. The experimental results on 12 transfer clustering tasks obtained from the 20newsgroups dataset show that TCE-TCES can find a better tradeoff between quality and diversity, as well as obtaining more desirable clustering results.},
   author = {Yifan Shi and Zhiwen Yu and C. L.Philip Chen and Jane You and Hau San Wong and Yide Wang and Jun Zhang},
   doi = {10.1109/TCYB.2018.2885585},
   issn = {21682275},
   issue = {6},
   journal = {IEEE Transactions on Cybernetics},
   keywords = {Clustering ensemble selection (CES),machine learning,multiobjective,transfer learning},
   month = {6},
   note = {\{'id': 127,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II","Bayesian"], <br/>'ML task': ["Clustering","Ensemble","Transfer Learning"], <br/>'Objective functions': ["Performance","Quality index","Diversity index"], <br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pages = {2872-2885},
   pmid = {30596592},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Transfer Clustering Ensemble Selection},
   volume = {50},
   year = {2020},
}
@article{,
   abstract = {The last decade has witnessed the proliferation of Deep Learning models in many applications, achieving unrivaled levels of predictive performance. Unfortunately, the black-box nature of Deep Learning models has posed unanswered questions about what they learn from data. Certain application scenarios have highlighted the importance of assessing the bounds under which Deep Learning models operate, a problem addressed by using assorted approaches aimed at audiences from different domains. However, as the focus of the application is placed more on non-expert users, it results mandatory to provide the means for him/her to trust the model, just like a human gets familiar with a system or process: by understanding the hypothetical circumstances under which it fails. This is indeed the angular stone for this research work: to undertake an adversarial analysis of a Deep Learning model. The proposed framework constructs counterfactual examples by ensuring their plausibility, e.g. there is a reasonable probability that a human could generate them without resorting to a computer program. Therefore, this work must be regarded as valuable auditing exercise of the usable bounds a certain model is constrained within, thereby allowing for a much greater understanding of the capabilities and pitfalls of a model used in a real application. To this end, a Generative Adversarial Network (GAN) and multi-objective heuristics are used to furnish a plausible attack to the audited model, efficiently trading between the confusion of this model, the intensity and plausibility of the generated counterfactual. Its utility is showcased within a human face classification task, unveiling the enormous potential of the proposed framework.},
   author = {Alejandro Barredo-Arrieta and Javier Del Ser},
   doi = {10.1109/IJCNN48605.2020.9206728},
   isbn = {9781728169262},
   journal = {Proceedings of the International Joint Conference on Neural Networks},
   keywords = {Counterfactuals,Deep Learning,Explainable Artificial Intelligence,Generative Adversarial Networks,Meta-heuristics,Multiobjective Optimization},
   month = {7},
   note = {\{'id': 128,<br/>'keep': True, <br/>'Optimization': ["Heuristic"], <br/>'ML task': ["Explainable AI","Deep Learning","Counterfactuals","GAN"], <br/>'Objective functions': ["Unlikeliness","Probability of not confusion","Intensity of adversarial changes"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Plausible Counterfactuals: Auditing Deep Learning Classifiers with Realistic Adversarial Examples},
   year = {2020},
}
@article{Kiziloz2020,
   abstract = {With the advance in technology, the volume of available data grows massively. Therefore, feature selection has become an essential preprocessing step to extract valuable information. Feature selection is the task of reducing the number of features by removing redundant features from data while preserving the classification accuracy. It is a multiobjective problem as there are two objectives. In general, multiobjective selection algorithms with machine learning techniques are utilized to find the most promising feature subsets; however, classification performances of these machine learning techniques are analyzed separately. In this study, we propose a new multiobjective selection model that dynamically searches for the best ensemble of five classifiers to extract the best representative feature subsets. We present the experiment results on 12 well-known datasets. The results show that the proposed method performs significantly better than all the machine learning techniques when they are executed separately. Moreover, the proposed method outperforms two existing ensemble algorithms, namely AdaBoost and Gradient Boosting.},
   author = {Hakan Ezgi Kiziloz and Ayca Deniz},
   doi = {10.1109/SMC42975.2020.9282969},
   isbn = {9781728185262},
   issn = {1062922X},
   journal = {Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
   keywords = {classifier ensemble,feature selection,machine learning,multiobjective optimization},
   month = {10},
   note = {\{'id': 129,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II"], <br/>'ML task': ["Feature Selection","Ensemble","Classification"], <br/>'Objective functions': ["Number of features","Accuracy"], <br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pages = {2038-2043},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Feature Selection with Dynamic Classifier Ensembles},
   volume = {2020-October},
   year = {2020},
}
@article{Fernandes2020,
   abstract = {Imbalanced datasets may negatively impact the predictive performance of most classical classification algorithms. This problem, commonly found in real-world, is known in machine learning domain as imbalanced learning. Most techniques proposed to deal with imbalanced learning have been proposed and applied only to binary classification. When applied to multiclass tasks, their efficiency usually decreases and negative side effects may appear. This paper addresses these limitations by presenting a novel adaptive approach, E-MOSAIC (Ensemble of Classifiers based on MultiObjective Genetic Sampling for Imbalanced Classification). E-MOSAIC evolves a selection of samples extracted from training dataset, which are treated as individuals of a MOEA. The multiobjective process looks for the best combinations of instances capable of producing classifiers with high predictive accuracy in all classes. E-MOSAIC also incorporates two mechanisms to promote the diversity of these classifiers, which are combined into an ensemble specifically designed for imbalanced learning. Experiments using twenty imbalanced multi-class datasets were carried out. In these experiments, the predictive performance of E-MOSAIC is compared with state-of-the-art methods, including methods based on presampling, active-learning, cost-sensitive, and boosting. According to the experimental results, the proposed method obtained the best predictive performance for the multiclass accuracy measures mAUC and G-mean.},
   author = {Everlandio R.Q. Fernandes and Andre C.P.L.F. De Carvalho and Xin Yao},
   doi = {10.1109/TKDE.2019.2898861},
   issn = {15582191},
   issue = {6},
   journal = {IEEE Transactions on Knowledge and Data Engineering},
   keywords = {Ensemble of classifiers,Evolutionary algorithm,Imbalanced datasets},
   month = {6},
   note = {\{'id': 130,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II"], <br/>'ML task': ["Imbalanced Data","Ensemble","Classification"], <br/>'Objective functions': ["Positive Predictive Value by class"], <br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pages = {1104-1115},
   publisher = {IEEE Computer Society},
   title = {Ensemble of classifiers based on multiobjective genetic sampling for imbalanced data},
   volume = {32},
   year = {2020},
}
@article{Omozaki2020,
   abstract = {In multi-label classification problems, multiple class labels are assigned to each instance. Two approaches have been studied in the literature. One is a data transformation approach, which transforms a multi-label dataset into a number of singlelabel datasets. However, this approach often loses the correlation information among classes in the multi-class assignment. The other is a method adaptation approach where a conventional classification method is extended to multi-label classification. Recently, some explainable classification models for multi-label classification have been proposed. Their high interpretability has also been discussed with respect to the transparency of the classification process. Although the explainability is a well-known advantage of fuzzy systems, their applications to multi-label classification have not been well studied. Since multi-label classification problems often have vague class boundaries, fuzzy systems seem to be a promising approach to multi-label classification. In this paper, we propose a new multiobjective evolutionary fuzzy system, which can be categorized as a method adaptation approach. The proposed algorithm produces nondominated classifiers with different tradeoffs between accuracy and complexity. We examine the behavior of the proposed algorithm using synthetic multi-label datasets. We also compare the proposed algorithm with five representative algorithms. Our experimental results on real-world datasets show that the obtained fuzzy classifiers with a small number of fuzzy rules have high transparency and comparable generalization ability to the other examined multi-label classification algorithms.},
   author = {Yuichi Omozaki and Naoki Masuyama and Yusuke Nojima and Hisao Ishibuchi},
   doi = {10.1109/FUZZ48607.2020.9177804},
   isbn = {9781728169323},
   issn = {10987584},
   journal = {IEEE International Conference on Fuzzy Systems},
   keywords = {Fuzzy rule-based classification system,Method adaptation approach,Multi-label classification,Multiobjective fuzzy genetics-based machine learning},
   month = {7},
   note = {\{'id': 131,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II"], <br/>'ML task': ["Classification","Multi-Label"], <br/>'Objective functions': ["Subset Accuracy","Hamming Loss","F-Measure","Number of rules"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Multiobjective fuzzy genetics-based machine learning for multi-label classification},
   volume = {2020-July},
   year = {2020},
}
@article{Nguyen2020,
   abstract = {Feature selection is an important task in machine learning that has two main objectives: 1) reducing dimensionality and 2) improving learning performance. Feature selection can be considered a multiobjective problem. However, it has its problematic characteristics, such as a highly discontinuous Pareto front, imbalance preferences, and partially conflicting objectives. These characteristics are not easy for existing evolutionary multiobjective optimization (EMO) algorithms. We propose a new decomposition approach with two mechanisms (static and dynamic) based on multiple reference points under the multiobjective evolutionary algorithm based on decomposition (MOEA/D) framework to address the above-mentioned difficulties of feature selection. The static mechanism alleviates the dependence of the decomposition on the Pareto front shape and the effect of the discontinuity. The dynamic one is able to detect regions in which the objectives are mostly conflicting, and allocates more computational resources to the detected regions. In comparison with other EMO algorithms on 12 different classification datasets, the proposed decomposition approach finds more diverse feature subsets with better performance in terms of hypervolume and inverted generational distance. The dynamic mechanism successfully identifies conflicting regions and further improves the approximation quality for the Pareto fronts.},
   author = {Bach Hoai Nguyen and Bing Xue and Peter Andreae and Hisao Ishibuchi and Mengjie Zhang},
   doi = {10.1109/TEVC.2019.2913831},
   issn = {19410026},
   issue = {1},
   journal = {IEEE Transactions on Evolutionary Computation},
   keywords = {Classification,feature selection,multiobjective evolutionary algorithm based on decomposition (MOEA/D),multiobjective optimization,partially conflicting},
   month = {2},
   note = {\{'id': 132,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","MOEA/D"], <br/>'ML task': ["Classification","Feature Selection"], <br/>'Objective functions': ["Classification error","Number of selected features"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {170-184},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Multiple Reference Points-Based Decomposition for Multiobjective Feature Selection in Classification: Static and Dynamic Mechanisms},
   volume = {24},
   year = {2020},
}
@article{Nguyen2020,
   abstract = {This paper introduces a new scalable multi-objective deep reinforcement learning (MODRL) framework based on deep Q-networks. We develop a high-performance MODRL framework that supports both single-policy and multi-policy strategies, as well as both linear and non-linear approaches to action selection. The experimental results on two benchmark problems (two-objective deep sea treasure environment and three-objective Mountain Car problem) indicate that the proposed framework is able to find the Pareto-optimal solutions effectively. The proposed framework is generic and highly modularized, which allows the integration of different deep reinforcement learning algorithms in different complex problem domains. This therefore overcomes many disadvantages involved with standard multi-objective reinforcement learning methods in the current literature. The proposed framework acts as a testbed platform that accelerates the development of MODRL for solving increasingly complicated multi-objective problems.},
   author = {Thanh Thi Nguyen and Ngoc Duy Nguyen and Peter Vamplew and Saeid Nahavandi and Richard Dazeley and Chee Peng Lim},
   doi = {10.1016/J.ENGAPPAI.2020.103915},
   issn = {0952-1976},
   journal = {Engineering Applications of Artificial Intelligence},
   keywords = {Deep learning,Multi-objective,Multi-policy,Reinforcement learning,Single-policy},
   month = {11},
   note = {\{'id': 133,<br/>'keep': False<br/>\}},
   pages = {103915},
   publisher = {Pergamon},
   title = {A multi-objective deep reinforcement learning framework},
   volume = {96},
   year = {2020},
}
@article{Chen2020,
   abstract = {Multi-objective reinforcement learning (MORL) algorithms aim to approximate the Pareto frontier uniformly in multi-objective decision making problems. In the scenario of deep reinforcement learning (RL), gradient-based methods are often adopted to learn deep policies/value functions due to the fast convergence speed, while pure gradient-based methods can not guarantee a uniformly approximated Pareto frontier. On the other side, evolution strategies straightly manipulate in the solution space to achieve a well-distributed Pareto frontier, but applying evolution strategies to optimize deep networks is still a challenging topic. To leverage the advantages of both kinds of methods, we propose a two-stage MORL framework combining a gradient-based method and an evolution strategy. First, an efficient multi-policy soft actor-critic algorithm is proposed to learn multiple policies collaboratively. The lower layers of all policy networks are shared. The first-stage learning can be regarded as representation learning. Secondly, the multi-objective covariance matrix adaptation evolution strategy (MO-CMA-ES) is applied to fine-tune policy-independent parameters to approach a dense and uniform estimation of the Pareto frontier. Experimental results on three benchmarks (Deep Sea Treasure, Adaptive Streaming, and Super Mario Bros) show the superiority of the proposed method.},
   author = {Diqi Chen and Yizhou Wang and Wen Gao},
   doi = {10.1007/S10489-020-01702-7/TABLES/8},
   issn = {15737497},
   issue = {10},
   journal = {Applied Intelligence},
   keywords = {Multi-objective reinforcement learning,Multi-policy reinforcement learning,Pareto frontier,Sampling efficiency},
   month = {10},
   note = {\{'id': 134,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","MO-CMA-ES"], <br/>'ML task': ["Reinforcement Learning","Multi-policy"], <br/>'Objective functions': ["Task objectives"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {3301-3317},
   publisher = {Springer},
   title = {Combining a gradient-based method and an evolution strategy for multi-objective reinforcement learning},
   volume = {50},
   url = {https://link.springer.com/article/10.1007/s10489-020-01702-7},
   year = {2020},
}
@article{Senhaji2020,
   abstract = {The paper presents a new approach to optimize the Multilayer Perceptron Neural Network (MLPNN), to deal with the generalization problem. As known, most supervised learning algorithms aim to minimize the training error. However, the mentioned methods, based only on error minimizing, may generate a solution with an insufficient generalization performance. This present work proposes a multiobjective modelling problem involving two objectives: accuracy and complexity since the learning problem is multiobjective by nature. The learning task is carried on by minimizing both objectives simultaneously, according to Pareto domination concept, using NSGAII (Non-dominated Sorting Genetic Algorithm II) as a solver. This method leads us to a set of solutions called Pareto front, being the optimal solutions set, the adequate MLPNN need to be extracted. We show empirically that the proposed method is capable of reducing the neural networks topology and improved generalization performance, in addition to a good classification rate compared to different methods.},
   author = {Kaoutar Senhaji and Hassan Ramchoun and Mohamed Ettaouil},
   doi = {10.1016/J.NEUCOM.2020.05.066},
   issn = {0925-2312},
   journal = {Neurocomputing},
   keywords = {L1/2 regularization,Learning algorithm,Multiobjective optimization,NSGAII,Neural network},
   month = {10},
   note = {\{'id': 135,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II"], <br/>'ML task': ["Multilayer Perceptron Neural Network"], <br/>'Objective functions': ["MSE","Modified L1/2regularizer"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {1-11},
   publisher = {Elsevier},
   title = {Training feedforward neural network via multiobjective optimization model using non-smooth L1/2 regularization},
   volume = {410},
   year = {2020},
}
@article{Riccio2020,
   abstract = {Neuroevolution has been used to train Deep Neural Networks on reinforcement learning problems. A few attempts have been made to extend it to address either multi-task or multi-objective optimization problems. This research work presents the Multi-Task Multi-Objective...},
   author = {Salvatore D. Riccio and Deyan Dyankov and Giorgio Jansen and Giuseppe Di Fatta and Giuseppe Nicosia},
   doi = {10.1007/978-3-030-61616-8_11},
   isbn = {9783030616151},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Atari 2600 games,Deep Neuroevolution,Evolution strategy,Hypervolume,Kullback-Leibler divergence,Multi-objective learning,Multi-task learning,Pareto front},
   note = {\{'id': 136,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Proposed","MTMO-ES"], <br/>'ML task': ["Multi-Task Learning"], <br/>'Objective functions': ["Utility functions for the multi-task learning"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {132-141},
   publisher = {Springer, Cham},
   title = {Pareto Multi-task Deep Learning},
   volume = {12397 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-61616-8_11},
   year = {2020},
}
@article{Lambert2020,
   abstract = {The concept of Pareto optimality has been utilized in fields such as engineering and economics to understand fluid dynamics and consumer behavior. In machine learning contexts, Pareto-optimality has been used to identify tuning parameters that best optimize a set of m criteria (multi-objective optimization). During the process of regression model selection, data scientists are often concerned with choosing a model which has the best single criterion (e.g., Akaike information criterion (AIC) or R-squared (R2)) before continuing to check a number of other regression model characteristics (e.g., model size, form, diagnostics, and interpretability). This strategy is multi-objective in nature but single objective in its numeric execution. This paper will first introduce a feasible solution algorithm (FSA) and explain how it can be applied to multi-objective problems for regression subset selection. Then we introduce the general framework of Pareto optimality within the regression setting. We then apply the algorithm in a simulation setting where we seek to estimate the first four Pareto boundaries for regression models using two model fit criteria. Finally, we present an application where we use a US communities and crime dataset.},
   author = {Joshua W. Lambert and Gregory S. Hawk},
   doi = {10.1007/S41060-020-00218-0/TABLES/1},
   issn = {23644168},
   issue = {3},
   journal = {International Journal of Data Science and Analytics},
   keywords = {Feasible solution,Multiple,Objective,Optimal,Pareto,Regression,Subset selection},
   month = {9},
   note = {\{'id': 137,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","FSA"], <br/>'ML task': ["Regression","Subset Selection"], <br/>'Objective functions': ["Residual","R squared"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {277-284},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Identifying Pareto-based solutions for regression subset selection via a feasible solution algorithm},
   volume = {10},
   url = {https://link.springer.com/article/10.1007/s41060-020-00218-0},
   year = {2020},
}
@article{Sun2020,
   abstract = {A novel machine learning optimization process coined Restrictive Federated Model Selection (RFMS) is proposed under the scenario, for example, when data from healthcare units can not leave the site it is situated on and it is forbidden to carry out training...},
   author = {Xudong Sun and Andrea Bommert and Florian Pfisterer and Jörg Rähenfürher and Michel Lang and Bernd Bischl},
   doi = {10.1007/978-3-030-29516-5_48},
   isbn = {9783030295158},
   issn = {21945365},
   journal = {Advances in Intelligent Systems and Computing},
   keywords = {Differential privacy,Distribution shift,Federated learning,High dimensional data,Model selection,Multi-objective Bayesian Optimization},
   note = {\{'id': 138,<br/>'keep': True, <br/>'Optimization': ["Bayesian Optimization","Parego algorithm"], <br/>'ML task': ["Model Selection","Federated Learning"], <br/>'Objective functions': ["Local loss","Remote loss"], <br/>'Single/Multi Solutions': "Single - Dominated Hypervolume Indicator"<br/>\}},
   pages = {629-647},
   publisher = {Springer, Cham},
   title = {High Dimensional Restrictive Federated Model Selection with Multi-objective Bayesian Optimization over Shifted Distributions},
   volume = {1037},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-29516-5_48},
   year = {2020},
}
@article{Chen2020,
   abstract = {In multi-objective decision making problems, multi-objective reinforcement learning (MORL) algorithms aim to approximate the Pareto frontier uniformly. A naive approach is to learn multiple policies by repeatedly running a single-objective reinforcement learning (RL) algorithm on scalarized rewards. The scalarization methods denote the preferences of objectives, which are different in each run. However, in this way, the model representation and computation are redundant. Furthermore, uniform preferences can not guarantee a uniformly approximated Pareto frontier. To address these problems and leverage the expressive power of deep neural networks, we propose a two-stage MORL framework integrating a multi-policy deep RL algorithm and an evolution strategy algorithm. Firstly, a multi-policy soft actor-critic algorithm is proposed to collaboratively learn multiple policies which are assigned with different scalarization weights. The lower layers of all policy networks are shared. The first-stage learning can be regarded as representation learning. Secondly, the multi-objective covariance matrix adaptation evolution strategy (MO-CMA-ES) is applied to fine-tune policy-independent parameters to approach a dense and uniform estimation of the Pareto frontier. Experimental results on two benchmarks (Deep Sea Treasure and Adaptive Streaming) show the superiority of the proposed method.},
   author = {Diqi Chen and Yizhou Wang and Wen Gao},
   doi = {10.3233/FAIA200202},
   isbn = {9781643681009},
   issn = {09226389},
   journal = {Frontiers in Artificial Intelligence and Applications},
   month = {8},
   note = {\{'id': 139,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","MO-CMA-ES"], <br/>'ML task': ["Reinforcement Learning"], <br/>'Objective functions': ["Soft Actor-Critic (SAC) objective functions"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {1063-1070},
   publisher = {IOS Press BV},
   title = {A two-stage multi-objective deep reinforcement learning framework},
   volume = {325},
   year = {2020},
}
@article{,
   abstract = {Deep Reinforcement Learning has shown promising results in learning policies for complex sequential decision-making tasks. However, different adversarial attack strategies have revealed the weakness of these policies to perturbations to their observations. Most of these attacks have been built on existing adversarial example crafting techniques used to fool classifiers, where an adversarial attack is considered a success if it makes the classifier outputs any wrong class. The major drawback of these approaches when applied to decision-making tasks is that they are blind for long-term goals. In contrast, this paper suggests that it is more appropriate to view the attack process as a sequential optimization problem, with the aim of learning a sequence of attacks, where the attacker must consider the long-term effects of each attack. In this paper, we propose that such an attack policy must be learned with two objectives in view. On the one hand, the attack must pursue the maximum performance loss of the attacked policy. On the other hand, it also should minimize the cost of the attacks. Therefore, in this paper we propose a novel modelization of the process of learning an attack policy as a Multi-objective Markov Decision Process with two objectives: maximizing the performance loss of the attacked policy and minimizing the cost of the attacks. We also reveal the conflicting nature of these two objectives and use a Multi-objective Reinforcement Learning algorithm to draw the Pareto fronts for four well-known tasks: the GridWorld, the Cartpole, the Mountain car and the Breakout.},
   author = {Javier García and Rubén Majadas and Fernando Fernández},
   doi = {10.1016/J.ENGAPPAI.2020.104021},
   issn = {0952-1976},
   journal = {Engineering Applications of Artificial Intelligence},
   keywords = {Adversarial reinforcement learning,Multi-objective reinforcement learning},
   month = {11},
   note = {\{'id': 140,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Reinforcement Learning"], <br/>'ML task': ["Reinforcement Learning","Adversarial attacks"], <br/>'Objective functions': ["Performance loss of the attacked policy","Cost of the attacks"], <br/>'Single/Multi Solutions': "Single - Weighted optimization"<br/>\}},
   pages = {104021},
   publisher = {Pergamon},
   title = {Learning adversarial attack policies through multi-objective reinforcement learning},
   volume = {96},
   year = {2020},
}
@article{Wang2020,
   abstract = {Ensemble learning is an important element in machine learning. However, two essential tasks, including training base classifiers and finding a suitable ensemble balance for the diversity and accuracy of these base classifiers, are need to be achieved. In this paper,...},
   author = {Jie Wang and Bo Wang and Jing Liang and Kunjie Yu and Caitong Yue and Xiangyang Ren},
   doi = {10.1007/978-981-15-3425-6_34},
   isbn = {9789811534249},
   issn = {18650937},
   journal = {Communications in Computer and Information Science},
   keywords = {Classifier parameter,Ensemble learning,Feature selection,Multimodal multiobjective optimization},
   note = {\{'id': 141,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","MMODE"], <br/>'ML task': ["Ensemble Learning","Feature Selection"], <br/>'Objective functions': ["Feature selection rate","Error rate"], <br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pages = {439-453},
   publisher = {Springer, Singapore},
   title = {Ensemble Learning via Multimodal Multiobjective Differential Evolution and Feature Selection},
   volume = {1159 CCIS},
   url = {https://link.springer.com/chapter/10.1007/978-981-15-3425-6_34},
   year = {2020},
}
@article{Vashishtha2020,
   abstract = {Feature selection is a pre-processing technique in which a subset or a small number of features, which are relevant and non-redundant, are selected for better classification performance. Multi-objective optimization is applied in the fields where finest decisions...},
   author = {Jyoti Vashishtha and Vijay Hasan Puri and Mukesh},
   doi = {10.1007/978-981-15-6318-8_10},
   isbn = {9789811563171},
   issn = {18650937},
   journal = {Communications in Computer and Information Science},
   keywords = {Feature selection,Multi-objective optimization,PSO},
   note = {\{'id': 142,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Particle Swarm Optimization"], <br/>'ML task': ["Feature Selection","Classification"], <br/>'Objective functions': ["Number of features","Classification error"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {106-119},
   publisher = {Springer, Singapore},
   title = {Feature Selection Using PSO: A Multi Objective Approach},
   volume = {1241 CCIS},
   url = {https://link.springer.com/chapter/10.1007/978-981-15-6318-8_10},
   year = {2020},
}
@article{Salazar2021,
   abstract = {One of the fundamental problems of machine ethics is to avoid the perpetuation and amplification of discrimination through machine learning applications. In particular, it is desired to exclude the...},
   author = {Ricardo Salazar and Felix Neutatz and Ziawasch Abedjan},
   doi = {10.14778/3461535.3463474},
   issn = {21508097},
   issue = {9},
   journal = {Proceedings of the VLDB Endowment},
   month = {5},
   note = {\{'id': 143,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II"], <br/>'ML task': ["Feature Selection","Feature Construction","Fairness"], <br/>'Objective functions': ["Ratio of Observational Discrimination","F1 score"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {1694-1702},
   publisher = {
		VLDB Endowment
		PUB4722
	},
   title = {Automated feature engineering for algorithmic fairness},
   volume = {14},
   url = {https://dl.acm.org/doi/abs/10.14778/3461535.3463474},
   year = {2021},
}
@article{Liuliakov2021,
   abstract = {Automated machine learning (AutoML) technologies constitute promising tools to automatically infer model architecture, meta-parameters or processing pipelines for specific machine learning tasks given suitable training data. At present, the main objective of such...},
   author = {Aleksei Liuliakov and Barbara Hammer},
   doi = {10.1007/978-3-030-91608-4_7},
   isbn = {9783030916077},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   note = {\{'id': 144,<br/>'keep': True, <br/>'Optimization': ["Greedy search"], <br/>'ML task': ["AutoML","Feature Selection"], <br/>'Objective functions': ["Accuracy","Number of selected features"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {65-75},
   publisher = {Springer, Cham},
   title = {AutoML Technologies for the Identification of Sparse Models},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-91608-4_7},
   year = {2021},
}
@article{Chaudhuri2021,
   abstract = {Naïve Bayes (NB) is a widely used classifier in the field of machine learning. However, its conditional independence assumption does not hold true in real-world applications. In literature, various feature weighting approaches have attempted to alleviate this assumption. Almost all of these approaches consider the relationship between feature-class (relevancy) and feature-feature (redundancy) independently, to determine the weights of features. We argue that these two relationships are mutually dependent and both cannot be improved simultaneously, i.e., form a trade-off. This paper proposes a new paradigm to determine the feature weight by formulating it as a multi-objective optimisation problem to balance the trade-off between relevancy and redundancy. Multi-objective artificial bee colony-based feature weighting technique for naïve Bayes (MOABC-FWNB) is proposed. An extensive experimental study was conducted on 20 benchmark UCI datasets. Experimental results show that MOABC-FWNB outperforms NB and other existing state-of-the-art feature weighting techniques.},
   author = {Abhilasha Chaudhuri and Tirath Prasad Sahu},
   doi = {10.1504/IJCSE.2021.113655},
   issn = {17427193},
   issue = {1},
   journal = {International Journal of Computational Science and Engineering},
   keywords = {Artificial bee colony,Feature weighting,Multi objective optimisation,Naïve Bayes},
   note = {\{'id': 145,<br/>'keep': True, <br/>'Optimization': ["Evolutive","Artificial bee colony"], <br/>'ML task': ["Classification","Naïve Bayes","Feature Weighting"], <br/>'Objective functions': ["Relevancy","Redundancy"], <br/>'Single/Multi Solutions': "Single - Maximum fitness value"<br/>\}},
   pages = {74-88},
   publisher = {Inderscience Publishers},
   title = {Feature weighting for naïve Bayes using multi objective artificial bee colony algorithm},
   volume = {24},
   year = {2021},
}
@article{Diana2021,
   abstract = {We consider a recently introduced framework in which fairness is measured by worst-case outcomes across groups, rather than by the more standard differences between group outcomes. In this framework we provide provably convergent oracle-efficient learning algorithms (or equivalently, reductions to non-fair learning) for minimax group fairness. Here the goal is that of minimizing the maximum loss across all groups, rather than equalizing group losses. Our algorithms apply to both regression and classification settings and support both overall error and false positive or false negative rates as the fairness measure of interest. They also support relaxations of the fairness constraints, thus permitting study of the tradeoff between overall accuracy and minimax fairness. We compare the experimental behavior and performance of our algorithms across a variety of fairness-sensitive data sets and show empirical cases in which minimax fairness is strictly and strongly preferable to equal outcome notions.},
   author = {Emily Diana and Wesley Gill and Michael Kearns and Krishnaram Kenthapadi and Aaron Roth},
   doi = {10.1145/3461702.3462523},
   isbn = {9781450384735},
   journal = {AIES 2021 - Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
   keywords = {fair machine learning,game theory,minimax fairness},
   month = {7},
   note = {\{'id': 146,<br/>'keep': True, <br/>'Optimization': ["Evolutive","Zero-sum game"], <br/>'ML task': ["Fairness","Classification","Regression"], <br/>'Objective functions': ["Group error"], <br/>'Single/Multi Solutions': "Single - Minimax"<br/>\}},
   pages = {66-76},
   publisher = {Association for Computing Machinery, Inc},
   title = {Minimax Group Fairness: Algorithms and Experiments},
   url = {https://doi.org/10.1145/3461702.3462523},
   year = {2021},
}
@article{Shah2021,
   abstract = {Group-fairness in classification aims for equality of a predictive utility across different sensitive sub-populations, e.g., race or gender. Equality or near-equality constraints in group-fairness often worsen not only the aggregate utility but also the utility for the least advantaged sub-population. In this paper, we apply the principles of Pareto-efficiency and least-difference to the utility being accuracy, as an illustrative example, and arrive at the Rawls classifier that minimizes the error rate on the worst-off sensitive sub-population. Our mathematical characterization shows that the Rawls classifier uniformly applies a threshold to an ideal score of features, in the spirit of fair equality of opportunity. In practice, such a score or a feature representation is often computed by a black-box model that has been useful but unfair. Our second contribution is practical Rawlsian fair adaptation of any given black-box deep learning model, without changing the score or feature representation it computes. Given any score function or feature representation and only its second-order statistics on the sensitive sub-populations, we seek a threshold classifier on the given score or a linear threshold classifier on the given feature representation that achieves the Rawls error rate restricted to this hypothesis class. Our technical contribution is to formulate the above problems using ambiguous chance constraints, and to provide efficient algorithms for Rawlsian fair adaptation, along with provable upper bounds on the Rawls error rate. Our empirical results show significant improvement over state-of-the-art group-fair algorithms, even without retraining for fairness. CCS CONCEPTS • Computing methodologies → Supervised learning by classification ; • Theory of computation → Machine learning theory; • Social and professional topics → Race and ethnicity; Gender.},
   author = {Kulin Shah and Pooja Gupta and Amit Deshpande and Chiranjib Bhattacharyya},
   city = {New York, NY, USA},
   doi = {10.1145/3461702},
   isbn = {9781450384735},
   journal = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
   keywords = {Fair Adaptation,Fairness for Deep Learning Classifiers,Rawlsian Fairness},
   note = {\{'id': 147,<br/>'keep': False<br/>\}},
   publisher = {ACM},
   title = {Rawlsian Fair Adaptation of Deep Learning Classifiers},
   volume = {1},
   url = {https://doi.org/10.1145/3461702.3462592},
   year = {2021},
}
@article{Raimundo2021,
   abstract = {Multinomial logistic loss and L2 regularization are often conflicting objectives as more robust regularization leads to restrained multinomial parameters. For many practical problems, leveraging the best of both worlds would be invaluable for better decision-making processes. This research proposes a novel framework to obtain representative and diverse L2-regularized multinomial models, based on valuable trade-offs between prediction error and model complexity. The framework relies upon the Non-Inferior Set Estimation (NISE) method – a deterministic multiobjective solver. NISE automatically implements hyperparameter tuning in a multiobjective context. Given the diverse set of efficient learning models, model selection and aggregation of the multiple models in an ensemble framework promote high performance in multiclass classification. Additionally, NISE uses the weighted sum method as scalarization, thus being able to deal with the learning formulation directly. Its deterministic nature and the convexity of the learning problem confer scalability to the proposal. The experiments show competitive performance in various setups, taking a broad set of multiclass classification methods as contenders.},
   author = {Marcos M. Raimundo and Thalita F. Drumond and Alan Caio R. Marques and Christiano Lyra and Anderson Rocha and Fernando J. Von Zuben},
   doi = {10.1016/J.NEUCOM.2020.12.087},
   issn = {0925-2312},
   journal = {Neurocomputing},
   keywords = {Diversity of Pareto-optimal models,Ensemble learning,Multiclass classification,Multiobjective optimization},
   month = {5},
   note = {\{'id': 148,<br/>'keep': True, <br/>'Optimization': ["Deterministic","NISE"], <br/>'ML task': ["Classification","Multiclass"], <br/>'Objective functions': ["Classification loss","L2"], <br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pages = {307-320},
   publisher = {Elsevier},
   title = {Exploring multiobjective training in multiclass classification},
   volume = {435},
   year = {2021},
}
@article{Cai2021,
   abstract = {Multi-task learning is a promising field in machine learning, which aims to improve the performance of multiple related learning tasks by taking advantage of useful information between them. Multi-task learning is essentially equivalent to multi-objective optimization problem, the purpose is to find the most appropriate weight, and because the performance of many deep learning systems based on multi-task learning largely depends on the relative weight of each task loss. It's a problem that we need to study how to calculate the weight value under some constraint conditions by reasonable method. Therefore, this paper employs a powerful method based on convex optimization theory, whose purpose is to find the Pareto optimal solution and get the specific task loss weight. The optimization process is closely related to the gradient in deep learning. In addition, to improve the accuracy, we add the modules of gradient normalization and weight standardization. The experimental results show that the performance of our method is better than that of single task experiment or multi-task experiment under fixed weight, and multi-task experiment based on uncertainty based adaptive learning, and the accuracy is further improved after adding the above modules.},
   author = {Tian Cai and Xing Gao and Liang Song and Minghong Liao},
   doi = {10.1145/3448734.3450463},
   isbn = {9781450389570},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Multi-task learning,Pareto improvement,multi-objective optimization},
   month = {1},
   note = {\{'id': 149,<br/>'keep': True, <br/>'Optimization': ["Deterministic","MGDA"], <br/>'ML task': ["Multi-task Learning"], <br/>'Objective functions': ["Loss of each class"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   publisher = {Association for Computing Machinery},
   title = {The multi-task learning with an application of Pareto improvement},
   volume = {PartF168982},
   url = {https://doi.org/10.1145/3448734.3450463},
   year = {2021},
}
@article{Cai2021,
   abstract = {Multi-task learning (MTL) is a promising research field of machine learning, in which the training process of the neural network is equivalent to multi-objective optimization. On one hand, MTL trains all the network weights simultaneously to converge the multi-task...},
   author = {Tian Cai and Liang Song and Guilin Li and Minghong Liao},
   doi = {10.1007/978-3-030-84529-2_42},
   isbn = {9783030845285},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Multi-objective optimization,Multi-task learning,Riemannian optimization},
   note = {\{'id': 150,<br/>'keep': True, <br/>'Optimization': ["Deterministic","MGDA"], <br/>'ML task': ["Multi-task Learning"], <br/>'Objective functions': ["Loss of each class"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {499-509},
   publisher = {Springer, Cham},
   title = {Multi-task Learning with Riemannian Optimization},
   volume = {12837 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-84529-2_42},
   year = {2021},
}
@article{Do2021,
   abstract = {A newly introduced training-free neural architecture search (TE-NAS) framework suggests that candidate network architectures can be ranked via a combined metric of expressivity and trainability. Expressivity is measured by the number of linear regions in the input...},
   author = {Tu Do and Ngoc Hoang Luong},
   doi = {10.1007/978-3-030-92270-2_29},
   isbn = {9783030922696},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Deep learning,Evolutionary computation,Multi-objective optimization,Neural architecture search,Neural tangent kernels},
   note = {\{'id': 151,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II"], <br/>'ML task': ["Neural Architecture Search","Neural Network"], <br/>'Objective functions': ["Few floating-point operations (FLOPs)","Number of linear regions","Condition number of neural tangent kernel"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {335-347},
   publisher = {Springer, Cham},
   title = {Training-Free Multi-objective Evolutionary Neural Architecture Search via Neural Tangent Kernel and Number of Linear Regions},
   volume = {13109 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-92270-2_29},
   year = {2021},
}
@article{Liu2021,
   abstract = {Many existing deep learning models are vulnerable to adversarial examples that are imperceptible to humans. To address this issue, various methods have been proposed to design network architectures that are robust to one particular type of adversarial attacks. It is practically impossible, however, to predict beforehand which type of attacks a machine learn model may suffer from. To address this challenge, we propose to search for deep neural architectures that are robust to five types of well-known adversarial attacks using a multi-objective evolutionary algorithm. To reduce the computational cost, a normalized error rate of a randomly chosen attack is calculated as the robustness for each newly generated neural architecture at each generation. All non-dominated network architectures obtained by the proposed method are then fully trained against randomly chosen adversarial attacks and tested on two widely used datasets. Our experimental results demonstrate the superiority of optimized neural architectures found by the proposed approach over state-of-the-art networks that are widely used in the literature in terms of the classification accuracy under different adversarial attacks.},
   author = {Jia Liu and Yaochu Jin},
   doi = {10.1016/J.NEUCOM.2021.04.111},
   issn = {0925-2312},
   journal = {Neurocomputing},
   keywords = {Adversarial attacks,Multi-objective evolutionary algorithm,Neural architecture search,Robustness},
   month = {9},
   note = {\{'id': 152,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II"], <br/>'ML task': ["Neural Architecture Search","Adversarial attacks","Robustness","Neural Network"], <br/>'Objective functions': ["Error on clean data","Error on adversarial data"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {73-84},
   publisher = {Elsevier},
   title = {Multi-objective search of robust neural architectures against multiple types of adversarial attacks},
   volume = {453},
   year = {2021},
}
@article{Villar2021,
   abstract = {Fairness is an increasingly important topic in the world of Artificial Intelligence. Machine learning techniques are widely used nowadays to solve huge amounts of problems, but those techniques may be biased against certain social groups due to different reasons....},
   author = {David Villar and Jorge Casillas},
   doi = {10.1007/978-3-030-85347-1_27},
   isbn = {9783030853464},
   issn = {18650937},
   journal = {Communications in Computer and Information Science},
   keywords = {Decision trees,Fairness in machine learning,Many objective evolutionary algorithm},
   note = {\{'id': 153,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-II","SMS-EMOA","GrEA"], <br/>'ML task': ["Fairness","Decision tree","Classification"], <br/>'Objective functions': ["Accuracy","Difference between False Positive Rate","Difference between Positive Predictive Val-ues"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {373-386},
   publisher = {Springer, Cham},
   title = {Facing Many Objectives for Fairness in Machine Learning},
   volume = {1439 CCIS},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-85347-1_27},
   year = {2021},
}
@article{Dokeroglu2021,
   abstract = {The Harris’ Hawks Optimization (HHO) is a recent metaheuristic inspired by the cooperative behavior of the hawks. These avians apply many intelligent techniques like surprise pounce (seven kills) while they are catching their prey according to the escaping patterns of the target. The HHO simulates these hunting patterns of the hawks to obtain the best/optimal solutions to the problems. In this study, we propose a new multiobjective HHO algorithm for the solution of the well-known binary classification problem. In this multiobjective problem, we reduce the number of selected features and try to keep the accuracy prediction as maximum as possible at the same time. We propose new discrete exploration (perching) and exploitation (besiege) operators for the hunting patterns of the hawks. We calculate the prediction accuracy of the selected features with four machine learning techniques, namely, Logistic Regression, Support Vector Machines, Extreme Learning Machines, and Decision Trees. To verify the performance of the proposed algorithm, we conduct comprehensive experiments on many benchmark datasets retrieved from the University of California, Irvine (UCI) Machine Learning Repository. Moreover, we apply it to a recent real-world dataset, i.e., a Coronavirus disease (COVID-19) dataset. Significant improvements are observed during the comparisons with state-of-the-art metaheuristic algorithms.},
   author = {Tansel Dokeroglu and Ayça Deniz and Hakan Ezgi Kiziloz},
   doi = {10.1016/J.KNOSYS.2021.107219},
   issn = {0950-7051},
   journal = {Knowledge-Based Systems},
   keywords = {Binary classification,Feature selection,Harris’ Hawks optimization,Multiobjective optimization},
   month = {9},
   note = {\{'id': 154,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Harris’ Hawks optimization"], <br/>'ML task': ["Feature Selection","Classification","Logistic Regression","Support Vector Machine","Extreme Learning Machines","Decision Trees"], <br/>'Objective functions': ["Number of features","Performance"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {107219},
   publisher = {Elsevier},
   title = {A robust multiobjective Harris’ Hawks Optimization algorithm for the binary classification problem},
   volume = {227},
   year = {2021},
}
@inproceedings{,
   abstract = {
Multi-objective multi-task learning aims to boost the performance of all tasks by leveraging their correlation and conflict appropriately. Nevertheless, in real practice, users may have preference for certain tasks, and other tasks simply serve as privileged or auxiliary tasks to assist the training of target tasks. The privileged tasks thus possess less or even no priority in the final task assessment by users. Motivated by this, we propose a privileged multiple descent algorithm to arbitrate the learning of target tasks and privileged tasks. Concretely, we introduce a privileged parameter so that the optimization direction does not necessarily follow the gradient from the privileged tasks, but concentrates more on the target tasks. Besides, we also encourage a priority parameter for the target tasks to control the potential distraction of optimization direction from the privileged tasks. In this way, the optimization direction can be more aggressively determined by weighting the gradients among target and privileged tasks, and thus highlight more the performance of target tasks under the unified multi-task learning context. Extensive experiments on synthetic and real-world datasets indicate that our method can achieve versatile Pareto solutions under varying preference for the target tasks. },
   author = {Yuru Song and Zan Lou and Shan You and Erkun Yang and Fei Wang and Chen Qian and Changshui Zhang and Xiaogang Wang},
   journal = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
   note = {\{'id': 155,<br/>'keep': True, <br/>'Optimization': ["Deterministic","Gradient","P-MGDA"], <br/>'ML task': ["Multi-task learning","Privileged tasks"], <br/>'Objective functions': ["Task losses"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   title = {Learning with Privileged Tasks},
   url = {https://openaccess.thecvf.com/content/ICCV2021/html/Song_Learning_With_Privileged_Tasks_ICCV_2021_paper.html},
   year = {2021},
}
@article{Li2021,
   abstract = {In statistical queries work, such as frequency estimation, the untrusted data collector could as an honest-but-curious (HbC) or malicious adversary to learn true values. Local differential privacy(LDP) protocols have been applied against the untrusted third party in data collecting. Nevertheless, excessive noise of LDP will reduce data utility, thus affecting the results of statistical queries. Therefore, it is significant to research the trade-off between privacy and utility. In this paper, we first measure the privacy loss by observing the maximum posterior confidence of the adversary (data collector). Then, through theoretical analysis and comparison we obtain the most suitable utility measure that is Wasserstein distance. Based on these, we introduce an originality framework for privacy-utility tradeoff framework, finding that this system conforms to the Pareto optimality state and formalizing a payoff function to find optimal equilibrium point under Pareto efficiency. Finally, we illustrate the efficacy of our system model by the Adult dataset from the UCI machine learning repository.},
   author = {Mengqian Li and Youliang Tian and Junpeng Zhang and Dandan Fan and Dongmei Zhao},
   doi = {10.1109/NANA53684.2021.00071},
   isbn = {9781665441582},
   journal = {Proceedings - 2021 International Conference on Networking and Network Applications, NaNA 2021},
   keywords = {Pareto optimality,data collecting,local differential privacy,privacy metric,utility metric},
   note = {\{'id': 156,<br/>'keep': True, <br/>'Optimization': ["Evolutionary", "NSGA-II"], <br/>'ML task': ["Privacy","Local differential privacy"], <br/>'Objective functions': ["Privacy Loss","Utility Loss"], <br/>'Single/Multi Solutions': "Single - Payoff function"<br/>\}},
   pages = {373-378},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {The Trade-off Between Privacy and Utility in Local Differential Privacy},
   year = {2021},
}
@article{Torfah2021,
   abstract = {We present a new multi-objective optimization approach for synthesizing interpretations that 'explain' the behavior of black-box machine learning models. Constructing human-understandable interpretations for black-box models often requires balancing conflicting objectives. A simple interpretation may be easier to understand for humans while being less precise in its predictions vis-a-vis a complex interpretation. Existing methods for synthesizing interpretations use a single objective function and are often optimized for a single class of interpretations. In contrast, we provide a more general and multi-objective synthesis framework that allows users to choose (1) the class of syntactic templates from which an interpretation should be synthesized, and (2) quantitative measures on both the correctness and explainability of an interpretation. For a given black-box, our approach yields a set of Pareto-optimal interpretations with respect to the correctness and explainability measures. We show that the underlying multi-objective optimization problem can be solved via a reduction to quantitative constraint solving, such as weighted maximum satisfiability. To demonstrate the benefits of our approach, we have applied it to synthesize interpretations for black-box neural-network classifiers. Our experiments show that there often exists a rich and varied set of choices for interpretations that are missed by existing approaches.},
   author = {Hazem Torfah and Shetal Shah and Supratik Chakraborty and S. Akshay and Sanjit A. Seshia},
   doi = {10.34727/2021/ISBN.978-3-85448-046-4_24},
   isbn = {9783854480464},
   journal = {Proceedings of the 21st Formal Methods in Computer-Aided Design, FMCAD 2021},
   note = {\{'id': 157,<br/>'keep': True, <br/>'Optimization': ["Deterministic","quantitative constraint solver","MaxSAT"], <br/>'ML task': ["Explainability"], <br/>'Objective functions': ["Explainability measure","Correctness measure"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {153-162},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Synthesizing Pareto-Optimal Interpretations for Black-Box Models},
   year = {2021},
}
@article{Chen2021,
   abstract = {While few-shot learning (FSL) aims for rapid generalization to new concepts with little supervision, self-supervised learning (SSL) constructs supervisory signals directly computed from unlabeled data. Exploiting the complementarity of these two manners, few-shot auxiliary learning has recently drawn much attention to deal with few labeled data. Previous works benefit from sharing inductive bias between the main task (FSL) and auxiliary tasks (SSL), where the shared parameters of tasks are optimized by minimizing a linear combination of task losses. However, it is challenging to select a proper weight to balance tasks and reduce task conflict. To handle the problem as a whole, we propose a novel approach named as Pareto self-supervised training (PSST) for FSL. PSST explicitly decomposes the few-shot auxiliary problem into multiple constrained multi-objective subproblems with different trade-off preferences, and here a preference region in which the main task achieves the best performance is identified. Then, an effective preferred Pareto exploration is proposed to find a set of optimal solutions in such a preference region. Extensive experiments on several public benchmark datasets validate the effectiveness of our approach by achieving state-of-the-art performance.},
   author = {Zhengyu Chen and Jixie Ge and Heshen Zhan and Siteng Huang and Donglin Wang},
   doi = {10.1109/CVPR46437.2021.01345},
   isbn = {9781665445092},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   note = {\{'id': 158,<br/>'keep': True, <br/>'Optimization': ["Deterministic","Constrained multi-objective subproblems","Gradient-based","steepest descent algorithm"], <br/>'ML task': ["Few-Shot Learning","Self-Supervised Learning"], <br/>'Objective functions': ["Loss for each task"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {13658-13667},
   publisher = {IEEE Computer Society},
   title = {Pareto self-supervised training for few-shot learning},
   year = {2021},
}
@article{Baioletti2021,
   abstract = {Generative Adversarial Network (GAN) is a family of machine learning algorithms designed to train neural networks able to imitate real data distributions. Unfortunately, GAN suffers from problems such as gradient vanishing and mode collapse. In Multi-Objective Evolutionary Generative Adversarial Network (MO-EGAN) these problems were addressed using an evolutionary technique combined with Multi-Objective selection, obtaining better results on synthetic datasets at the expense of larger computation times. In this works, we present the Smart Multi-Objective Evolutionary Generative Adversarial Network (SMO-EGAN) algorithm, which reduces the computational cost of MO-EGAN and achieves better results on real data distributions.},
   author = {Marco Baioletti and Gabriele Di Bari and Valentina Poggioni and Carlos Artemio Coello Coello},
   doi = {10.1109/CEC45853.2021.9504858},
   isbn = {9781728183923},
   journal = {2021 IEEE Congress on Evolutionary Computation, CEC 2021 - Proceedings},
   note = {\{'id': 159,<br/>'keep': True, <br/>'Optimization': ["Evolutionary"], <br/>'ML task': ["Generative Adversarial Networ"], <br/>'Objective functions': ["Jensen-Shannon divergence","probability of the discriminator being mistaken","Least square"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {2218-2225},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Smart Multi-Objective Evolutionary GaN},
   year = {2021},
}
@article{Kannan2021,
   abstract = {Current state of the art methods for generating Pareto-optimal solutions for multi-objective optimization problems mostly rely on optimizing the hyper-parameters of the models (HPO - hyper-parameter Optimization). Few recent, less studied methods focus on optimizing over the space of model parameters, leveraging the problem specific knowledge. We present a generic first-of-a-kind method, referred to as HyperASPO, that combines optimization over the spaces of both hyper-parameters and model parameters for multi-objective optimization of learning problems. HyperASPO consists of two stages. First, we perform a coarse HPO to determine a set of favorable hyper-parameter configurations. In the second step, for each of these configurations, we solve a sequence of weighted single objective optimization problems for estimating Pareto-optimal solutions. We generate the weights in the second step using an adaptive mesh constructed iteratively based on the metrics of interest, resulting in further refinement of Pareto frontier efficiently. We consider the widely used XGBoost (Gradient Boosted Trees) model and validate our method on multiple classification datasets. Our proposed method shows up to 20% improvement over the hypervolumes of Pareto fronts obtained through state of the art HPO based methods with up to 2× reduction in computational time.},
   author = {Aswin Kannan and Anamitra Roy Choudhury and Vaibhav Saxena and Saurabh Raje and Parikshit Ram and Ashish Verma and Yogish Sabharwal},
   doi = {10.1109/BIGDATA52589.2021.9671604},
   isbn = {9781665439022},
   journal = {2021 IEEE International Conference on Big Data (Big Data)},
   month = {1},
   note = {\{'id': 160,<br/>'keep': True, <br/>'Optimization': ["Deterministic","Gradient based","Adaptively Scalarized"], <br/>'ML task': ["Hyper Parameter Optimization"], <br/>'Objective functions': ["Chosen given the task"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {790-800},
   publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
   title = {HyperASPO: Fusion of Model and Hyper Parameter Optimization for Multi-objective Machine Learning},
   year = {2021},
}
@article{Ruchte2021,
   abstract = {Multi-objective optimization is important for various Deep Learning applications, however, no prior multi-objective method suits very deep networks. Existing approaches either require training a new network for every solution on the Pareto front or add a considerable overhead to the number of parameters by introducing hyper-networks conditioned on modifiable preferences. In this paper, we present a novel method that contextualizes the network directly on the preferences by adding them to the input space. In addition, we ensure a well-spread Pareto front by forcing the solutions to preserve a small angle to the preference vector. Through extensive experiments, we demonstrate that our Pareto fronts achieve state-of-the-art quality despite being computed significantly faster. Furthermore, we demonstrate the scalability as our method approximates the full Pareto front on the CelebA dataset with an EfficientNet network at a marginal training time overhead of 7% compared to a single-objective optimization. We make the code publicly available at https://github.com/ruchtem/cosmos.},
   author = {Michael Ruchte and Josif Grabocka},
   doi = {10.1109/ICDM51629.2021.00162},
   isbn = {9781665423984},
   issn = {15504786},
   journal = {2021 IEEE International Conference on Data Mining (ICDM)},
   month = {1},
   note = {\{'id': 161,<br/>'keep': True, <br/>'Optimization': ["Deterministic","Linear scalarization","Gradient based"], <br/>'ML task': ["Deep Learning"], <br/>'Objective functions': ["Loss for each task"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {1306-1311},
   publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
   title = {Scalable Pareto Front Approximation for Deep Multi-Objective Learning},
   year = {2021},
}
@article{Hu2021,
   abstract = {Feature selection (FS) is an important data processing technique in the field of machine learning. There have been various FS methods, but all assume that the cost associated with a feature is precise, which restricts their real applications. Focusing on the FS problem with fuzzy cost, a fuzzy multiobjective FS method with particle swarm optimization, called PSOMOFS, is studied in this article. The proposed method develops a fuzzy dominance relationship to compare the goodness of candidate particles and defines a fuzzy crowding distance measure to prune the elitist archive and determine the global leader of particles. Also, a tolerance coefficient is introduced into the proposed method to ensure that the Pareto-optimal solutions obtained satisfy decision makers' preferences. The developed method is used to tackle a series of the UCI datasets and is compared with three fuzzy multiobjective evolutionary methods and three typical multiobjective FS methods. Experimental results show that the proposed method can achieve feature sets with superior performances in approximation, diversity, and feature cost.},
   author = {Ying Hu and Yong Zhang and Dunwei Gong},
   doi = {10.1109/TCYB.2020.3015756},
   issn = {21682275},
   issue = {2},
   journal = {IEEE Transactions on Cybernetics},
   keywords = {Feature selection (FS),fuzzy cost,multiobjective optimization,particle swarm optimization (PSO)},
   month = {2},
   note = {\{'id': 162,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Particle Swarm Optimization"], <br/>'ML task': ["Feature Selection","Fuzzy Cost"], <br/>'Objective functions': ["Average error rate","Maximal fuzzy cost"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {874-888},
   pmid = {32924943},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Multiobjective Particle Swarm Optimization for Feature Selection with Fuzzy Cost},
   volume = {51},
   year = {2021},
}
@article{Luo2021,
   abstract = {Feature selection is an important research field in machine learning since high-dimensionality is a common characteristic of real-world data. It has two main objectives, which are to maximize the classification accuracy while minimizing the number of selected features. As the two objectives are usually in conflict with each other, it makes feature selection a multiobjective problem. However, the large search space and discrete Pareto front makes it not easy for existing evolutionary multiobjective algorithms. In order to deal with the above mentioned difficulties in feature selection, an entropy driven multiobjective particle swarm optimization algorithm is proposed to remove redundant feature and decrease computational complexity. First, its basic idea is to model feature selection as a multiobjective optimization problem by optimizing the number of features and the classification accuracy in supervised condition simultaneously. Second, a particle initialization strategy based on information entropy is designed to improve the quality of initial solutions, and an adaptive velocity update rule is used to swap between local search and global search. Besides, a specified discrete nondominated sorting is designed. These strategies enable the proposed algorithm to gain better performance on both the quality and size of feature subset. The experimental results show that the proposed algorithm can maintain or improve the quality of Pareto fronts evolved by the state-of-the-art algorithms for feature selection.},
   author = {Juanjuan Luo and Dongqing Zhou and Lingling Jiang and Huadong Ma},
   doi = {10.1109/CEC45853.2021.9504837},
   isbn = {9781728183923},
   journal = {2021 IEEE Congress on Evolutionary Computation, CEC 2021 - Proceedings},
   keywords = {Feature selection,Multiobjective optimization,Particle swarm optimization},
   note = {\{'id': 163,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Particle Swarm Optimization"], <br/>'ML task': ["Feature Selection","Classification"], <br/>'Objective functions': ["Number of features","Classification error"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {768-775},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {An Entropy Driven Multiobjective Particle Swarm Optimization Algorithm for Feature Selection},
   year = {2021},
}
@article{Bai2021,
   author = {Lixia Bai and Hong Li and Weifeng Gao},
   doi = {10.1109/CIS54983.2021.00017},
   journal = {2021 17th International Conference on Computational Intelligence and Security (CIS)},
   month = {2},
   note = {\{'id': 164,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","MOEA/D"], <br/>'ML task': ["Ensemble Learning","Classification","Neural Network","Extreme learning machine"], <br/>'Objective functions': ["Training RMSE","Validation RMSE","Network complexity"], <br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pages = {40-44},
   publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
   title = {A Selective Ensemble Classifier Using Multiobjective Optimization Based Extreme Learning Machine Algorithm},
   year = {2021},
}
@article{Shu2021,
   abstract = {Feature selection is extremely important in machine learning and data mining. Typical two-objective feature selection methods aim to minimize the number of features and maximize classification performance. However, they overlook the fact that there may be multiple subsets with similar information content for a given cardinality. The paper presents a many-objective feature selection approach to address this problem. Firstly, we establish a five-objective optimization model, which consists of classification accuracy, the number of features, feature relevance, feature redundancy, and feature complementarity. Therefore, the proposed model can enlarge the search space with more Pareto solutions. Secondly, we propose a wrapper structure for many-objective feature selection, which integrates a learning algorithm. Thirdly, in order to reduce the computional overhead, we propose a filter structure, which separates the learning algorithm. For implementation, we adopt NSGA-III multi-objective evolutionary algorithm and extreme learning machine. The experiments on mainstream datasets confirm the superiority of the proposed method.},
   author = {Lingxuan Shu and Fazhi He and Xun Hu and Haoran Li},
   doi = {10.1109/CSCWD49262.2021.9437707},
   isbn = {9781728165974},
   journal = {Proceedings of the 2021 IEEE 24th International Conference on Computer Supported Cooperative Work in Design, CSCWD 2021},
   keywords = {classification,collaborative processing of big data,extreme learning machine,feature selection,intelligent cloud manufacturing,many-objective optimization,optimization driven design},
   month = {5},
   note = {\{'id': 165,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA-­III"], <br/>'ML task': ["Feature Selection","Extreme learning machine","Classification"], <br/>'Objective functions': ["Accuracy","Number of features","Feature relevance","Feature redundancy","Feature complementarity"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {684-689},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Novel Feature Selection with Many-Objective Optimization and Learning Mechanism},
   year = {2021},
}
@article{,
   abstract = {Multi-label classification is a machine learning task to construct a model for assigning an entity in the dataset to two or more class labels. In order to improve the performance of multi-label classification, a multi-objective feature selection algorithm has been proposed in this paper. Feature selection as a preprocessing task for Multi-label classification problems aims to choose a subset of relevant features. Selecting a small number of high-quality features decreases the computational cost and at the same time maximizes the classification performance. However extreme decreasing the number of features causes the failure of classification. As a result, feature selection has two conflicting objectives, namely, minimizing the classification error and minimizing the number of selected features. This paper proposes a multi-objective optimization algorithm to tackle the multi-label feature selection. The task is to find a set of solutions (a subset of features) in a sophisticated large-scale search space using a reference-based multi-objective optimization method. The proposed algorithm utilizes an opposition-based binary operator to generate more diverse solutions. Injection of extreme point of the Pareto-front is another component of the algorithm which aims to find feature subsets with less classification error. The proposed method is compared with two other existing methods on eight multi-label benchmark datasets. The experimental results show that the proposed method outperforms existing algorithms in terms of various multi-objective evaluation measures, such as Hyper-volume indicator, Pure diversity, Two-set coverage, and Pareto-front proportional contribution. The proposed method leads to get a set of well-distributed trade-off solutions which reach less classification error in comparing with competitors, even with the fewer number of features.},
   author = {Azam Asilian Bidgoli and Hossein Ebrahimpour-Komleh and Shahryar Rahnamayan},
   doi = {10.1016/J.INS.2020.08.004},
   issn = {0020-0255},
   journal = {Information Sciences},
   keywords = {Evolutionary algorithm,Feature selection,Multi-label classification,Multi-objective optimization,Opposition-based computation},
   month = {2},
   note = {\{'id': 166,<br/>'keep': True, <br/>'Optimization': ["Evolutionary"], <br/>'ML task': ["Classification","Multi-label","Feature Selection"], <br/>'Objective functions': ["Number of features","Hamming loss"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {1-17},
   publisher = {Elsevier},
   title = {Reference-point-based multi-objective optimization algorithm with opposition-based voting scheme for multi-label feature selection},
   volume = {547},
   year = {2021},
}
@article{Wang2021,
   abstract = {As multi-task models gain popularity in a wider range of machine learning applications, it is becoming increasingly important for practitioners to understand the fairness implications associated with those models. Most existing fairness literature focuses on learning a single task more fairly, while how ML fairness interacts with multiple tasks in the joint learning setting is largely under-explored. In this paper, we are concerned with how group fairness (e.g., equal opportunity, equalized odds) as an ML fairness concept plays out in the multi-task scenario. In multi-task learning, several tasks are learned jointly to exploit task correlations for a more efficient inductive transfer. This presents a multi-dimensional Pareto frontier on (1) the trade-off between group fairness and accuracy with respect to each task, as well as (2) the trade-offs across multiple tasks. We aim to provide a deeper understanding on how group fairness interacts with accuracy in multi-task learning, and we show that traditional approaches that mainly focus on optimizing the Pareto frontier of multi-task accuracy might not perform well on fairness goals. We propose a new set of metrics to better capture the multi-dimensional Pareto frontier of fairness-accuracy trade-offs uniquely presented in a multi-task learning setting. We further propose a Multi-Task-Aware Fairness (MTA-F) approach to improve fairness in multi-task learning. Experiments on several real-world datasets demonstrate the effectiveness of our proposed approach.},
   author = {Yuyan Wang and Xuezhi Wang and Alex Beutel and Flavien Prost and Jilin Chen and Ed H. Chi},
   doi = {10.1145/3447548.3467326},
   isbn = {9781450383325},
   journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   keywords = {fairness,multi-task learning,multi-task-aware fairness treatment,pareto frontier},
   month = {8},
   note = {\{'id': 167,<br/>'keep': True, <br/>'Optimization': ["Evolutionary"], <br/>'ML task': ["Classification","Multi-label","Fairness"], <br/>'Objective functions': ["Accuracy loss","Task-specific Fairness loss","Shared Fairness loss"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {1748-1757},
   publisher = {Association for Computing Machinery},
   title = {Understanding and Improving Fairness-Accuracy Trade-offs in Multi-Task Learning},
   year = {2021},
}
@article{Liu2021,
   abstract = {Support Vector Machines (SVMs), originally proposed for classifications of two classes, have become a very popular technique in the machine learning field. For multi-class classifications, various single-objective models and multi-objective ones have been proposed. However,in most single-objective models, neither the different costs of different misclassifications nor the users’ preferences were considered. This drawback has been taken into account in multi-objective models.In these models, large and hard second-order cone programs(SOCPs) were constructed ane weakly Pareto-optimal solutions were offered. In this paper, we propose a Projected Multi-objective SVM (PM), which is a multi-objective technique that works in a higher dimensional space than the object space. For PM, we can characterize the associated Pareto-optimal solutions. Additionally, it significantly alleviates the computational bottlenecks for classifications with large numbers of classes. From our experimental results, we can see PM outperforms the single-objective multi-class SVMs (based on an all-together method, one-against-all method and one-against-one method) and other multi-objective SVMs. Compared to the single-objective multi-class SVMs, PM provides a wider set of options designed for different misclassifications, without sacrificing training time. Compared to other multi-objective methods, PM promises the out-of-sample quality of the approximation of the Pareto frontier, with a considerable reduction of the computational burden.},
   author = {Ling Liu and Belén Martín-Barragán and Francisco J. Prieto},
   doi = {10.1016/J.CIE.2021.107425},
   issn = {0360-8352},
   journal = {Computers & Industrial Engineering},
   keywords = {Multi-class multi-objective SVM,Multiple objective programming,Pareto-optimal solution,Support vector machine},
   month = {8},
   note = {\{'id': 168,<br/>'keep': True, <br/>'Optimization': ["Evolutionary"], <br/>'ML task': ["Classification","Multi-class","Support Vector Machine"], <br/>'Objective functions': ["Geometric margin for each class"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {107425},
   publisher = {Pergamon},
   title = {A projection multi-objective SVM method for multi-class classification},
   volume = {158},
   year = {2021},
}
@article{Wei2021,
   abstract = {With the rapid development of computer hardware in the past three decades, various classic algorithms such as neural computing and bionic optimization computing have been widely used in practical problems. This paper extended the new bionic algorithm-flock algorithm proposed in 2014 and obtained a multi-objective flock algorithm to solve the multi-objective problem. This study used aggregate functions to define social ranks, and simulated the foraging behavior of chickens in the process of searching for food in the objective space and found the balance between diversity and convergence when looking for the best Pareto solution. The algorithm took five types of bi-objective functions and four types of three-objective functions as objects and compared it with four more widely used algorithms in multi-objective problems. The results demonstrate that the MOCSO (multi-objective chicken swarm optimization) algorithm shows better results in the optimization of multi-objective problems.},
   author = {Qianzhou Wei and Dongru Huang and Yu Zhang},
   doi = {10.1007/S11227-021-03770-Z/TABLES/9},
   issn = {15730484},
   issue = {11},
   journal = {Journal of Supercomputing},
   keywords = {Bionic optimization calculation,Chicken swarm optimization algorithm,Deep learning,Multi-objective optimization,Neural computing,Pareto solution set},
   month = {11},
   note = {\{'id': 169,<br/>'keep': False<br/>\}},
   pages = {13069-13089},
   publisher = {Springer},
   title = {Artificial chicken swarm algorithm for multi-objective optimization with deep learning},
   volume = {77},
   url = {https://link.springer.com/article/10.1007/s11227-021-03770-z},
   year = {2021},
}
@article{Ottervanger2021,
   abstract = {Early time series classification (EarlyTSC) involves the prediction of a class label based on partial observation of a given time series. Most EarlyTSC algorithms consider the trade-off between accuracy and earliness as two competing objectives, using a single dedicated hyperparameter. To obtain insights into this trade-off requires finding a set of non-dominated (Pareto efficient) classifiers. So far, this has been approached through manual hyperparameter tuning. Since the trade-off hyperparameters only provide indirect control over the earliness-accuracy trade-off, manual tuning is tedious and tends to result in many sub-optimal hyperparameter settings. This complicates the search for optimal hyperparameter settings and forms a hurdle for the application of EarlyTSC to real-world problems. To address these issues, we propose an automated approach to hyperparameter tuning and algorithm selection for EarlyTSC, building on developments in the fast-moving research area known as automated machine learning (AutoML). To deal with the challenging task of optimising two conflicting objectives in early time series classification, we propose MultiETSC, a system for multi-objective algorithm selection and hyperparameter optimisation (MO-CASH) for EarlyTSC. MultiETSC can potentially leverage any existing or future EarlyTSC algorithm and produces a set of Pareto optimal algorithm configurations from which a user can choose a posteriori. As an additional benefit, our proposed framework can incorporate and leverage time-series classification algorithms not originally designed for EarlyTSC for improving performance on EarlyTSC; we demonstrate this property using a newly defined, “naïve” fixed-time algorithm. In an extensive empirical evaluation of our new approach on a benchmark of 115 data sets, we show that MultiETSC performs substantially better than baseline methods, ranking highest (avg. rank 1.98) compared to conceptually simpler single-algorithm (2.98) and single-objective alternatives (4.36).},
   author = {Gilles Ottervanger and Mitra Baratchi and Holger H. Hoos},
   doi = {10.1007/S10618-021-00781-5/TABLES/10},
   issn = {1573756X},
   issue = {6},
   journal = {Data Mining and Knowledge Discovery},
   keywords = {Automated machine learning,Early classification,Time series classification},
   month = {11},
   note = {\{'id': 170,<br/>'keep': True, <br/>'Optimization': ["Deterministic","MO-ParamILS","Iterated local search"], <br/>'ML task': ["Classification","Time Series","Early time series classification","hyperparameter tuning"," algorithm selection","AutoML"], <br/>'Objective functions': ["Error rate","Earliness"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {2602-2654},
   publisher = {Springer},
   title = {MultiETSC: automated machine learning for early time series classification},
   volume = {35},
   url = {https://link.springer.com/article/10.1007/s10618-021-00781-5},
   year = {2021},
}
@article{Cai2021,
   abstract = {Neural architecture search (NAS) is an exciting new field in automating machine learning. It can automatically search for the architecture of neural networks. But the current NAS has extremely high requirements for hardware equipment and time costs. In this work, we propose a predictor based on Radial basis function neural network (RBFNN) as a surrogate model of Bayesian optimization to predict the performance of neural architecture. The existing work does not consider the difficulty of directly searching for neural architectures that meet the performance requirements of NAS in real-world applications. Meanwhile, NAS needs to execute multiple times independently when facing multiple similar tasks. Therefore, we further propose a multi-task learning surrogate model with multiple RBFNNs. The model not only functions as a predictor, but also learns knowledge of similar tasks jointly. The performance of NAS is improved by processing multiple tasks simultaneously. Also, the current NAS is committed to searching for very high-performance networks and does not take into account that neural architectures are limited by device memory during actual deployment. The scale of architecture also needs to be considered. We use a multi-objective optimization algorithm to simultaneously balance the performance and the scale, and build a multi-objective evolutionary search framework to find the Pareto optimal front. Once the NAS is completed, decision-makers can choose the appropriate architecture for deployment according to different performance requirements and hardware conditions. Compared with existing NAS work, our proposed MT-ENAS algorithm is able to find a neural architecture with competitive performance and smaller scale in a shorter time.},
   author = {Ronghong Cai and Jianping Luo},
   doi = {10.1109/CEC45853.2021.9504721},
   isbn = {9781728183923},
   journal = {2021 IEEE Congress on Evolutionary Computation, CEC 2021 - Proceedings},
   keywords = {Multi-objective optimization,Multi-task learning,Neural architecture search,Surrogate model},
   note = {\{'id': 171,<br/>'keep': True, <br/>'Optimization': ["Evolutionary"], <br/>'ML task': ["Neural Architecture Search","Multi-task","AutoML","Neural Networks"], <br/>'Objective functions': ["Task loss"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {1680-1687},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Multi-Task Learning for Multi-Objective Evolutionary Neural Architecture Search},
   year = {2021},
}
@article{Li2021,
   abstract = {Multi-label learning has widely applied in machine learning and data mining. The purpose of feature selection is to select an approximately optimal feature subset to characterize the original feature space. Similar to single-label data, feature selection is an import preprocessing step to enhance the performance of multi-label classification model. In this paper, we propose a multi-label feature selection approach with Pareto optimality for continuous data, called MLFSPO. It maps multi-label features to high-dimensional space to evaluate the correlation between features and labels by utilizing the Hilbert-Schmidt Independence Criterion (HSIC). Then, the feature subset obtains by combining the Pareto optimization with feature ordering criteria and label weighting. Eventually, extensive experimental results on publicly available data sets show the effectiveness of the proposed algorithm in multi-label tasks.},
   author = {Guohe Li and Yong Li and Yifeng Zheng and Ying Li and Yunfeng Hong and Xiaoming Zhou},
   doi = {10.1007/S10489-021-02228-2/TABLES/6},
   issn = {15737497},
   issue = {11},
   journal = {Applied Intelligence},
   keywords = {Feature selection,Hilbert-Schmidt independence criterion,Multi-label learning,Pareto optimality},
   month = {11},
   note = {\{'id': 172,<br/>'keep': False<br/>\}},
   pages = {7794-7811},
   publisher = {Springer},
   title = {A novel feature selection approach with Pareto optimality for multi-label data},
   volume = {51},
   url = {https://link.springer.com/article/10.1007/s10489-021-02228-2},
   year = {2021},
}
@article{Valdivia2021,
   abstract = {Fair machine learning has been focusing on the development of equitable algorithms that address discrimination. Yet, many of these fairness-aware approaches aim to obtain a unique solution to the problem, which leads to a poor understanding of the statistical limits of bias mitigation interventions. In this study, a novel methodology is presented to explore the tradeoff in terms of a Pareto front between accuracy and fairness. To this end, we propose a multiobjective framework that seeks to optimize both measures. The experimental framework is focused on logistiregression and decision tree classifiers since they are well-known by the machine learning community. We conclude experimentally that our method can optimize classifiers by being fairer with a small cost on the classification accuracy. We believe that our contribution will help stakeholders of sociotechnical systems to assess how far they can go being fair and accurate, thus serving in the support of enhanced decision making where machine learning is used.},
   author = {Ana Valdivia and Javier Sánchez-Monedero and Jorge Casillas},
   doi = {10.1002/INT.22354},
   issn = {1098-111X},
   issue = {4},
   journal = {International Journal of Intelligent Systems},
   keywords = {algorithmic fairness,group fairness,multiobjective optimization},
   month = {4},
   note = {\{'id': 173,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA‐II"], <br/>'ML task': ["Fairness","Logistic Regression","Decision tree","Classification"], <br/>'Objective functions': ["Geometric Mean","Difference of False-Positive Rate"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {1619-1643},
   publisher = {John Wiley & Sons, Ltd},
   title = {How fair can we go in machine learning? Assessing the boundaries of accuracy and fairness},
   volume = {36},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/int.22354 https://onlinelibrary.wiley.com/doi/abs/10.1002/int.22354 https://onlinelibrary.wiley.com/doi/10.1002/int.22354},
   year = {2021},
}
@article{Yu2021,
   abstract = {Recently, various Cross-Domain Collaborative Filtering (CDCF) algorithms are presented to address the sparsity problem, leveraging ratings of auxiliary domains to improve target domain's recommendation performance. Therein, two-sided CDCF algorithms have shown better performance, given the fact that they can extract both user and item information. However, as the auxiliary domains are not all related to the target domain, utilizing information from all the auxiliary domains may not be optimal and would lead to low efficiency. A Two-Sided CDCF model based on Selective Ensemble learning considering both Accuracy and Efficiency (TSSEAE) is proposed to balance recommendation accuracy and efficiency. In TSSEAE, user-sided and item-sided auxiliary domains are firstly combined to improve performance of target domain. Then, CDCF problems are converted to ensemble learning problems, with each combination corresponding to a classifier. In this way, the problem of selecting combinations can be converted to that of selecting classifiers, which is a selective ensemble learning problem. Finally, a bi-objective optimization problem is solved to obtain Pareto optimal solutions for the selective ensemble learning problem. The experimental result on Amazon dataset shows the effectiveness of TSSEAE.},
   author = {Xu Yu and Qinglong Peng and Lingwei Xu and Feng Jiang and Junwei Du and Dunwei Gong},
   doi = {10.1016/J.IPM.2021.102691},
   issn = {0306-4573},
   issue = {6},
   journal = {Information Processing & Management},
   keywords = {Bi-objective optimization problem,Cross-domain collaborative filtering,Ensemble learning,Pareto optimal solutions,Selective ensemble},
   month = {11},
   note = {\{'id': 174,<br/>'keep': False<br/>\}},
   pages = {102691},
   publisher = {Pergamon},
   title = {A selective ensemble learning based two-sided cross-domain collaborative filtering algorithm},
   volume = {58},
   year = {2021},
}
@article{Sun2021,
   abstract = {Ensemble learning is one of the most frequently used techniques for handling concept drift, which is the greatest challenge for learning high-performance models from big evolving data streams. In this paper, a Pareto-based multi-objective optimization technique is introduced to learn high-performance base classifiers. Based on this technique, a multi-objective evolutionary ensemble learning scheme, named Pareto-optimal ensemble for a better accuracy and diversity (PAD), is proposed. The approach aims to enhance the generalization ability of ensemble in evolving data stream environment by balancing the accuracy and diversity of ensemble members. In addition, an adaptive window change detection mechanism is designed for tracking different kinds of drifts constantly. Extensive experiments show that PAD is capable of adapting to dynamic change environments effectively and efficiently in achieving better performance.},
   author = {Yange Sun and Honghua Dai},
   doi = {10.1007/S00521-020-05386-5/FIGURES/10},
   issn = {14333058},
   issue = {11},
   journal = {Neural Computing and Applications},
   keywords = {Classifier selection,Concept drift,Data streams,Diversity,Ensemble learning,Multi-objective optimization},
   month = {6},
   note = {\{'id': 175,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA‐II"], <br/>'ML task': ["Ensemble Learning","Classification","Data streams"], <br/>'Objective functions': ["Accuracy","Diversity"], <br/>'Single/Multi Solutions': "Single - Ensemble"<br/>\}},
   pages = {6119-6132},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Constructing accuracy and diversity ensemble using Pareto-based multi-objective learning for evolving data streams},
   volume = {33},
   url = {https://link.springer.com/article/10.1007/s00521-020-05386-5},
   year = {2021},
}
@article{Zhang2021,
   abstract = {Dilemma between model accuracy and fairness in machine learning models has been shown theoretically and empirically. So far, dozens of fairness measures have been proposed, among which incompatibility and complementarity exist. However, no fairness measure has been...},
   author = {Qingquan Zhang and Jialin Liu and Zeqi Zhang and Junyi Wen and Bifei Mao and Xin Yao},
   doi = {10.1007/978-3-030-86380-7_10},
   isbn = {9783030863791},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {AI ethics,Discrimination in machine learning,Fairness in machine learning,Fairness measures,Multi-objective learning},
   note = {\{'id': 176,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA‐II"], <br/>'ML task': ["Fairness","Classification"], <br/>'Objective functions': ["Model Error","Individual unfairness","Group unfairness"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {111-123},
   publisher = {Springer, Cham},
   title = {Fairer Machine Learning Through Multi-objective Evolutionary Learning},
   volume = {12894 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-86380-7_10},
   year = {2021},
}
@article{Wegier2022,
   abstract = {One of the vital problems with the imbalanced data classifier training is the definition of an optimization criterion. Typically, since the exact cost of misclassification of the individual classes is unknown, combined metrics and loss functions that roughly balance the cost for each class are used. However, this approach can lead to a loss of information, since different trade-offs between class misclassification rates can produce similar combined metric values. To address this issue, this paper discusses a multi-criteria ensemble training method for the imbalanced data. The proposed method jointly optimizes precision and recall, and provides the end-user with a set of Pareto optimal solutions, from which the final one can be chosen according to the user's preference. The proposed approach was evaluated on a number of benchmark datasets and compared with the single-criterion approach (where the selected criterion was one of the chosen metrics). The results of the experiments confirmed the usefulness of the obtained method, which on the one hand guarantees good quality, i.e., not worse than the one obtained with the use of single-criterion optimization, and on the other hand, offers the user the opportunity to choose the solution that best meets their expectations regarding the trade-off between errors on the minority and the majority class.},
   author = {Weronika Wegier and Michal Koziarski and Micha Wozniak and Weronika Wegier},
   doi = {10.1109/ACCESS.2022.3149914},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Classifier ensemble,imbalanced data,multi-objective optimization,pattern classification},
   note = {\{'id': 177,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","NSGA‐II"], <br/>'ML task': ["Multicriteria","Classification","Imbalanced data"], <br/>'Objective functions': ["Recall","Precision"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {16807-16818},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Multicriteria Classifier Ensemble Learning for Imbalanced Data},
   volume = {10},
   year = {2022},
}
@article{Kumar2022,
   abstract = {Standard classification algorithms give biased results when data sets are imbalanced. Genetic Programming, a machine learning algorithm based on the evolution of species in nature, also suffers from the same issue. In this research work, we introduced a logarithmic...},
   author = {Arvind Kumar and Shivani Goel and Nishant Sinha and Arpit Bhardwaj},
   doi = {10.1007/978-3-030-95502-1_23},
   isbn = {9783030955014},
   issn = {18650937},
   note = {\{'id': 178\}},
   pages = {294-304},
   publisher = {Springer, Cham},
   title = {A Logarithmic Distance-Based Multi-Objective Genetic Programming Approach for Classification of Imbalanced Data},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-95502-1_23},
   year = {2022},
}
@article{Reiners2022,
   abstract = {Overparameterization and overfitting are common concerns when designing and training deep neural networks, that are often counteracted by pruning and regularization strategies. However, these strategies remain secondary to most learning approaches and suffer from time and computational intensive procedures. We suggest a multiobjective perspective on the training of neural networks by treating its prediction accuracy and the network complexity as two individual objective functions in a biobjective optimization problem. As a showcase example, we use the cross entropy as a measure of the prediction accuracy while adopting an l1-penalty function to assess the total cost (or complexity) of the network parameters. The latter is combined with an intra-training pruning approach that reinforces complexity reduction and requires only marginal extra computational cost. From the perspective of multiobjective optimization, this is a truly large-scale optimization problem. We compare two different optimization paradigms: On the one hand, we adopt a scalarization-based approach that transforms the biobjective problem into a series of weighted-sum scalarizations. On the other hand we implement stochastic multi-gradient descent algorithms that generate a single Pareto optimal solution without requiring or using preference information. In the first case, favorable knee solutions are identified by repeated training runs with adaptively selected scalarization parameters. Numerical results on exemplary convolutional neural networks confirm that large reductions in the complexity of neural networks with negligible loss of accuracy are possible.},
   author = {Malena Reiners and Kathrin Klamroth and Fabian Heldmann and Michael Stiglmayr},
   doi = {10.1016/J.COR.2021.105676},
   issn = {0305-0548},
   journal = {Computers & Operations Research},
   keywords = {Automated machine learning,Multiobjective learning,Stochastic multi-gradient descent,Unstructured pruning,l1-regularization},
   month = {5},
   note = {\{'id': 179,<br/>'keep': True, <br/>'Optimization': ["Deterministic","Stochastic Multi-Gradient Descent","Gradient based"], <br/>'ML task': ["Neural Networks","Pruning","AutoML","Classification"], <br/>'Objective functions': ["Cross entropy","L1"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {105676},
   publisher = {Pergamon},
   title = {Efficient and sparse neural networks by pruning weights in a multiobjective learning approach},
   volume = {141},
   year = {2022},
}
@article{Zhou2022,
   abstract = {Multi-task learning technique is widely utilized in machine learning modeling where commonalities and differences across multiple tasks are exploited. However, multiple conflicting objectives often occur in multi-task learning. Conventionally, a common compromise is to minimize the weighted sum of multiple objectives which may be invalid if the objectives are competing. In this paper, a novel multi-objective machine learning approach is proposed to solve this challenging issue, which reformulates the multi-task learning as multi-objective optimization. To address the issues contributed by existing multi-objective optimization algorithms, a multi-gradient descent algorithm is introduced for the multi-objective machine learning problem by which an innovative gradient-based optimization is leveraged to converge to an optimal solution of the Pareto set. Moreover, the gradient surgery for the multi-gradient descent algorithm is proposed to obtain a stable Pareto optimal solution. As most of the edge computing devices are computational resource-constrained, the proposed method is implemented for optimizing the edge device's memory, computation and communication demands. The proposed method is applied to the multiple license plate recognition problem. The experimental results show that the proposed method outperforms state-of-the-art learning methods and can successfully find solutions that balance multiple objectives of the learning task over different datasets.},
   author = {Xiaojun Zhou and Yuan Gao and Chaojie Li and Zhaoke Huang},
   doi = {10.1109/TNSE.2021.3067454},
   issn = {23274697},
   issue = {1},
   journal = {IEEE Transactions on Network Science and Engineering},
   keywords = {Deep neural network,Edge computing,Multi-objective machine learning,Multi-task learning,Multiple gradient descent},
   note = {\{'id': 180,<br/>'keep': False<br/>\}},
   pages = {121-133},
   publisher = {IEEE Computer Society},
   title = {A Multiple Gradient Descent Design for Multi-Task Learning on Edge Computing: Multi-Objective Machine Learning Approach},
   volume = {9},
   year = {2022},
}
@article{Liu2021,
   abstract = {Optimization of conflicting functions is of paramount importance in decision making, and real world applications frequently involve data that is uncertain or unknown, resulting in multi-objective optimization (MOO) problems of stochastic type. We study the stochastic multi-gradient (SMG) method, seen as an extension of the classical stochastic gradient method for single-objective optimization. At each iteration of the SMG method, a stochastic multi-gradient direction is calculated by solving a quadratic subproblem, and it is shown that this direction is biased even when all individual gradient estimators are unbiased. We establish rates to compute a point in the Pareto front, of order similar to what is known for stochastic gradient in both convex and strongly convex cases. The analysis handles the bias in the multi-gradient and the unknown a priori weights of the limiting Pareto point. The SMG method is framed into a Pareto-front type algorithm for calculating an approximation of the entire Pareto front. The Pareto-front SMG algorithm is capable of robustly determining Pareto fronts for a number of synthetic test problems. One can apply it to any stochastic MOO problem arising from supervised machine learning, and we report results for logistic binary classification where multiple objectives correspond to distinct-sources data groups.},
   author = {S. Liu and L. N. Vicente},
   doi = {10.1007/S10479-021-04033-Z/TABLES/4},
   issn = {15729338},
   journal = {Annals of Operations Research},
   keywords = {Multi-objective optimization,Pareto front,Stochastic gradient descent,Supervised machine learning},
   month = {3},
   note = {\{'id': 181,<br/>'keep': False,<br/>'obs': "Optimization paper"<br/>\}},
   pages = {1-30},
   publisher = {Springer},
   title = {The stochastic multi-gradient algorithm for multi-objective optimization and its application to supervised machine learning},
   url = {https://link.springer.com/article/10.1007/s10479-021-04033-z},
   year = {2021},
}
@article{Abdollahzadeh2021,
   abstract = {Feature selection (FS) is a critical step in data mining, and machine learning algorithms play a crucial role in algorithms performance. It reduces the processing time and accuracy of the categories. In this paper, three different solutions are proposed to FS. In the first solution, the Harris Hawks Optimization (HHO) algorithm has been multiplied, and in the second solution, the Fruitfly Optimization Algorithm (FOA) has been multiplied, and in the third solution, these two solutions are hydride and are named MOHHOFOA. The results were tested with MOPSO, NSGA-II, BGWOPSOFS and B-MOABC algorithms for FS on 15 standard data sets with mean, best, worst, standard deviation (STD) criteria. The Wilcoxon statistical test was also used with a significance level of 5% and the Bonferroni–Holm method to control the family-wise error rate. The results are shown in the Pareto front charts, indicating that the proposed solutions' performance on the data set is promising.},
   author = {Benyamin Abdollahzadeh and Farhad Soleimanian Gharehchopogh},
   doi = {10.1007/S00366-021-01369-9/TABLES/7},
   issn = {14355663},
   journal = {Engineering with Computers},
   keywords = {Bonferroni–Holm,Family-wise error rate,Feature selection,Fruitfly optimization algorithm,Harris hawks optimization,Multiobjective},
   month = {3},
   note = {\{'id': 182,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Harris Hawks Optimization","Fruitfly Optimization Algorithm"], <br/>'ML task': ["Feature Selection","Classification"], <br/>'Objective functions': ["Classification error","Number of features"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {1-19},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {A multi-objective optimization algorithm for feature selection problems},
   volume = {1},
   url = {https://link.springer.com/article/10.1007/s00366-021-01369-9},
   year = {2021},
}
@article{Qasem2010,
   abstract = {The problem of unsupervised and supervised learning of RBF networks is discussed with Multi-Objective Particle Swarm Optimization (MOPSO). This study presents an evolutionary multi-objective selection method of RBF networks structure. The candidates of RBF networks structures are encoded into particles in PSO. These particles evolve toward Pareto-optimal front defined by several objective functions with model accuracy and complexity. This study suggests an approach of RBF network training through simultaneous optimization of architectures and connections with PSO-based multi-objective algorithm. Present goal is to determine whether MOPSO can train RBF networks and the performance is validated on accuracy and complexity. The experiments are conducted on two benchmark datasets obtained from the machine learning repository. The results show that; the best results are obtained for our proposed method that has obtained 100 and 80.21 % classification accuracy from the experiments made on the data taken from breast cancer and diabetes diseases database, respectively. The results also show that our approach provides an effective means to solve multi-objective RBF networks and outperforms multi-objective genetic algorithm. © 2010 Asian Network for Scientific Information.},
   author = {Sultan Noman Qasem and S. M. Shamsuddin},
   doi = {10.3923/JAI.2010.1.16},
   issn = {19945450},
   issue = {1},
   journal = {Journal of Artificial Intelligence},
   keywords = {Elitist non-dominated sorting genetic algorithm,Hybrid learning,Multi-objective optimization,Multi-objective particle swarm optimization,Radial basis function network},
   note = {\{'id': 183,<br/>'keep': True, <br/>'Optimization': ["Evolutionary","Particle Swarm Optimization"], <br/>'ML task': ["RBF Network","Classification"], <br/>'Objective functions': ["Mean-Squared Error","L2"], <br/>'Single/Multi Solutions': "Multi"<br/>\}},
   pages = {1-16},
   title = {Generalization improvement of radial basis function network based on multi-objective particle swarm optimization},
   volume = {3},
   year = {2010},
}
