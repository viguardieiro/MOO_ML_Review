@article{10.1109/MM.2017.49,
author = {Khazraee, Moein and Gutierrez, Luis Vega and Magaki, Ikuo and Taylor, Michael Bedford},
title = {Specializing a Planet's Computation: ASIC Clouds},
year = {2017},
issue_date = {2017},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {37},
number = {3},
issn = {0272-1732},
url = {https://doi.org/10.1109/MM.2017.49},
doi = {10.1109/MM.2017.49},
abstract = {GPU- and FPGA-based clouds have been deployed to accelerate computationally intensive workloads. ASIC-based clouds are a natural evolution as cloud services expand across the planet. ASIC Clouds are purpose-built datacenters comprising large arrays of ASIC accelerators that optimize the total cost of ownership (TCO) of large, high-volume scale-out computations. On the surface, ASIC Clouds may seem improbable due to high nonrecurring engineering (NRE) costs and ASIC inflexibility, but large-scale ASIC Clouds have been deployed for the Bitcoin cryptocurrency system. This article distills lessons from these Bitcoin ASIC Clouds and applies them to other large-scale workloads, including YouTube-style video-transcoding and Deep Learning, showing superior TCO versus CPU and GPU. It derives Pareto-optimal ASIC Cloud servers based on accelerator properties, by jointly optimizing ASIC architecture, DRAM, motherboard, power delivery, cooling, and operating voltage. Finally, the authors examine the impact of ASIC NRE and when it makes sense to build an ASIC Cloud.},
journal = {IEEE Micro},
month = {jan},
pages = {62–69},
numpages = {8}
}

@inproceedings{10.1145/3351095.3372857,
author = {Hu, Lily and Chen, Yiling},
title = {Fair Classification and Social Welfare},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372857},
doi = {10.1145/3351095.3372857},
abstract = {Now that machine learning algorithms lie at the center of many important resource allocation pipelines, computer scientists have been unwittingly cast as partial social planners. Given this state of affairs, important questions follow. How do leading notions of fairness as defined by computer scientists map onto longer-standing notions of social welfare? In this paper, we present a welfare-based analysis of fair classification regimes. Our main findings assess the welfare impact of fairness-constrained empirical risk minimization programs on the individuals and groups who are subject to their outputs. We fully characterize the ranges of Δ'e perturbations to a fairness parameter 'e in a fair Soft Margin SVM problem that yield better, worse, and neutral outcomes in utility for individuals and by extension, groups. Our method of analysis allows for fast and efficient computation of "fairness-to-welfare" solution paths, thereby allowing practitioners to easily assess whether and which fair learning procedures result in classification outcomes that make groups better-off. Our analyses show that applying stricter fairness criteria codified as parity constraints can worsen welfare outcomes for both groups. More generally, always preferring "more fair" classifiers does not abide by the Pareto Principle---a fundamental axiom of social choice theory and welfare economics. Recent work in machine learning has rallied around these notions of fairness as critical to ensuring that algorithmic systems do not have disparate negative impact on disadvantaged social groups. By showing that these constraints often fail to translate into improved outcomes for these groups, we cast doubt on their effectiveness as a means to ensure fairness and justice.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {535–545},
numpages = {11},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1109/INFOCOM41043.2020.9155487,
author = {Yuan, Yali and Srikant Adhatarao, Sripriya and Lin, Mingkai and Yuan, Yachao and Liu, Zheli and Fu, Xiaoming},
title = {ADA: Adaptive Deep Log Anomaly Detector},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/INFOCOM41043.2020.9155487},
doi = {10.1109/INFOCOM41043.2020.9155487},
abstract = {Large private and government networks are often subjected to attacks like data extrusion and service disruption. Existing anomaly detection systems use offline supervised learning and employ experts for labeling. Hence they cannot detect anomalies in real-time. Even though unsupervised algorithms are increasingly used nowadays, they cannot readily adapt to newer threats. Moreover, many such systems also suffer from high cost of storage and require extensive computational resources. In this paper, we propose ADA: Adaptive Deep Log Anomaly Detector, an unsupervised online deep neural network framework that leverages LSTM networks and regularly adapts to newer log patterns to ensure accurate anomaly detection. In ADA, an adaptive model selection strategy is designed to choose pareto-optimal configurations and thereby utilize resources efficiently. Further, a dynamic threshold algorithm is proposed to dictate the optimal threshold based on recently detected events to improve the detection accuracy. We also use the predictions to guide storage of abnormal data and effectively reduce the overall storage cost. We compare ADA with state-of-the-art approaches through leveraging the Los Alamos National Laboratory cyber security dataset and show that ADA accurately detects anomalies with high F1-score ~95% and it is 97 times faster than existing approaches and incurs very low storage cost.},
booktitle = {IEEE INFOCOM 2020 - IEEE Conference on Computer Communications},
pages = {2449–2458},
numpages = {10},
location = {Toronto, ON, Canada}
}

@inproceedings{10.1109/CEC.2016.7743899,
author = {Zhang, Bin and Shafi, Kamran and Abbass, Hussein A.},
title = {Hybrid Knowledge-Based Evolutionary Many-Objective Optimization},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CEC.2016.7743899},
doi = {10.1109/CEC.2016.7743899},
abstract = {Knowledge-based optimization is a recent direction in evolutionary optimization research which aims at understanding the optimization process, discovering relationships between decision variables and performance parameters, and using discovered knowledge to improve the optimization process, using machine learning techniques. A novel evolutionary optimization framework that incorporates a knowledge-based representation to search for Pareto optimal patterns in decision space was proposed earlier. This paper extends this framework to problems with four and more objectives, commonly referred to as many-objective optimization problems, using a hybridization approach with NSGA3. Experimental results on standard test functions are presented to demonstrate the advantages of the proposed hybrid algorithm in both objective and decision spaces.},
booktitle = {2016 IEEE Congress on Evolutionary Computation (CEC)},
pages = {1007–1014},
numpages = {8},
location = {Vancouver, BC, Canada}
}

@article{10.1145/3305218.3305227,
author = {Sun, Xiao and Le, Tan N. and Chowdhury, Mosharaf and Liu, Zhenhua},
title = {Fair Allocation of Heterogeneous and InterchangeableResources},
year = {2019},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {2},
issn = {0163-5999},
url = {https://doi.org/10.1145/3305218.3305227},
doi = {10.1145/3305218.3305227},
abstract = {Motivated by the proliferation of heterogeneous processors such as multi-core CPUs, GPUs, TPUs, and other accelerators for machine learning, we formulate a novel multiinterchangeable resource allocation (MIRA) problem where some resources are interchangeable. The challenge is how to allocate interchangeable resources to users in a sharing system while maintaining desirable properties such as sharing incentive, Pareto efficiency, and envy-freeness. In this paper, we first show that existing algorithms, including the Dominant Resource Fairness used in production systems, fail to provide these properties for interchangeable resources. Then we characterize the tradeoff between performance and strategyproofness, and design the Budget-based (BUD) algorithm, which preserves Pareto efficiency, sharing incentive and envyfreeness while providing better performance over currently used algorithms.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jan},
pages = {21–23},
numpages = {3}
}

@article{10.1109/TIT.2011.2182178,
author = {Agarwal, Alekh and Bartlett, Peter L. and Ravikumar, Pradeep and Wainwright, Martin J.},
title = {Information-Theoretic Lower Bounds on the Oracle Complexity of Stochastic Convex Optimization},
year = {2012},
issue_date = {May 2012},
publisher = {IEEE Press},
volume = {58},
number = {5},
issn = {0018-9448},
url = {https://doi.org/10.1109/TIT.2011.2182178},
doi = {10.1109/TIT.2011.2182178},
abstract = {Relative to the large literature on upper bounds on complexity of convex optimization, lesser attention has been paid to the fundamental hardness of these problems. Given the extensive use of convex optimization in machine learning and statistics, gaining an understanding of these complexity-theoretic issues is important. In this paper, we study the complexity of stochastic convex optimization in an oracle model of computation. We introduce a new notion of discrepancy between functions, and use it to reduce problems of stochastic convex optimization to statistical parameter estimation, which can be lower bounded using information-theoretic methods. Using this approach, we improve upon known results and obtain tight minimax complexity estimates for various function classes.},
journal = {IEEE Trans. Inf. Theor.},
month = {may},
pages = {3235–3249},
numpages = {15}
}

@article{10.1145/3399734,
author = {Taylor, Michael Bedford and Vega, Luis and Khazraee, Moein and Magaki, Ikuo and Davidson, Scott and Richmond, Dustin},
title = {ASIC Clouds: Specializing the Datacenter for Planet-Scale Applications},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/3399734},
doi = {10.1145/3399734},
abstract = {Planet-scale applications are driving the exponential growth of the Cloud, and datacenter specialization is the key enabler of this trend. GPU- and FPGA-based clouds have already been deployed to accelerate compute-intensive workloads. ASIC-based clouds are a natural evolution as cloud services expand across the planet. ASIC Clouds are purpose-built datacenters comprised of large arrays of ASIC accelerators that optimize the total cost of ownership (TCO) of large, high-volume scale-out computations. On the surface, ASIC Clouds may seem improbable due to high NREs and ASIC inflexibility, but large-scale ASIC Clouds have already been deployed for the Bitcoin cryptocurrency system. This paper distills lessons from these Bitcoin ASIC Clouds and applies them to other large scale workloads such as YouTube-style video-transcoding and Deep Learning, showing superior TCO versus CPU and GPU. It derives Pareto-optimal ASIC Cloud servers based on accelerator properties, by jointly optimizing ASIC architecture, DRAM, motherboard, power delivery, cooling, and operating voltage. Finally, the authors examine the impact of ASIC NRE and when it makes sense to build an ASIC Cloud.},
journal = {Commun. ACM},
month = {jun},
pages = {103–109},
numpages = {7}
}

@inproceedings{10.1145/3109761.3158394,
author = {Gupta, Aparana and Garg, Anshul and Rawat, Namrata and Chigurupati, Sandeep and Kumar, U Dinesh},
title = {Every Drop Counts: Unleashing the Prospective Locations for Water Harvesting Using Geospatial Analytics},
year = {2017},
isbn = {9781450352437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109761.3158394},
doi = {10.1145/3109761.3158394},
abstract = {Water is at the heart of 'Sustainable Development Goals (SDGs)' set by United Nations - with an objective to balance the three dimensions of sustainable development: Environment, Social and Economic - and is indirectly associated with the success of all the other Goals. But, with changing climatic patterns, untimely rains, prolonged dry spells, depleting ground water and drought making every drop of water extremely precious, the need of the hour is to gauge and work towards the major aspects of water harvesting - 'Catchment'. Water Harvesting must be a key element of any strategy to bring an end to India's perennial swings between drought and flood and to meet the following SDGs for sustained development. This study presents a structured and meticulous approach, wielding 'Geospatial Analytics' to identify the prospective locations for Water Harvesting in arid and semi-arid parts of the country for sustainable development. This paper is structured as follows. Section 1 describes the background and motivation for this idea. Section 2 details out the objective. In section 3 we present the 'Literature Survey' on the work that has already been carried out in this field. While section 4 discerns our area of study, Section 5 provides process flow starting from Data gathering, Data extraction, Data pre-processing, Model selection and Multi Criteria Decision Making (Model Application). In Section 6, we present and validate our experimental results achieved using the proposed methodology. Section 7 concludes our study followed by Section 8 on Recommendations for future enhancements and next steps.},
booktitle = {Proceedings of the 1st International Conference on Internet of Things and Machine Learning},
articleno = {46},
numpages = {13},
keywords = {image processing, flood fill model, water tanks, sliding window algorithm, geospatial analytics, AHP, LAND-SAT-8, RWH optimum location selection, rain water harvesting, analytical hierarchy process, smart water, digital elevation model, GIS},
location = {Liverpool, United Kingdom},
series = {IML '17}
}

@inproceedings{10.5555/3437539.3437657,
author = {Prabakaran, Bharath Srinivas and Mrazek, Vojtech and Vasicek, Zdenek and Sekanina, Lukas and Shafique, Muhammad},
title = {ApproxFPGAs: Embracing ASIC-Based Approximate Arithmetic Components for FPGA-Based Systems},
year = {2020},
isbn = {9781450367257},
publisher = {IEEE Press},
abstract = {There has been abundant research on the development of Approximate Circuits (ACs) for ASICs. However, previous studies have illustrated that ASIC-based ACs offer asymmetrical gains in FPGA-based accelerators. Therefore, an AC that might be pareto-optimal for ASICs might not be pareto-optimal for FPGAs. In this work, we present the ApproxFPGAs methodology that uses machine learning models to reduce the exploration time for analyzing the state-of-the-art ASIC-based ACs to determine the set of pareto-optimal FPGA-based ACs. We also perform a case-study to illustrate the benefits obtained by deploying these pareto-optimal FPGA-based ACs in a state-of-the-art automation framework to systematically generate pareto-optimal approximate accelerators that can be deployed in FPGA-based systems to achieve high performance or low-power consumption.},
booktitle = {Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference},
articleno = {118},
numpages = {6},
keywords = {arithmetic units, adder, approximate computing, machine learning, models, multiplier, synthesis, statistics, ASIC, FPGA},
location = {Virtual Event, USA},
series = {DAC '20}
}

@article{10.1145/3357334,
author = {Saini, Naveen and Saha, Sriparna and Bhattacharyya, Pushpak and Tuteja, Himanshu},
title = {Textual Entailment--Based Figure Summarization for Biomedical Articles},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3357334},
doi = {10.1145/3357334},
abstract = {This article proposes a novel unsupervised approach (FigSum++) for automatic figure summarization in biomedical scientific articles using a multi-objective evolutionary algorithm. The problem is treated as an optimization problem where relevant sentences in the summary for a given figure are selected based on various sentence scoring features (or objective functions), such as the textual entailment score between sentences in the summary and a figure’s caption, the number of sentences referring to that figure, semantic similarity between sentences and a figure’s caption, and the number of overlapping words between sentences and a figure’s caption. These objective functions are optimized simultaneously using multi-objective binary differential evolution (MBDE). MBDE consists of a set of solutions, and each solution represents a subset of sentences to be selected in the summary. MBDE generally uses a single differential evolution variant, but in the current study, an ensemble of two different differential evolution variants measuring diversity among solutions and convergence toward global optimal solution, respectively, is employed for efficient search. Usually, in any summarization system, diversity among sentences (called anti-redundancy) in the summary is a very critical feature, and it is calculated in terms of similarity (like cosine similarity) among sentences. In this article, a new way of measuring diversity in terms of textual entailment is proposed. To represent the sentences of the article in the form of numeric vectors, the recently proposed BioBERT pre-trained language model in biomedical text mining is utilized. An ablation study has also been presented to determine the importance of different objective functions. For evaluation of the proposed technique, two benchmark biomedical datasets containing 91 and 84 figures are considered. Our proposed system obtains 5% and 11% improvements in terms of the F-measure metric over two datasets, compared to the state-of-the-art unsupervised methods.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {apr},
articleno = {35},
numpages = {24},
keywords = {evolutionary computing, textual entailment, multi-objective optimization (MOO), Figure-assisted text summarization}
}

@article{10.1109/TEVC.2010.2073471,
author = {Cartlidge, J. and Ait-Boudaoud, D.},
title = {Autonomous Virulence Adaptation Improves Coevolutionary Optimization},
year = {2011},
issue_date = {April 2011},
publisher = {IEEE Press},
volume = {15},
number = {2},
issn = {1089-778X},
url = {https://doi.org/10.1109/TEVC.2010.2073471},
doi = {10.1109/TEVC.2010.2073471},
abstract = {A novel approach for the autonomous virulence adaptation (AVA) of competing populations in a coevolutionary optimization framework is presented. Previous work has demonstrated that setting an appropriate virulence, v, of populations accelerates coevolutionary optimization by avoiding detrimental periods of disengagement. However, since the likelihood of disengagement varies both between systems and over time, choosing the ideal value of v is problematic. The AVA technique presented here uses a machine learning approach to continuously tune v as system engagement varies. In a simple, abstract domain, AVA is shown to successfully adapt to the most productive values of v. Further experiments, in more complex domains of sorting networks and maze navigation, demonstrate AVA's efficiency over reduced virulence and the layered Pareto coevolutionary archive.},
journal = {Trans. Evol. Comp},
month = {apr},
pages = {215–229},
numpages = {15},
keywords = {machine learning, optimization methods, coevolution, sorting networks, genetic algorithms, Autonomous virulence adaptation, reduced virulence, maze navigation, disengagement}
}

@article{10.1155/2022/7275433,
author = {Kaka, Jhansi Rani and Satya Prasad, K. and G, Thippa Reddy},
title = {Differential Evolution and Multiclass Support Vector Machine for Alzheimer’s Classification},
year = {2022},
issue_date = {2022},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2022},
issn = {1939-0114},
url = {https://doi.org/10.1155/2022/7275433},
doi = {10.1155/2022/7275433},
abstract = {Early diagnosis of Alzheimer’s helps a doctor to decide the treatment for the patient based on the stages. The existing methods involve applying the deep learning methods for Alzheimer’s classification and have the limitations of overfitting problems. Some researchers were involved in applying the feature selection based on the optimization method, having limitations of easily trapping into local optima and poor convergence. In this research, Differential Evolution-Multiclass Support Vector Machine (DE-MSVM) is proposed to increase the performance of Alzheimer’s classification. The image normalization method is applied to enhance the quality of the image and represent the features effectively. The AlexNet model is applied to the normalized images to extract the features and also applied for feature selection. The Differential Evolution method applies Pareto Optimal Front for nondominated feature selection. This helps to select the feature that represents the characteristics of the input images. The selected features are applied in the MSVM method to represent in high dimension and classify Alzheimer’s. The DE-MSVM method has accuracy of 98.13% in the axial slice, and the existing whale optimization with MSVM has 95.23% accuracy.},
journal = {Sec. and Commun. Netw.},
month = {jan},
numpages = {13}
}

@article{10.1145/3308897.3308965,
author = {Garcia, Johan and Korhonen, Topi},
title = {On Runtime and Classification Performance of the Discretize-Optimize (DISCO) Classification Approach},
year = {2019},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {3},
issn = {0163-5999},
url = {https://doi.org/10.1145/3308897.3308965},
doi = {10.1145/3308897.3308965},
abstract = {Using machine learning in high-speed networks for tasks such as flow classification typically requires either very resource efficient classification approaches, large amounts of computational resources, or specialized hardware. Here we provide a sketch of the discretize-optimize (DISCO) approach which can construct an extremely efficient classifier for low dimensional problems by combining feature selection, efficient discretization, novel bin placement, and lookup. As feature selection and discretization parameters are crucial, appropriate combinatorial optimization is an important aspect of the approach. A performance evaluation is performed for a YouTube classification task using a cellular traffic data set. The initial evaluation results show that the DISCO approach can move the Pareto boundary in the classification performance versus runtime trade-off by up to an order of magnitude compared to runtime optimized random forest and decision tree classifiers.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jan},
pages = {167–170},
numpages = {4},
keywords = {runtime, machine learning, classification}
}

@inbook{10.1145/3459637.3481944,
author = {Mills, Keith G. and Han, Fred X. and Zhang, Jialin and Changiz Rezaei, Seyed Saeed and Chudak, Fabian and Lu, Wei and Lian, Shuo and Jui, Shangling and Niu, Di},
title = {Profiling Neural Blocks and Design Spaces for Mobile Neural Architecture Search},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3481944},
abstract = {Neural architecture search automates neural network design and has achieved state-of-the-art results in many deep learning applications. While recent literature has focused on designing networks to maximize accuracy, little work has been conducted to understand the compatibility of architecture design spaces to varying hardware. In this paper, we analyze the neural blocks used to build Once-for-All (MobileNetV3), ProxylessNAS and ResNet families, in order to understand their predictive power and inference latency on various devices, including Huawei Kirin 9000 NPU, RTX 2080 Ti, AMD Threadripper 2990WX, and Samsung Note10. We introduce a methodology to quantify the friendliness of neural blocks to hardware and the impact of their placement in a macro network on overall network performance via only end-to-end measurements. Based on extensive profiling results, we derive design insights and apply them to hardware-specific search space reduction. We show that searching in the reduced search space generates better accuracy-latency Pareto frontiers than searching in the original search spaces, customizing architecture search according to the hardware. Moreover, insights derived from measurements lead to notably higher ImageNet top-1 scores on all search spaces investigated.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {4026–4035},
numpages = {10}
}

@inbook{10.1145/3377929.3398088,
author = {Hamilton, Nolan H. and Fulp, Errin W.},
title = {An Evolutionary Approach for Constructing Multi-Stage Classifiers},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3398088},
abstract = {Multi-stage classification is a supervised learning approach that distributes a set of features, each with an associated importance and cost of generation, across a series of classifiers (stages). Inputs are processed in a pipeline fashion through the stages, each of which utilizes only a subset of the complete feature set, until a confident classification decision can be made or until all features and stages have been exhausted. This design benefits from processing inputs in parallel and ensures that labels are assigned using only the necessary features, but the number and composition of stages used by the model can have significant impact on overall performance. Unfortunately, identifying these critical design aspects becomes more difficult as the number of features and possible stages increases, often making brute-force search or human intuition impractical.This paper introduces a novel evolutionary approach for discovering multi-stage configurations that provide high classification performance and fast processing times. Using this approach, multistage classifier configurations are modeled as chromosomes, and a series of selection, recombination, and mutation operations are iteratively performed to find better configurations. Since the problem has multiple objectives, a Pareto-based fitness measure is developed to rank chromosomes, where better chromosomes have high accuracy, high conclusiveness, and fast processing time. Experimental results indicate this approach is able to consistently find accurate and fast multi-stage classifier configurations under various conditions, including an increasing number of features and different feature synthesis time distributions.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {1730–1738},
numpages = {9}
}

@article{10.1016/j.sysarc.2016.07.004,
author = {Malazgirt, Gorker Alp and Yurdakul, Arda},
title = {Prenaut},
year = {2017},
issue_date = {January 2017},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {72},
number = {C},
issn = {1383-7621},
url = {https://doi.org/10.1016/j.sysarc.2016.07.004},
doi = {10.1016/j.sysarc.2016.07.004},
abstract = {Prenaut is a design space exploration method for finding the best on-chip architectures given processor cores, Level 1, Level 2, and Level 3 caches.Current design space exploration methods mostly explore cache, processor configurations with a fixed architecture. Our method explores architectures with fixed cache and processor configurations.Prenaut is to build a data oriented design space exploration method that exploits simulation data to its full extent rather than discarding it.Prenaut uses the available simulation data and applies machine learning methods for estimating design parameters. As embedded systems have evolved to appear in many different domains, symmetric multiprocessing (SMP) has been the design choice from low-end to high-end devices. In this paper we present Prenaut, a design space exploration method for finding the best on-chip SMP architectures given processor cores, Level 1, Level 2, and Level 3 caches. Unlike traditional design space exploration tools that are majorly concerned with optimizations in processor, memory and cache structures with a fixed on-chip architecture, Prenaut explores architectures that have not been considered in symmetric multiprocessing domain. These architectures consist of shared instruction caches between cores and heterogeneous cache topologies that feature bypassing a level in the cache hierarchy. The design idea behind Prenaut is to build a data oriented design space exploration method that exploits simulation data to its full extent rather than discarding it. Therefore, Prenaut uses simulation data and applies machine learning methods for estimating design parameters. This provides very rapid estimation of the Pareto set and guides designers through the overall system design process. The design space is pruned by topological clustering of design points which groups similar topologies and new simulation points are selected via an ordered look up table that prevents infeasible random jumps in the design space. For the selected benchmarks, Prenaut can estimate the Pareto set up to 147x faster and the clustering information can reduce the design space up to 82% in comparison with a state-of-the-art evolutionary algorithm.},
journal = {J. Syst. Archit.},
month = {jan},
pages = {3–18},
numpages = {16},
keywords = {Clustering, Design space exploration, Machine learning, Symmetric multiprocessing}
}

@inproceedings{10.1145/3308558.3313564,
author = {Jia, Yuting and Zhang, Qinqin and Zhang, Weinan and Wang, Xinbing},
title = {CommunityGAN: Community Detection with Generative Adversarial Nets},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313564},
doi = {10.1145/3308558.3313564},
abstract = {Community detection refers to the task of discovering groups of vertices sharing similar properties or functions so as to understand the network data. With the recent development of deep learning, graph representation learning techniques are also utilized for community detection. However, the communities can only be inferred by applying clustering algorithms based on learned vertex embeddings. These general cluster algorithms like K-means and Gaussian Mixture Model cannot output much overlapped communities, which have been proved to be very common in many real-world networks. In this paper, we propose CommunityGAN, a novel community detection framework that jointly solves overlapping community detection and graph representation learning. First, unlike the embedding of conventional graph representation learning algorithms where the vector entry values have no specific meanings, the embedding of CommunityGAN indicates the membership strength of vertices to communities. Second, a specifically designed Generative Adversarial Net (GAN) is adopted to optimize such embedding. Through the minimax competition between the motif-level generator and discriminator, both of them can alternatively and iteratively boost their performance and finally output a better community structure. Extensive experiments on synthetic data and real-world tasks demonstrate that CommunityGAN achieves substantial community detection performance gains over the state-of-the-art methods.},
booktitle = {The World Wide Web Conference},
pages = {784–794},
numpages = {11},
keywords = {Generative Adversarial Nets, Community Detection, Graph Representation Learning},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/2025113.2025133,
author = {Sidiroglou-Douskos, Stelios and Misailovic, Sasa and Hoffmann, Henry and Rinard, Martin},
title = {Managing Performance vs. Accuracy Trade-Offs with Loop Perforation},
year = {2011},
isbn = {9781450304436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2025113.2025133},
doi = {10.1145/2025113.2025133},
abstract = {Many modern computations (such as video and audio encoders, Monte Carlo simulations, and machine learning algorithms) are designed to trade off accuracy in return for increased performance. To date, such computations typically use ad-hoc, domain-specific techniques developed specifically for the computation at hand. Loop perforation provides a general technique to trade accuracy for performance by transforming loops to execute a subset of their iterations. A criticality testing phase filters out critical loops (whose perforation produces unacceptable behavior) to identify tunable loops (whose perforation produces more efficient and still acceptably accurate computations). A perforation space exploration algorithm perforates combinations of tunable loops to find Pareto-optimal perforation policies. Our results indicate that, for a range of applications, this approach typically delivers performance increases of over a factor of two (and up to a factor of seven) while changing the result that the application produces by less than 10%.},
booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
pages = {124–134},
numpages = {11},
keywords = {profiling, loop perforation, quality of service},
location = {Szeged, Hungary},
series = {ESEC/FSE '11}
}

@article{10.1145/3464308,
author = {Yin, Jianfei and Wang, Ruili and Guo, Yeqing and Bai, Yizhe and Ju, Shunda and Liu, Weili and Huang, Joshua Zhexue},
title = {Wealth Flow Model: Online Portfolio Selection Based on Learning Wealth Flow Matrices},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3464308},
doi = {10.1145/3464308},
abstract = {This article proposes a deep learning solution to the online portfolio selection problem based on learning a latent structure directly from a price time series. It introduces a novel wealth flow matrix for representing a latent structure that has special regular conditions to encode the knowledge about the relative strengths of assets in portfolios. Therefore, a wealth flow model (WFM) is proposed to learn wealth flow matrices and maximize portfolio wealth simultaneously. Compared with existing approaches, our work has several distinctive benefits: (1) the learning of wealth flow matrices makes our model more generalizable than models that only predict wealth proportion vectors, and (2) the exploitation of wealth flow matrices and the exploration of wealth growth are integrated into our deep reinforcement algorithm for the WFM. These benefits, in combination, lead to a highly-effective approach for generating reasonable investment behavior, including short-term trend following, the following of a few losers, no self-investment, and sparse portfolios. Extensive experiments on five benchmark datasets from real-world stock markets confirm the theoretical advantage of the WFM, which achieves the Pareto improvements in terms of multiple performance indicators and the steady growth of wealth over the state-of-the-art algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {sep},
articleno = {30},
numpages = {27},
keywords = {wealth flow matrix, Online portfolio selection, regret bound, deep reinforcement learning}
}

@inproceedings{10.1145/3488560.3498401,
author = {Wang, Xuesi and Huzhang, Guangda and Lin, Qianying and Da, Qing},
title = {Learning-To-Ensemble by Contextual Rank Aggregation in E-Commerce},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498401},
doi = {10.1145/3488560.3498401},
abstract = {Ensemble models in E-commerce combine predictions from multiple sub-models for ranking and revenue improvement. Industrial ensemble models are typically deep neural networks, following the supervised learning paradigm to infer conversion rate given inputs from sub-models. However, this process has the following two problems. Firstly, the point-wise scoring approach disregards the relationships between items and leads to homogeneous displayed results, while diversified display benefits user experience and revenue. Secondly, the learning paradigm focuses on the ranking metrics and does not directly optimize the revenue. In our work, we propose a new Learning-To-Ensemble (LTE) framework RA-EGO, which replaces the ensemble model with a contextual Rank Aggregator (RA) and explores the best weights of sub-models by the Evaluator-Generator Optimization (EGO). To achieve the best online performance, we propose a new rank aggregation algorithm TournamentGreedy as a refinement of classic rank aggregators, which also produces the best average weighted Kendall Tau Distance (KTD) amongst all the considered algorithms with quadratic time complexity. Under the assumption that the best output list should be Pareto Optimal on the KTD metric for sub-models, we show that our RA algorithm has higher efficiency and coverage in exploring the optimal weights. Combined with the idea of Bayesian Optimization and gradient descent, we solve the online contextual Black-Box Optimization task that finds the optimal weights for sub-models given a chosen RA model. RA-EGO has been deployed in our online system and has improved the revenue significantly.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {1036–1044},
numpages = {9},
keywords = {rank aggregation, contextual black-box optimization},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@article{10.1145/3487910,
author = {Burrello, Alessio and Pagliari, Daniele Jahier and Rapa, Pierangelo Maria and Semilia, Matilde and Risso, Matteo and Polonelli, Tommaso and Poncino, Massimo and Benini, Luca and Benatti, Simone},
title = {Embedding Temporal Convolutional Networks for Energy-Efficient PPG-Based Heart Rate Monitoring},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2691-1957},
url = {https://doi.org/10.1145/3487910},
doi = {10.1145/3487910},
abstract = {Photoplethysmography (PPG) sensors allow for non-invasive and comfortable heart rate (HR) monitoring, suitable for compact wrist-worn devices. Unfortunately, motion artifacts (MAs) severely impact the monitoring accuracy, causing high variability in the skin-to-sensor interface. Several data fusion techniques have been introduced to cope with this problem, based on combining PPG signals with inertial sensor data. Until now, both commercial and reasearch solutions are computationally efficient but not very robust, or strongly dependent on hand-tuned parameters, which leads to poor generalization performance. In this work, we tackle these limitations by proposing a computationally lightweight yet robust deep learning-based approach for PPG-based HR estimation. Specifically, we derive a diverse set of Temporal Convolutional Networks for HR estimation, leveraging Neural Architecture Search. Moreover, we also introduce ActPPG, an adaptive algorithm that selects among multiple HR estimators depending on the amount of MAs, to improve energy efficiency. We validate our approaches on two benchmark datasets, achieving as low as 3.84 beats per minute of Mean Absolute Error on PPG-Dalia, which outperforms the previous state of the art. Moreover, we deploy our models on a low-power commercial microcontroller (STM32L4), obtaining a rich set of Pareto optimal solutions in the complexity vs. accuracy space.},
journal = {ACM Trans. Comput. Healthcare},
month = {mar},
articleno = {19},
numpages = {25},
keywords = {wearable devices, medical IoT, heart rate monitoring, deep learning, Temporal convolutional networks}
}

@article{10.1109/TASL.2006.879805,
author = {Jiang, Hui and Li, Xinwei and Liu, Chaojun},
title = {Large Margin Hidden Markov Models for Speech Recognition},
year = {2006},
issue_date = {September 2006},
publisher = {IEEE Press},
volume = {14},
number = {5},
issn = {1558-7916},
url = {https://doi.org/10.1109/TASL.2006.879805},
doi = {10.1109/TASL.2006.879805},
abstract = {In this paper, motivated by large margin classifiers in machine learning, we propose a novel method to estimate continuous-density hidden Markov model (CDHMM) for speech recognition according to the principle of maximizing the minimum multiclass separation margin. The approach is named large margin HMM. First, we show this type of large margin HMM estimation problem can be formulated as a constrained minimax optimization problem. Second, we propose to solve this constrained minimax optimization problem by using a penalized gradient descent algorithm, where the original objective function, i.e., minimum margin, is approximated by a differentiable function and the constraints are cast as penalty terms in the objective function. The new training method is evaluated in the speaker-independent isolated E-set recognition and the TIDIGITS connected digit string recognition tasks. Experimental results clearly show that the large margin HMMs consistently outperform the conventional HMM training methods. It has been consistently observed that the large margin training method yields significant recognition error rate reduction even on top of some popular discriminative training methods},
journal = {Trans. Audio, Speech and Lang. Proc.},
month = {sep},
pages = {1584–1595},
numpages = {12},
keywords = {Continuous-density hidden Markov models (CDHMMs), gradient descent search, support vector machine, large margin classifiers, minimax optimization}
}

@inbook{10.1145/3450267.3450538,
author = {Yuan, Yukun and Ma, Meiyi and Han, Songyang and Zhang, Desheng and Miao, Fei and Stankovic, John and Lin, Shan},
title = {DeResolver: A Decentralized Negotiation and Conflict Resolution Framework for Smart City Services},
year = {2021},
isbn = {9781450383530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450267.3450538},
abstract = {As various smart services are increasingly deployed in modern cities, many unexpected conflicts arise due to various physical world couplings. Existing solutions for conflict resolution often rely on centralized control to enforce predetermined and fixed priorities of different services, which is challenging due to the inconsistent and private objectives of the services. Also, the centralized solutions miss opportunities to more effectively resolve conflicts according to their spatiotemporal locality of the conflicts. To address this issue, we design a decentralized negotiation and conflict resolution framework named DeResolver, which allows services to resolve conflicts by communicating and negotiating with each other to reach a Pareto-optimal agreement autonomously and efficiently. Our design features a two-level semi-supervised learning-based algorithm to predict acceptable proposals and their rankings of each opponent through the negotiation. Our design is evaluated with a smart city case study of three services: intelligent traffic light control, pedestrian service, and environmental control. In this case study, a data-driven evaluation is conducted using a large data set consisting of the GPS locations of 246 surveillance cameras and an automatic traffic monitoring system with more than 3 million records per day to extract real-world vehicle routes. The evaluation results show that our solution achieves much more balanced results, i.e., only increasing the average waiting time of vehicles, the measurement metric of intelligent traffic light control service, by 6.8% while reducing the weighted sum of air pollutant emission, measured for environment control service, by 12.1%, and the pedestrian waiting time, the measurement metric of pedestrian service, by 33.1%, compared to priority-based solution.},
booktitle = {Proceedings of the ACM/IEEE 12th International Conference on Cyber-Physical Systems},
pages = {98–109},
numpages = {12}
}

@inproceedings{10.1145/2694344.2694373,
author = {Mishra, Nikita and Zhang, Huazhe and Lafferty, John D. and Hoffmann, Henry},
title = {A Probabilistic Graphical Model-Based Approach for Minimizing Energy Under Performance Constraints},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694373},
doi = {10.1145/2694344.2694373},
abstract = {In many deployments, computer systems are underutilized -- meaning that applications have performance requirements that demand less than full system capacity. Ideally, we would take advantage of this under-utilization by allocating system resources so that the performance requirements are met and energy is minimized. This optimization problem is complicated by the fact that the performance and power consumption of various system configurations are often application -- or even input -- dependent. Thus, practically, minimizing energy for a performance constraint requires fast, accurate estimations of application-dependent performance and power tradeoffs. This paper investigates machine learning techniques that enable energy savings by learning Pareto-optimal power and performance tradeoffs. Specifically, we propose LEO, a probabilistic graphical model-based learning system that provides accurate online estimates of an application's power and performance as a function of system configuration. We compare LEO to (1) offline learning, (2) online learning, (3) a heuristic approach, and (4) the true optimal solution. We find that LEO produces the most accurate estimates and near optimal energy savings.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {267–281},
numpages = {15},
keywords = {probabilistic graphical models},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694373,
author = {Mishra, Nikita and Zhang, Huazhe and Lafferty, John D. and Hoffmann, Henry},
title = {A Probabilistic Graphical Model-Based Approach for Minimizing Energy Under Performance Constraints},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694373},
doi = {10.1145/2775054.2694373},
abstract = {In many deployments, computer systems are underutilized -- meaning that applications have performance requirements that demand less than full system capacity. Ideally, we would take advantage of this under-utilization by allocating system resources so that the performance requirements are met and energy is minimized. This optimization problem is complicated by the fact that the performance and power consumption of various system configurations are often application -- or even input -- dependent. Thus, practically, minimizing energy for a performance constraint requires fast, accurate estimations of application-dependent performance and power tradeoffs. This paper investigates machine learning techniques that enable energy savings by learning Pareto-optimal power and performance tradeoffs. Specifically, we propose LEO, a probabilistic graphical model-based learning system that provides accurate online estimates of an application's power and performance as a function of system configuration. We compare LEO to (1) offline learning, (2) online learning, (3) a heuristic approach, and (4) the true optimal solution. We find that LEO produces the most accurate estimates and near optimal energy savings.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {267–281},
numpages = {15},
keywords = {probabilistic graphical models}
}

@article{10.1145/2786763.2694373,
author = {Mishra, Nikita and Zhang, Huazhe and Lafferty, John D. and Hoffmann, Henry},
title = {A Probabilistic Graphical Model-Based Approach for Minimizing Energy Under Performance Constraints},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694373},
doi = {10.1145/2786763.2694373},
abstract = {In many deployments, computer systems are underutilized -- meaning that applications have performance requirements that demand less than full system capacity. Ideally, we would take advantage of this under-utilization by allocating system resources so that the performance requirements are met and energy is minimized. This optimization problem is complicated by the fact that the performance and power consumption of various system configurations are often application -- or even input -- dependent. Thus, practically, minimizing energy for a performance constraint requires fast, accurate estimations of application-dependent performance and power tradeoffs. This paper investigates machine learning techniques that enable energy savings by learning Pareto-optimal power and performance tradeoffs. Specifically, we propose LEO, a probabilistic graphical model-based learning system that provides accurate online estimates of an application's power and performance as a function of system configuration. We compare LEO to (1) offline learning, (2) online learning, (3) a heuristic approach, and (4) the true optimal solution. We find that LEO produces the most accurate estimates and near optimal energy savings.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {267–281},
numpages = {15},
keywords = {probabilistic graphical models}
}

@article{10.1109/TEVC.2019.2913831,
author = {Nguyen, Bach Hoai and Xue, Bing and Andreae, Peter and Ishibuchi, Hisao and Zhang, Mengjie},
title = {Multiple Reference Points-Based Decomposition for Multiobjective Feature Selection in Classification: Static and Dynamic Mechanisms},
year = {2020},
issue_date = {Feb. 2020},
publisher = {IEEE Press},
volume = {24},
number = {1},
issn = {1089-778X},
url = {https://doi.org/10.1109/TEVC.2019.2913831},
doi = {10.1109/TEVC.2019.2913831},
abstract = {Feature selection is an important task in machine learning that has two main objectives: 1) reducing dimensionality and 2) improving learning performance. Feature selection can be considered a multiobjective problem. However, it has its problematic characteristics, such as a highly discontinuous Pareto front, imbalance preferences, and partially conflicting objectives. These characteristics are not easy for existing evolutionary multiobjective optimization (EMO) algorithms. We propose a new decomposition approach with two mechanisms (static and dynamic) based on multiple reference points under the multiobjective evolutionary algorithm based on decomposition (MOEA/D) framework to address the above-mentioned difficulties of feature selection. The static mechanism alleviates the dependence of the decomposition on the Pareto front shape and the effect of the discontinuity. The dynamic one is able to detect regions in which the objectives are mostly conflicting, and allocates more computational resources to the detected regions. In comparison with other EMO algorithms on 12 different classification datasets, the proposed decomposition approach finds more diverse feature subsets with better performance in terms of hypervolume and inverted generational distance. The dynamic mechanism successfully identifies conflicting regions and further improves the approximation quality for the Pareto fronts.},
journal = {Trans. Evol. Comp},
month = {feb},
pages = {170–184},
numpages = {15}
}

@article{10.1145/3264945,
author = {Song, Qun and Gu, Chaojie and Tan, Rui},
title = {Deep Room Recognition Using Inaudible Echos},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3264945},
doi = {10.1145/3264945},
abstract = {Recent years have seen the increasing need of location awareness by mobile applications. This paper presents a room-level indoor localization approach based on the measured room's echos in response to a two-millisecond single-tone inaudible chirp emitted by a smartphone's loudspeaker. Different from other acoustics-based room recognition systems that record full-spectrum audio for up to ten seconds, our approach records audio in a narrow inaudible band for 0.1 seconds only to preserve the user's privacy. However, the short-time and narrowband audio signal carries limited information about the room's characteristics, presenting challenges to accurate room recognition. This paper applies deep learning to effectively capture the subtle fingerprints in the rooms' acoustic responses. Our extensive experiments show that a two-layer convolutional neural network fed with the spectrogram of the inaudible echos achieve the best performance, compared with alternative designs using other raw data formats and deep models. Based on this result, we design a RoomRecognize cloud service and its mobile client library that enable the mobile application developers to readily implement the room recognition functionality without resorting to any existing infrastructures and add-on hardware. Extensive evaluation shows that RoomRecognize achieves 99.7%, 97.7%, 99%, and 89% accuracy in differentiating 22 and 50 residential/office rooms, 19 spots in a quiet museum, and 15 spots in a crowded museum, respectively. Compared with the state-of-the-art approaches based on support vector machine, RoomRecognize significantly improves the Pareto frontier of recognition accuracy versus robustness against interfering sounds (e.g., ambient music).},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {135},
numpages = {28},
keywords = {smartphone, Room recognition, inaudible sound}
}

@inproceedings{10.1145/3306618.3314277,
author = {Noriega-Campero, Alejandro and Bakker, Michiel A. and Garcia-Bulle, Bernardo and Pentland, Alex 'Sandy'},
title = {Active Fairness in Algorithmic Decision Making},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314277},
doi = {10.1145/3306618.3314277},
abstract = {Society increasingly relies on machine learning models for automated decision making. Yet, efficiency gains from automation have come paired with concern for algorithmic discrimination that can systematize inequality. Recent work has proposed optimal post-processing methods that randomize classification decisions for a fraction of individuals, in order to achieve fairness measures related to parity in errors and calibration. These methods, however, have raised concern due to the information inefficiency, intra-group unfairness, and Pareto sub-optimality they entail. The present work proposes an alternativeactive framework for fair classification, where, in deployment, a decision-maker adaptively acquires information according to the needs of different groups or individuals, towards balancing disparities in classification performance. We propose two such methods, where information collection is adapted to group- and individual-level needs respectively. We show on real-world datasets that these can achieve: 1) calibration and single error parity (e.g.,equal opportunity ); and 2) parity in both false positive and false negative rates (i.e.,equal odds ). Moreover, we show that by leveraging their additional degree of freedom,active approaches can substantially outperform randomization-based classifiers previously considered optimal, while avoiding limitations such as intra-group unfairness.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {77–83},
numpages = {7},
keywords = {active feature acquisition, algorithmic fairness, adaptive inquiry},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@article{10.1109/TEVC.2014.2305671,
author = {Pu Wang and Emmerich, Michael and Rui Li and Ke Tang and Back, Thomas and Xin Yao},
title = {Convex Hull-Based Multiobjective Genetic Programming for Maximizing Receiver Operating Characteristic Performance},
year = {2015},
issue_date = {April 2015},
publisher = {IEEE Press},
volume = {19},
number = {2},
issn = {1089-778X},
url = {https://doi.org/10.1109/TEVC.2014.2305671},
doi = {10.1109/TEVC.2014.2305671},
abstract = {The receiver operating characteristic (ROC) is commonly used to analyze the performance of classifiers in data mining. An important topic in ROC analysis is the ROC convex hull (ROCCH), which is the least convex majorant (LCM) of the empirical ROC curve and covers potential optima for a given set of classifiers. ROCCH maximization problems have been taken as multiobjective optimization problem (MOPs) in some previous work. However, the special characteristics of ROCCH maximization problem makes it different from traditional MOPs. In this paper, the difference will be discussed in detail and a new convex hull-based multiobjective genetic programming (CH-MOGP) is proposed to solve ROCCH maximization problems. Specifically, convex hull-based without redundancy sorting (CWR-sorting) is introduced, which is an indicator-based selection scheme that aims to maximize the area under the convex hull. A novel selection procedure is also proposed based on the proposed sorting scheme. It is hypothesized that by using a tailored indicator-based selection, CH-MOGP becomes more efficient for ROC convex hull approximation than algorithms that compute all Pareto optimal points. Empirical studies are conducted to compare CH-MOGP to both existing machine learning approaches and multiobjective genetic programming (MOGP) methods with classical selection schemes. Experimental results show that CH-MOGP outperforms the other approaches significantly.},
journal = {Trans. Evol. Comp},
month = {apr},
pages = {188–200},
numpages = {13},
keywords = {genetic programming, receiver operating characteristic (ROC) convex hull, evolutionary multiobjective algorithm, memetic algorithm, Classification}
}

@article{10.1109/TEVC.2012.2199119,
author = {Bhowan, Urvesh and Johnston, Mark and Zhang, Mengjie and Yao, Xin},
title = {Evolving Diverse Ensembles Using Genetic Programming for Classification With Unbalanced Data},
year = {2013},
issue_date = {June 2013},
publisher = {IEEE Press},
volume = {17},
number = {3},
issn = {1089-778X},
url = {https://doi.org/10.1109/TEVC.2012.2199119},
doi = {10.1109/TEVC.2012.2199119},
abstract = {In classification, machine learning algorithms can suffer a performance bias when data sets are unbalanced. Data sets are unbalanced when at least one class is represented by only a small number of training examples (called the minority class), while the other class(es) make up the majority. In this scenario, classifiers can have good accuracy on the majority class, but very poor accuracy on the minority class(es). This paper proposes a multiobjective genetic programming (MOGP) approach to evolving accurate and diverse ensembles of genetic program classifiers with good performance on both the minority and majority of classes. The evolved ensembles comprise of nondominated solutions in the population where individual members vote on class membership. This paper evaluates the effectiveness of two popular Pareto-based fitness strategies in the MOGP algorithm (SPEA2 and NSGAII), and investigates techniques to encourage diversity between solutions in the evolved ensembles. Experimental results on six (binary) class imbalance problems show that the evolved ensembles outperform their individual members, as well as single-predictor methods such as canonical GP, naive Bayes, and support vector machines, on highly unbalanced tasks. This highlights the importance of developing an effective fitness evaluation strategy in the underlying MOGP algorithm to evolve good ensemble members.},
journal = {Trans. Evol. Comp},
month = {jun},
pages = {368–386},
numpages = {19}
}

@article{10.1016/j.cie.2021.107445,
author = {chen, Cheng and Xu, Xianhao and Zou, Bipan and Peng, Hongxia and Li, Zhiwen},
title = {Optimal Decision of Multiobjective and Multiperiod Anticipatory Shipping under Uncertain Demand: A Data-Driven Framework},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {159},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2021.107445},
doi = {10.1016/j.cie.2021.107445},
journal = {Comput. Ind. Eng.},
month = {sep},
numpages = {15},
keywords = {Online retail, Multiobjective, Anticipatory shipping, Integer programming model, Data-driven}
}

@inproceedings{10.1145/3229762.3229765,
author = {Hadidi, Ramyad and Cao, Jiashen and Woodward, Matthew and Ryoo, Michael S. and Kim, Hyesoon},
title = {Real-Time Image Recognition Using Collaborative IoT Devices},
year = {2018},
isbn = {9781450359238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229762.3229765},
doi = {10.1145/3229762.3229765},
abstract = {Internet of things (IoT) devices capture and create various forms of sensor data such as images and videos. However, such resource-constrained devices lack the capability to efficiently process data in a timely and real-time manner. Therefore, IoT systems strongly rely on a powerful server (either local or on the cloud) to extract useful information from data. In addition, during communication with servers, unprocessed, sensitive, and private data is transmitted throughout the Internet, a serious vulnerability. What if we were able to harvest the aggregated computational power of already existing IoT devices in our system to locally process this data? In this artifact, we utilize Musical Chair, which enables efficient, localized, and dynamic real-time recognition by harvesting the aggregated computational power of these resource-constrained IoT devices. We apply Musical chair to two well-known image recognition models, AlexNet and VGG16, and implement them on a network of Raspberry PIs (up to 11). We compare inference per second and energy per inference of our systems with Tegra TX2, an embedded low-power platform with a six-core CPU and a GPU. We demonstrate that the collaboration of IoT devices, enabled by Musical Chair, achieves similar real-time performance without the extra costs of maintaining a server.},
booktitle = {Proceedings of the 1st on Reproducible Quality-Efficient Systems Tournament on Co-Designing Pareto-Efficient Deep Learning},
articleno = {4},
location = {Williamsburg, VA, USA},
series = {ReQuEST '18}
}

@article{10.1007/s10994-020-05895-3,
author = {Haghir Chehreghani, Morteza and Haghir Chehreghani, Mostafa},
title = {Learning Representations from Dendrograms},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {109},
number = {9–10},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-020-05895-3},
doi = {10.1007/s10994-020-05895-3},
abstract = {We propose unsupervised representation learning and feature extraction from dendrograms. The commonly used Minimax distance measures correspond to building a dendrogram with single linkage criterion, with defining specific forms of a level function and a distance function over that. Therefore, we extend this method to arbitrary dendrograms. We develop a generalized framework wherein different distance measures and representations can be inferred from different types of dendrograms, level functions and distance functions. Via an appropriate embedding, we compute a vector-based representation of the inferred distances, in order to enable many numerical machine learning algorithms to employ such distances. Then, to address the model selection problem, we study the aggregation of different dendrogram-based distances respectively in solution space and in representation space in the spirit of deep representations. In the first approach, for example for the clustering problem, we build a graph with positive and negative edge weights according to the consistency of the clustering labels of different objects among different solutions, in the context of ensemble methods. Then, we use an efficient variant of correlation clustering to produce the final clusters. In the second approach, we investigate the combination of different distances and features sequentially in the spirit of multi-layered architectures to obtain the final features. Finally, we demonstrate the effectiveness of our approach via several numerical studies.},
journal = {Mach. Learn.},
month = {sep},
pages = {1779–1802},
numpages = {24},
keywords = {Dendrogram, Ensemble method, Representation learning, Feature extraction, Unsupervised learning}
}

@inproceedings{10.1145/3447548.3467326,
author = {Wang, Yuyan and Wang, Xuezhi and Beutel, Alex and Prost, Flavien and Chen, Jilin and Chi, Ed H.},
title = {Understanding and Improving Fairness-Accuracy Trade-Offs in Multi-Task Learning},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467326},
doi = {10.1145/3447548.3467326},
abstract = {As multi-task models gain popularity in a wider range of machine learning applications, it is becoming increasingly important for practitioners to understand the fairness implications associated with those models. Most existing fairness literature focuses on learning a single task more fairly, while how ML fairness interacts with multiple tasks in the joint learning setting is largely under-explored. In this paper, we are concerned with how group fairness (e.g., equal opportunity, equalized odds) as an ML fairness concept plays out in the multi-task scenario. In multi-task learning, several tasks are learned jointly to exploit task correlations for a more efficient inductive transfer. This presents a multi-dimensional Pareto frontier on (1) the trade-off between group fairness and accuracy with respect to each task, as well as (2) the trade-offs across multiple tasks. We aim to provide a deeper understanding on how group fairness interacts with accuracy in multi-task learning, and we show that traditional approaches that mainly focus on optimizing the Pareto frontier of multi-task accuracy might not perform well on fairness goals. We propose a new set of metrics to better capture the multi-dimensional Pareto frontier of fairness-accuracy trade-offs uniquely presented in a multi-task learning setting. We further propose a Multi-Task-Aware Fairness (MTA-F) approach to improve fairness in multi-task learning. Experiments on several real-world datasets demonstrate the effectiveness of our proposed approach.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {1748–1757},
numpages = {10},
keywords = {multi-task-aware fairness treatment, pareto frontier, fairness, multi-task learning},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1109/ASP-DAC47756.2020.9045442,
author = {Lin, Zhe and Zhao, Jieru and Sinha, Sharad and Zhang, Wei},
title = {HL-Pow: A Learning-Based Power Modeling Framework for High-Level Synthesis},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASP-DAC47756.2020.9045442},
doi = {10.1109/ASP-DAC47756.2020.9045442},
abstract = {High-level synthesis (HLS) enables designers to customize hardware designs efficiently. However, it is still challenging to foresee the correlation between power consumption and HLS-based applications at an early design stage. To overcome this problem, we introduce HL-Pow, a power modeling framework for FPGA HLS based on state-of-the-art machine learning techniques. HL-Pow incorporates an automated feature construction flow to efficiently identify and extract features that exert a major influence on power consumption, simply based upon HLS results, and a modeling flow that can build an accurate and generic power model applicable to a variety of designs with HLS. By using HL-Pow, the power evaluation process for FPGA designs can be significantly expedited because the power inference of HL-Pow is established on HLS instead of the time-consuming register-transfer level (RTL) implementation flow. Experimental results demonstrate that HL-Pow can achieve accurate power modeling that is only 4.67% (24.02 mW) away from onboard power measurement. To further facilitate power-oriented optimizations, we describe a novel design space exploration (DSE) algorithm built on top of HL-Pow to trade off between latency and power consumption. This algorithm can reach a close approximation of the real Pareto frontier while only requiring running HLS flow for 20% of design points in the entire design space.},
booktitle = {2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC)},
pages = {574–580},
numpages = {7},
location = {Beijing, China}
}

@article{10.1016/j.ipm.2015.07.002,
title = {A Cross-Benchmark Comparison of 87 Learning to Rank Methods},
year = {2015},
issue_date = {November 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {51},
number = {6},
issn = {0306-4573},
url = {https://doi.org/10.1016/j.ipm.2015.07.002},
doi = {10.1016/j.ipm.2015.07.002},
abstract = {We propose a novel way to compare learning to rank methods.We perform a meta-analysis on a large set of papers that report ranking accuracy on a benchmark dataset.LRUF, FSMRank, FenchelRank, SmoothRank and ListNet are the most accurate, with increasing certainty. Learning to rank is an increasingly important scientific field that comprises the use of machine learning for the ranking task. New learning to rank methods are generally evaluated on benchmark test collections. However, comparison of learning to rank methods based on evaluation results is hindered by the absence of a standard set of evaluation benchmark collections. In this paper we propose a way to compare learning to rank methods based on a sparse set of evaluation results on a set of benchmark datasets. Our comparison methodology consists of two components: (1) Normalized Winning Number, which gives insight in the ranking accuracy of the learning to rank method, and (2) Ideal Winning Number, which gives insight in the degree of certainty concerning its ranking accuracy. Evaluation results of 87 learning to rank methods on 20 well-known benchmark datasets are collected through a structured literature search. ListNet, SmoothRank, FenchelRank, FSMRank, LRUF and LARF are Pareto optimal learning to rank methods in the Normalized Winning Number and Ideal Winning Number dimensions, listed in increasing order of Normalized Winning Number and decreasing order of Ideal Winning Number.},
journal = {Inf. Process. Manage.},
month = {nov},
pages = {757–772},
numpages = {16}
}

@article{10.1016/j.asoc.2016.09.035,
author = {Ojha, Varun Kumar and Abraham, Ajith and Snel, Vclav},
title = {Ensemble of Heterogeneous Flexible Neural Trees Using Multiobjective Genetic Programming},
year = {2017},
issue_date = {March 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {52},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.09.035},
doi = {10.1016/j.asoc.2016.09.035},
abstract = {Graphical abstractDisplay Omitted HighlightsA heterogeneous flexible neural tree (FNT) for function approximation was proposed.FNT was studied under Pareto-based multiobjective genetic programming framework.A diversity-index was introduced to maintain diversity in genetic population.FNT was found competitive with other algorithm when cross validated over datasets.Evolutionary weighted ensemble of HFNTs further improved FNT performance. Machine learning algorithms are inherently multiobjective in nature, where approximation error minimization and model's complexity simplification are two conflicting objectives. We proposed a multiobjective genetic programming (MOGP) for creating a heterogeneous flexible neural tree (HFNT), tree-like flexible feedforward neural network model. The functional heterogeneity in neural tree nodes was introduced to capture a better insight of data during learning because each input in a dataset possess different features. MOGP guided an initial HFNT population towards Pareto-optimal solutions, where the final population was used for making an ensemble system. A diversity index measure along with approximation error and complexity was introduced to maintain diversity among the candidates in the population. Hence, the ensemble was created by using accurate, structurally simple, and diverse candidates from MOGP final population. Differential evolution algorithm was applied to fine-tune the underlying parameters of the selected candidates. A comprehensive test over classification, regression, and time-series datasets proved the efficiency of the proposed algorithm over other available prediction methods. Moreover, the heterogeneous creation of HFNT proved to be efficient in making ensemble system from the final population.},
journal = {Appl. Soft Comput.},
month = {mar},
pages = {909–924},
numpages = {16},
keywords = {Pareto-based multiobjectives, Feature selection, Approximation, Flexible neural tree, Ensemble}
}

@inproceedings{10.1145/3316781.3317890,
author = {Liu, Gai and Primmer, Joseph and Zhang, Zhiru},
title = {Rapid Generation of High-Quality RISC-V Processors from Functional Instruction Set Specifications},
year = {2019},
isbn = {9781450367257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316781.3317890},
doi = {10.1145/3316781.3317890},
abstract = {The increasing popularity of compute acceleration for emerging domains such as artificial intelligence and computer vision has led to the growing need for domain-specific accelerators, often implemented as specialized processors that execute a set of domain-optimized instructions. The ability to rapidly explore (1) various possibilities of the customized instruction set, and (2) its corresponding micro-architectural features is critical to achieve the best quality-of-results (QoRs). However, this ability is frequently hindered by the manual design process at the register transfer level (RTL). Such an RTL-based methodology is often expensive and slow to react when the design specifications change at the instruction-set level and/or micro-architectural level.We address this deficiency in domain-specific processor design with ASSIST, a behavior-level synthesis framework for RISC-V processors. From an untimed functional instruction set description, ASSIST generates a spectrum of RISC-V processors implementing varying micro-architectural design choices, which enables effective tradeoffs between different QoR metrics. We demonstrate the automatic synthesis of more than 60 in-order processor implementations with varying pipeline structures from the RISC-V 32I instruction set, some of which dominate the manually optimized counterparts in the area-performance Pareto frontier. In addition, we propose an autotuning-based approach for optimizing the implementations under a given performance constraint and the technology target. We further present case studies of synthesizing various custom instruction extensions and customized instruction sets for cryptography and machine learning applications.},
booktitle = {Proceedings of the 56th Annual Design Automation Conference 2019},
articleno = {122},
numpages = {6},
location = {Las Vegas, NV, USA},
series = {DAC '19}
}

@inproceedings{10.1145/3316781.3317781,
author = {Mrazek, Vojtech and Hanif, Muhammad Abdullah and Vasicek, Zdenek and Sekanina, Lukas and Shafique, Muhammad},
title = {AutoAx: An Automatic Design Space Exploration and Circuit Building Methodology Utilizing Libraries of Approximate Components},
year = {2019},
isbn = {9781450367257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316781.3317781},
doi = {10.1145/3316781.3317781},
abstract = {Approximate computing is an emerging paradigm for developing highly energy-efficient computing systems such as various accelerators. In the literature, many libraries of elementary approximate circuits have already been proposed to simplify the design process of approximate accelerators. Because these libraries contain from tens to thousands of approximate implementations for a single arithmetic operation it is intractable to find an optimal combination of approximate circuits in the library even for an application consisting of a few operations. An open problem is "how to effectively combine circuits from these libraries to construct complex approximate accelerators". This paper proposes a novel methodology for searching, selecting and combining the most suitable approximate circuits from a set of available libraries to generate an approximate accelerator for a given application. To enable fast design space generation and exploration, the methodology utilizes machine learning techniques to create computational models estimating the overall quality of processing and hardware cost without performing full synthesis at the accelerator level. Using the methodology, we construct hundreds of approximate accelerators (for a Sobel edge detector) showing different but relevant tradeoffs between the quality of processing and hardware cost and identify a corresponding Pareto-frontier. Furthermore, when searching for approximate implementations of a generic Gaussian filter consisting of 17 arithmetic operations, the proposed approach allows us to identify approximately 103 highly relevant implementations from 1023 possible solutions in a few hours, while the exhaustive search would take four months on a high-end processor.},
booktitle = {Proceedings of the 56th Annual Design Automation Conference 2019},
articleno = {123},
numpages = {6},
location = {Las Vegas, NV, USA},
series = {DAC '19}
}

@inproceedings{10.5555/2492708.2492964,
author = {Zuluaga, Marcela and Bonilla, Edwin and Topham, Nigel},
title = {Predicting Best Design Trade-Offs: A Case Study in Processor Customization},
year = {2012},
isbn = {9783981080186},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Given the high level description of a task, many different hardware modules may be generated while meeting its behavioral requirements. The characteristics of the generated hardware can be tailored to favor energy efficiency, performance, accuracy or die area. The inherent trade-offs between such metrics need to be explored in order to choose a solution that meets design and cost expectations. We address the generic problem of automatically deriving a hardware implementation from a high-level task description. In this paper we present a novel technique that exploits previously explored implementation design spaces in order to find optimal trade-offs for new high-level descriptions. This technique is generalizable to a range of high-level synthesis problems in which trade-offs can be exposed by changing the parameters of the hardware generation tool. Our strategy, based upon machine learning techniques, models the impact of the parameterization of the tool on the target objectives, given the characteristics of the input. Thus, a predictor is able to suggest a subset of parameters that are likely to lead to optimal hardware implementations. The proposed method is evaluated on a resource sharing problem which is typical in high level synthesis, where the trade-offs between area and performance need to be explored. In this case study, we show that the technique can reduce by two orders of magnitude the number of design points that need to be explored in order to find the Pareto optimal solutions.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe},
pages = {1030–1035},
numpages = {6},
location = {Dresden, Germany},
series = {DATE '12}
}

@inproceedings{10.1145/1553374.1553384,
author = {Boutilier, Craig and Regan, Kevin and Viappiani, Paolo},
title = {Online Feature Elicitation in Interactive Optimization},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553384},
doi = {10.1145/1553374.1553384},
abstract = {Most models of utility elicitation in decision support and interactive optimization assume a predefined set of "catalog" features over which user preferences are expressed. However, users may differ in the features over which they are most comfortable expressing their preferences. In this work we consider the problem of feature elicitation: a user's utility function is expressed using features whose definitions (in terms of "catalog" features) are unknown. We cast this as a problem of concept learning, but whose goal is to identify only enough about the concept to enable a good decision to be recommended. We describe computational procedures for identifying optimal alternatives w.r.t. minimax regret in the presence of concept uncertainty; and describe several heuristic query strategies that focus on reduction of relevant concept uncertainty.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {73–80},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@article{10.1016/j.asoc.2014.12.036,
author = {Boryczka, Urszula and Kozak, Jan},
title = {Enhancing the Effectiveness of Ant Colony Decision Tree Algorithms by Co-Learning},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {30},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2014.12.036},
doi = {10.1016/j.asoc.2014.12.036},
abstract = {Graphical abstractDisplay Omitted HighlightsACO techniques provide a way to efficiently search for solutions via colearning.ACO applied to data mining tasks is one of these methods and the focus of this paper..The ACDT approach generates solutions efficiently and effectively.The ACDT approach is tested in the context of the bi-criteria evaluation function.The empirical results clearly show that the ACDT algorithm creates good solutions. Data mining and visualization techniques for high-dimensional data provide helpful information to substantially augment decision-making. Optimization techniques provide a way to efficiently search for these solutions. ACO applied to data mining tasks - a decision tree construction - is one of these methods and the focus of this paper. The Ant Colony Decision Tree (ACDT) approach generates solutions efficiently and effectively but scales poorly to large problems. This article merges the methods that have been developed for better construction of decision trees by ants. The ACDT approach is tested in the context of the bi-criteria evaluation function by focusing on two problems: the size of the decision trees and the accuracy of classification obtained during ACDT performance. This approach is tested in co-learning mechanism, it means agents-ants can interact during the construction decision trees via pheromone values. This cooperation is a chance of getting better results. The proposed methodology of analysis of ACDT is tested in a number of well-known benchmark data sets from the UCI Machine Learning Repository. The empirical results clearly show that the ACDT algorithm creates good solutions which are located in the Pareto front. The software that implements the ACDT algorithm used to generate the results of this study can be downloaded freely from http://www.acdtalgorithm.com.},
journal = {Appl. Soft Comput.},
month = {may},
pages = {166–178},
numpages = {13},
keywords = {Ant Colony Decision Trees, Ant Colony Optimization, Quality of decision trees, Decision trees, Pareto front, Ant-Miner}
}

@inproceedings{10.1145/3442188.3445887,
author = {Mehrotra, Anay and Celis, L. Elisa},
title = {Mitigating Bias in Set Selection with Noisy Protected Attributes},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445887},
doi = {10.1145/3442188.3445887},
abstract = {Subset selection algorithms are ubiquitous in AI-driven applications, including, online recruiting portals and image search engines, so it is imperative that these tools are not discriminatory on the basis of protected attributes such as gender or race. Currently, fair subset selection algorithms assume that the protected attributes are known as part of the dataset. However, protected attributes may be noisy due to errors during data collection or if they are imputed (as is often the case in real-world settings). While a wide body of work addresses the effect of noise on the performance of machine learning algorithms, its effect on fairness remains largely unexamined. We find that in the presence of noisy protected attributes, in attempting to increase fairness without considering noise, one can, in fact, decrease the fairness of the result!Towards addressing this, we consider an existing noise model in which there is probabilistic information about the protected attributes (e.g., [19, 32, 44, 56]), and ask is fair selection possible under noisy conditions? We formulate a "denoised" selection problem which functions for a large class of fairness metrics; given the desired fairness goal, the solution to the denoised problem violates the goal by at most a small multiplicative amount with high probability. Although this denoised problem turns out to be NP-hard, we give a linear-programming based approximation algorithm for it. We evaluate this approach on both synthetic and real-world datasets. Our empirical results show that this approach can produce subsets which significantly improve the fairness metrics despite the presence of noisy protected attributes, and, compared to prior noise-oblivious approaches, has better Pareto-tradeoffs between utility and fairness.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {237–248},
numpages = {12},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@article{10.1007/s00778-020-00613-w,
author = {Yang, Jingru and Fan, Ju and Wei, Zhewei and Li, Guoliang and Liu, Tongyu and Du, Xiaoyong},
title = {A Game-Based Framework for Crowdsourced Data Labeling},
year = {2020},
issue_date = {Nov 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {29},
number = {6},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-020-00613-w},
doi = {10.1007/s00778-020-00613-w},
abstract = {Data labeling, which assigns data with multiple classes, is indispensable for many applications, such as machine learning and data integration. However, existing labeling solutions either incur expensive cost for large datasets or produce noisy results. This paper introduces a cost-effective labeling approach and focuses on the labeling rule generation problem that aims to generate high-quality rules to largely reduce the labeling cost while preserving quality. To address the problem, we first generate candidate rules and then devise a game-based crowdsourcing approach CrowdGame to select high-quality rules by considering coverage and accuracy. CrowdGame employs two groups of crowd workers: One group answers rule validation tasks (whether a rule is valid) to play a role of rule generator, while the other group answers tuple checking tasks (whether the label of a data tuple is correct) to play a role of rule refuter. We let the two groups play a two-player game: Rule generator identifies high-quality rules with large coverage, while rule refuter tries to refute its opponent rule generator by checking some tuples that provide enough evidence to reject rules with low accuracy. This paper studies the challenges in CrowdGame. The first is to balance the trade-off between coverage and accuracy. We define the loss of a rule by considering the two factors. The second is rule accuracy estimation. We utilize Bayesian estimation to combine both rule validation and tuple checking tasks. The third is to select crowdsourcing tasks to fulfill the game-based framework for minimizing the loss. We introduce a minimax strategy and develop efficient task selection algorithms. We also develop a hybrid crowd-machine method for effective label assignment under budget-constrained crowdsourcing settings. We conduct experiments on entity matching and relation extraction, and the results show that our method outperforms state-of-the-art solutions.},
journal = {The VLDB Journal},
month = {nov},
pages = {1311–1336},
numpages = {26},
keywords = {Crowdsourcing, Labeling rules, Data labeling}
}

@article{10.14778/3275536.3275541,
author = {Yang, Jingru and Fan, Ju and Wei, Zhewei and Li, Guoliang and Liu, Tongyu and Du, Xiaoyong},
title = {Cost-Effective Data Annotation Using Game-Based Crowdsourcing},
year = {2018},
issue_date = {September 2018},
publisher = {VLDB Endowment},
volume = {12},
number = {1},
issn = {2150-8097},
url = {https://doi.org/10.14778/3275536.3275541},
doi = {10.14778/3275536.3275541},
abstract = {Large-scale data annotation is indispensable for many applications, such as machine learning and data integration. However, existing annotation solutions either incur expensive cost for large datasets or produce noisy results. This paper introduces a cost-effective annotation approach, and focuses on the labeling rule generation problem that aims to generate high-quality rules to largely reduce the labeling cost while preserving quality. To address the problem, we first generate candidate rules, and then devise a game-based crowdsourcing approach CROWDGAME to select high-quality rules by considering coverage and precision. CROWDGAME employs two groups of crowd workers: one group answers rule validation tasks (whether a rule is valid) to play a role of rule generator, while the other group answers tuple checking tasks (whether the annotated label of a data tuple is correct) to play a role of rule refuter. We let the two groups play a two-player game: rule generator identifies high-quality rules with large coverage and precision, while rule refuter tries to refute its opponent rule generator by checking some tuples that provide enough evidence to reject rules covering the tuples. This paper studies the challenges in CROWDGAME. The first is to balance the trade-off between coverage and precision. We define the loss of a rule by considering the two factors. The second is rule precision estimation. We utilize Bayesian estimation to combine both rule validation and tuple checking tasks. The third is to select crowdsourcing tasks to fulfill the game-based framework for minimizing the loss. We introduce a minimax strategy and develop efficient task selection algorithms. We conduct experiments on entity matching and relation extraction, and the results show that our method outperforms state-of-the-art solutions.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {57–70},
numpages = {14}
}

@article{10.1145/3314326,
author = {Schmuck, Manuel and Benini, Luca and Rahimi, Abbas},
title = {Hardware Optimizations of Dense Binary Hyperdimensional Computing: Rematerialization of Hypervectors, Binarized Bundling, and Combinational Associative Memory},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1550-4832},
url = {https://doi.org/10.1145/3314326},
doi = {10.1145/3314326},
abstract = {Brain-inspired hyperdimensional (HD) computing models neural activity patterns of the very size of the brain’s circuits with points of a hyperdimensional space, that is, with hypervectors. Hypervectors are D-dimensional (pseudo)random vectors with independent and identically distributed (i.i.d.) components constituting ultra-wide holographic words: D=10,000 bits, for instance. At its very core, HD computing manipulates a set of seed hypervectors to build composite hypervectors representing objects of interest. It demands memory optimizations with simple operations for an efficient hardware realization. In this article, we propose hardware techniques for optimizations of HD computing, in a synthesizable open-source VHDL library, to enable co-located implementation of both learning and classification tasks on only a small portion of Xilinx UltraScale FPGAs: (1) We propose simple logical operations to rematerialize the hypervectors on the fly rather than loading them from memory. These operations massively reduce the memory footprint by directly computing the composite hypervectors whose individual seed hypervectors do not need to be stored in memory. (2) Bundling a series of hypervectors over time requires a multibit counter per every hypervector component. We instead propose a binarized back-to-back bundling without requiring any counters. This truly enables on-chip learning with minimal resources as every hypervector component remains binary over the course of training to avoid otherwise multibit components. (3) For every classification event, an associative memory is in charge of finding the closest match between a set of learned hypervectors and a query hypervector by using a distance metric. This operator is proportional to hypervector dimension (D), and hence may take O(D) cycles per classification event. Accordingly, we significantly improve the throughput of classification by proposing associative memories that steadily reduce the latency of classification to the extreme of a single cycle. (4) We perform a design space exploration incorporating the proposed techniques on FPGAs for a wearable biosignal processing application as a case study. Our techniques achieve up to 2.39\texttimes{} area saving, or 2,337\texttimes{} throughput improvement. The Pareto optimal HD architecture is mapped on only 18,340 configurable logic blocks (CLBs) to learn and classify five hand gestures using four electromyography sensors.},
journal = {J. Emerg. Technol. Comput. Syst.},
month = {oct},
articleno = {32},
numpages = {25},
keywords = {binarized temporal bundling, biosignals, rematerialization, on-chip learning, single-cycle associative memory, electromyography, Hyperdimensional computing, FPGA}
}

