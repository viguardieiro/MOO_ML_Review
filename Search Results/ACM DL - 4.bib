@inproceedings{10.1145/3337821.3337833,
author = {Fan, Kaijie and Cosenza, Biagio and Juurlink, Ben},
title = {Predictable GPUs Frequency Scaling for Energy and Performance},
year = {2019},
isbn = {9781450362955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3337821.3337833},
doi = {10.1145/3337821.3337833},
abstract = {Dynamic voltage and frequency scaling (DVFS) is an important solution to balance performance and energy consumption, and hardware vendors provide management libraries that allow the programmer to change both memory and core frequencies. The possibility to manually set these frequencies is a great opportunity for application tuning, which can focus on the best application-dependent setting. However, this task is not straightforward because of the large set of possible configurations and because of the multi-objective nature of the problem, which minimizes energy consumption and maximizes performance.This paper proposes a method to predict the best core and memory frequency configurations on GPUs for an input OpenCL kernel. Our modeling approach, based on machine learning, first predicts speedup and normalized energy over the default frequency configuration. Then, it combines the two models into a multi-objective one that predicts a Pareto-set of frequency configurations. The approach uses static code features, is built on a set of carefully designed micro-benchmarks, and can predict the best frequency settings of a new kernel without executing it. Test results show that our modeling approach is very accurate on predicting extrema points and Pareto set for ten out of twelve test benchmarks, and discover frequency configurations that dominate the default configuration in either energy or performance.},
booktitle = {Proceedings of the 48th International Conference on Parallel Processing},
articleno = {52},
numpages = {10},
keywords = {Frequency scaling, GPUs, Modeling, Energy efficiency},
location = {Kyoto, Japan},
series = {ICPP 2019}
}

@inproceedings{10.1145/3319619.3321962,
author = {Wang, Hongyan and Xu, Hua and Yuan, Yuan and Sun, Xiaomin and Deng, Junhui},
title = {Balancing Exploration and Exploitation in Multiobjective Batch Bayesian Optimization},
year = {2019},
isbn = {9781450367486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319619.3321962},
doi = {10.1145/3319619.3321962},
abstract = {Many applications such as hyper-parameter tunning in Machine Learning can be casted to multiobjective black-box problems and it is challenging to optimize them. Bayesian Optimization (BO) is an effective method to deal with black-box functions. This paper mainly focuses on balancing exploration and exploitation in multi-objective black-box optimization problems by multiple samplings in BBO. In each iteration, multiple recommendations are generated via two different trade-off strategies respectively the expected improvement (EI) and a multiobjective framework with the mean and variance function of the GP posterior forming two conflict objectives. We compare our algorithm with ParEGO by running on 12 test functions. Hypervolume (HV, also known as S-metric) results show that our algorithm works well in exploration-exploitation trade-off for multiobjective black-box optimization problems.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {237–238},
numpages = {2},
keywords = {exploration and exploitation, expensive multiobjective optimization, batch bayesian optimization, ParEGO, gaussian process},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@inproceedings{10.1145/3377930.3390151,
author = {Wilkins, Zachary and Zincir-Heywood, Nur},
title = {COUGAR: Clustering of Unknown Malware Using Genetic Algorithm Routines},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3390151},
doi = {10.1145/3377930.3390151},
abstract = {Through malware, cyber criminals can leverage our computing resources to disrupt our work, steal our information, and even hold it hostage. Security professionals seek to classify these malicious software so as to prevent their distribution and execution, but the sheer volume of malware complicates these efforts. In response, machine learning algorithms are actively employed to alleviate the workload. One such approach is evolutionary computation, where solutions are bred, rather than built. In this paper, we design, develop and evaluate a system, COUGAR, to reduce high-dimensional malware behavioural data, and optimize clustering behaviour using a multi-objective genetic algorithm. Evaluations demonstrate that each of our chosen clustering algorithms can successfully highlight groups of malware. We also present an example real-world scenario, based on the testing data, to demonstrate practical applications.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {1195–1203},
numpages = {9},
keywords = {clustering, multi-objective optimization, evolution, malware, machine learning, cyber attack, cyber security},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1145/2739480.2754756,
author = {Urbanowicz, Ryan and Moore, Jason},
title = {Retooling Fitness for Noisy Problems in a Supervised Michigan-Style Learning Classifier System},
year = {2015},
isbn = {9781450334723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739480.2754756},
doi = {10.1145/2739480.2754756},
abstract = {An accuracy-based rule fitness is a hallmark of most modern Michigan-style learning classifier systems (LCS), a powerful, flexible, and largely interpretable class of machine learners. However, rule-fitness based solely on accuracy is not ideal for identifying 'optimal' rules in supervised learning. This is particularly true for noisy problem domains where perfect rule accuracy essentially guarantees over-fitting. Rule fitness based on accuracy alone is unreliable for reflecting the global 'value' of a given rule since rule accuracy is based on a subset of the training instances. While moderate over-fitting may not dramatically hinder LCS classification or prediction performance, the interpretability of the solution is likely to suffer. Additionally, over-fitting can impede algorithm learning efficiency and leads to a larger number of rules being required to capture relationships. The present study seeks to develop an intuitive multi-objective fitness function that will encourage the discovery, preservation, and identification of 'optimal' rules through accuracy, correct coverage of training data, and the prior probability of the specified attribute states and class expressed by a given rule. We demonstrate the advantages of our proposed fitness by implementing it into the ExSTraCS algorithm and performing evaluations over a large spectrum of complex, noisy, simulated datasets.},
booktitle = {Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {591–598},
numpages = {8},
keywords = {multiple solutions, bioinformatics, multi-objective optimization, pattern recognition and classification, classifier systems},
location = {Madrid, Spain},
series = {GECCO '15}
}

@article{10.1109/TNN.2010.2041468,
author = {Caballero, Juan Carlos Fern\'{a}ndez and Mart\'{\i}nez, Francisco Jos\'{e} and Herv\'{a}s, C\'{e}sar and Guti\'{e}rrez, Pedro Antonio},
title = {Sensitivity versus Accuracy in Multiclass Problems Using Memetic Pareto Evolutionary Neural Networks},
year = {2010},
issue_date = {May 2010},
publisher = {IEEE Press},
volume = {21},
number = {5},
issn = {1045-9227},
url = {https://doi.org/10.1109/TNN.2010.2041468},
doi = {10.1109/TNN.2010.2041468},
abstract = {This paper proposes a multiclassification algorithm using multilayer perceptron neural network models. It tries to boost two conflicting main objectives of multiclassifiers: a high correct classification rate level and a high classification rate for each class. This last objective is not usually optimized in classification, but is considered here given the need to obtain high precision in each class in real problems. To solve this machine learning problem, we use a Pareto-based multiobjective optimization methodology based on a memetic evolutionary algorithm. We consider a memetic Pareto evolutionary approach based on the NSGA2 evolutionary algorithm (MPENSGA2). Once the Pareto front is built, two strategies or automatic individual selection are used: the best model in accuracy and the best model in sensitivity (extremes in the Pareto front). These methodologies are applied to solve 17 classification benchmark problems obtained from the University of California at Irvine (UCI) repository and one complex real classification problem. The models obtained show high accuracy and a high classification rate for each class.},
journal = {Trans. Neur. Netw.},
month = {may},
pages = {750–770},
numpages = {21},
keywords = {multiobjective evolutionary algorithms, neural networks, multiclassification, local search, Accuracy, accuracy, sensitivity}
}

@inproceedings{10.1145/2248418.2248436,
author = {Zuluaga, Marcela and Krause, Andreas and Milder, Peter and P\"{u}schel, Markus},
title = {"Smart" Design Space Sampling to Predict Pareto-Optimal Solutions},
year = {2012},
isbn = {9781450312127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2248418.2248436},
doi = {10.1145/2248418.2248436},
abstract = {Many high-level synthesis tools offer degrees of freedom in mapping high-level specifications to Register-Transfer Level descriptions. These choices do not affect the functional behavior but span a design space of different cost-performance tradeoffs. In this paper we present a novel machine learning-based approach that efficiently determines the Pareto-optimal designs while only sampling and synthesizing a fraction of the design space. The approach combines three key components: (1) A regression model based on Gaussian processes to predict area and throughput based on synthesis training data. (2) A "smart" sampling strategy, GP-PUCB, to iteratively refine the model by carefully selecting the next design to synthesize to maximize progress. (3) A stopping criterion based on assessing the accuracy of the model without access to complete synthesis data. We demonstrate the effectiveness of our approach using IP generators for discrete Fourier transforms and sorting networks. However, our algorithm is not specific to this application and can be applied to a wide range of Pareto front prediction problems.},
booktitle = {Proceedings of the 13th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, Tools and Theory for Embedded Systems},
pages = {119–128},
numpages = {10},
keywords = {machine learning, area and performance estimation, high-level synthesis, Pareto optimality},
location = {Beijing, China},
series = {LCTES '12}
}

@article{10.1145/2345141.2248436,
author = {Zuluaga, Marcela and Krause, Andreas and Milder, Peter and P\"{u}schel, Markus},
title = {"Smart" Design Space Sampling to Predict Pareto-Optimal Solutions},
year = {2012},
issue_date = {MAY 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {5},
issn = {0362-1340},
url = {https://doi.org/10.1145/2345141.2248436},
doi = {10.1145/2345141.2248436},
abstract = {Many high-level synthesis tools offer degrees of freedom in mapping high-level specifications to Register-Transfer Level descriptions. These choices do not affect the functional behavior but span a design space of different cost-performance tradeoffs. In this paper we present a novel machine learning-based approach that efficiently determines the Pareto-optimal designs while only sampling and synthesizing a fraction of the design space. The approach combines three key components: (1) A regression model based on Gaussian processes to predict area and throughput based on synthesis training data. (2) A "smart" sampling strategy, GP-PUCB, to iteratively refine the model by carefully selecting the next design to synthesize to maximize progress. (3) A stopping criterion based on assessing the accuracy of the model without access to complete synthesis data. We demonstrate the effectiveness of our approach using IP generators for discrete Fourier transforms and sorting networks. However, our algorithm is not specific to this application and can be applied to a wide range of Pareto front prediction problems.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {119–128},
numpages = {10},
keywords = {machine learning, high-level synthesis, area and performance estimation, Pareto optimality}
}

@inproceedings{10.5555/3491440.3491715,
author = {Bresson, Roman and Cohen, Johanne and H\"{u}llermeier, Eyke and Labreuche, Christophe and Sebag, Mich\`{e}le},
title = {Neural Representation and Learning of Hierarchical 2-Additive Choquet Integrals},
year = {2021},
isbn = {9780999241165},
abstract = {Multi-Criteria Decision Making (MCDM) aims at modelling expert preferences and assisting decision makers in identifying options best accommodating expert criteria. An instance of MCDM model, the Choquet integral is widely used in real-world applications, due to its ability to capture interactions between criteria while retaining interpretability. Aimed at a better scalability and modularity, hierarchical Choquet integrals involve intermediate aggregations of the interacting criteria, at the cost of a more complex elicitation. The paper presents a machine learning-based approach for the automatic identification of hierarchical MCDM models, composed of 2-additive Choquet integral aggregators and of marginal utility functions on the raw features from data reflecting expert preferences. The proposed NEUR-HCI framework relies on a specific neural architecture, enforcing by design the Choquet model constraints and supporting its end-to-end training. The empirical validation of NEUR-HCI on real-world and artificial benchmarks demonstrates the merits of the approach compared to state-of-art baselines.},
booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
articleno = {275},
numpages = {8},
location = {Yokohama, Yokohama, Japan},
series = {IJCAI'20}
}

@article{10.1002/sam.11545,
author = {Zhao, Yifan and Yang, Xian and Bolnykh, Carolina and Harenberg, Steve and Korchiev, Nodirbek and Yerramsetty, Saavan Raj and Vellanki, Bhanu Prasad and Kodumagulla, Ramakanth and Samatova, Nagiza F.},
title = {Predictive Models with End User Preference},
year = {2022},
issue_date = {February 2022},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {15},
number = {1},
issn = {1932-1864},
url = {https://doi.org/10.1002/sam.11545},
doi = {10.1002/sam.11545},
abstract = {Classical machine learning models typically try to optimize the model based on the most discriminatory features of the data; however, they do not usually account for end user preferences. In certain applications, this can be a serious issue as models not aware of user preferences could become costly, untrustworthy, or privacy‐intrusive to use, thus becoming irrelevant and/or uninterpretable. Ideally, end users with domain knowledge could propose preferable features that the predictive model could then take into account. In this paper, we propose a generic modeling method that respects end user preferences via a relative ranking system to express multi‐criteria preferences and a regularization term in the model's objective function to incorporate the ranked preferences. In a more generic perspective, this method is able to plug user preferences into existing predictive models without creating completely new ones. We implement this method in the context of decision trees and are able to achieve a comparable classification accuracy while reducing the use of undesirable features.},
journal = {Stat. Anal. Data Min.},
month = {jan},
pages = {69–82},
numpages = {14},
keywords = {decision tree, predictive model, child support, relative ranking, user preference, regularization}
}

@article{10.1016/j.knosys.2016.06.033,
author = {Kumar Sikdar, Utpal and Ekbal, Asif and Saha, Sriparna},
title = {A Generalized Framework for Anaphora Resolution in Indian Languages},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {109},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2016.06.033},
doi = {10.1016/j.knosys.2016.06.033},
abstract = {In this paper, we propose a joint model of feature selection and ensemble learning for anaphora resolution in the resource-poor environment like the Indian languages. The proposed approach is based on multi-objective differential evolution (DE) that optimises five coreference resolution scorers, namely Muc, Bcub, Ceafm, Ceafe and Blanc. The main goal is to determine the best combination of different mention classifiers and the most relevant set of features for anaphora resolution. The proposed method is evaluated for three leading Indian languages, namely Hindi, Bengali and Tamil. Experiments on the benchmark datasets of ICON-2011 Shared Task on Anaphora Resolution in Indian Languages show that our proposed approach attains good level of accuracies, which are often better with respect to the state-of-the-art systems. It achieves the F-measure values of 71.89%, 59.61%, 52.55% 34.45% and 72.52% for Muc, Bcub, Ceafm, Ceafe and Blanc, respectively, for Bengali language. For Hindi we obtain the F-measure values of 33.27%, 63.06%, 49.59%, 49.06% and 55.45% for Muc, Bcub, Ceafm, Ceafe and Blanc metrics, respectively. In order to further show the efficacy of our proposed algorithm, we evaluate with Tamil, a language that belongs to a different family. This shows the F-measure values of 31.79%, 64.67%, 46.81%, 45.29% and 52.80% for Muc, Bcub, Ceafm, Ceafe and Blanc metrics, respectively. Experiments on Dutch show the F-measure values of 17.67%, 74.43%, 58.08%, 59.21% and 55.58% for Muc, Bcub, Ceafm, Ceafe and Blanc metrics, respectively.},
journal = {Know.-Based Syst.},
month = {oct},
pages = {147–159},
numpages = {13},
keywords = {Single objective optimization (SOO), Support vector machine (SVM), Conditional random field (CRF), Multiobjective optimization (MOO)}
}

@inproceedings{10.1145/2723372.2723735,
author = {Peng, Peng and Wong, Raymong Chi-Wing},
title = {K-Hit Query: Top-k Query with Probabilistic Utility Function},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2723735},
doi = {10.1145/2723372.2723735},
abstract = {Multi-criteria decision making problem has been well studied for many years. One popular query for multi-criteria decision making is top-k queries which require each user to specify an exact utility function. In many cases, the utility function of a user is probabilistic and finding the distribution on the utility functions has been widely explored in the machine learning areas, such as user's recommender systems, Bayesian learning models and user's preference elicitation, for improving user's experience. Motivated by this, we propose a new type of queries called k-hit queries, which has not been studied before. Given a set D of tuples in the database, the distribution θ on utility functions and a positive integer k, we would like to select a set of k tuples from D in order to maximize the probability that at least one of tuples in the selection set is the favorite of a user. All applications for top-k queries can naturally be used in k-hit queries. In this paper, we present various interesting properties of k-hit queries. Besides, based on these properties, we propose a novel algorithm called k-hit_Alg for k-hit queries. Finally, we conducted comprehensive experiments to show that the performance of our proposed method, k hit_Alg, is superior compared with other existing algorithms which were originally used to answer other existing queries.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {577–592},
numpages = {16},
keywords = {sampling, top-k query, algorithm, geometry, preference query, skyline query},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@article{10.1504/ijbic.2019.099179,
author = {Tawhid, Mohamed A. and Ali, Ahmed F.},
title = {Multidirectional Harmony Search Algorithm for Solving Integer Programming and Minimax Problems},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {13},
number = {3},
issn = {1758-0366},
url = {https://doi.org/10.1504/ijbic.2019.099179},
doi = {10.1504/ijbic.2019.099179},
abstract = {Integer programming and minimax problems are essential tools in solving various problems that arise in data mining and machine learning such as multi-class data classification and feature selection problems. In this paper, we propose a new hybrid harmony search algorithm by combining the harmony search algorithm with the multidirectional search method in order to solve the integer programming and minimax problems. The proposed algorithm is called multidirectional harmony search algorithm (MDHSA). MDHSA starts the search by applying the standard harmony search for numbers of iteration then the best-obtained solution is passing to the multidirectional search method as an intensification process in order to accelerate the search and overcome the slow convergence of the standard harmony search algorithm. The proposed algorithm is balancing between the global exploration of the harmony search algorithm and the deep exploitation of the multidirectional search method. MDHSA algorithm is tested on seven integer programming problems and 15 minimax problems and compared against 12 algorithms for solving integer programming problems and 11 algorithms for solving minimax problems. The experiments results show the efficiency of the proposed algorithm and its ability to solve integer programming and minimax problems in reasonable time.},
journal = {Int. J. Bio-Inspired Comput.},
month = {jan},
pages = {141–158},
numpages = {17},
keywords = {multidirectional search, integer programming problems, minimax problems, evolutionary computation, global optimisation, harmony search algorithm, direct search algorithm}
}

@article{10.1007/s10994-020-05886-4,
author = {Haghir Chehreghani, Morteza},
title = {Unsupervised Representation Learning with Minimax Distance Measures},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {109},
number = {11},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-020-05886-4},
doi = {10.1007/s10994-020-05886-4},
abstract = {We investigate the use of Minimax distances to extract in a nonparametric way the features that capture the unknown underlying patterns and structures in the data. We develop a general-purpose and computationally efficient framework to employ Minimax distances with many machine learning methods that perform on numerical data. We study both computing the pairwise Minimax distances for all pairs of objects and as well as computing the Minimax distances of all the objects to/from a fixed (test) object. We first efficiently compute the pairwise Minimax distances between the objects, using the equivalence of Minimax distances over a graph and over a minimum spanning tree constructed on that. Then, we perform an embedding of the pairwise Minimax distances into a new vector space, such that their squared Euclidean distances in the new space equal to the pairwise Minimax distances in the original space. We also study the case of having multiple pairwise Minimax matrices, instead of a single one. Thereby, we propose an embedding via first summing up the centered matrices and then performing an eigenvalue decomposition to obtain the relevant features. In the following, we study computing Minimax distances from a fixed (test) object which can be used for instance in K-nearest neighbor search. Similar to the case of all-pair pairwise Minimax distances, we develop an efficient and general-purpose algorithm that is applicable with any arbitrary base distance measure. Moreover, we investigate in detail the edges selected by the Minimax distances and thereby explore the ability of Minimax distances in detecting outlier objects. Finally, for each setting, we perform several experiments to demonstrate the effectiveness of our framework.},
journal = {Mach. Learn.},
month = {nov},
pages = {2063–2097},
numpages = {35},
keywords = {Minimax distances, Computational efficiency, Distance measure, Representation learning}
}

@inproceedings{10.5555/2740769.2740804,
author = {Dalip, Daniel H. and Lima, Harlley and Gon\c{c}alves, Marcos Andr\'{e} and Cristo, Marco and Calado, P\'{a}vel},
title = {Quality Assessment of Collaborative Content with Minimal Information},
year = {2014},
isbn = {9781479955695},
publisher = {IEEE Press},
abstract = {Content generated by users is one of the most interesting phenomena of published media. However, the possibility of unrestricted edition is a source of doubts about its quality. This issue has motivated many studies on how to automatically assess content quality in collaborative web sites. Generally, these studies use machine learning techniques to combine large number of quality indicators into a single value representing the overall quality of the document. This need for a high number of indicators, however, has detrimental implications both on the efficiency and on the effectiveness of the quality assessment algorithms. In this work, we exploit and extend a feature selection method based on the SPEA2 multi-objective genetic algorithm. Results show that we can reduce the feature set to a fraction of 15% through 25% of the original, while obtaining error rates comparable to the state of the art.},
booktitle = {Proceedings of the 14th ACM/IEEE-CS Joint Conference on Digital Libraries},
pages = {201–210},
numpages = {10},
keywords = {machine learning, genetic algorithm, quality assessment, Wikipedia, feature selection},
location = {London, United Kingdom},
series = {JCDL '14}
}

@article{10.1145/3508461,
author = {Ma, Karima and Gharbi, Michael and Adams, Andrew and Kamil, Shoaib and Li, Tzu-Mao and Barnes, Connelly and Ragan-Kelley, Jonathan},
title = {Searching for Fast Demosaicking Algorithms},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0730-0301},
url = {https://doi.org/10.1145/3508461},
doi = {10.1145/3508461},
abstract = {We present a method to automatically synthesize efficient, high-quality demosaicking algorithms, across a range of computational budgets, given a loss function and training data. It performs a multi-objective, discrete-continuous optimization which simultaneously solves for the program structure and parameters that best trade off computational cost and image quality. We design the method to exploit domain-specific structure for search efficiency. We apply it to several tasks, including demosaicking both Bayer and Fuji X-Trans color filter patterns, as well as joint demosaicking and super-resolution. In a few days on 8 GPUs, it produces a family of algorithms that significantly improves image quality relative to the prior state-of-the-art across a range of computational budgets from 10s to 1000s of operations per pixel (1dB–3dB higher quality at the same cost, or 8.5–200 \texttimes{} higher throughput at same or better quality). The resulting programs combine features of both classical and deep learning-based demosaicking algorithms into more efficient hybrid combinations, which are bandwidth-efficient and vectorizable by construction. Finally, our method automatically schedules and compiles all generated programs into optimized SIMD code for modern processors.},
note = {Just Accepted},
journal = {ACM Trans. Graph.},
month = {dec},
keywords = {differentiable programming, data driven methods, domain specific programming, demosaicking, neural architecture search, super-resolution}
}

@inproceedings{10.1145/3393527.3393564,
author = {Jiang, Wenjie and Hu, Donghui and Yu, Cong and Li, Meng and Zhao, Zhong-qiu},
title = {A New Steganography Without Embedding Based on Adversarial Training},
year = {2020},
isbn = {9781450375344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3393527.3393564},
doi = {10.1145/3393527.3393564},
abstract = {Steganography is an art to hide information in the carriers to prevent from being detected, while steganalysis is the opposite art to detect the presence of the hidden information. With the development of deep learning, several state-of-the-art steganography and steganalysis based on deep learning techniques have been proposed to improve hiding or detection capabilities. Generative Adversarial Networks (GANs) based steganography directly uses the minimax game between the generator and discriminator, to automatically generate steganography algorithms resisting being detected by powerful steganalysis. The steganography without embedding (SWE) based on GANs, where the generated cover images themselves are stego ones carrying secret information has shown its state-of-the-art steganography performance. However, SWE based on GANs has serious weaknesses, such as low information recovery accuracy, low steganography capacity and poor natural showing. To solve these problems, this paper proposes a new SWE based on adversarial training, with carefully designed generator, discriminator and extractor, as well as their loss functions and optimized training mode. The proposed method can achieve a very high information recovery accuracy (100% in some cases), and at the same time improve the steganography capacity and image quality.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
pages = {219–223},
numpages = {5},
keywords = {steganography, steganalysis, steganography without embedding, generative adversarial networks},
location = {Hefei, China},
series = {ACM TURC'20}
}

@article{10.1109/TITS.2020.2971385,
author = {Yao, Yu and Atkins, Ella},
title = {The Smart Black Box: A Value-Driven High-Bandwidth Automotive Event Data Recorder},
year = {2021},
issue_date = {March 2021},
publisher = {IEEE Press},
volume = {22},
number = {3},
url = {https://doi.org/10.1109/TITS.2020.2971385},
doi = {10.1109/TITS.2020.2971385},
abstract = {Autonomous vehicles require reliable and resilient sensor suites and ongoing validation through fleet-wide data collection. This paper proposes a Smart Black Box (SBB) to augment traditional low-bandwidth data logging with value-driven high-bandwidth data capture. The SBB caches short-term histories of data as buffers through a deterministic Mealy machine based on data value and similarity. Compression quality for each frame is determined by optimizing the trade-off between value and storage cost. With finite storage, prioritized data recording discards low-value buffers to make room for new data. This paper formulates SBB compression decision making as a constrained multi-objective optimization problem with novel value metrics and filtering. The SBB has been evaluated on a traffic simulator which generates trajectories containing events of interest (EOIs) and corresponding first-person view videos. SBB compression efficiency is assessed by comparing storage requirements with different compression quality levels and event capture ratios. Performance is evaluated by comparing results with a traditional first-in-first-out (FIFO) recording scheme. Deep learning performance on images recorded at different compression levels is evaluated to illustrate the reproducibility of SBB recorded data.},
journal = {Trans. Intell. Transport. Sys.},
month = {mar},
pages = {1484–1496},
numpages = {13}
}

@article{10.1155/2019/7560872,
author = {Hu, Ruihan and Zhou, Songbin and Liu, Yisen and Tang, Zhiri and Lin, Cheng-Jian},
title = {Margin-Based Pareto Ensemble Pruning: An Ensemble Pruning Algorithm That Learns to Search Optimized Ensembles},
year = {2019},
issue_date = {2019},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2019},
issn = {1687-5265},
url = {https://doi.org/10.1155/2019/7560872},
doi = {10.1155/2019/7560872},
abstract = {The ensemble pruning system is an effective machine learning framework that combines several learners as experts to classify a test set. Generally, ensemble pruning systems aim to define a region of competence based on the validation set to select the most competent ensembles from the ensemble pool with respect to the test set. However, the size of the ensemble pool is usually fixed, and the performance of an ensemble pool heavily depends on the definition of the region of competence. In this paper, a dynamic pruning framework called margin-based Pareto ensemble pruning is proposed for ensemble pruning systems. The framework explores the optimized ensemble pool size during the overproduction stage and finetunes the experts during the pruning stage. The Pareto optimization algorithm is used to explore the size of the overproduction ensemble pool that can result in better performance. Considering the information entropy of the learners in the indecision region, the marginal criterion for each learner in the ensemble pool is calculated using margin criterion pruning, which prunes the experts with respect to the test set. The effectiveness of the proposed method for classification tasks is assessed using datasets. The results show that margin-based Pareto ensemble pruning can achieve smaller ensemble sizes and better classification performance in most datasets when compared with state-of-the-art models.},
journal = {Intell. Neuroscience},
month = {jan},
numpages = {12}
}

@inproceedings{10.5555/3522802.3523000,
author = {Dehghanimohammadabadi, Mohammad and Belsare, Sahil and Thiesing, Renee},
title = {Simulation-Optimization of Digital Twin},
year = {2021},
publisher = {IEEE Press},
abstract = {With rapid advancements in Cyber-Physical manufacturing, the Internet of Things, Simulation software, and Machine Learning algorithms, the applicability of Industry 4.0 is gaining momentum. The demand for real-time decision-making in the manufacturing industry has given significant attention to the field of Digital Twin (DT). The whole idea revolves around creating a digital counterpart of the physical system based on enterprise data to exploit the effects of numerous parameters and make informed decisions. Based on that, this paper proposes a simulation-optimization framework for the DT model of a Beverage Manufacturing Plant. A data-driven simulation model developed in Simio is integrated with Python to perform Multi-Objective optimization. The framework explores optimal solutions by simulating multiple scenarios by altering the availability of operators and dispatching/scheduling rules. The results show that simulation optimization can be integrated into the Digital-Twin models as part of real-time production planning and scheduling.},
booktitle = {Proceedings of the Winter Simulation Conference},
articleno = {234},
numpages = {10},
location = {Phoenix, Arizona},
series = {WSC '21}
}

@inproceedings{10.1145/3295500.3356217,
author = {Patki, Tapasya and Thiagarajan, Jayaraman J. and Ayala, Alexis and Islam, Tanzima Z.},
title = {Performance Optimality or Reproducibility: That is the Question},
year = {2019},
isbn = {9781450362290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295500.3356217},
doi = {10.1145/3295500.3356217},
abstract = {The era of extremely heterogeneous supercomputing brings with itself the devil of increased performance variation and reduced reproducibility. There is a lack of understanding in the HPC community on how the simultaneous consideration of network traffic, power limits, concurrency tuning, and interference from other jobs impacts application performance.In this paper, we design a methodology that allows both HPC users and system administrators to understand the trade-off space between optimal and reproducible performance. We present a first-of-its-kind dataset that simultaneously varies multiple system- and user-level parameters on a production cluster, and introduce a new metric, called the desirability score, which enables comparison across different system configurations. We develop a novel, model-agnostic machine learning methodology based on the graph signal theory for comparing the influence of parameters on application predictability, and using a new visualization technique, make practical suggestions for best practices for multi-objective HPC environments.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {77},
numpages = {30},
keywords = {graph signal analysis, machine learning, performance reproducibility, visualization},
location = {Denver, Colorado},
series = {SC '19}
}

@article{10.1016/j.future.2015.07.016,
author = {Pop, Florin and Potop-Butucaru, Maria},
title = {ARMCO},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2015.07.016},
doi = {10.1016/j.future.2015.07.016},
abstract = {Cloud Computing can be seen as one of the latest major evolution in computing offering unlimited possibility to use ICT in various domains: business, smart cities, medicine, environmental computing, mobile systems, design and implementation of cyber-infrastructures. The recent expansion of Cloud Systems has led to adapting resource management solutions for large number of wide distributed and heterogeneous datacenters. The adaptive methods used in this context are oriented on: self-stabilizing, self-organizing and autonomic systems; dynamic, adaptive and machine learning based distributed algorithms; fault tolerance, reliability, availability of distributed systems. The pay-per-use economic model of Cloud Computing comes with a new challenge: maximizing the profit for service providers, minimizing the total cost for customers and being friendly with the environment.This special issue presents advances in virtual machine assignment and placement, multi-objective and multi-constraints job scheduling, resource management in federated Clouds and in heterogeneous environments, dynamic topology for data distribution, workflow performance improvement, energy efficiency techniques and assurance of Service Level Agreements.},
journal = {Future Gener. Comput. Syst.},
month = {jan},
pages = {79–81},
numpages = {3},
keywords = {Ubiquitous systems, Adaptive methods, Task scheduling, Resource management, Cloud computing}
}

@inproceedings{10.1145/2330163.2330234,
author = {Phan, Dung H. and Suzuki, Junichi and Hayashi, Isao},
title = {Leveraging Indicator-Based Ensemble Selection in Evolutionary Multiobjective Optimization Algorithms},
year = {2012},
isbn = {9781450311779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330163.2330234},
doi = {10.1145/2330163.2330234},
abstract = {Various evolutionary multiobjective optimization algorithms (EMOAs) have replaced or augmented the notion of dominance with quality indicators and leveraged them in selection operators. Recent studies show that indicator-based EMOAs outperform traditional dominance-based EMOAs. This paper proposes and evaluates an ensemble learning method that constructs an ensemble of existing indicators with a novel boosting algorithm called Pdi-Boosting. The proposed method is carried out with a training problem in which Pareto-optimal solutions are known. It can work with a simple training problem, and an ensemble of indicators can effectively aid parent selection and environmental selection in order to solve harder problems. Experimental results show that the proposed method is efficient thanks to its dynamic adjustment of training data. An ensemble of indicators outperforms existing individual indicators in optimality, diversity and robustness. The proposed ensemble-based evolutionary algorithm outperforms a well-known dominance-based EMOA and existing indicator-based EMOAs.},
booktitle = {Proceedings of the 14th Annual Conference on Genetic and Evolutionary Computation},
pages = {497–504},
numpages = {8},
keywords = {quality indicators, indicator-based ensemble selection, evolutionary multiobjective optimization algorithms, boosting},
location = {Philadelphia, Pennsylvania, USA},
series = {GECCO '12}
}

@article{10.1007/s00500-018-3639-2,
author = {Sehra, Sumeet Kaur and Brar, Yadwinder Singh and Kaur, Navdeep and Sehra, Sukhjit Singh},
title = {Software Effort Estimation Using FAHP and Weighted Kernel LSSVM Machine},
year = {2019},
issue_date = {Nov 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {21},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-018-3639-2},
doi = {10.1007/s00500-018-3639-2},
abstract = {In the life cycle of software product development, the software effort estimation&nbsp;(SEE) has always been a critical activity. The researchers have proposed numerous estimation methods since the inception of software engineering as a research area. The diversity of estimation approaches is very high and increasing, but it has been interpreted that no single technique performs consistently for each project and environment. Multi-criteria decision-making (MCDM) approach generates more credible estimates, which is subjected to expert’s experience. In this paper, a hybrid model has been developed to combine MCDM (for handling uncertainty) and machine learning algorithm (for handling imprecision) approach to predict the effort more accurately. Fuzzy analytic hierarchy process (FAHP) has been used effectively for feature ranking. Ranks generated from FAHP have been integrated into weighted kernel least square support vector machine for effort estimation. The model developed has been empirically validated on data repositories available for SEE. The combination of weights generated by FAHP and the radial basis function (RBF) kernel has resulted in more accurate effort estimates in comparison with bee colony optimisation and basic RBF kernel-based model.},
journal = {Soft Comput.},
month = {nov},
pages = {10881–10900},
numpages = {20},
keywords = {Software effort estimation, Fuzzy analytic hierarchy process, Least square support vector machine}
}

@article{10.1007/s11277-020-07091-x,
author = {Ahmed, Syed Thouheed and Sandhya, M. and Sankar, Sharmila},
title = {TelMED: Dynamic User Clustering Resource Allocation Technique for MooM Datasets Under Optimizing Telemedicine Network},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {112},
number = {2},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-020-07091-x},
doi = {10.1007/s11277-020-07091-x},
abstract = {Tele-Medical data communication via general purpose networking protocols and techniques are major set-back under low line channels and adequate resources such as bandwidth, frequency, power spectrum for transmission and impacts the Quality of Data (QoD) on transmission line. In this paper, a heterogeneous multi-input multi-out (MIMO) based dynamic user clustering technique is proposed and the protocol is termed as TelMED. The proposed technique introduces machine learning terminology on networking nodes for dynamic user grouping and classification resulting in the formation of clusters with reflective similarity indexing ratio. The dynamic clustered users of TelMED protocol are allocated with resources for the transmission of Multi-Objective Optimized Medical datasets resulting in creation of virtual telemedicine networking environment with a given typical network space. The technique is designed on clustered user grouping size of maximum 32 users for reliable results over a fixed networking space and optimized resources for low line transmission channels of rural or remote networks. The resulting technique proves an efficiency of 92.3% over dynamic MIMO user grouping.},
journal = {Wirel. Pers. Commun.},
month = {may},
pages = {1061–1077},
numpages = {17},
keywords = {Telemedicine networks, Dynamic user grouping, Optimized resource allocation, Low line channel transmission}
}

@inbook{10.1145/3377929.3398139,
author = {Liou, Jhe-Yu and Wang, Xiaodong and Forrest, Stephanie and Wu, Carole-Jean},
title = {GEVO-ML: A Proposal for Optimizing ML Code with Evolutionary Computation},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3398139},
abstract = {Parallel accelerators, such as GPUs, are a key enabler of large-scale Machine Learning (ML) applications. However, programmers often lack detailed knowledge of the underlying architecture and fail to fully leverage their computational power. This paper proposes GEVO-ML, a tool for automatically discovering optimization opportunities and tuning the performance of ML kernels. GEVO-ML extends earlier work on GEVO (Gpu optimization using EVOlutionary computation) by focusing directly on ML frameworks, intermediate languages, and target architectures. It retains the multi-objective evolutionary search developed for GEVO, which searches for edits to GPU code compiled to LLVM-IR and improves performance on desired criteria while retaining required functionality. In earlier work, we studied some ML workloads in GPU settings and found that GEVO could improve kernel speeds by factors ranging from 1.7X to 2.9X, even with access to only a small portion of the overall ML framework. This workshop paper examines the limitations and constraints of GEVO for ML workloads and discusses our GEVO-ML design, which we are currently implementing.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {1849–1856},
numpages = {8}
}

@article{10.1007/s00521-019-04171-3,
author = {Ghosh, Manosij and Guha, Ritam and Sarkar, Ram and Abraham, Ajith},
title = {A Wrapper-Filter Feature Selection Technique Based on Ant Colony Optimization},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {12},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-019-04171-3},
doi = {10.1007/s00521-019-04171-3},
abstract = {Ant colony optimization (ACO) is a well-explored meta-heuristic algorithm, among whose many applications feature selection (FS) is an important one. Most existing versions of ACO are either wrapper based or filter based. In this paper, we propose a wrapper-filter combination of ACO, where we introduce subset evaluation using a filter method instead of using a wrapper method to reduce computational complexity. A memory to keep the best ants and feature dimension-dependent pheromone update has also been used to perform FS in a multi-objective manner. Our proposed approach has been evaluated on various real-life datasets, taken from UCI Machine Learning repository and NIPS2003 FS challenge, using K-nearest neighbors and multi-layer perceptron classifiers. The experimental outcomes have been compared to some popular FS methods. The comparison of results clearly shows that our method outperforms most of the state-of-the-art algorithms used for FS. For measuring the robustness of the proposed model, it has been additionally evaluated on facial emotion recognition and microarray datasets.},
journal = {Neural Comput. Appl.},
month = {jun},
pages = {7839–7857},
numpages = {19},
keywords = {Ant colony optimization, NIPS2003 challenge, Feature selection, Wrapper-filter method}
}

@article{10.1145/3462761,
author = {Zhou, Zhou and Li, Fangmin and Yang, Shuiqiao},
title = {A Novel Resource Optimization Algorithm&nbsp;Based on Clustering and Improved Differential Evolution Strategy Under a Cloud Environment},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3462761},
doi = {10.1145/3462761},
abstract = {Resource optimization algorithm based on clustering and improved differential evolution strategy, as a new global optimized algorithm, has wide applications in language translation, language processing, document understanding, cloud computing, and edge computing due to high efficiency. With the development of deep learning technology and the rise of big data, the resource optimization algorithm encounters a series of challenges, such as the workload imbalance and low resource utilization. To address the preceding problems, this study proposes a novel resource optimization algorithm based on clustering and an improved differential evolution strategy (Multi-objective Task Scheduling Strategy (MTSS)). Three indexes, namely task completion time, execution cost, and workload, of virtual machines are selected and used to build the fitness function of the MTSS algorithm. At the same time, the preprocessing state is set up to cluster according to the resource and task characteristics to reduce the magnitude of their matching scale. Moreover, to solve the workload imbalance among different resource sets, local resource tasks are reallocated using the Q-value method in the MTSS strategy to achieve workload balance of global resources and improve the resource utilization rate. Experiments are carried out to evaluate the effectiveness of the proposed algorithm. Results show that the proposed algorithm outperforms other algorithms in terms of task completion time, execution cost, and workload balancing.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jul},
articleno = {91},
numpages = {15},
keywords = {low resource utilization, deep learning, multi-objective optimization, Natural language processing, resource optimization}
}

@article{10.1007/s00354-016-0005-8,
author = {de Campos, Cassio P. and Benavoli, Alessio},
title = {Joint Analysis of Multiple Algorithms and Performance Measures},
year = {2017},
issue_date = {Jan 2017},
publisher = {Ohmsha},
address = {JPN},
volume = {35},
number = {1},
issn = {0288-3635},
url = {https://doi.org/10.1007/s00354-016-0005-8},
doi = {10.1007/s00354-016-0005-8},
abstract = {There has been an increasing interest in the development of new methods using Pareto optimality to deal with multi-objective criteria (for example, accuracy and time complexity). Once one has developed an approach to a problem of interest, the problem is then how to compare it with the state of art. In machine learning, algorithms are typically evaluated by comparing their performance on different data sets by means of statistical tests. Standard tests used for this purpose are able to consider jointly neither performance measures nor multiple competitors at once. The aim of this paper is to resolve these issues by developing statistical procedures that are able to account for multiple competing measures at the same time and to compare multiple algorithms altogether. In particular, we develop two tests: a frequentist procedure based on the generalized likelihood ratio test and a Bayesian procedure based on a multinomial-Dirichlet conjugate model. We further extend them by discovering conditional independences among measures to reduce the number of parameters of such models, as usually the number of studied cases is very reduced in such comparisons. Data from a comparison among general purpose classifiers are used to show a practical application of our tests.},
journal = {New Gen. Comput.},
month = {jan},
pages = {69–86},
numpages = {18},
keywords = {Conditional Independence, Null Hypothesis Significance Test, Dominance Statement, Bayesian Network, Generalize Likelihood Ratio Test}
}

@inproceedings{10.1145/3492324.3494159,
author = {Ceesay, Sheriffo and Lin, Yuhui and Barker, Adam},
title = {Benchmarking and Performance Modelling of Dataflow with Cycles},
year = {2021},
isbn = {9781450391641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3492324.3494159},
doi = {10.1145/3492324.3494159},
abstract = { Over the years, the popularity of iterative data-intensive applications such as machine learning applications has grown immensely. Unlike batch applications, iterative applications such as k-means, regression or classification algorithms require multiple access to the input data to train it sufficiently for convergence. In the context of big data, these applications are executed on distributed computing frameworks such as Apache Spark. These frameworks are simple to deploy and use, however, under the hood they are complex and highly configurable. To perform an exhaustive study of the impact of these ubiquitous parameters on application performance would be cumbersome due to the exponential amount of their combinations. In this paper, we group applications based on a common dataflow and communication pattern. We then present a multi-objective performance prediction framework to model the performance of these applications. The models can predict the execution time of a given application with high accuracy. The framework can be used to infer optimal configuration parameters to meet application execution deadlines. Based on these optimal configurable values, we recommend the best EC2 instances in terms of cost. The average error rate of the prediction results is ± 14% from the measured value.},
booktitle = {2021 IEEE/ACM 8th International Conference on Big Data Computing, Applications and Technologies (BDCAT '21)},
pages = {91–100},
numpages = {10},
keywords = {Dataflow With Cycles, Modelling, Communication Patterns, Machine Learning, Big Data},
location = {Leicester, United Kingdom},
series = {BDCAT '21}
}

@inproceedings{10.1145/3492805.3492819,
author = {Pochelu, Pierrick and Petiton, Serge G. and Conche, Bruno},
title = {A Deep Neural Networks Ensemble Workflow from Hyperparameter Search to Inference Leveraging GPU Clusters},
year = {2022},
isbn = {9781450384988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3492805.3492819},
doi = {10.1145/3492805.3492819},
abstract = { Automated Machine Learning with ensembling (or AutoML with ensembling) seeks to automatically build ensembles of Deep Neural Networks (DNNs) to achieve qualitative predictions. Ensemble of DNNs are well known to avoid over-fitting but they are memory and time consuming approaches. Therefore, an ideal AutoML would produce in one single run time different ensembles regarding accuracy and inference speed. While previous works on AutoML focus to search for the best model to maximize its generalization ability, we rather propose a new AutoML to build a larger library of accurate and diverse individual models to then construct ensembles. First, our extensive benchmarks show asynchronous Hyperband is an efficient and robust way to build a large number of diverse models to combine them. Then, a new ensemble selection method based on a multi-objective greedy algorithm is proposed to generate accurate ensembles by controlling their computing cost. Finally, we propose a novel algorithm to optimize the inference of the DNNs ensemble in a GPU cluster based on allocation optimization. The produced AutoML with ensemble method shows robust results on two datasets using efficiently GPU clusters during both the training phase and the inference phase.},
booktitle = {International Conference on High Performance Computing in Asia-Pacific Region},
pages = {61–71},
numpages = {11},
keywords = {neural networks},
location = {Virtual Event, Japan},
series = {HPCAsia2022}
}

@inproceedings{10.1145/2628071.2628092,
author = {Ansel, Jason and Kamil, Shoaib and Veeramachaneni, Kalyan and Ragan-Kelley, Jonathan and Bosboom, Jeffrey and O'Reilly, Una-May and Amarasinghe, Saman},
title = {OpenTuner: An Extensible Framework for Program Autotuning},
year = {2014},
isbn = {9781450328098},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2628071.2628092},
doi = {10.1145/2628071.2628092},
abstract = {Program autotuning has been shown to achieve better or more portable performance in a number of domains. However, autotuners themselves are rarely portable between projects, for a number of reasons: using a domain-informed search space representation is critical to achieving good results; search spaces can be intractably large and require advanced machine learning techniques; and the landscape of search spaces can vary greatly between different problems, sometimes requiring domain specific search techniques to explore efficiently.This paper introduces OpenTuner, a new open source framework for building domain-specific multi-objective program autotuners. OpenTuner supports fully-customizable configuration representations, an extensible technique representation to allow for domain-specific techniques, and an easy to use interface for communicating with the program to be autotuned. A key capability inside OpenTuner is the use of ensembles of disparate search techniques simultaneously; techniques that perform well will dynamically be allocated a larger proportion of tests. We demonstrate the efficacy and generality of OpenTuner by building autotuners for 7 distinct projects and 16 total benchmarks, showing speedups over prior techniques of these projects of up to 2.8x with little programmer effort.},
booktitle = {Proceedings of the 23rd International Conference on Parallel Architectures and Compilation},
pages = {303–316},
numpages = {14},
keywords = {optimization, autotuner},
location = {Edmonton, AB, Canada},
series = {PACT '14}
}

@inproceedings{10.1145/3457682.3457754,
author = {Qayyum, Shazib and Fan, Xiaoping and Faizan Ali, Rao},
title = {Feature-Based Selection for Open-Source Video Conferencing Software System Using Fuzzy Analytical Hierarchy Process (FAHP)},
year = {2021},
isbn = {9781450389310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457682.3457754},
doi = {10.1145/3457682.3457754},
abstract = {There has been an increase in the number of video conferencing tools used during the COVID19 outbreak. Video Conferencing has become a more prevalent and reliable tool when traveling, and face-to-face meetings are not an option. Evaluating and choosing the best conferencing tool that meets the requirement of the organization is difficult. A reliable conferencing tool provides the dual benefit of allowing users to communicate effectively with their teams and clients while also making the Information Technology (IT) department's job easier. Choosing an inappropriate conferencing tool may critically affect the functioning and processes of the business. Our study's core purpose is to evaluate and select the video conferencing platform, which is based upon the methodology called Multi-criteria decision making (MCDM). Numerous proceedings were considered during the evaluation process, and the best software packages selected were based upon a set of matrix outcomes using Fuzzy AHP. The experimental observation demonstrates that two software packages, zoom, and blue jeans, can give better results based upon their ranking scores among the selected alternatives.},
booktitle = {2021 13th International Conference on Machine Learning and Computing},
pages = {471–476},
numpages = {6},
keywords = {Multi-criteria decision making (MCDM), Open source software, Video Conferencing, Fuzzy Analytical Hierarchy Process (AHP)},
location = {Shenzhen, China},
series = {ICMLC 2021}
}

@article{10.1016/j.patcog.2017.01.018,
author = {Bhattacharya, Gautam and Ghosh, Koushik and Chowdhury, Ananda S.},
title = {Granger Causality Driven AHP for Feature Weighted KNN},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {66},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2017.01.018},
doi = {10.1016/j.patcog.2017.01.018},
abstract = {The kNN algorithm remains a popular choice for pattern classification till date due to its non-parametric nature, easy implementation and the fact that its classification error is bounded by twice the Bayes error. In this paper, we show that the performance of the kNN classifier improves significantly from the use of (training) class-wise group-statistics based two criteria during pairwise comparison of features in a given dataset. Granger causality is employed to assign preferences to each criteria. Analytic Hierarchy Process (AHP) is applied to obtain weights for different features from the two criteria and their preferences. Finally, these weights are used to build a weighted distance function for the kNN classification. Comprehensive experimentation on fifteen benchmark datasets of the UCI Machine Learning Repository clearly reveals the supremacy of the proposed Granger causality driven AHP induced kNN algorithm over the kNN method with many different distance metrics, and, with various feature selection strategies. In addition, the proposed method is also shown to perform well on high-dimensional face and hand-writing recognition datasets. HighlightsFeature weighting for kNN by a multi-criteria based decision analysis tool called AHP.Automated weight assignment in criteria matrix of AHP using group-statistics.Criteria preference selection in AHP with Granger Causality.Superior classification performance over kNN with many other feature weighting/selection methods.},
journal = {Pattern Recogn.},
month = {jun},
pages = {425–436},
numpages = {12},
keywords = {KNN Classification, Feature Weighting, Analytic Hierarchy Process, Granger causality}
}

@inproceedings{10.1145/3207719.3207727,
author = {Georgiou, Kyriakos and Blackmore, Craig and Xavier-de-Souza, Samuel and Eder, Kerstin},
title = {Less is More: Exploiting the Standard Compiler Optimization Levels for Better Performance and Energy Consumption},
year = {2018},
isbn = {9781450357807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3207719.3207727},
doi = {10.1145/3207719.3207727},
abstract = {This paper presents the interesting observation that by performing fewer of the optimizations available in a standard compiler optimization level such as -02, while preserving their original ordering, significant savings can be achieved in both execution time and energy consumption. This observation has been validated on two embedded processors, namely the ARM Cortex-M0 and the ARM Cortex-M3, using two different versions of the LLVM compilation framework; v3.8 and v5.0. Experimental evaluation with 71 embedded benchmarks demonstrated performance gains for at least half of the benchmarks for both processors. An average execution time reduction of 2.4% and 5.3% was achieved across all the benchmarks for the Cortex-M0 and Cortex-M3 processors, respectively, with execution time improvements ranging from 1% up to 90% over the -02. The savings that can be achieved are in the same range as what can be achieved by the state-of-the-art compilation approaches that use iterative compilation or machine learning to select flags or to determine phase orderings that result in more efficient code. In contrast to these time consuming and expensive to apply techniques, our approach only needs to test a limited number of optimization configurations, less than 64, to obtain similar or even better savings. Furthermore, our approach can support multi-criteria optimization as it targets execution time, energy consumption and code size at the same time.},
booktitle = {Proceedings of the 21st International Workshop on Software and Compilers for Embedded Systems},
pages = {35–42},
numpages = {8},
keywords = {phase-ordering, execution time, Autotuning, embedded systems, compiler optimizations, energy consumption},
location = {Sankt Goar, Germany},
series = {SCOPES '18}
}

@article{10.1109/TCBB.2018.2843339,
author = {Alden, Kieran and Cosgrove, Jason and Coles, Mark and Timmis, Jon},
title = {Using Emulation to Engineer and Understand Simulations of Biological Systems},
year = {2020},
issue_date = {Jan.-Feb. 2020},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {17},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2018.2843339},
doi = {10.1109/TCBB.2018.2843339},
abstract = {Modeling and simulation techniques have demonstrated success in studying biological systems. As the drive to better capture biological complexity leads to more sophisticated simulators, it becomes challenging to perform statistical analyses that help translate predictions into increased understanding. These analyses may require repeated executions and extensive sampling of high-dimensional parameter spaces: analyses that may become intractable due to time and resource limitations. Significant reduction in these requirements can be obtained using surrogate models, or emulators, that can rapidly and accurately predict the output of an existing simulator. We apply emulation to evaluate and enrich understanding of a previously published agent-based simulator of lymphoid tissue organogenesis, showing an ensemble of machine learning techniques can reproduce results obtained using a suite of statistical analyses within seconds. This performance improvement permits incorporation of previously intractable analyses, including multi-objective optimization to obtain parameter sets that yield a desired response, and Approximate Bayesian Computation to assess parametric uncertainty. To facilitate exploitation of emulation in simulation-focused studies, we extend our open source statistical package, <italic>spartan</italic>, to provide a suite of tools for emulator development, validation, and application. Overcoming resource limitations permits enriched evaluation and refinement, easing translation of simulator insights into increased biological understanding.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {jan},
pages = {302–315},
numpages = {14}
}

@article{10.1016/j.asoc.2021.108022,
author = {Mekouar, Soufiana},
title = {Classifiers Selection Based on Analytic Hierarchy Process and Similarity Score for Spam Identification},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {113},
number = {PB},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.108022},
doi = {10.1016/j.asoc.2021.108022},
journal = {Appl. Soft Comput.},
month = {dec},
numpages = {13},
keywords = {Images spam, Multi-layer perceptron, Analytic hierarchy process, Machine learning classifiers, Corpus spam}
}

@inproceedings{10.1145/3313950.3313963,
author = {Zhong, Juping and Gao, Jing and Chen, Rongjun and Li, Jun},
title = {Digital Recognition of Street View House Numbers Based on DCGAN},
year = {2019},
isbn = {9781450360920},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313950.3313963},
doi = {10.1145/3313950.3313963},
abstract = {Deep learning algorithms have surpassed human resolution in applications such as face recognition and object classification. However, it can only produce very blurred, lack of details of the image. Generative Adversarial Network is a game training of minimax antagonism between generator G and discriminator D, and ultimately achieves Nash equilibrium. We use deep convolutional GAN that recognizes sequence numbers and without split characters. First we use convolution network to extract character features. Second we construct a convolution neural network to recognize digits of natural scene house number. DCGAN is used to improve the resolution of the number of fuzzy houses, so as to extract more abundant data features in data set training. It can better recognize the numbers in the natural street.},
booktitle = {Proceedings of the 2nd International Conference on Image and Graphics Processing},
pages = {19–22},
numpages = {4},
keywords = {deep convolutional generative adversarial networks, image processing, machine learning, street view house numbers},
location = {Singapore, Singapore},
series = {ICIGP '19}
}

@article{10.1016/j.eswa.2017.08.020,
author = {Singh, Bikesh Kumar and Verma, Kesari and Panigrahi, Lipismita and Thoke, A.S.},
title = {Integrating Radiologist Feedback with Computer Aided Diagnostic Systems for Breast Cancer Risk Prediction in Ultrasonic Images},
year = {2017},
issue_date = {December 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {90},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2017.08.020},
doi = {10.1016/j.eswa.2017.08.020},
abstract = {New hybrid classification approach by integrating BPANN and SVM is developed.Radiologist opinion is incorporated in CAD system.Integrating radiologists opinion in CAD systems improves its overall performance.Proposed method outperforms existing ones. With advancements in machine learning algorithms and computer aided diagnostic (CAD) systems, the performance of automated analysis of radiological images has improved substantially in recent times. However, the lack of integration between the radiologist and CAD systems restrains the rate of progress as well as the reach of such advancements in clinical use. This article aims to improve the clinical efficiency of ultrasound based CAD systems for classification of breast lesions by integrating back-propagation artificial neural network (BPANN), support vector machine (SVM) and radiologist feedback. The acquired breast ultrasound images were subjected to wavelet based filtering in order to reduce speckle noise followed by feature extraction, feature selection and classification. Experiments on a database of 178 ultrasound images of breast anomalies (88 benign and 90 malignant) show that the proposed methodology achieves classification accuracy of 98.621% and 98.276%, respectively, when all 457 and 19 most relevant features selected by multi-criteria feature selection method were used for classification. The accuracy achieved is significantly higher than that using conventional classifiers based on BPANN and SVM. Further, it is found that integrating expert opinion in CAD systems improves its overall performance. The quantitative results obtained are discussed in light of some recently reported studies.},
journal = {Expert Syst. Appl.},
month = {dec},
pages = {209–223},
numpages = {15},
keywords = {Ultrasound, Support vector machine, Radiologist opinion, Neural network, Machine learning, Breast tumor classification}
}

@article{10.1016/j.eswa.2016.08.038,
author = {Arag\~{a}o, Marcelo V.C. and Frigieri, Edielson Prevato and Ynoguti, Carlos A. and Paiva, Anderson P.},
title = {Factorial Design Analysis Applied to the Performance of SMS Anti-Spam Filtering Systems},
year = {2016},
issue_date = {December 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {64},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2016.08.038},
doi = {10.1016/j.eswa.2016.08.038},
abstract = {An anti-spam filtering system for SMS is proposed.Different system parameters are evaluated through factorial design analysis.The optimal configuration has reached over 98% classification accuracy.The number of features increases system performance but adds computational cost.Linguistic techniques do not represent significant performance boost for some setups. Over the last few decades, the advent of telecommunication systems has allowed a growing exchange of electronic messages around the world. Unfortunately, irrelevant and/or unsolicited content corresponds to the majority of this volume of data, and to decide whether to keep or discard each message is a known challenge in the context of machine learning. This paper proposes an anti-spam filtering approach base on linguistic techniques. The real effect of each system parameter is evaluated through design factorial analysis using two different classifiers: first using Support Vector Machine (SVM) and second applying Naive Bayesian (NB) classification. This analysis is detailed and discussed providing a step-by-step guide for developers and users of anti-spam filters. Based on different system metrics, multi-objective optimization is applied in order to obtain the optimal filter setup. Evaluation of anti-spam filter under optimal configuration showed that SVM-based system achieved an accuracy performance above 98% whereas the NB-based system reached 87%. Results also reveal that linguistic techniques are relevant for the NB classifier but do not contribute to improve the SVM-based system performance.},
journal = {Expert Syst. Appl.},
month = {dec},
pages = {589–604},
numpages = {16},
keywords = {Factorial design, Naive bayesian (NB), Short message service (SMS), Spam filtering, Support vector machine (SVM)}
}

@inproceedings{10.5555/3049877.3049882,
author = {Chan, Arwin and Zulkernine, Farhana H.},
title = {ArchiGen: A Conceptual Form Design Tool Using an Evolutionary Computing Approach},
year = {2016},
publisher = {IBM Corp.},
address = {USA},
abstract = {Computer aided design (CAD), the use of computer systems for design and documentation, is prevalent in industrial and architectural design, but largely features passive software to follow user interaction. In the past decade, there have been multiple efforts in implementing multi-objective optimization algorithms and machine learning for data analytics in the area of computational optimization of building designs. This research initially explored the technical review of presented designs, and subsequently began to explore the creation of novel forms based on design constraints in addition to parameter optimization. Most notably in the conceptualization phase of the process, the designer is largely unassisted as current existing CAD software focuses on the modeling and basic structural analysis of already created designs. In this position paper, we propose a conceptual framework to leverage computer-assisted creativity in building and form design using evolutionary algorithms, complimented with a comprehensive review of the approaches of other research. We present the preliminary results of our rudimentary implementation of ArchiGen (Architectural Generator), a tool for assisting designers in the conceptualization of a design by presenting alternative forms based on design constraints. ArchiGen uses Genetic Algorithms (GA) to create alternative designs of a pillar-pod-antenna structured observation tower as a case study and explores the potential of combining optimal and sub-optimal solutions based on the specified design constraints.},
booktitle = {Proceedings of the 26th Annual International Conference on Computer Science and Software Engineering},
pages = {50–55},
numpages = {6},
keywords = {computer-aided design, artificial intelligence, conceptual form design, social media, cognitive science, collaborative design, evolutionary algorithm, CAD, genetic algorithm, archigen},
location = {Toronto, Ontario, Canada},
series = {CASCON '16}
}

@inproceedings{10.1145/3109761.3109786,
author = {Rahmani, Rahim and Kanter, Theo},
title = {Autonomous Cooperative Decision-Making in Massively Distributed IoT via Heterogenous Networks},
year = {2017},
isbn = {9781450352437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109761.3109786},
doi = {10.1145/3109761.3109786},
abstract = {This paper presents a disruptive approach "Immersive Networking" enabling massively distributed IoT nodes to participate in autonomous and cooperative decision-making. The approach is mandated by perceived limitations in 5G networking architecture maintaining control in the edge gateway. In our approach, control may be delegated to clusters of IoT nodes beyond the edge gateway. The communication is event-based involving publish-subscribe between related nodes. Clusters are identified in an autonomic fashion based on multi-criteria proximity. Local decisions can combine global and local context information to establish network slices in a decentralized fashion based on application demands. Moreover, such decisions may be part of a collaborative effort (map-reduce) based on either local or global context. Application demands expressed as such are modeled compatible with Open Data initiatives. We demonstrated feasibility of the approach and evaluate its advantages over the 5G architecture involving an edge gateway.},
booktitle = {Proceedings of the 1st International Conference on Internet of Things and Machine Learning},
articleno = {25},
numpages = {5},
keywords = {autonomic edge gateway, distributed IoT, IoT-middleware, wireless sensor networks, fog computing},
location = {Liverpool, United Kingdom},
series = {IML '17}
}

@article{10.1109/TIT.2009.2027479,
author = {Jones, Lee K.},
title = {Local Minimax Learning of Functions with Best Finite Sample Estimation Error Bounds: Applications to Ridge and Lasso Regression, Boosting, Tree Learning, Kernel Machines, and Inverse Problems},
year = {2009},
issue_date = {December 2009},
publisher = {IEEE Press},
volume = {55},
number = {12},
issn = {0018-9448},
url = {https://doi.org/10.1109/TIT.2009.2027479},
doi = {10.1109/TIT.2009.2027479},
abstract = {Optimal local estimation is formulated in the minimax sense for inverse problems and nonlinear regression. This theory provides best mean squared finite sample error bounds for some popular statistical learning algorithms and also for several optimal improvements of other existing learning algorithms such as smoothing splines and kernel regularization. The bounds and improved algorithms are not based on asymptotics or Bayesian assumptions and are truly local for each query, not depending on cross validating estimates at other queries to optimize modeling parameters. Results are given for optimal local learning of approximately linear functions with side information (context) using real algebraic geometry. In particular, finite sample error bounds are given for ridge regression and for a local version of lasso regression. The new regression methods require only quadratic programming with linear or quadratic inequality constraints for implementation. Greedy additive expansions are then combined with local minimax learning via a change in metric. An optimal strategy is presented for fusing the local minimax estimators of a class of experts--providing optimal finite sample prediction error bounds from (random) forests. Local minimax learning is extended to kernel machines. Best local prediction error bounds for finite samples are given for Tikhonov regularization. The geometry of reproducing kernel Hilbert space is used to derive improved estimators with finite sample mean squared error (MSE) bounds for class membership probability in two class pattern classification problems. A purely local, cross validation free algorithm is proposed which uses Fisher information with these bounds to determine best local kernel shape in vector machine learning. Finally, a locally quadratic solution to the finite Fourier moments problem is presented. After reading the first three sections the reader may proceed directly to any of the subsequent applications sections.},
journal = {IEEE Trans. Inf. Theor.},
month = {dec},
pages = {5700–5727},
numpages = {28},
keywords = {fusion, Fusion, ridge regression, minimax, reproducing kernel, inverse problem}
}

@inproceedings{10.1109/CEC.2016.7743901,
author = {He, Hongmei and Tiwari, Ashutosh and Mehnen, J\"{o}rn and Watson, Tim and Maple, Carsten and Jin, Yaochu and Gabrys, Bogdan},
title = {Incremental Information Gain Analysis of Input Attribute Impact on RBF-Kernel SVM Spam Detection},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CEC.2016.7743901},
doi = {10.1109/CEC.2016.7743901},
abstract = {The massive increase of spam is posing a very serious threat to email and SMS, which have become an important means of communication. Not only do spams annoy users, but they also become a security threat. Machine learning techniques have been widely used for spam detection. Email spams can be detected through detecting senders' behaviour, the contents of an email, subject and source address, etc, while SMS spam detection usually is based on the tokens or features of messages due to short content. However, a comprehensive analysis of email/SMS content may provide cures for users to aware of email/SMS spams. We cannot completely depend on automatic tools to identify all spams. In this paper, we propose an analysis approach based on information entropy and incremental learning to see how various features affect the performance of an RBF-based SVM spam detector, so that to increase our awareness of a spam by sensing the features of a spam. The experiments were carried out on the spambase and SMSSpemCollection databases in UCI machine learning repository. The results show that some features have significant impacts on spam detection, of which users should be aware, and there exists a feature space that achieves Pareto efficiency in True Positive Rate and True Negative Rate.},
booktitle = {2016 IEEE Congress on Evolutionary Computation (CEC)},
pages = {1022–1029},
numpages = {8},
location = {Vancouver, BC, Canada}
}

@article{10.1007/s11042-020-09382-8,
author = {Cheng, Yang-Jin and Hou, Muzhou and Wang, Juan},
title = {An Improved Optimal Trigonometric ELM Algorithm for Numerical Solution to Ruin Probability of Erlang(2) Risk Model},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {41–42},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09382-8},
doi = {10.1007/s11042-020-09382-8},
abstract = {In this paper, we focus on accurately calculating the numerical solution of the integral-differential equation for ruin probability in Erlang(2) renewal risk model with arbitrary claim distribution. Because the analytical solutions of the equation do not usually exist, firstly, using machine learning method in modern artificial intelligence, the activation functions in the ELM model are changed to trigonometric function, the initial conditions in the integral-differential equation are added to the ELM linear solver to get the ITELM model, and the steps and feasibility of the algorithm are strictly deduced in theory. As the analytic solution for the integral-differential equation only exists when the claim is subject to exponential distribution, and the numerical solution can be gotten with the pareto distribution. And, since the number of hidden neurons in the ITELM model is uncertain, a good numerical value of hidden neurons can only be determined through a large number of iterative tests and comparisons in the actual calculation. Then, we construct a multi-objective optimization model and algorithm, which can get the optimal number of hidden neurons to obtain the IOTELM model and algorithm. Then, in the above two cases for exponential distribution and pareto distribution, the optimal number of hidden neurons is calculated by IOTELM model and algorithm, and then corresponding ITELM models and algorithms are constructed to calculate the corresponding ruin probability. Compared with the previous numerical experiments, it can be seen that the numerical accuracy is greatly improved, which verified the versatility, feasibility and superiority of the proposed IOTELM model and algorithm.},
journal = {Multimedia Tools Appl.},
month = {nov},
pages = {30235–30255},
numpages = {21},
keywords = {Renewal integral-differential equation, Ruin probability, IOTELM algorithm, Erlang(2) risk model}
}

@article{10.1007/s00500-019-03997-2,
author = {Suresh, K. and Dillibabu, R.},
title = {A Novel Fuzzy Mechanism for Risk Assessment in Software Projects},
year = {2020},
issue_date = {Feb 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {3},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-019-03997-2},
doi = {10.1007/s00500-019-03997-2},
abstract = {Risk management is a vital factor for ensuring better quality software development processes. Moreover, risks are the events that could adversely affect the organization activities or the development of projects. Effective prioritization of software project risks play a significant role in determining whether the project will be successful in terms of performance characteristics or not. In this work, we develop a new hybrid fuzzy-based machine learning mechanism for performing risk assessment in software projects. This newly developed hybridized risk assessment scheme can be used to determine and rank the significant software project risks that support the decision making during the software project lifecycle. For better assessment of the software project risks, we have incorporated fuzzy decision making trial and evaluation laboratory, adaptive neuro-fuzzy inference system-based multi-criteria decision making (ANFIS MCDM) and intuitionistic fuzzy-based TODIM (IF-TODIM) approaches. More significantly, for the newly introduced ANFIS MCDM approach, the parameters of ANFIS are adjusted using a traditional crow search algorithm (CSA) which applies only a reasonable as well as small changes in variables. The main activity of CSA in ANFIS is to find the best parameter to achieve most accurate software risk estimate. Experimental validation was conducted on NASA 93 dataset having 93 software project values. The result of this method exhibits a vivid picture that provides software risk factors that are key determinant for achievement of the project performance. Experimental outcomes reveal that our proposed integrated fuzzy approaches can exhibit better and accurate performance in the assessment of software project risks compared to other existing approaches.},
journal = {Soft Comput.},
month = {feb},
pages = {1683–1705},
numpages = {23},
keywords = {MCDM, Performance, Adaptive neuro-fuzzy inference system, Crow search algorithm, IF-TODIM, Fuzzy DEMATEL, Project risk}
}

@inproceedings{10.1145/3437801.3441620,
author = {Cai, Zixian and Liu, Zhengyang and Maleki, Saeed and Musuvathi, Madanlal and Mytkowicz, Todd and Nelson, Jacob and Saarikivi, Olli},
title = {Synthesizing Optimal Collective Algorithms},
year = {2021},
isbn = {9781450382946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437801.3441620},
doi = {10.1145/3437801.3441620},
abstract = {Collective communication algorithms are an important component of distributed computation. Indeed, in the case of deep-learning, collective communication is the Amdahl's bottleneck of data-parallel training.This paper introduces SCCL (for Synthesized Collective Communication Library), a systematic approach to synthesizing collective communication algorithms that are explicitly tailored to a particular hardware topology. SCCL synthesizes algorithms along the Pareto-frontier spanning from latency-optimal to bandwidth-optimal implementations of a collective. The paper demonstrates how to encode the synthesis problem as a quantifier-free SMT formula which can be discharged to a theorem prover. We show how our carefully built encoding enables SCCL to scale.We synthesize novel latency and bandwidth optimal algorithms not seen in the literature on two popular hardware topologies. We also show how SCCL efficiently lowers algorithms to implementations on two hardware architectures (NVIDIA and AMD) and demonstrate competitive performance with hand optimized collective communication libraries.},
booktitle = {Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {62–75},
numpages = {14},
keywords = {network, interconnection, synthesis, GPU, collective communication},
location = {Virtual Event, Republic of Korea},
series = {PPoPP '21}
}

@article{10.1016/j.engappai.2016.06.007,
author = {Garc\'{\i}a-Rudolph, Alejandro and Gibert, Karina},
title = {Understanding Effects of Cognitive Rehabilitation under a Knowledge Discovery Approach},
year = {2016},
issue_date = {October 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {55},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2016.06.007},
doi = {10.1016/j.engappai.2016.06.007},
abstract = {Traumatic brain injury (TBI) is the leading cause of death and disability in children and young adults worldwide. Cognitive rehabilitation (CR) plans consist of a sequence of CR tasks targeting main cognitive functions. There is not enough on-field experience yet regarding which specific intervention (tasks or exercise assignment) is more appropriate to help therapists to design plans with significant effectiveness on patient improvement. The selection of specific tasks to be prescribed to the patient and the order in which they might be executed is currently decided by the therapists based on their experience.In this paper a new data mining methodology is proposed, combining several tools from Artificial Intelligence, clustering and post-processing analysis to identify regularities in the sequences of tasks in such a way that treatment profiles (classes) can be discovered. Due to the cumulative effect of rehabilitation tasks, small variations within the sequence of tasks performed by the patient do not significantly change the final outcomes in rehabilitation and makes it difficult to find discriminant rules by using the traditional machine learning inductive methods. However, by relaxing the formalization of the problem to find patterns that might include small variations, and introducing motif discovery techniques in the proposed methodology, the complexity of the neurorehabilitation phenomenon can be better captured and a global structure of successful treatment task sequences can be devised.Following this, the relationship between the discovered patterns and the CR treatment response are analyzed, offering a richer perspective than that provided by the single task focus traditionally used in the CR field.The paper provides a definition of the whole methodological approach proposed from a formal point of view, and its application to a real dataset. Comparisons with traditional AI approaches are also presented and the contribution of the proposed methodology to the AI field discussed. SAIMAP is an innovative combination of pre-processing, clustering, motif discovery and post-processing in a hybrid methodological frame,SAIMAP associates sequential patterns of a predefined set of events including high order interactions and cumulative effects.Effects of treatment are measured through multi-criteria improvement indicators set, referring tot a predefined set of areas.SAIMAP provides the possibility to understand underlying structure in Cognitive Rehabilitation PatternsSAIMAP analyses characteristics of different types of treatments. Length of treatment is associated with different treatment schemesDecision criteria to understand when long or short treatment is required can be also learned.Shorter treaments work for patients with short-term memory mild impaired; high impairment in recognition memory require longer treatment.SAIMAP overcomes classical machine learning approaches for this complex scenarios.SAIMAP is suitable for other research fields out of neuropsychology, provided they fit the formal structure of problem described in the paper.},
journal = {Eng. Appl. Artif. Intell.},
month = {oct},
pages = {165–185},
numpages = {21},
keywords = {Knowledge discovery, Cognitive rehabilitation, Clustering-based on rules, Motifs, Traumatic brain injury, Post-processing}
}

@inproceedings{10.1145/3400302.3415731,
author = {Marchisio, Alberto and Massa, Andrea and Mrazek, Vojtech and Bussolino, Beatrice and Martina, Maurizio and Shafique, Muhammad},
title = {NASCaps: A Framework for Neural Architecture Search to Optimize the Accuracy and Hardware Efficiency of Convolutional Capsule Networks},
year = {2020},
isbn = {9781450380263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400302.3415731},
doi = {10.1145/3400302.3415731},
abstract = {Deep Neural Networks (DNNs) have made significant improvements to reach the desired accuracy to be employed in a wide variety of Machine Learning (ML) applications. Recently the Google Brain's team demonstrated the ability of Capsule Networks (CapsNets) to encode and learn spatial correlations between different input features, thereby obtaining superior learning capabilities compared to traditional (i.e., non-capsule based) DNNs. However, designing CapsNets using conventional methods is a tedious job and incurs significant training effort. Recent studies have shown that powerful methods to automatically select the best/optimal DNN model configuration for a given set of applications and a training dataset are based on the Neural Architecture Search (NAS) algorithms. Moreover, due to their extreme computational and memory requirements, DNNs are employed using the specialized hardware accelerators in IoT-Edge/CPS devices.In this paper, we propose NASCaps, an automated framework for the hardware-aware NAS of different types of DNNs, covering both traditional convolutional DNNs and CapsNets. We study the efficacy of deploying a multi-objective Genetic Algorithm (e.g., based on the NSGA-II algorithm). The proposed framework can jointly optimize the network accuracy and the corresponding hardware efficiency, expressed in terms of energy, memory, and latency of a given hardware accelerator executing the DNN inference. Besides supporting the traditional DNN layers (such as, convolutional and fully-connected), our framework is the first to model and supports the specialized capsule layers and dynamic routing in the NAS-flow. We evaluate our framework on different datasets, generating different network configurations, and demonstrate the tradeoffs between the different output metrics. We will open-source the complete framework and configurations of the Pareto-optimal architectures at https://github.com/ehw-fit/nascaps.},
booktitle = {Proceedings of the 39th International Conference on Computer-Aided Design},
articleno = {114},
numpages = {9},
keywords = {design space, deep neural networks, memory, evolutionary algorithms, hardware accelerators, neural architecture search, energy efficiency, multi-objective, DNNs, genetic algorithms, latency, optimization, capsule networks, accuracy},
location = {Virtual Event, USA},
series = {ICCAD '20}
}

@article{10.1016/j.ins.2016.05.026,
author = {Zhao, Jiaqi and Basto Fernandes, Vitor and Jiao, Licheng and Yevseyeva, Iryna and Maulana, Asep and Li, Rui and B\"{a}ck, Thomas and Tang, Ke and T.M. Emmerich, Michael},
title = {Multiobjective Optimization of Classifiers by Means of 3D Convex-Hull-Based Evolutionary Algorithms},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {367},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2016.05.026},
doi = {10.1016/j.ins.2016.05.026},
abstract = {The receiver operating characteristic (ROC) and detection error tradeoff (DET) curves are frequently used in the machine learning community to analyze the performance of binary classifiers. Recently, the convex-hull-based multiobjective genetic programming algorithm was proposed and successfully applied to maximize the convex hull area for binary classification problems by minimizing false positive rate and maximizing true positive rate at the same time using indicator-based evolutionary algorithms. The area under the ROC curve was used for the performance assessment and to guide the search. Here we extend this research and propose two major advancements: Firstly we formulate the algorithm in detection error tradeoff space, minimizing false positives and false negatives, with the advantage that misclassification cost tradeoff can be assessed directly. Secondly, we add complexity as an objective function, which gives rise to a 3D objective space (as opposed to a 2D previous ROC space). A domain specific performance indicator for 3D Pareto front approximations, the volume above DET surface, is introduced, and used to guide the indicator-based evolutionary algorithm to find optimal approximation sets. We assess the performance of the new algorithm on designed theoretical problems with different geometries of Pareto fronts and DET surfaces, and two application-oriented benchmarks: (1) Designing spam filters with low numbers of false rejects, false accepts, and low computational cost using rule ensembles, and (2) finding sparse neural networks for binary classification of test data from the UCI machine learning benchmark. The results show a high performance of the new algorithm as compared to conventional methods for multicriteria optimization.},
journal = {Inf. Sci.},
month = {nov},
pages = {80–104},
numpages = {25},
keywords = {Convex hull, Evolutionary multiobjective optimization, Classification, ROC analysis, Anti-spam filters, Parsimony}
}

@inproceedings{10.5555/2616606.2617115,
author = {Nepal, Kumud and Li, Yueting and Bahar, R. Iris and Reda, Sherief},
title = {ABACUS: A Technique for Automated Behavioral Synthesis of Approximate Computing Circuits},
year = {2014},
isbn = {9783981537024},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {Many classes of applications, especially in the domains of signal and image processing, computer graphics, computer vision, and machine learning, are inherently tolerant to inaccuracies in their underlying computations. This tolerance can be exploited to design approximate circuits that perform within acceptable accuracies but have much lower power consumption and smaller area footprints (and often better run times) than their exact counterparts. In this paper, we propose a new class of automated synthesis methods for generating approximate circuits directly from behavioral-level descriptions. In contrast to previous methods that operate at the Boolean level or use custom modifications, our automated behavioral synthesis method enables a wider range of possible approximations and can operate on arbitrary designs. Our method first creates an abstract synthesis tree (AST) from the input behavioral description, and then applies variant operators to the AST using an iterative stochastic greedy approach to identify the optimal inexact designs in an efficient way. Our method is able to identify the optimal designs that represent the Pareto frontier trade-off between accuracy and power consumption. Our methodology is developed into a tool we call ABACUS, which we integrate with a standard ASIC experimental flow based on industrial tools. We validate our methods on three realistic Verilog-based benchmarks from three different domains --- signal processing, computer vision and machine learning. Our tool automatically discovers optimal designs, providing area and power savings of up to 50% while maintaining good accuracy.},
booktitle = {Proceedings of the Conference on Design, Automation &amp; Test in Europe},
articleno = {361},
numpages = {6},
location = {Dresden, Germany},
series = {DATE '14}
}

@article{10.1016/j.jbi.2015.12.001,
author = {Bashir, Saba and Qamar, Usman and Khan, Farhan Hassan},
title = {IntelliHealth},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {59},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2015.12.001},
doi = {10.1016/j.jbi.2015.12.001},
abstract = {Display Omitted Extensive research has been conducted on disease prediction.An optimal combination of classifiers is presented with multi-layer classification.The ensemble approach uses bagging with multi-objective optimized weighted.Comparison with existing techniques show superiority of our ensemble.An application named "IntelliHealth" has been developed. Accuracy plays a vital role in the medical field as it concerns with the life of an individual. Extensive research has been conducted on disease classification and prediction using machine learning techniques. However, there is no agreement on which classifier produces the best results. A specific classifier may be better than others for a specific dataset, but another classifier could perform better for some other dataset. Ensemble of classifiers has been proved to be an effective way to improve classification accuracy. In this research we present an ensemble framework with multi-layer classification using enhanced bagging and optimized weighting. The proposed model called "HM-BagMoov" overcomes the limitations of conventional performance bottlenecks by utilizing an ensemble of seven heterogeneous classifiers. The framework is evaluated on five different heart disease datasets, four breast cancer datasets, two diabetes datasets, two liver disease datasets and one hepatitis dataset obtained from public repositories. The analysis of the results show that ensemble framework achieved the highest accuracy, sensitivity and F-Measure when compared with individual classifiers for all the diseases. In addition to this, the ensemble framework also achieved the highest accuracy when compared with the state of the art techniques. An application named "IntelliHealth" is also developed based on proposed model that may be used by hospitals/doctors for diagnostic advice.},
journal = {J. of Biomedical Informatics},
month = {feb},
pages = {185–200},
numpages = {16},
keywords = {Classification, Machine learning, Multi-layer, Disease prediction, Bagging, Ensemble technique}
}

